{"pdf_id": "0704.0002", "content": "A k-arborescence is a graph that admits a decomposition into k edge-disjoint spanning trees. Figure 1(a) shows an example of a 3-arborescence. The k-arborescent graphs are described by the well-known theorems of Tutte [23] and Nash-Williams [17] as exactly the (k,k)-tight graphs. A map-graph is a graph that admits an orientation such that the out-degree of each vertex isexactly one. A k-map-graph is a graph that admits a decomposition into k edge-disjoint map graphs. Figure 1(b) shows an example of a 2-map-graphs; the edges are oriented in one possible configuration certifying that each color forms a map-graph. Map-graphs may be equivalently defined (see, e.g., [18]) as having exactly one cycle per connected component.1"}
{"pdf_id": "0704.0002", "content": "Fig. 2. (a) A graph with a 3T2 decomposition; one of the three trees is a single vertex in the bottom right corner. (b) The highlighted subgraph inside the dashed countour has three black tree-pieces and one gray tree-piece. (c) The highlighted subgraph inside the dashed countour has three gray tree-pieces (one is a single vertex) and one black tree-piece."}
{"pdf_id": "0704.0002", "content": "We now present the pebble game with colors. The game is played by a single player on a fixed finite set of vertices. The player makes a finite sequence of moves; a move consists in the addition and/or orientation of an edge. At any moment of time, the state of the game is captured by a directed graph H, with colored pebbles on vertices and edges. The edges of H are colored by the pebbles on them. While playing the pebble game all edges are directed, and we use the notation vw to indicate a directed edge from v to w. We describe the pebble game with colors in terms of its initial configuration and the allowed moves."}
{"pdf_id": "0704.0002", "content": "Fig. 4. A (2,2)-tight graph with one possible pebble-game decomposition. The edges are oriented to show (1,0)-sparsity for each color. (a) The graph K4 with a pebble-game decomposition. There is an empty black tree at the center vertex and a gray spanning tree. (b) The highlighted subgraph has two black trees and a gray tree; the black edges are part of a larger cycle but contribute a tree to the subgraph. (c) The highlighted subgraph (with a light gray background) has three empty gray trees; the black edges contain a cycle and do not contribute a piece of tree to the subgraph."}
{"pdf_id": "0704.0002", "content": "In this section we prove Theorem 1, a strengthening of results from [12] to the pebble game with colors. Since many of the relevant properties of the pebble game with colors carry over directly from the pebble games of [12], we refer the reader there for the proofs. We begin by establishing some invariants that hold during the execution of the pebble game."}
{"pdf_id": "0704.0002", "content": "Proof. (I1), (I2), and (I3) come directly from [12]. (I4) This invariant clearly holds at the initialization phase of the pebble game with colors. That add-edge and pebble-slide moves preserve (I4) is clear from inspection. (I5) By (I4), a monochromatic path of edges is forced to end only at a vertex with a pebble of the same color on it. If there is no pebble of that color reachable, then the path must eventually visit some vertex twice."}
{"pdf_id": "0704.0002", "content": "Proof. Observe that the preconditions in the statement of the lemma are implied by Lemma 7. By Lemma 12 monochromatic cycles form when the last pebble of color ci is removed from a connected monochromatic subgraph. (M1) and (M2) are the only ways to do this in a pebble game construction, since the color of an edge only changes when it is inserted the first time or a new pebble is put on it by a pebble-slide move."}
{"pdf_id": "0704.0002", "content": "Figure 5(a) and Figure 5(b) show examples of (M1) and (M2) map-graph creation moves, respectively, in a (2,0)-pebble game construction.We next show that if a graph has a pebble game construction, then it has a canonical pebble game construction. This is done in two steps, considering the cases (M1) and (M2) sepa rately. The proof gives two constructions that implement the canonical add-edge and canonical pebble-slide moves."}
{"pdf_id": "0704.0002", "content": "Remark: We note that in the upper range, there is always a repeated color, so no canonical add-edge moves create cycles in the upper range. The canonical pebble-slide move is defined by a global condition. To prove that we obtain the same class of graphs using only canonical pebble-slide moves, we need to extend Lemma 9 to only canonical moves. The main step is to show that if there is any sequence of moves that reorients a path from v to w, then there is a sequence of canonical moves that does the same thing."}
{"pdf_id": "0704.0002", "content": "Since no edges change color in the beginning of the new sequence, we have eliminated the (M2) move. Because our construction does not change any of the edges involved in the remaining tail of the original sequence, the part of the original path that is left in the new sequence will still be a simple path in H, meeting our initial hypothesis. The rest of the lemma follows by induction."}
{"pdf_id": "0704.0002", "content": "Complexity. We start by observing that the running time of Algorithm 17 is the time taken to process O(n) edges added to H and O(m) edges not added to H. We first consider the cost of an edge of G that is added to H. Each of the pebble game moves can be implemented in constant time. What remains is to describe an efficient way to find and move the pebbles. We use the following algorithm as a subroutine of Algorithm 17 to do this."}
{"pdf_id": "0704.1267", "content": "There is a huge amount of historical documents in libraries and in various National Archives that have not been  exploited electronically. Although automatic reading of complete pages remains, in most cases, a long-term  objective, tasks such as word spotting, text/image alignment, authentication and extraction of specific fields are  in use today. For all these tasks, a major step is document segmentation into text lines. Because of the low  quality and the complexity of these documents (background noise, artifacts due to aging, interfering lines),  automatic text line segmentation remains an open research field. The objective of this paper is to present a  survey of existing methods, developed during the last decade, and dedicated to documents of historical interest."}
{"pdf_id": "0704.1267", "content": "GUI as in the Viadocs project [11][18]. However, document structure can also be used when  no transcription is available. Word spotting techniques [22] [55] [46] can retrieve similar  words in the image document through an image query. When words of the image document  are extracted by top down segmentation, which is generally the case, text lines are extracted  first."}
{"pdf_id": "0704.1267", "content": "To have a good idea of the physical structure of a document image, one only needs to look at  it from a certain distance: the lines and the blocks are immediately visible. These blocks  consist of columns, annotations in margins, stanzas, etc... As blocks generally have no  rectangular shape in historical documents, the text line structure becomes the dominant  physical structure. We first give some definitions about text line components and text line  segmentation. Then we describe the factors which make this text line segmentation hard.  Finally, we describe how a text line can be represented."}
{"pdf_id": "0704.1267", "content": "baseline: fictitious line which follows and joins the lower part of the character bodies in a text  line (Fig. 2)  median line: fictitious line which follows and joins the upper part of the character bodies in a  text line.  upper line: fictitious line which joins the top of ascenders.  lower line: fictitious line which joins the bottom of descenders.  overlapping components: overlapping components are descenders and ascenders located in  the region of an adjacent line (Fig. 2).  touching components: touching components are ascenders and descenders belonging to  consecutive lines which are thus connected. These components are large but hard to  discriminate before text lines are known."}
{"pdf_id": "0704.1267", "content": "line spacing: lines that are rather widely spaced lines are easy to find. The process of  extracting text lines grows more difficult as interlines are narrowing; the lower baseline of the  first line is becoming closer to the upper baseline of the second line; also, descenders and  ascenders start to fill the blank space left for separating two adjacent text lines (Fig. 3)."}
{"pdf_id": "0704.1267", "content": "stroke fragmentation and merging: punctuation, dots and broken strokes due to low-quality  images and/or binarization may produce many connected components; conversely, words,  characters and strokes may be split into several connected components. The broken  components are no longer linked to the median baseline of the writing and become ambiguous  and hard to segment into the correct text line (Fig. 3)."}
{"pdf_id": "0704.1267", "content": "separating paths and delimited strip: separating lines (or paths) are continuous fictitious lines  which can be uniformly straight, made of straight segments, or of curving joined strokes. The  delimited strip between two consecutive separating lines receives the same text line label. So  the text line can be represented by a strip with its couple of separating lines (Fig. 4)."}
{"pdf_id": "0704.1267", "content": "strings: strings are lists of spatially aligned and ordered units. Each string represents one text  line.  baselines: baselines follow line fluctuations but partially define a text line. Units connected to  a baseline are assumed to belong to it. Complementary processing has to be done to cluster  non-connected units and touching components."}
{"pdf_id": "0704.1267", "content": "Projection-profiles are commonly used for printed document segmentation. This technique can also be adapted to handwritten documents with little overlap. The vertical projection profile is obtained by summing pixel values along the horizontal axis for each y value. From  the vertical profile, the gaps between the text lines in the vertical direction can be observed  (Fig. 5)."}
{"pdf_id": "0704.1267", "content": "The RXY cuts method applied in He and Downton [18], uses alternating projections along the  X and the Y axis. This results in a hierarchical tree structure. Cuts are found within white  spaces. Thresholds are necessary to derive inter-line or inter-block distances. This method can  be applied to printed documents (which are assumed to have these regular distances) or well  separated handwritten lines."}
{"pdf_id": "0704.1267", "content": "These methods consist in building alignments by aggregating units in a bottom-up strategy.  The units may be pixels or of higher level, such as connected components, blocks or other  features such as salient points. Units are then joined together to form alignments. The joining  scheme relies on both local and global criteria, which are used for checking local and global  consistency respectively."}
{"pdf_id": "0704.1267", "content": "The Hough transform can also be applied to fluctuating lines of handwritten drafts such as in  Pu and Shi [45]. The Hough transform is first applied to minima points (units) in a vertical  strip on the left of the image. The alignments in the Hough domain are searched starting from  a main direction, by grouping cells in an exhaustive search in 6 directions. Then a moving  window, associated with a clustering scheme in the image domain, assigns the remaining units  to alignments. The clustering scheme (Natural Learning Algorithm) allows the creation of  new lines starting in the middle of the page."}
{"pdf_id": "0704.1267", "content": "This  section only deals with methods where ambiguous components (overlapping or touching) are  actually detected before, during or after text line segmentation  Such criteria as component size, the fact that the component belongs to several alignments, or  on the contrary to no alignment, can be used for detecting ambiguous components"}
{"pdf_id": "0704.1267", "content": "The manuscripts studied in Likforman-Sulem et al. [27], are written in Hebrew, in a so-called  squared writing as most characters are made of horizontal and vertical strokes. They are  reproducing the biblical text of the Pentateuch. Characters are calligraphed by skilled scribes  with a quill or a calamus. The Scrolls, intended to be used in the synagogue, do not include  diacritics. Characters and words are written properly separated but digitization make some  characters touch. Cases of overlapping components occur as characters such as Lamed, Kaf,  and final letters include ascenders and descenders. Since the majority of characters are  composed of one connected component, it is more convenient to perform text line"}
{"pdf_id": "0704.1267", "content": "Projection, smearing and Hough-based methods, classically adapted to straight lines and  easier to implement, had to be completed and enriched by local considerations (piecewise  projections, clustering in Hough space, use of a moving window, ascender and descender  skipping), so as to solve some problems including: line proximity, overlapping or even  touching strokes, fluctuating close lines, shape fragmentation occurrences"}
{"pdf_id": "0704.1267", "content": "Concerning text line fluctuations, baseline-based representations seem to fit naturally.  Methods using straight line-based representations must be modified as previously to give non  linear results (by piecewise projections or neighboring considerations in Hough space). The  more fluctuating the text line, the more refined local criteria must be. Accurate locally  oriented processing and careful grouping rules make smearing and grouping methods  convenient. The stochastic methods also seem suited, for they can generate non linear  segmentation paths to separate overlapping characters, and even more to derive non linear  cutting paths from touching characters by identifying the shortest paths."}
{"pdf_id": "0704.1394", "content": "Three important features are required of a tool that implements interactive configu ration: it should be complete (all valid configurations should be reachable through user interaction), backtrack-free (a user is never forced to change an earlier choice due to incompleteness in the logical deductions), and it should provide real-time performance (feedback should be fast enough to allow real-time interactions)"}
{"pdf_id": "0704.1394", "content": "Important requirement for online user-interaction is the guaranteed real-time expe rience of user-configurator interaction. Therefore, the algorithms that are executing in the online phase must be provably efficient in the size of the BDD representation. This is what we call the real-time guarantee. As the CV D functionality is NP-hard, and theonline algorithms are polynomial in the size of generated BDD, there is no hope of pro viding polynomial size guarantees for the worst-case BDD representation. However, it suffices that the BDD size is small enough for all the configuration instances occurring in practice [10]."}
{"pdf_id": "0704.1675", "content": "Figure 1: Graphical representations of the probabilistic La tent Semantic Model (left) and Multi-way Aspect Model (right) R, U, T and Z denote variables \"Resource\", \"User\", \"Tag\" and \"Topic\" repectively. Nt represents a number of tag occurrences for a particular resource; D represents a number of resources. Meanwhile, Nb represents a number of all resource-user-tag co-occurrences in the social annotation system. Note that filled circles represent observed variables."}
{"pdf_id": "0704.1675", "content": "The sys tem provides three types of pages: a tag page — listing all resources that are tagged with a particular keyword; a user page — listing all resources that have been bookmarked by a particular user; and a resource page — listing all the tags the users have associated with that resource"}
{"pdf_id": "0704.1675", "content": "We use probabilistic models in order to find a compresseddescription of the collected resources in terms of topic de scriptions. This description is a vector of probabilities ofhow a particular resource is likely to be described by different topics. The topic distribution of the resource is subsequently used to compute similarity between resources us ing Jensen-Shannon divergence (Lin 1991). For the rest of this section, we describe the probabilistic models. We firstbrieny describe two existing models: the probabilistic La tent Semantic Analysis (pLSA) model and the Three-Way Aspect model (MWA). We then introduce a new model that explicitly takes into account users' interests and resources' topics. We compare performance of these models on the three del.icio.us datasets."}
{"pdf_id": "0704.1675", "content": "Interest-Topic Model (ITM)The motivation to implement the model proposed in this paper comes from the observation that users in a social anno tation system have very broad interests. A set of tags in a particular bookmark could renect both users' interests and resources' topics. As in the three-way aspect model, using asingle latent variable to represent both \"interests\" and \"top ics\" may not be appropriate, as intermixing between these two may skew the final similarity scores computed from the topic distribution over resources."}
{"pdf_id": "0704.1675", "content": "Figure 2: Graphical representation on the proposed model. R, U, T , I and Z denote variables \"Resource\", \"User\", \"Tag\", \"Interest\" and \"Topic\" repectively. Nt represents anumber of tag occurrences for a one bookmark (by a partic ular user to a particular resource); D represents a number of all bookmarks in social annotation system."}
{"pdf_id": "0704.1675", "content": "Instead, we propose to explicitly separate the latent vari ables into two: one representing user interests, I; another representing resource topics, Z. According to the proposed model, the process of resource-user-tag co-occurrence could be described as a stochastic process: • User u finds a resource r interesting and she would like to bookmark it• User u has her own interest profile i; meanwhile the re source has a set of topics z.• Tag t is then chosen based on users's interest and re source's topic The process is depicted in a graphical form in Figure 2. From the process described above, the joint probability of resource, user and tag is written as"}
{"pdf_id": "0704.1675", "content": "unique users for the geocoder seed; (c) 6,327,211 triples with 7,176 unique resources; 77,056 unique tags and 45,852 unique users for the wunderground seed. Next, we trained all three models on the data: pLSA, MWA and ITM. We then used the learned topic distributions to compute the similarity of the resources in each dataset tothe seed, and ranked the resources by similarity. We evalu ated the performance of each model by manually checking the top 100 resources produced by the model according to the criteria below:"}
{"pdf_id": "0704.1675", "content": "Figure 3: Performance of different models on the three datasets. Each model was trained with 40 or 100 topics. For ITM, we fix interest to 20 interests across all different datasets. The bars show the number of resources within the top 100 returned by each model that had the same functionality as the seed or contained a link to a resource with the same functionality as the seed."}
{"pdf_id": "0704.1675", "content": "Popular methods for finding documents relevant to a userquery rely on analysis of word occurrences (including meta data) in the document and across the document collection. Information sources that generate their contents dynamicallyin response to a query cannot be adequately indexed by con ventional search engines. Since they have sparse metadata,"}
{"pdf_id": "0704.1676", "content": "Social media sites share four characteristics: (1) Users create or contribute content in a variety of media types;(2) Users annotate content with tags; (3) Users evaluate con tent, either actively by voting or passively by using content; and (4) Users create social networks by designating otherusers with similar interests as contacts or friends"}
{"pdf_id": "0704.1676", "content": "Ratherthan forcing the image into a hierarchy or multiple hierar chies based on the equipment used to take the photo, the place where the image was taken, type of animal depicted, or even the animal's provenance, tagging system allows the user to locate the image by any of its properties by filtering the entire image set on any of the tags"}
{"pdf_id": "0704.1676", "content": "Contacts Flickr allows users to designate others as friends or contacts and makes it easy to track their activities. A single click on the \"Contacts\" hyperlink shows the user the latest images from his or her contacts. Tracking activities of friends is a common feature of many social media sites and is one of their major draws."}
{"pdf_id": "0704.1676", "content": "Search results We manually evaluated the top 500 images in each data set and marked each as relevant if it was related to the first sense of the search term listed above, not relevant or undecided, if the evaluator could not understand the image well enough to judge its relevance"}
{"pdf_id": "0704.1676", "content": "Flickr encourages users to designate others as contacts by making is easy to view the latest images submitted by them through the \"Contacts\" interface. Users add contacts for a variety of reasons, including keeping in touch with friends and family, as well as to track photographers whose work is of interest to them. We claim that the latter reason is the most dominant of the reasons. Therefore, we view user'scontacts as an expression of the user's interests. In this section we show that we can improve tag search results by filter ing through the user's contacts. To personalize search results for a particular user, we simply restrict the images returned by the tag search to those created by the user's contacts."}
{"pdf_id": "0704.1676", "content": "Table 2 shows how many of the 500 images in each data set came from a user's contacts. The column labeled \"# L1\"gives the number of user's Level 1 contacts. The follow ing columns show how many of the images were marked as relevant or not relevant by the filtering method, as well as precision and recall relative to the 500 images in each dataset. Recall measures the fraction of relevant retrieved im ages relative to all relevant images within the data set. Thelast column \"improv\" shows percent improvement in preci sion over the plain (unfiltered) tag search."}
{"pdf_id": "0704.1676", "content": "As Table 2 shows, filtering by contacts improves the pre cision of tag search for most users anywhere from 22% toover 100% when compared to plain search results in Ta ble 1. The best performance is attained for users within thenewborn set, with a large number of relevant images cor rectly identified as being relevant, and no irrelevant images admitted into the result set. The tiger set shows an average precision gain of 42% over four users, while the beetle set shows an 85% gain."}
{"pdf_id": "0704.1676", "content": "Increase in precision is achieved by reducing the numberof false positives, or irrelevant images that are marked as rel evant by the search method. Unfortunately, this gain comes at the expense of recall: many relevant images are missedby this filtering method. In order to increase recall, we en large the contacts set by considering two levels of contacts: user's contacts (Level 1) and her contacts' contacts (Level2). The motivation for this is that if the contact relationship expresses common interests among users, user's inter ests will also be similar to those of her contacts' contacts."}
{"pdf_id": "0704.1676", "content": "The second half of Table 2 shows the performance of filtering the search results by the combined set of user's Level 1 and Level 2 contacts. This method identifies manymore relevant images, although it also admits more irrele vant images, thereby decreasing precision. This method stillshows precision improvement over plain search, with pre cision gain of 9%, 16% and 11% respectively for the three data sets."}
{"pdf_id": "0704.1676", "content": "Figure 2: Graphical representation for model-based infor mation filtering. U, T , G and Z denote variables \"User\", \"Tag\", \"Group\", and \"Topic\" respectively. Nt represents a number of tag occurrences for a one photo (by the photo owner); D represents a number of all photos on Flickr.Meanwhile, Ng denotes a number of groups for a particu lar photo."}
{"pdf_id": "0704.1676", "content": "The process is depicted in a graphical form in Figure 2. We do not treat the image i as a variable in the model but view it as a co-occurrence of a user, a set of tags and a set of groups. From the process described above, we can represent the joint probability of user, tag and group for a particular photo as"}
{"pdf_id": "0704.1676", "content": "Note that it is straightforward to exclude photo's group information from the above equation simply by omitting the terms relevant to g. nt and ng is a number of all possible tags and groups respectively in the data set. Meanwhile, ni(t) and ni(g) act as indicator functions: ni(t) = 1 if an image i is tagged with tag t; otherwise, it is 0. Similarly, ni(g) = 1 if an image i is submitted to group g; otherwise, it is 0. k is the predefined number of topics. The joint probability of photos in the data set I is defined as p(I) ="}
{"pdf_id": "0704.1676", "content": "In order to estimate parameters p(z|ui), p(ti|z), and p(gi|z),we define a log likelihood L, which measures how the esti mated parameters fit the observed data. According to the EM algorithm (Dempster et al. 1977), L will be used as an objective function to estimate all parameters. L is defined as"}
{"pdf_id": "0704.1676", "content": "tacular Nature, Let's Play Tag, etc.). We postulate that group information would help estimate p(i|z) in cases where the photo has few or no tags. Group information would help filling in the missing data by using group name as another tag. We also trained the model on the data with 15 topics, but found no significant difference in results."}
{"pdf_id": "0704.1676", "content": "We presented two methods for personalizing results of im age search on Flickr.Both methods rely on the meta data users create through their everyday activities on Flickr, namely user's contacts and the tags they used for annotating their images. We claim that this information captures user's tastes and preferences in photography and can be used to personalize search results to the individual user. We showed"}
{"pdf_id": "0704.1676", "content": "tribute reports for Governmental purposes notwithstandingany copyright annotation thereon. The views and conclu sions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of any of the above organizations or any person connected with them."}
{"pdf_id": "0704.2010", "content": "One of the major tasks in computational molecular biology is toaid large-scale protein annotation and biological knowledge disco very. Functional characterization of unknown-function proteins is often inferred through sequence similarity search methods, such as BLAST (Altschul et al., 1990) and FASTA (Pearson, 1985). However, when the evolutionary relationship among proteins is distant, methods based on profile hidden Markov models (pHMMs) (Eddy, 1996; Krogh et al., 1994) are known to outperform methods"}
{"pdf_id": "0704.2010", "content": "Profile HMMs represent conserved regions in sequences as sequences of match (M) states. Inserted material is represented as insert states (I), anddeleted regions as delete states (D). The parameters of pHMMs are pro babilities of two events: a transition probability from a state to another state, and a probability that a specific state will emit a specific residue (say,a specific amino-acid when comparing proteins), called emission probabi lity. Obviously, only match and insert states generate characters and have"}
{"pdf_id": "0704.2010", "content": "In the spirit of PSSMs, we propose to reinforce residues that correspond to preserved regions in the protein. Our motivation is that when homologue proteins are structurally aligned, spatial overlapping of an atom set occurs.This set is called the invariant core or core structure, and can be used to cha racterize homologue proteins. We argue that the residues in the core structure should carry more weight rather than the residues outside the core. Thus, wepropose sequence-weighting method that gives different weight to each resi due in the same protein, based on structural relevance. We will represent such \"structural\" weights by a matrix Ms, where each residue of the same protein has a different weight."}
{"pdf_id": "0704.2010", "content": "In a second step, we join the models built from these matrices to forma library of structural models aiming at building a single model to repre sent the structural patterns under different aspects. We used the hmmpfam HMMER tool to combine the models together. Library of models have been used in a number of studies, such as (Bateman et al., 2004; Haft et al., 2003; Gough et al., 2001), and they are known to achieve better results than those achieved by single models."}
{"pdf_id": "0704.2010", "content": "As a first step, we build a model for each structural property and evaluate it according to the methodology described in the Methodssection. The ROC curves are presented in figure 4 and the Preci sion/Recall curves in figure 5. Both figures show all models, that is, pHMM2D (secondary structural model), pHMMOi (Ooi measuremodel), pHMMAcc (inaccessibility model) and pHMM3D (three dimensional structure model) outperforming the HMMER model."}
{"pdf_id": "0704.2010", "content": "Next, we compare the performance of the model library with respect to the initial HMMER model. To do so, we joined the five models, one for each structural property, and scored the test sequences using hmmpfam. Figure 6 shows the ROC curve forthe results. Figure 7 shows graphically the results through Precison/Recall curves. Both figures show HMMER-STRUCT outperfor ming HMMER. Table 3 displays significance results. The difference between HMMER-STRUCT and HMMER results are statistically significant according to paired two tailed t-test. The two tailed t-test also indicate significant differences between HMMER-STRUCT and each HMMER-STRUCT component, i.e, HMMER, pHMM2D, pHMM3D, pHMMAcc and pHMMOi."}
{"pdf_id": "0704.2010", "content": "We believe that the good results obtained with the pHMMoi model can be attributed to the fact that tight packing is important for protein stability, and follow well-known results that indicate that amino-acids located in the core protein are more conserved than amino-acids located in other sites (Privalov, 2000)"}
{"pdf_id": "0704.2010", "content": "property. Therefore, combining the models increases sensitivity by exploring the different structural properties. Our method shows that structural information can be added during the training phase of pHMM to improve sensitivity, without much changes to the usage of pHMM methodology, and applied to recently discovered proteins for which there is little structural information."}
{"pdf_id": "0704.2902", "content": "An important goal for digital libraries is to enable researchers to more easily explore related work. While citation data is often used as an indicator of relatedness, in this paper we demonstrate that digital access records (e.g. http-server logs) can be used as indicators as well. In particular, weshow that measures based on co-access provide better cov erage than co-citation, that they are available much sooner, and that they are more accurate for recent papers."}
{"pdf_id": "0704.2902", "content": "related. We evaluate how well this measure predicts future co-citations on the arXiv e-Print archive [1]. Our resultsshow that access-based measures have vastly larger coverage and are more accurate at finding related work than co citation for recently published papers. Additional and more detailed results can be found in [7]."}
{"pdf_id": "0704.2963", "content": "Bondi, H. 1952, MNRAS, 112, 195 Brown, G.E. 1995, ApJ, 440, 270 Burrows, A., & Woosley, S. 1986, ApJ, 308, 680 Cannon, R.C. 1993, MNRAS, 263, 817 Cannon, R.C., Eggleton, P.P., Zytkow, A.N., P. 1992, ApJ, 386, 206 Chevalier, R.A. 1989, ApJ, 346, 847 (C89) Chevalier, R.A. 1993, ApJ, 411,L33 Colgate, S.A. 1971, ApJ, 163, 221 Colgate, S.A., Herant, M., & Benz, W. 1993, Phys. Rep., 227, 157 (CHB) Cox, A.N., Vauclair, S., & Zahn, J.P. 1983, Astrophysical Processes in Upper Main Sequence Stars, (CH-1290 Sauverny : Geneva Observatory) Davies, R.E., & Pringle, J. 1980, MNRAS, 191, 599 Davies, M.B. & Benz, W., 1995, MNRAS, submitted Table 2.1: Excerpt of a typical reference section in physics papers."}
{"pdf_id": "0704.3157", "content": "In this section we brieny describe the syntax and the semantics of the reasoninglanguage adopted by the DLVDB system. This is basically Disjunctive Logic Pro gramming (DLP) with Aggregate functions under the Answer Set Semantics; we refer to this language as DLPA in the following. The interested reader can find all details about DLPA in (Faber et al. 2004). Before starting the presentation, it is worth pointing out that the direct databaseexecution modality supports only a strict subset of the reasoning language sup ported by the main-memory execution. In particular, while DLVIO supports the whole language of DLV (including disjunction, unlimited negation, and stratified"}
{"pdf_id": "0704.3157", "content": "Definition 2.3 ((Faber et al. 2004)) Given a ground DLPA program P and a total interpretation I, let PI denote the transformed program obtained from P by deleting all rules in which a body literal is false w.r.t. I. I is an answer set of a program P if it is a minimal model of Ground(P)I."}
{"pdf_id": "0704.3157", "content": "As pointed out in the Introduction, the presented system allows for two typologies of execution: (i) direct database execution (DLVDB), which is capable of handling massive amounts of data but with some limitations on the expressiveness of the query program (see Section 2), and (ii) main-memory execution (DLVIO) which allows the user to take full advantage of the expressiveness of DLPA and to import data residing on DBMSs, but with some limitations on the quantity of data to reason about, given by the amount of available main-memory"}
{"pdf_id": "0704.3157", "content": "Three main peculiarities characterize the DLVDB system in this execution modality: (i) its ability to evaluate logic programs directly and completely on databases with a very limited usage of main-memory resources, (ii) its capability to map programpredicates to (possibly complex and distributed) database views, and (iii) the pos sibility to easily specify which data is to be considered as input or as output for the program. This is the main contribution of our work."}
{"pdf_id": "0704.3157", "content": "An #import command retrieves data from a table \"row by row\" through the query specified by the user in SQL and creates one atom for each selected tuple. The name of each imported atom is set to predname, and is considered as a fact of the program. The #export command generates a new tuple into tablename for each new truth value derived for predname by the program evaluation. An alternative form of the #export command is the following:"}
{"pdf_id": "0704.3157", "content": "which can be used to remove from tablename the tuples of predname for which the\"REPLACE where\" condition holds; it can be useful for deleting tuples correspond ing to violated integrity constraints. It is worth pointing out that if a DLPA program contains at least one #export command, the system can compute only the first valid answer set; this limitation has been introduced mainly to avoid an exponential space complexity of the system. In fact, the number of answer sets can be exponential in the input."}
{"pdf_id": "0704.3157", "content": "Example 3.2 Consider again the scenario introduced in Example 3.1, and assume that the amount of input data allows the evaluation to be carried out in main-memory. The built-in commands that must be added to the DLPA program of Example 3.1 to implement the necessary mappings are: #import(dbAirports, \"airportUser\", \"airportPasswd\" , \"SELECT * FROM night"}
{"pdf_id": "0704.3157", "content": "Note that the syntax of DLVIO directives is simpler than that of DLVDB auxiliary directives. This is because DLVIO is intended to provide an easy mechanism to load data into the logic program and then store its results back to mass-memory, whereas DLVDB is oriented to more sophisticated applications handling distributed data and mass-memory-based reasoning and, consequently, it must provide a richer set of options in defining the mappings."}
{"pdf_id": "0704.3157", "content": "In this section we describe the general functions exploited to translate DLPA rules in SQL statements. Functions are presented in pseudocode and, for the sake of presentation clarity, they omit some details; moreover, since there is a one-to-one correspondence between the predicates in the logic program and the relations in the database, in the following, when this is not confusing, we use the terms predicate and relation interchangeably. It is worth recalling that these one-to-one correspondences are determined both from the user specifications in the auxiliary directives and from the mappings automatically derived by the system. In order to provide examples for the presented functions, we exploit the following reference schema:"}
{"pdf_id": "0704.3157", "content": "Translating Positive Rules.Intuitively, the SQL statement for positive rules is composed as follows: the SE LECT part is determined by the variable bindings between the head and the bodyof the rule. The FROM part of the statement is determined by the predicates com posing the body of the rule; variable bindings between body atoms and constants determine the WHERE conditions of the statement. Finally, an EXCEPT part isadded in order to eliminate tuple duplications. The behaviour of function Trans latePositiveRule is well described by the following example."}
{"pdf_id": "0704.3157", "content": "Translating rules with negated atoms. Intuitively, the construction of the SQL statement for this kind of rule is carried out as follows: the positive part of the rule is handled in a way very similar to what has been shown for function TranslatePositiveRule; then, each negated atom is handled by a corresponding NOT IN part in the statement. The behaviour of function TranslateRuleWithNegation is well illustrated by the following example."}
{"pdf_id": "0704.3157", "content": "As an example, aggregate atoms can not contain predicates mutually recursive with the head of the rule they are placed in; from our point of view, this implies that the truth values of each aggregate function can be computed once and for all before evaluating the corresponding rule (which can be, in its turn, recursive)"}
{"pdf_id": "0704.3157", "content": "of f and, consequently, it may have far less (and can not have more) attributes than those present in Conj. In our approach we rely on this standardization to translate this kind of rule to SQL; clearly only the second rule, containing the aggregate function, is handled by the function we are presenting next; in fact, the first rule is automatically translated by one of the already presented functions. Intuitively, the objective of our translation is to create an SQL view auxAtom"}
{"pdf_id": "0704.3157", "content": "Example 4.5 Consider the situation in which we need to know whether the employee e1 is the boss of the employee en either directly or by means of a number of employees e2, .., en such that e1 is the boss of e2, e2 is the boss of e3, etc. Then, we have to evaluate the program:"}
{"pdf_id": "0704.3157", "content": "Moreover, as we pointed out in the Introduction, other logic-based systems such as ASSAT, Cmodels, and CLASP have not been tested since they use the same grounding layer of Smodels (LParse) and, as it will be clear in the following, the benchmark programs are completely solved by this layer"}
{"pdf_id": "0704.3157", "content": "On the contrary, DB-C implements a large subset of SQL99 features andsupports recursion but, as far as recursive queries are concerned, it exploits pro prietary constructs which do not follow the standard SQL99 notation, and whose expressiveness is lower than that of SQL99; as an example, it is not possible to express unbound queries within recursive statements (e"}
{"pdf_id": "0704.3157", "content": "The LDL++ system (Arni et al. 2003) integrates rule-based programming with ef ficient secondary memory access, transaction management recovery and integrity control. The underlying database engine has been developed specifically within theLDL project and is designed as a virtual-memory record manager, which is opti mized for the situation where the pages containing frequently used data can reside in main-memory. LDL++ can also be interfaced with external DBMSs, but it isnecessary to implement vendor-specific drivers to handle data conversion and lo cal SQL dialects (Arni et al. 2003). The LDL++ language supports complex terms within facts and rules, stratified negation, and don't care non-determinism based"}
{"pdf_id": "0704.3157", "content": "It is the direct database execution of our system; in our tests we used a commercial database as DBMS for the working database. However, to guarantee fairness with the other systems, we did not set any additional index or key information for the involved relations. We point out again that any DBMS supporting ODBC could be easily coupled with DLVDB."}
{"pdf_id": "0704.3157", "content": "a sequence of edges in E. The input is provided by a relation edge(X, Y ) where a fact edge(a, b) states that b is directly reachable by an edge from a. In database terms, determining all pairs of reachable nodes in G amounts to computing the transitive closure of the relation storing the edges."}
{"pdf_id": "0704.3157", "content": "Given a parent-child relationship (a tree), the Same Generation problem aims to find pairs of persons belonging to the same generation. Two persons belong to the same generation either if they are siblings, or if they are children of two persons of the same generation. The input is provided by a relation parent(X, Y ) where a fact parent(thomas, moritz) means that thomas is the parent of moritz."}
{"pdf_id": "0704.3157", "content": "From the analysis of these figures we can observe that, in several cases, the performance of DLVDB (the black triangle in the graphs) is better than all the other systems with orders of magnitude and that DLVDB allows almost always the handling of the greatest amount of data; moreover, there is no system which can be considered the \"competitor\" of DLVDB in all the tests"}
{"pdf_id": "0704.3157", "content": "Surprisingly enough, DBMSs often have the worst performance (their times are near to the vertical axis) and they can handle very limited amounts of input data. Finally, as expected, DLVIO is capable of handling lower amounts of data w.r.t. DLVDB; however, in several cases it was one of the best three performing systems, especially on bound queries. This result is mainly due to the magic sets optimization technique it implements. A rather surprising result is that DLVIO has almost always higher execution times than DLVDB even for not very high input data sizes. The motivation for this result can be justified by the following reasoning. Both DLVDB and DLVIO"}
{"pdf_id": "0704.3157", "content": "for logical query optimization (like, e.g., magic sets). (iii) A proper combination and a well-engineered implementation of the above ideas. Moreover, the usage of a purely mass-memory evaluation strategy, improves previous deductive systems eliminating, in practice, any limitation in the dimension of the input data. In the future we plan to extend the language supported by the direct database execution and to exploit the system in interesting research fields, such as data integration and data warehousing. Moreover, a mixed approach exploiting both DLVDB and DLVIO executions to evaluate hard problems partially on mass-memory and partially in main-memory will be explored."}
{"pdf_id": "0704.3316", "content": "1. INTRODUCTION The paradigm of collaborative tagging [1, 2] has been swiftly adopted and deployed in a wide range of systems,motivating a surge of interest in understanding their structure and evolution. Folksonomies have been known to ex hibit striking statistical regularities and activity patterns [3, 4].In this context, a natural topic for investigation is the vo cabulary of tags that is used within a given system, and in particular its evolution over time, as new users, resources and tags come into play. Some insights in this direction arereported in [3] and [5], but a systematic attempt at charac"}
{"pdf_id": "0704.3316", "content": "2. EXPERIMENTAL DATA Our analysis will focus on del.icio.us for several reasons: i) it was the first system to deploy the ideas and technologies of collaborative tagging, so it has acquired a paradigmaticcharacter and it is the natural starting point for any quan titative study. ii) because of its popularity, it has a large community of active users and comprises a precious body of raw data on the structure and evolution of a folksonomy. iii) it is a broad folksonomy [7], i.e. single tagging events (posts) retain their identity and can be individually retrieved. This allows to define and measure the multiplicity (or frequency)"}
{"pdf_id": "0704.3316", "content": "It is remarkable that the above statistical regularities holdthroughout the history of del.icio.us, while the system un dergoes a huge change in the size of its user base, the numberof bookmarked resources, several changes in the user inter face are made, tag suggestion is introduced, and so on. The above observations constitute the core facts of the present study, and in the following we will shift from the global view of the system to a local one, to see whether these facts stay valid, and to deepen our analysis."}
{"pdf_id": "0704.3316", "content": "6. ACKNOWLEDGMENTS This research has been partly supported by the TAGoraproject (FP6-IST5-34721) funded by the Future and Emerging Technologies program (IST-FET) of the European Com mission. The information provided is the sole responsibilityof the authors and does not renect the Commission's opin ion. The Commission is not responsible for any use that may be made of data appearing in this publication."}
{"pdf_id": "0704.3359", "content": "• Good performance with respect to the empirical risk Remp[f, X, Y ] does not result in good performance on an unseen test set. In practice, strict minimization of the empirical risk virtually ensures bad performance on a test set due to overfitting. This issue has been discussed extensively in the machine learning literature (see e.g. [Vapnik, 1982])."}
{"pdf_id": "0704.3359", "content": "Solving the optimization problem (7) presents a formidable challenge. In particular, for largeZ (e.g. the space of all permutations over a set of documents) the number of variables is pro hibitively large and it is essentially impossible to find an optimal solution within a practical amount of time. Instead, one may use column generation [Tsochantaridis et al., 2005] to find an approximate solution in polynomial time. The key idea in this is to check the constraints (5b) to find out which of them are violated for the current set of parameters and to use this information to improve the value of the optimization problem. That is, one needs to find"}
{"pdf_id": "0704.3359", "content": "assignment problem (ignoring log-factors). Finally, Orlin and Lee [1993] propose a linear time algorithm for large problems. Since in our case the number of pages is fairly small (in the order of 50 to 200), we used an existing implementation due to Jonker and Volgenant [1987]. See Section 6.3 for runtime details. The latter uses modern techniques for computing the shortest path problem arising in (26). This means that we can check whether a particular set of documents and an associated query (Di, qi) satisfies the inequality constraints of the structured estimation problem (5). Hence we have the subroutine necessary to make the algorithm of Section 2 work. In particular, this is the only subroutine we need to replace in SVMStruct [Tsochantaridis et al., 2005]."}
{"pdf_id": "0704.3359", "content": "Imagine the following scenario: when searching for 'Jordan', we will find many relevant webpages containing information on this subject. They will cover a large range of topics, such as a basketball player (Mike Jordan), a country (the kingdom of Jordan), a river (in the Middle East), a TV show (Crossing Jordan), a scientist (Mike Jordan), a city (both in Minnesota and in Utah), and many more. Clearly, it is desirable to provide the user with a diverse mix of references, rather than exclusively many pages from the same site or domain or topic range. One way to achieve this goal is to include an interaction term between the items to be ranked. This leads to optimization problems of the form"}
{"pdf_id": "0704.3359", "content": "Protocol Since WebSearch provided a validation set, we used the latter for model selection. Otherwise, 10-fold cross validation was used to adjust the regularization constant C. We used linear kernels throughout, except for the EachMovie datasets, where we followed the protocols of [Basilico and Hofmann, 2004] and [Yu et al., 2006]. This was done to show that the performance improvement we observe is due to our choice of a better loss function rather than the function class. Note that NDCG, MRR were rescaled from [0, 1] to [0, 100] for better visualization."}
{"pdf_id": "0704.3359", "content": "NDCG In a second experiment, we mimicked the experimental protocol of [Yu et al., 2006] on EachMovie. Here, we treat each movie as a document and each user as a query. After filtering out all the unpopular documents and queries (as in [Yu et al., 2006]) we have 1075 documents and 100 users. For each user, we randomly select 10, 20 and 50 labeled items for training and perform prediction on the rest. The process is repeated 10 times independently. The methods for"}
{"pdf_id": "0704.3359", "content": "In this paper we proposed a general scheme to deal with a large range of criteria commonly used in the context of web page ranking and collaborative filtering. Unlike previous work, which mainly focuses on pairwise comparisons we aim to minimize the multivariate performance measures (or rather a convex upper bound on them) directly. This has both computational savings, leading to a faster algorithm and practical ones, leading to better performance. In a way, our work follows the mantra of [Vapnik, 1982] of estimating directly the desired quantities rather than optimizing a surrogate function. There are clear extensions of the current work:"}
{"pdf_id": "0704.3359", "content": "• The key point of our paper was to construct a well-designed loss function for optimization. In this form it is completely generic and can be used as a drop-in replacement in many settings. We completely ignored language models [Ponte and Croft, 1998] to parse the queries in any sophisticated fashion."}
{"pdf_id": "0704.3359", "content": "• The present algorithm can be extended to learn matching problems on graphs. This is achieved by extending the linear assignment problem to a quadratic one. The price one needs to pay in this case is that the Hungarian Marriage algorithm is no longer feasible, as the optimization problem itself is NP hard."}
{"pdf_id": "0704.3359", "content": "Note that the choice of a Hilbert space for the scoring functions is done for reasons of conve nience. If the applications demand Neural Networks or similar (harder to deal with) function classes instead of kernels, we can still apply the large margin formulation. That said, we find that the kernel approach is well suited to the problem."}
{"pdf_id": "0704.3359", "content": "Acknowledgments: We are indebted to Thomas Hofmann, Chris Burges, and Shipeng Yufor providing us with their datasets for the purpose of ranking. This was invaluable in ob taining results comparable with their own publications (as reported in the experiments). We thank Yasemin Altun, Chris Burges, Tiberio Caetano, David Hawking, Bhaskar Mehta, Bob"}
{"pdf_id": "0704.3395", "content": "instruct the CPU to 1) read the word in the memory cell at memory address 2 in RAM and store it in CPU register 3, 2) read the word at memory address 1 and store it in register 2, 3) add the contents of register 1 and 2 and store the result in register 3, and finally 4) store the word in register 3 into memory address 2 of RAM. Modern day computer languages are written at a much higher level of abstraction than both machine and assembly language. For instance, the previous instructions could be represented by a single statement as"}
{"pdf_id": "0704.3395", "content": "The example SPARQL query will bind the variable ?x to all URIs that are the subject of the triples with a predicate of rdf:type and objects of ComputerScientist and CognitiveScientist. For the example RDF network diagrammed in Figure 1, ?x would bind to Marko. Thus, the query above would return Marko.4"}
{"pdf_id": "0704.3395", "content": "where X is the set of URIs that bind to ?x and G is the RDF network represented as an edge list. The above syntax's semantics is \"X is the set of all elements ?x such that ?x is the head of the triple ending with rdf:type, ComputerScientist and the head of the triple ending with rdf:type, CognitiveScientist, where both triples are in the triple list G\". Only recently has there been a proposal to extend SPARQL to support writing and deleting triples to and from an RDF network. SPARQL/Update (Seaborne & Manjunath, 2007) can be used to add the fact that Marko is also an rdf:type of Human."}
{"pdf_id": "0704.3395", "content": "4Many triple-store applications support reasoning about resources during a query (at run-time). For example, suppose that the triple (Marko, rdf:type, ComputerScientist) does not exist in the RDF network, but instead there exist the triples (Marko, rdf:type, ComputerEngineer) and (ComputerEngineer, owl:sameAs, ComputerScientist). With OWL reasoning, ?x would still bind to Marko because ComputerEngineer and ComputerScientist are the same according to OWL semantics. The RDF computing concepts presented in this article primarily focus on triple pattern matching and thus, beyond direct URI and literal name matching, no other semantics are used."}
{"pdf_id": "0704.3395", "content": "5In this article, ontology diagrams will not explicitly represent the constructs rdfs:domain, rdfs:range, nor the owl:Restriction anonymous URIs. These URIs are assumed to be apparent from the diagram. For example, the restriction shown as [0..1] in Figure 2 is represented by an owl:Restriction for the hasFriend property where the maxCardinality is 1 and Human is an rdfs:subClassOf of this owl:Restriction."}
{"pdf_id": "0704.3395", "content": "declares that there exists an abstract class called Human. A Human has one field calledhasFriend. The hasFriend field refers to an object of type Human. Furthermore, accord ing to the class declaration, a Human has a method called makeFriend. The makeFriend method takes a single argument that is of type Human and sets its hasFriend field to the Human provided in the argument. The this keyword makes explicit that the hasFriend field is the field of the object for which the makeFriend method was invoked.In many object-oriented languages, an instance of Human is created with the new oper ator. For instance,"}
{"pdf_id": "0704.3395", "content": "creates a Human named (referenced as) Marko. The new operator is analogous to the rdf:type property. Thus, after this code is executed, a similar situation exists as that which is represented in Figure 2. However, the ontological model diagrammed in the top half of Figure 2 does not have the makeFriend method URI. The relationship between object-oriented programming and OWL is presented in Table 1."}
{"pdf_id": "0704.3395", "content": "This article unifies all of the concepts presented hitherto into a framework for computing on RDF networks. In this framework, the state of a computing virtual machine, the API, and the low-level instructions are all represented in RDF. Furthermore, unlike the current programming paradigm, there is no stack of representation. The lowest level of computing and the highest level of computing are represented in the same substrate: URIs, literals, and triples. This article proposes the concept of OWL APIs, RDF triple-code, and RDF virtual machines (RVM). Human readable/writeable source code is compiled to create an OWL ontology that abstractly represents how instructions should be united to form instruction sequences.6 When objects and their methods are instantiated from an OWL API, RDF"}
{"pdf_id": "0704.3395", "content": "2. The Semantic Web is no longer an information gathering infrastructure, but a dis tributed information processing infrastructure (the process can move to the data, thedata doesn't have to move to the process). An RVM can be \"GETed\" from a web server as an RDF/XML document or \"SELECTed\" from an RDF triple-store. RDF programs and RVM states are \"first-class\" web-entities. The ramifications of this is that an RVM can move between triple-store environments and can compute on local"}
{"pdf_id": "0704.3395", "content": "OWL supports the specification of class interactions. However, class interactions are speci fied in terms of property relationships, not method invocations. OWL has no formal way of specifying class behaviors (i.e. methods). However, in OWL, it is possible to define method and instruction classes and formally specify restrictions that dictate how instructions should be interrelated within a method. The method and instruction ontology presented in this article makes RDF a programming framework and not just a data modeling framework."}
{"pdf_id": "0704.3395", "content": "An instance of the machine architecture is an RDF virtual machine (RVM). The purpose of the RVM is to represent its state (stacks, program counter, etc.) in the same RDF network as the triple-code instructions. However, the RDF-based RVM is not a \"true\" computer. The RVM simply represents its state in RDF. The RVM requires a software implementation outside the triple-store to compute its instructions. This requires the machine level discussed next."}
{"pdf_id": "0704.3395", "content": "The machine level is where the actual computation is executed. An RDF network is a data structure. RDF is not a processor in the common sense—it has no way of evolving itself. In order to process RDF data, some external process must read and write to the RDF network. The reading and writing of the RDF network evolves the RVM and the objects on which it is computing. This section discusses the machine level that is diagrammed in Figure 3."}
{"pdf_id": "0704.3395", "content": "The virtual machine process is represented in software on a particular host machine. TheRVM processor must be compatible with both the triple-store interface (e.g. SPARQL/Up date) and the underlying host machine. The RVM's host machine can be the physical machine (hardware CPU) or another virtual machine. For instance, if the RVM's machine process is implemented in the Java language, then the machine process runs in the JVM. This is diagrammed in Figure 3 by the ... component in between the virtual machine process and the physical machine."}
{"pdf_id": "0704.3395", "content": "The physical machine is the actual hardware CPU. The RVM implementation translates the RDF triple-code to the host machine's instruction set. For example, if the RVM process is running on the Intel Core Duo, then it is the role of the RVM process to translate the RDF triple-code to that specified by the Intel Core Duo instruction set. Thus, portability"}
{"pdf_id": "0704.3395", "content": "of this architectural model relies on a per host implementation of the RVM. Finally, to complete the computational stack, the laws of physics compute the hardware CPU. Much like the RDF representation of the RVM is a \"snap-shot\" representation of a computation, the hardware CPU is a silicon/electron \"snap-shot\" representation of a computation."}
{"pdf_id": "0704.3395", "content": "Throughout the remainder of this article, Universally Unique Identifiers (UUIDs) will be continually used (Leach, 2005). The set of all UUIDs is a subset of the set of all URIs. A UUID is a 128-bit (16-byte) string that can be created in disparate environments with a near zero probability of ever being reproduced. To understand the number of UUIDs that are possible at 128-bits, it would require 1 trillion unique UUIDs to be created every"}
{"pdf_id": "0704.3395", "content": "nanosecond for 10 billion years to exhaust the space of all possible UUIDs.7 A UUID canbe represented as a 36 character hexadecimal string. For example, 6c3f8afe-ec3d-11db-8314 0800200c9a66, is a UUID. The hexadecimal representation will be used in all the following examples. However, for the sake of brevity, since 36 characters is too lengthy for theexamples and diagrams, only the first 8 characters will be used. Thus, 6c3f8afe-ec3d-11db 8314-0800200c9a66 will be represented as 6c3f8afe. Furthermore, UUIDs, when used as URIs are namespaced as"}
{"pdf_id": "0704.3395", "content": "While Neno is an object-oriented language, it is also a semantic network programming language. Neno is more in line with the concepts of RDF than it is with those of Java and C++. One of the major distinguishing features of an object in Neno is that objects can have multi-instance fields. This means that a single field (predicate) can have more than one value (object). For instance, in Java"}
{"pdf_id": "0704.3395", "content": "will initially set the hasName field of the Human object referenced by the variable name marko to \"Marko Rodriguez\". The invocation of the setName method of marko will replace \"Marko Rodriguez\" with \"Marko Antonio Rodriguez\". Thus, the field hasName has a cardinality of 1. All fields in Java have a cardinality of 1 and are universally quantified for the specified class (though taxonomical subsumption is supported). In Neno, it is possible for a field to have a cardinality greater than one. In Neno, when a class' fields are declared, the cardinality specifier is used to denote how many properties of this type are allowed for an instance of this class. Thus, in the Neno code at the start of this section,"}
{"pdf_id": "0704.3395", "content": "For a multi-instance field, the = is a very destructive operator. For a [0..1] or [1] field, = behaves as one would expect in any other object-oriented language. Furthermore, for a [0..1] or [1] field, =+ is not allowed as it will cause the insertion of more than one property of the same predicate. In order to control the removal of fields from a multi-instance field, the =- and =/ operators can be used. For example, suppose the following method declaration in Neno"}
{"pdf_id": "0704.3395", "content": "In many cases, a field (i.e. property) will have many instances. In computer programming terms, fields can be thought of as arrays. However, these \"arrays\" are not objects, but simply greater than one cardinality fields. In Java, arrays are objects and high-level array objects like the java.util.ArrayList provide functions to search an array. In Neno, there are no methods that support such behaviors since fields are not objects. Instead, Neno provides language constructs that support field querying. For example, suppose the following method"}
{"pdf_id": "0704.3395", "content": "It is important to note that these statements need not have the literal type specifier (e.g. xsd:integer) on every hardcoded literal. The literal type can be inferred from its context and thus, is automatically added by the compiler. For example, since i is an xsd:integer, it is assumed that 10 is also."}
{"pdf_id": "0704.3395", "content": "In object-oriented languages the \"dot\" operator is used to access a method or field of an object. For instance, in this.hasName, on the left of the \"dot\" is the object and on the right of the \"dot\" is the field. Whether the right hand side of the operator is a field or method can be deduced by the compiler from its context. If this resolves to the URI urn:uuid:2db4a1d2, then the following Neno code"}
{"pdf_id": "0704.3395", "content": "According to the previous query, everything that binds to ?h will be set to the variable h. The above query says \"locate all Human hasFriends of this object.\" However, Neno provides another concept not found in other object-oriented languages called the \"dot dot\" operator. The \"dot dot\" operator provides support for what is called inverse field referencing (and inverse method invocation discussed next). Assume the following line in some method of some class,"}
{"pdf_id": "0704.3395", "content": "the true and false block of the if statement can read the variable a, but the true block can not read the c in the false block and the false block can not read the b in the true block. Also, methods are out of scope from one another. The only way methods communicate are through parameter passing, return values, and object manipulations."}
{"pdf_id": "0704.3395", "content": "Behind the scenes, Fhat would also remove all the method references of urn:uuid:55b2a3b0, internal variable references to urn:uuid:55b2a3b0, and the rdf:type relationships that relate the object to the ontological-layer. When an object is properly destroyed, only its instance is removed from the RDF network. The object's class specification still exists in the ontological-layer."}
{"pdf_id": "0704.3395", "content": "In Neno, there are no static methods. Thus, there does not exist something like the public static void main(String[] args) method in Java. Instead, Fhat is provided a class URI and a method for that class that takes no arguments. The class is automatically instantiated by Fhat and the specified no-argument method is invoked. For example, if Fhat is pointed to the following Test class and main method, then the main method creates a Human, changes its name, then exits. When main exits, Fhat halts."}
{"pdf_id": "0704.3395", "content": "This section describes how a developer would typically use the Neno/Fhat environment. The terminal commands below ensure that the NenoFhat compiler translates Neno source code to a Fhat OWL API, loads the Fhat OWL API into the triple-store, instantiates a Fhat RVM, and points the RVM to the demo:Test class with a main method. Note that the third command is broken into four lines for display purposes. Do not assume that there is a newline character at the end of the first three lines of the third statement."}
{"pdf_id": "0704.3395", "content": "The programLocation is a pointer to the current instruction being executed by Fhat. Fhat executes one instruction at a time and thus, the programLocation must always point to a single instruction. The \"while\" loop of Fhat simply moves the programLocation from one instruction to the next. At each instruction, Fhat interprets what the instruction is (by its rdf:type \"opcode\") and uses its various components appropriately. When there are no more instructions (i.e. when there no longer exists a programLocation property), Fhat halts."}
{"pdf_id": "0704.3395", "content": "Fhat is a frame-based processor. This means that each invoked method is provided a Frame, or local environment, for its variables (i.e. FrameVariables). Due to how variables are scoped in object-oriented languages and because Neno does not support global variables,each method can only communicate with one another through parameter (i.e. method ar guments) passing, return value passing, or object manipulations. When method A callsmethod B, the parameters passed by method A are stored in method B's Frame accord ing to the variable names in the method description. For example, assume the following method,"}
{"pdf_id": "0704.3395", "content": "A Fhat RVM and the triple-code that it is executing are in the same address space and thus, can reference one another. It is the UUID address space of Neno/Fhat that makes it a unique programming environment in that Neno is not only a completely renective language, but also that it removes the representational stack found in most other programming environments."}
{"pdf_id": "0704.3395", "content": "Language renection means that the program can modify itself during its execution. Many scripting languages and even Java (through the java.lang.reflect package) support language renection. However, not only does Neno/Fhat support language renection, it also supports machine renection. A Fhat can modify itself during its execution. There are no true boundaries between the various components of the computation. This idea is represented in Figure 9, where a Fhat RVM has its program counter (programLocation) pointing to a Push instruction. The Push instruction is instructing Fhat to push a reference to itself on its operand stack. With a reference to the Fhat instance in the Fhat operand stack, Fhat can manipulate its own components. Thus, the Fhat RVM is executing triple-code that is manipulating itself."}
{"pdf_id": "0704.3395", "content": "In order for Neno software to run on a Fhat machine instance, it must be compiled to a Fhat OWL API that is compliant with the Fhat instruction set (the Fhat OWL API owl:imports the Fhat instruction set ontology). A Fhat RVM uses the Fhat OWL API as a \"blueprint\" for constructing the instance-level representation of the RDF triple-code. It is the instance-level triple-code that the Fhat RVM \"walks\" when a program is executing."}
{"pdf_id": "0704.3395", "content": "In Neno, the only process code that exists is that which is in a Method Block. Figure 10 defines the OWL ontology of a Method. A Method has an ArgumentDescriptor that is of rdfs:subClassOf rdf:Seq and a return descriptor that is of type rdfs:Resource. The sequence of the ArgumentDescriptor Argument denotes the placement of the Method parameter in the method declaration. For instance,"}
{"pdf_id": "0704.3395", "content": "The hasHumanCode property can be used, if desired, to point to the original human readable/writeable source code that describes that class and its methods. By using the hasHumanCode property, it is possible for \"in-network\" or run-time compiling of source code. In principle, a Neno compiler can be written in Neno and be executed by a Fhat RVM. The Neno compiler can compile the representation that results from resolving the URI that is the value of the xsd:anyURI."}
{"pdf_id": "0704.3395", "content": "A Method has a single Block. A Block is an rdfs:subClassOf Instruction and is composed of a sequence of Instructions. The Instruction sequence is denoted by the nextInst property. The Instruction rdf:type is the \"opcode\" of the Instruction. The set of all Instructions is the instruction set of the Fhat architecture. Figure 11 provides a collection of the super class Instructions that can exist in a Block of code and their relationship to one another. Examples of these super classes are itemized below.13"}
{"pdf_id": "0704.3395", "content": "• Variable: LocalVariable, FieldVariable, ObjectVariable. When a Fhat instance enters a Method it creates a new Frame. When a Variable is declared, that Variable is specified in the Frame and according to the current Block of the Fhat instance as denoted by Fhat's blockTop property. A Block is used for variable scoping. When Fhat leaves a Block, it destroys all the FrameVariables in the current Frame that have that Block as their fromBlock property (refer to Figure 5). However, entering a new Block is not exiting the old Block. Parent Block FrameVariables can be accessed by child Blocks. For instance, in the following Neno code fragment,"}
{"pdf_id": "0704.3395", "content": "When this code is compiled, it compiles to a Fhat OWL API. When an instance of demo:Human is created, the Fhat RVM will start its journey at the URI demo:Human and move through the ontology creating instance UUID URIs for all the components of the demo:Human class. This includes, amongst its hard-coded properties, its Methods, their Blocks, and their Instructions. When the demo:Human class is instantiated, an instance will appear in the RDF network as diagrammed in Figure 14."}
{"pdf_id": "0704.3453", "content": "Consider the example of the HIV protease, a protein  produced by the human immunodeficiency virus. The  target identification stage involves the discovery of this  HIV protease and the identification of this protein as a  disease causing agent. The objective of drug design is to  design a molecule that will bind to and inhibit the drug  target. A great deal of time and money can be saved if the  effect of molecules can be determined before these  molecules are actually synthesised in a laboratory.  Bioinformatics tools are used to predict the structures and  hence the functions of the molecules under design and to  determine if they will have any effect on the drug target."}
{"pdf_id": "0704.3453", "content": "body. Many classification systems have been developed  over the years based on machine learning to classify  sequences as belonging to one of the GPCR families, and  have shown great success in this task. These classification  systems  produce  static  classifiers  which  cannot  accommodate any new sequences that may be discovered."}
{"pdf_id": "0704.3453", "content": "CNS diseases [7]. This obvious importance of the GPCRs  is the reason they are used in this research.  The key features of the GPCRs are that they share no  overall sequence homology and have only one structural  feature in common [5]. The GPCR superfamily consists  of five major families and several putative families, of  which each family is further divided into level I and then  into level II subfamilies. The extreme divergence among  GPCR sequences is the primary reason for the difficulty  of classifying these sequences [1], and another important  reason as to why they are used in this research."}
{"pdf_id": "0704.3453", "content": "In this research eight GPCR families are considered from  the number of families available in the GPCRDB. The  GPCR sequences are stored in the EMBL format, which  consists of a number of labelled fields considering  aspects of a sequence such as identifiers in a number of  databases, the date of discovery and relevant publications  dealing with the protein sequence. The database itself is  updated every three to four months."}
{"pdf_id": "0704.3453", "content": "We can use this as an indication  that the data used is sufficiently representative of the  protein data in general and that results from experiments  that are conducted can be used to show that the  algorithms are not highly dependant on sequence lengths  for classification"}
{"pdf_id": "0704.3453", "content": "The GA selects the 4 best classifiers that minimises the  cost function of equation 5. The Genetic Algorithm was  designed to produce 50 generations of solutions with each  generation being a population 30 possible solutions. The  crossover rate was set to a high value of 0.8 and a  mutation rate of 0.4, and was empirically determined to  be the best values for the experiment. The crossover  functions are modified from the standard crossover  functions in this case, to ensure that unique classifiers are  selected during each generation, that is, preventing the  same classifier from being selected twice in a particular  generation."}
{"pdf_id": "0704.3453", "content": "These selected classifiers are then used in parallel, with  each of the five classifiers in the system producing an  independent set of predictions. These predictions must  then be fused together to form the final decision. A  number of decision fusion techniques exist. Some of  these include the majority and weighted majority voting,  trained combiner fusion, median, min and max combiner  rules [38]. We adopt the majority voting decision fusion  scheme, which simply considers each of the predictions  produced by the five classifiers as a vote, with the final  prediction for any given pattern given by the prediction  that receives the largest number of votes.  9.1. Incremental Learning of Protein Data"}
{"pdf_id": "0704.3453", "content": "1. It is possible to add new sequence information for  families which the classifier has already been trained  with.  2. Data of completely new classes can be added to the  system, increasing the knowledge that the system has  of the general protein domain.  The base system will in general be trained with data of a  number of classes. Once new data becomes available,  incremental learning of the system is based on  incrementally training each of the 5 FAM classifiers in  the system with the new data. The system can now be  tested with data from all classes it has been trained with,  including classes which have been incrementally added to  the system."}
{"pdf_id": "0704.3453", "content": "We compare the Fuzzy ARTMAP with other more  common machine learning tools such as the Support  Vector (SVM) Machines and Multi-layer perceptron  (MLP). These have been chosen since they have found  widespread use in the literature [1, 3, 19]. Table 3 shows  the performance of the classifiers that were considered in  the experiment. The parameters that are used for each of  the classifiers is included in the table. The classifiers are  trained with all the training data combined into a single  training set and tested on the test set"}
{"pdf_id": "0704.3453", "content": "error is the error of the system on the validation data set.  The GA for this data set selected classifiers 2,3, 4,  and 12 to form the final ensemble system. Again, the  system consisting of the elite classifier and the four  classifiers selected by the GA are incrementally trained  using databases"}
{"pdf_id": "0704.3453", "content": "and the Support Vector Machines. While these systems  have allowed a wider set of evolutionary mechanisms  involving proteins to be included in the design of  classification systems, such as invariance to the order of  amino acid motifs in a sequence, they remain static  structures which cannot incorporate newly discovered  proteins into their models."}
{"pdf_id": "0704.3453", "content": "With this in mind, Incremental Learning was proposed as  a machine learning approach to the classification of  proteins. The system presented is based on an  evolutionary strategy and the fuzzy ARTMAP classifier.  The results presented indicate that the fuzzy ARTMAP is  a suitable machine learning tool for the classification of  protein sequences into structural families, which is  comparable to many of the more established tools. An  analysis of the sequences also showed that the system is  able to classify proteins of varying lengths, and thus the  length of the protein sequences used is not important."}
{"pdf_id": "0704.3515", "content": "Abstract. Noise, corruptions and variations in face images can seriously hurt theperformance of face recognition systems. To make such systems robust, multiclass neural network classifiers capable of learning from noisy data have been suggested. However on large face data sets such systems cannot provide the robustness at a high level. In this paper we explore a pairwise neural-network system as an alternative approach to improving the robustness of face recognition. In our experiments this approach is shown to outperform the multiclass neural-network system in terms of the predictive accuracy on the face images corrupted by noise."}
{"pdf_id": "0704.3515", "content": "From this plot we can observe that the noise components corrupt the boundary of the given classes, and therefore the performance of a face recognition system can be affected. From these plots we can also observe that the boundaries between pairs of the classes can remain almost the same. This inspire us to exploit such a classification scheme to implement a pairwise neural-network system for face recognition."}
{"pdf_id": "0704.3515", "content": "The goal of our experiments is to compare the robustness of the proposed pairwise and standard multiclass neural-network systems on the Cambridge ORL face image data set [5] (in a full paper, the experiments will run on different face image data sets). To estimate the robustness we add noise components to the data and then estimate the performance on the test data within 5 fold cross-validation. The performances of the pairwise and multiclass systems are listed in Table 1 and shown in Fig. 4."}
{"pdf_id": "0704.3515", "content": "1. S.Y. Kung, M.W. Mak and S.H. Lin. Biometric Authentication: A Machine Learning Approach. Pearson Education, 2005 2. C. Liu and H. Wechler. Robust coding scheme for indexing and retrieval from large face database. IEEE Trans Image Processing, 9(1), 132-137, 2000 3. A.S. Tolba, A.H. El-Baz and A.A. El-Harby. Face Recognition: A Literature Review. IJSP. 2(2), 88-103, 2005 4. T. Hastie and R. Tibshirani. Classification by pairwise coupling. Advances in NIPS, 10, 507-513, 1998 5. E.S. Samaria. Face recognition using hidden Markov models. PhD thesis. University of Cambridge, 1994"}
{"pdf_id": "0704.3647", "content": "We found that  curation of personal digital materials in online stores bears some  striking similarities to the curation of similar materials stored  locally in that study participants continue to archive personal  assets by relying on a combination of benign neglect, sporadic  backups, and unsystematic file replication"}
{"pdf_id": "0704.3647", "content": "However, we have also  identified issues specific to Internet-based material: how risk is  spread by distributing the files among multiple servers and  services; the circular reasoning participants use when they discuss  the safety of their digital assets; and the types of online material  that are particularly vulnerable to loss"}
{"pdf_id": "0704.3647", "content": "ine the broader problems of the ad hoc IT practices characteristic  of home and small business users. We will also examine the ways  in which respondents lost their web-based digital belongings, how they discovered the loss, and whether this loss (and potential re covery) has changed their behavior at all. Finally, we reflect on  what these findings imply for personal digital archiving."}
{"pdf_id": "0704.3647", "content": "Study Description This study combines two different data sources: a self administered online survey that was offered to people who were  attempting to recover web-based assets using Warrick from the  Internet Archive's Wayback Machine or search engine caches and  follow-up in-depth interviews of survey-takers who were willing to submit to more extensive questioning"}
{"pdf_id": "0704.3647", "content": "The survey had 52 respondents, 34 of which were trying to  recover a website that they had personally created, maintained, or owned, and 18 of which were trying to recover a website for some one else, a friend, relative, client, or in a few cases, for themselves  to use as a resource; these responses were sufficiently complete to form a reliable picture of what happened"}
{"pdf_id": "0704.3647", "content": "The survey covered four basic areas: (1) a characterization of  the website itself; (2) questions pertaining to the development and  curation of the website, including where it was hosted and how it  was backed up; (3) questions probing particular aspects of the loss  and how it was discovered; and (4) questions about the restoration  and how it did or did not influence the curation practices of the  respondent"}
{"pdf_id": "0704.3647", "content": "To ground and focus the interviews, we asked preliminary  questions that enabled us to look at the restored website whenever  possible and center our questions around it; in one case, this was not possible, since the formerly public website was being recov ered as a personal resource and was not destined for republication"}
{"pdf_id": "0704.3647", "content": "tween website-specific curation practices and practices that pertain  to digital belongings in general.  We also looked back on the data collected for a past study,  described in [1], to extend the reach of the limited set of interviews conducted for this study. We isolated the portions of those 12 in terviews that pertained to online material and used this data to triangulate the data gathered during our current study and to confirm or question the findings. Hence we had 19 sources of inter view data as a window into general practices for curating online  personal information."}
{"pdf_id": "0704.3647", "content": "Respondents' Websites and Their Value What kind of websites did survey respondents and interview ees think were sufficiently valuable to restore from caches and  public archives? What made these websites valuable? The websites described in the survey and discussed in the in terviews spanned a spectrum of uses, from topical resources such  as a Frank Sinatra fan site to web-based magazines to personal  websites that respondents had created earlier in their lives (some quite extensive) to commercially important websites that adver tised, provided information, and supported e-commerce for small businesses"}
{"pdf_id": "0704.3647", "content": "Table 1 shows the breakdown of website genres, cate gorized by whether they were predominantly personal websites,  had commercial value, were topical resources, were fan sites, were  computer games, were publications, or were principally social  venues; of course, this categorization is rough, and some of the  websites spanned multiple genres"}
{"pdf_id": "0704.3647", "content": "It should be no surprise that a significant proportion of the re covered websites had commercial value; what is more puzzling is  why a commercially valuable website was lost to begin with.  Three of the interviewees described commercial websites they  were recovering; in all three cases, the web sites were not the main  revenue source of the businesses they represented, yet they played  a fundamental role. One supported the activities of a sports league  (where the sports league itself was a revenue source for its two  coordinators):"}
{"pdf_id": "0704.3647", "content": "\"That's a big part of what we do. Just sort of enabling our  players and our members to communicate with each  other, be kept up-to-date in terms of what's going on with  the league and games and stuff. So, I mean, the website is  really a vital component of what we do.\""}
{"pdf_id": "0704.3647", "content": "In each case, a different aspect of the website was considered  valuable (besides the basic contact information); for the painter, it  was the photos of his recently completed jobs; for the law firm, it  was the extensive textual content, especially the transcription of a long speech; and for the sports league coordinator, it was the func tionality and social nexus provided by the website"}
{"pdf_id": "0704.3647", "content": "namically. Additional functionality varied too. Blogs were part of 21% of the lost websites, and forums were present in 31%. Interestingly, when asked specifically about this sort of facility, recovering personal blogs was considered important; other social con tent was adjudged to be ephemeral, especially given the difficulty  of fully recovering it. This distinction between important and ephemeral content of ten hinges its role. A respondent who recovered both his personal  website and a commercial site, both with extensive blogs, said:"}
{"pdf_id": "0704.3647", "content": "It is impossible to predict whether a website is important by  looking at the type or quantity of content or even by knowing its original purpose. Participants had a variety of reasons for recovering these websites including their emotional importance, the diffi culty (or impossibility) of recreating the content, the time and cost  involved in the original effort, the value of the information as a resource, an interest in reviving a community, and sometimes sim ply curiosity."}
{"pdf_id": "0704.3647", "content": "De facto Archiving Strategies  Consumer strategies for keeping online digital material safe  and archived for long-term access reflect a blend of opportunism,  optimism, and benign neglect. We noticed three basic trends that  arise from the characteristics of the current online environment and  extend the way local digital belongings are handled:"}
{"pdf_id": "0704.3647", "content": "•  Materials are often opportunistically distributed over a variety  of servers and services;  •  Consumers employ circular reasoning about data safety; and  •  Strategies based on benign neglect fail to take into account the  server-side authoring capabilities offered by many current web  hosting, blogging, and media sharing services."}
{"pdf_id": "0704.3647", "content": "Distributing the files and spreading the risk  First, consumers have learned to spread their risk and take advantage of the different free and low-cost storage services avail able on the Internet. Thus they might store photos on Flickr and  videos on YouTube, create a blog on Blogger, publish a website on their ISP's server, and so on. Whether consciously or unconsciously, they realize that this mediates the risk of \"losing every thing\" and provides them with functionality appropriate to the media type and their purposes. For example, an art student (specializing in animation) who has already lost several different por tions of his personal webpage describes his strategy this way:"}
{"pdf_id": "0704.3647", "content": "\"I keep backup lists because my site, blog, and podcast is  currently on the free (for students here) website space our  school generously provides. The problem is, I can't  vouch for its permanence and so I set up backup lists for  my peace of mind.\""}
{"pdf_id": "0704.3647", "content": "Because each service has slightly varying capabilities, the copies  are not necessarily equivalent. Some, as he notes, are better than  others: one of his blog sites he has chosen because it allows him to  have an easy-to-remember name; another he has chosen because he can partition the posts by subject. Remembering just where everything is and keeping all the mirrors up-to-date imposes a discern able tax on this strategy. It was not unusual during the interviews  for a participant to suddenly recall a forgotten online store midway  through our conversation: \"I've posted some photos to, like, um,  [pause] gosh I'm drawing a blank—oh! Pbase.\""}
{"pdf_id": "0704.3647", "content": "Circularity of reasoning: what protects what? Second, in part owing to this distribution of materials, re spondents exhibit a pervasive circularity of reasoning about the  safety of the files, databases, and code they rely on. First they  might assert that even if the service or their account disappeared,  they would still have the copy that they originally uploaded; then,  in almost the same breath, they rationalize their home curatorial  practices by saying that they would simply download the files  from the web service they are using (never mind that they have  reduced resolution or otherwise culled material to post it online).  For example, one respondent told us he did not worry unduly  about his valuable photos:"}
{"pdf_id": "0704.3647", "content": "\"The good thing about the photos is that there's always an  intermediary step. I mean like the photos go off of my  camera onto my computer before they go up to Flickr. So  I always have master copies on my PC. So that's why I  don't care so much about Flickr evaporating.\""}
{"pdf_id": "0704.3647", "content": "But these websites represent material that is crawled and  cached by a number of different public stores. What of other types  of web-based personal material such as email? Even if they are  distinctly valuable, respondents seem to give little thought to their  long-term safety. One participant said:"}
{"pdf_id": "0704.3647", "content": "After some thought, he realized that because he used POP, he had a second copy of these important files, but there was scant evi dence that he felt he should expend any extra effort to ensure that  these files were archived. In fact, he described this way of thinking  as \"quaint.\""}
{"pdf_id": "0704.3647", "content": "neglect of distributed and augmented materials that we described in the previous section. Many individuals are unaware of the spe cific IT practices of their ISPs (for example, how regularly their  files are backed up or whether they are backed up at all); nor do  they keep careful track of the status of their various accounts or the ISPs policies regarding account dormancy. In fact, the survey responses indicate that the respondents regard their websites as ar chival or permanent, and the service providers do not."}
{"pdf_id": "0704.3647", "content": "•  There is a mismatch between an owner's expectation of asset  value and their ISP's notification policies and procedures;  • There is often a greater temporal gap between the site's disap pearance, detection of the loss, and recovery of the material  than we would expect; and  •  There is often a discrepancy between site owner's perception  of the permanence of online materials and the actual ad hoc  nature of many network services."}
{"pdf_id": "0704.3647", "content": "\"They did a lot of research and they had a lot of very spe cific drug fact information on there. And then they built it and had someone hosting it for them. And then that per son, they couldn't contact anymore. They wanted to make  changes, and then the website went down, and they couldn't find him anymore. So he just kind of disap peared.\""}
{"pdf_id": "0704.3647", "content": "Site owners in our survey usually noticed the site's disappearance in under a week (al most 65% did) and began to substantially restore it in under a  week (about 45%); but over 40% of non-owners waited more than  a year (sometimes significantly more than a year) after the site  disappeared to restore it"}
{"pdf_id": "0704.3647", "content": "In a few cases, this loss was a wake-up call that provoked  respondents to consider instituting some sort of backup procedure in the future (however at this writing, even 6 months after the sur vey, these good intentions have not been realized); but in other  cases, the respondents simply retrenched after recovering some or  all of their lost material"}
{"pdf_id": "0704.3653", "content": "Four central archiving themes emerged from the  data: (1) people find it difficult to evaluate the worth of  accumulated materials; (2) personal storage is highly distributed  both on- and offline; (3) people are experiencing magnified  curatorial problems associated with managing files in the  aggregate, creating appropriate metadata, and migrating  materials to maintainable formats; and (4) facilities for long-term  access are not supported by the current desktop metaphor"}
{"pdf_id": "0704.3653", "content": "Four  environmental factors further complicate archiving in consumer  settings: the pervasive influence of malware; consumer reliance on  ad hoc IT providers; an accretion of minor system and registry  inconsistencies;  and  strong  consumer  beliefs  about  the  incorruptibility of digital forms, the reliability of digital  technologies, and the social vulnerability of networked storage"}
{"pdf_id": "0704.3653", "content": "Of course, in our minds eyes, we have strategies for keeping  our personal digital belongings safe: we might promise ourselves  that we will track the development of new storage media,  refreshing what we have already stored as needed; or we might  intend to migrate our files to new formats as they become accepted  standards"}
{"pdf_id": "0704.3653", "content": "In the study we report in this paper, we examine three central  questions that will allow us to design a service for personal digital  archiving:  •  What kinds of digital belongings do people have and what do  they value?  •  How do people archive their digital belongings now?  •  What are the central archiving challenges stemming from  current practice, digital genres, and home technology  environments that will guide archiving service design?  We first briefly describe our study and then go on to discuss  our findings and their implications"}
{"pdf_id": "0704.3653", "content": "Study  We performed a field study to understand how consumers  acquire, keep, and access their digital belongings with a focus on  determining the extent of what they had kept, which of these  belongings they cared about the most over the long term, and what  obstacles they had encountered in maintaining them"}
{"pdf_id": "0704.3653", "content": "Our field  study consisted of three parts: an eight-interview pilot study to  identify potential data collection difficulties; the main portion of  the study, which included twelve in-depth interviews; and an  opportunistic collection of stories about saving or recovering  digital material that we gathered outside the primary interviews"}
{"pdf_id": "0704.3653", "content": "From their stories, we identified five basic strategies for archiving:  (1) using system backups as archives; (2) moving files wholesale  from older computers to newer computers (or to other household  computers); (3) replicating specific valuable files on removable  media such as CDs, DVDs, or floppy disks; (4) using email  attachments as ad hoc archival storage; and (5) retaining old  computers as a means of saving and accessing the files created on  them"}
{"pdf_id": "0704.3653", "content": "While we encountered a few instances where informants  said they would print a file to save it, none thought of  comprehensive hardcopy production as a viable way of keeping  their digital belongings safe; hardcopy was a stop-gap when the  threat was immediate or the item had already been lost"}
{"pdf_id": "0704.3653", "content": "What do these principles and the contradictory behaviors we  observed tell us? They speak volumes about value: it is difficult to  state, admit, or predict the value of individual files, but consumers  readily demonstrate value by what they do with a file, for example,  by writing it to a CD or sending it to a friend"}
{"pdf_id": "0704.3653", "content": "It is also apparent  that value is a nuanced concept that has many factors, including  the personal labor and creativity that a particular digital item  represents; how much emotional impact a given item has; and how  hard it will be to replace, either by finding it again, reconstituting  it from component parts, or by substituting something similar"}
{"pdf_id": "0704.3653", "content": "We  also see that sometimes it is easier to assess the value of digital  assets in aggregate than it is to cull individual components; so, for  example, it is easier to declare, \"my email is important\" than it is  to assess the value of each of 10,000 messages"}
{"pdf_id": "0704.3653", "content": "Taken together, the unimplemented strategies and belied  principles suggest that a service will need to be semi-automated  without appearing to save too much dross or too much that is  easily replaceable; that value will need to be interpreted through  action and by taking a variety of important factors into account;  and that an archiving service will need to be aligned with both  abstract principles and with realistic practice"}
{"pdf_id": "0704.3653", "content": "Second, consumers often rely on ad hoc IT  support from family, friends, and other members of their extended  social networks; they neither do their own IT nor call in a  professional; naturally, this ad hoc support is performed with  varying levels of understanding of the underlying problems"}
{"pdf_id": "0704.3653", "content": "Although we tend to assume a \"perfect world\" when we design  this sort of service, what we observed is that every one of our  informants experienced an overall aggregation of minor problems  on their computers, likely due to inconsistencies in the registry or  partially installed software"}
{"pdf_id": "0704.3653", "content": "Guided by our four challenges (accumulation, distribution,  curation, and long-term access) and our complicating environment  factors (malware, ad hoc IT support, platform inconsistencies, and  consumer sensitivities) we have identified four aspects of storage,  preservation, and access that must be addressed by a service  design"}
{"pdf_id": "0704.3653", "content": "Long term storage must be  designed with the idea that any centralized repository will contain  both full digital objects and metadata or indices that represent  digital objects held elsewhere (sometimes in long-term digital  libraries and institutional stores, and sometimes in shorter-term  backends such as free email accounts, personal web sites, and  media-sharing venues)"}
{"pdf_id": "0704.3653", "content": "The architecture must also be layered to  handle local storage (as it is currently distributed among local  computers and devices), intermediate storage (as it is currently  distributed among servers and media centers, both local and  remote), and a network-based backend (which ultimately tracks  distributed sources and is the final repository for unique content)"}
{"pdf_id": "0704.3653", "content": "It is more practical to store digital  objects with an eye toward how they will be used later,  maintaining a canonical form wherever possible [12]; some uses  such as editing or custom interaction might demand emulation  [13], while others will simply require that the digital asset be  viewable or playable with reasonable (but possibly not complete)  fidelity"}
{"pdf_id": "0704.3886", "content": "Young are considered to be second-intension logical concepts, namely  properties that may or may not be true of first-intension (ontological)  concepts3. Moreover, and unlike first-intension ontological concepts (such as  human), logical concepts such as Artist and Young are assumed to be defined  by virtue of logical expressions,"}
{"pdf_id": "0704.3905", "content": "3. ENSEMBLE LEARNING FOR FREEAfter the above discussion, Evolutionary Ensemble Learn ing (EEL) involves two critical issues: i) how to enforce both the predictive accuracy and the diversity of the classifiers inthe population, and across generations; ii) how to best se lect the ensemble classifiers, from either the final population"}
{"pdf_id": "0704.3905", "content": "4.1 Datasets Experiments are conducted on the six UCI datasets [19] presented in Table 1. The performance of each algorithm is measured after a standard stratified 10-fold cross-validation procedure. The dataset is partitioned into 10 folds with same class distribution. Iteratively, all folds but the i-th one are used to train a classifier, and the error rate of thisclassifier on the remaining i-th fold is recorded. The per formance of the algorithm is averaged over 10 runs for each fold, and over the 10 folds."}
{"pdf_id": "0704.3905", "content": "Table 3: Results on the UCI datasets based on 10-folds cross-validation, using 10 independent runs over each fold. Values are averages (standard deviations) over the 100 runs. Statistical tests are p-values of paired t-tests on the test error rate compared to that of the best method on the dataset (in bold)."}
{"pdf_id": "0705.0197", "content": "In the data processing stage the measured vibration data need to be processed. This is  mainly due to the fact that the measured vibration data, which are in the time domain,  are difficult to use in raw form. Thus far the time-domain vibration data may be  transformed to the modal analysis, frequency domain analysis and time-frequency  domain [2,3]. In this paper the time-domain vibration data set is transformed into the  modal domain where it is represented as natural frequencies and mode shapes."}
{"pdf_id": "0705.0197", "content": "shells [2,3,4]. The importance of fault identification process in a population of  nominally identical structures is particularly important in areas such as the automated  manufacturing process in the assembly line. Thus far various forms of neural networks  such as MLP and Bayesian neural networks have been successfully used to classify  faults in structures [8]. Worden and Lane [9] used SVMs to identify damage in  structures. However, SVMs have not been used for fault classification in a population of  cylinders. Based on the successes of SVMs observed in other areas, we therefore  propose in this paper SVMs and GMMs for classifying faults in a population of  nominally identical cylindrical shells."}
{"pdf_id": "0705.0197", "content": "2. NEURAL NETWORKS  Neural networks are parameterised graphs that make probabilistic assumptions about data  and in this paper these data are modal domain data and their respective classes of faults.  In this paper multi-layer perceptron neural networks are trained to give a relationship  between modal domain data and the fault classes."}
{"pdf_id": "0705.0197", "content": "the EM algorithm is used since it has reasonable fast computational time when  compared to other algorithms. The EM algorithm finds the optimum model parameters  by iteratively refining GMM parameters to increase the likelihood of the estimated  model for the given fault feature modal vector. For the EM equations for training a  GMM, the reader is referred to [19]. Fault detection or diagnosis using this classifier is  then achieved by computing the likelihood of the unknown modal data of the different  fault models. This likelihood is given by [18]"}
{"pdf_id": "0705.0197", "content": "5.1 Principal Component Analysis  In this paper we use the principal component analysis (PCA) [20;21] to reduce the input  data into independent input data. The PCA orthogonalizes the components of the input  vector so that they are uncorrelated with each other. In the PCA, correlations and  interactions among variables in the data are summarised in terms of a small number of  underlying factors."}
{"pdf_id": "0705.0197", "content": "6. FOUNDATIONS OF DYNAMICS  As indicated earlier, in this paper modal properties i.e. natural frequencies and mode  shapes are extracted from the measured vibration data and used for fault classification.  For this reason the foundation of these parameters are described in this section. All  elastic structures may be described the time domain as [22]"}
{"pdf_id": "0705.0214", "content": "The generalization of the methods used for scalar- and vector-valued data to tensor-valued data is being pursued with mainly three formalisms: the use of geometric invariants of tensors like eigenvalues, determinant, trace; the generalization of Di Zenzo's concept of a structure tensor for vector-valued images to tensor-valued data; and recently, differential-geometric methods"}
{"pdf_id": "0705.0214", "content": "Riemannian geometry of the space of symmetric positive-definite (SPD) matrices. The remainder of this paper is organized as follows. In Section 2 we give a compilationof results that gives the differential geometry of the Riemannian manifold of symmet ric positive-definite matrices. In Section 3 we fix notation and recall some facts about immersions between Riemannian manifolds and their mean curvature. We explain inSection 4 how to describe a DT-MR image by differential-geometric concepts. Sec tion 5 is the key of our paper in which we extend several mean curvature-based nows for the denoising and segmentation from the scalar and vector setting to the tensor one. In Section 6 we present some numerical results."}
{"pdf_id": "0705.0214", "content": "The role of c is to reduce the magnitude of smoothing near edges. In the scalar case, this equation does not have the same action as the Perona-Malik equation of enhancing edges. Indeed, Perona-Malik equation has variable diffusivity function and has been shown to selectively produce a \"negative diffusion\" which can increase the contrast of edges. Equation of he form (17) have always positive or forward diffusion, and the term c merely reduces the magnitude of that smoothing. To correct this situation, Sapiro have proposed the self-snakes formalism [20], which we present in the next subsection and generalize to the matrix-valued data setting."}
{"pdf_id": "0705.0588", "content": "created (by merging or splitting) and others disappear (bymerging, or by other reasons). Together these points con stitute the evolving model P, where points correspond with frequent itemsets. We will first explain how we use the stream of records to update the supports of the elements of P, we then presentan outline of the algorithm; next we describe how the co ordinates of the elements change in accordance with the corresponding supports, and finally mention our method of growing and shrinking the number of sets present in P: the merge and split part of the algorithm."}
{"pdf_id": "0705.0588", "content": "3.3 Distance We now describe how the coordinates of the points change as their supports vary when the new records from the streamcome in. In our model for distance (p1, p2) we take the Eu clidean distance between the 2-dimensional coordinates of the points corresponding with the two patterns p1 and p2. These points are pulled closer to one another if they occur in the current transaction and they are pushed apart if not.Furthermore nothing is done if both do not occur. In ev ery time step a random selection of the pairs undergoes this process. To pull two points together we set the goal distance to 0 and to push them apart the goal distance is"}
{"pdf_id": "0705.0588", "content": "Next we split patterns, when they contain more than one item, if they do not occur often enough and they have been in the model for at least a certain number of records (they are\"old enough\"). Split combinations are generated by remov ing each item from the original pattern once. The remaining items form one new itemset, so in this way a size k itemset will result in k combinations after splitting."}
{"pdf_id": "0705.0588", "content": "Finally, the newly formed patterns in Q are united with those in P. Of course, when patterns occur more than one time, only one copy — the oldest one — is maintained. And those patterns from P that are contained in a larger one in P are removed, unless — as stated above — they have size 1: we focus on the maximal patterns."}
{"pdf_id": "0705.0588", "content": "Figure 4 displays the cluster model (only patterns with age at least 50 are shown) after seeing 20,000 transactions produced by repeating the real dataset. Some patterns, i.e., itemsets, are clearly placed far apart from each other orclose together. Table 1 displays some examples on the co occurrences of patterns. The first thing to notice is that all"}
{"pdf_id": "0705.0588", "content": "the patterns occur often and so they should be in the clus ter model. Secondly the first and the second itemset occur often together, so we expect them to be close together in the model. Finally the last itemset does not occur less often with the other two, we expect them to be placed further apart. Figure 4 displays all these facts in one picture."}
{"pdf_id": "0705.0588", "content": "This distance is used to merge patterns together if it is smaller than a user-defined threshold, because we want only maximal frequent itemsets (itemsets that are often a subset of a transaction but they are never a subset of a bigger frequent itemsets) such that the model does not grow too big"}
{"pdf_id": "0705.0593", "content": "The information we need to store concerning the occurrence of subgraph patternscan be huge. However, in some cases the user might want to have this informa tion, e.g., in our working example the scientist might want to closer investigate molecules (transactions) contain a specific pattern.Interesting information for any user is to see how often the groups (clus ters) of subgraphs occur in the same transactions (graphs) within the dataset."}
{"pdf_id": "0705.0593", "content": "Our final experiment was done to show how the runtime is innuenced by the maxdist threshold and how much the preprocessing step innuences runtime. Here we assume the distances between clusters can be stored in memory. In Figure 6 the innuence on runtime is shown. The time for preprocessing appears to be more or less stable, but the total runtime drops significantly."}
{"pdf_id": "0705.0593", "content": "The model can be built faster with the clustering algorithm because of thegrouping of the subgraphs, the preprocessing step. The groups also remove redundant points from the visualization that represents very similar subgraph pat terns. Finally the model enables the user to quickly select the right subgraphs for which the user wants to investigate the graphs (or molecules) in which the frequent subgraphs occur. In the future we want to take a closer look at grouping where the types of vertices and edges and their corresponding weight also decide their group.Furthermore, we want to investigate how we can compress occurrence more ef ficiently and access it faster."}
{"pdf_id": "0705.0693", "content": "The agent must be  able to learn not only about the inherent nature of the game it  is playing, but also must be capable of learning trends  emerging from its opponent's behaviour, since bluffing is  only plausible when one can anticipate the opponent's  reactions to one's own actions"}
{"pdf_id": "0705.0693", "content": "As with any optimisation system, very careful consideration  needs to be taken with regards to how the system is  structured, since the implications of these decisions can often  result in unintentional assumptions made by the system  created. With this in mind, the Lerpa Multi-Agent System  (MAS) has been designed to allow the maximum amount of  freedom to the system, and the agents within, while also  allowing for generalisation and swift convergence in order to  allow the intelligent agents to interact unimpeded by human  assumptions, intended or otherwise."}
{"pdf_id": "0705.0693", "content": "Each hand played will be viewed as an independent,  stochastic event, and as such only information about the  current hand will be available to the agent, who will have to  draw on its own learned knowledge base to draw deductions  not from previous hands"}
{"pdf_id": "0705.0693", "content": "With each agent implemented as described above, and  interacting with each other as specified in Section III, we can  now perform the desired task, namely that of utilising a  multi-agent model to analyse the given game, and develop  strategies that may \"solve\" the game given differing  circumstances. Only once agents know how to play a certain  hand can they then begin to outplay, and potentially bluff  each other."}
{"pdf_id": "0705.0693", "content": "Fig. 5. Agent performance, averaged over 40 hands  nature of cards being dealt. As can be seen, AIden is  consistently performing better than its counterparts, and  continues to learn the game as it plays.  1) Cowardice: In the learning phase of the abovementioned  intelligent agent, an interesting and somewhat enlightening  problem arises. When initially learning, the agent does not in  fact continue to learn. Instead, the agent quickly determines  that it is losing chips, and decides that it is better off not  playing, and keeping its chips! This is illustrated in Fig. 6."}
{"pdf_id": "0705.0693", "content": "C. MAS Learning Patterns  With all of the agents learning in the same manner, it is  noteworthy that the overall rewards they obtain are far better  than those obtained by the random agents, and even by the  intelligent agent that was playing against the random agents  [3]. A sample of these results is depicted in Fig. 8.  R1 to R3 are the Random agents, while AI1 is the intelligent  agent playing against the random agents. AI2 to AI 5 depict  intelligent agents playing against each other. As can be seen,  the agents learn far better when playing against intelligent  opponents, an attribute that is in fact mirrored in human  competitive learning."}
{"pdf_id": "0705.0693", "content": "F. Bluffing  A bluff is an action, usually in the context of a card game that  misrepresents one's cards with the intent of causing one's  opponents to drop theirs. There are two opposing schools of  thought regarding bluffing. One school claims that bluffing is  purely psychological, while the other maintains that a bluff is  a purely statistical act, and therefore no less sensible than any  other strategy. Astoundingly enough, the intelligent agents do  in fact learn to bluff! A classic example is illustrated in  Fig. 11, which depicts a hand in which bluffing was  evidenced"}
{"pdf_id": "0705.0734", "content": "In the recent years there has been a growing interest in soft constraint satisfac tion. Various extensions of the classical constraint satisfaction problems (CSPs)[10, 9] have been introduced in the literature, e.g., Fuzzy CSP [11, 5, 12], Prob abilistic CSP [6], Weighted CSP [15, 7], Possibilistic CSP [13], and Valued CSP [14]. Roughly speaking, these extensions are just like classical CSPs except that each assignment of values to variables in the constraints is associated to an element taken from a semiring. Furthermore, nearly all of these extensions, as well as classical CSPs, can be cast by the semiring-based constraint solving framework, called SCSP (for Semiring CSP), proposed by Bistarelli, Montanari and Rossi [3]."}
{"pdf_id": "0705.0751", "content": "An approximate textual retrieval algorithm for searching sources with high levels of defects is presented. It considers splitting the words in a query into two overlapping segments and subsequently building composite regular expressions from interlacing subsets of the segments. This procedure reduces the probability of missed occurrences due to source defects, yet diminishes the retrieval of irrelevant, non-contextual occurrences."}
{"pdf_id": "0705.0751", "content": "• ...or -icus. Thus the name of 'Bupleurum chinense' is incorrect and the correct name is \"Bupleurum chinensis\" as shown in Table 1. There are also... • ...II Grammatical error 86.6 Bupleurum chinense Bupleurum chinensis Collection II Grammatical error 89.5... • ...alba Collection II 29 36 Bupleurum chinense Collection II 23 28 Cinnamomum... • ...sachalinense Phellodendron chinense 84.2 Salvia przewalskii Sabina..."}
{"pdf_id": "0705.0751", "content": "• ...references about the relation of approximate string matching and information retrieval are Wag ner and Fisher [1974... • ...2000. Blockaddressing indices for approximate text retrieval. J. Am. Soc. Inf. Sci. (JASIS) 51... • ...SCHULMAN, E. 1997. Applications of approximate word matching in information retrieval. In Proceedings of the 6th ACM..."}
{"pdf_id": "0705.0751", "content": "from the references [1], [2], [3], and [4], respectively. Note that the three words in the query appear in two, and only two, component expressions. Therefore, if the segment Approximate textual retrieval had been in the texts, the occurrence would have certainly been retrieved, provided that the errors did not extend to more than one of the three words."}
{"pdf_id": "0705.0781", "content": "Abstract— This paper presents deformable templates as a  tool for segmentation and localization of biological structures  in medical images. Structures are represented by a prototype  template, combined with a parametric warp mapping used to  deform the original shape. The localization procedure is achieved using a multi-stage, multi-resolution algorithm de signed to reduce computational complexity and time. The  algorithm initially identifies regions in the image most likely to  contain the desired objects and then examines these regions at  progressively increasing resolutions. The final stage of the  algorithm involves warping the prototype template to match  the localized objects. The algorithm is presented along with  the results of four example applications using MRI, x-ray and  ultrasound images."}
{"pdf_id": "0705.0781", "content": "The deformable template model presented has been applied to different biological structures in a number of func tional medical images. The first test experiment presented involves the segmen tation of the Corpus Callosum in four different MRI images.  The prototype template used is the first Corpus Callosum  shape. This experiment is designed to illustrate the warp  capabilities of the algorithm, and the template image is  initialized at the center of the base images. Figure 2 shows  the initial and final base images. As can be seen, all four  Corpus Callosums are localized and segmented, even  though there is considerable shape variation between the  images."}
{"pdf_id": "0705.0781", "content": "Fig. 4. Cardiac MRI segmentation.  The final experiment involves the detection of Carpal  bones in x-ray images. This experiment shows how the  algorithm can be adapted to object tracking tasks. X-ray  images were taken of the hand and wrist moving in an ark. In each consecutive image, the final template from the pre vious image is used as the initial template for the current image. In this way, full localization is not required, result ing in speed and computational efficiency. Figure 5 shows  the x-ray images, clockwise in consecutive order."}
{"pdf_id": "0705.0781", "content": "achieve computation efficiency. The algorithm begins by  identifying regions of interest in the image, and proceeds to  search these regions at progressively finer resolutions. Once  an object is located, the template is deformed to fit it using a Particle Swarm optimized, LWM warp routine. Experimental results have been presented showing invariant localiza tion of objects in MRI, x-ray and ultrasound images."}
{"pdf_id": "0705.0828", "content": "used since the algorithm may constantly compare the re stored image with the real image. However, when dealing  with NM images this comparison is can not be made. The  easiest solution would be to provide NM physicians with a \"movie\" of the restoration process and allow the NM physician to view the iterated image of choice. A more mathe matical approach at achieving the correct stopping criteria is  suggested by using the noise and prior Hamiltonians as  enhancement indicators."}
{"pdf_id": "0705.0828", "content": "Phantom images were used extensively in the development of a MFA algorithm and MFA parameters. Experi mental empirical methods were used on numerous phantom  images such as Fig. 3 to determine optimal parameters.  It is evident from Fig. 3A & C that the MFA algorithm  with the correct parameters can reduce noise substantially  without damaging edge integrity. Fig. 3B shows a Wiener  filter restored image. Comparative noise reduction and edge"}
{"pdf_id": "0705.0828", "content": "classification is evident from Fig. 3E & F, which displays  the Sobel edges.  Looking carefully at Fig. 3A, B & C, it is noticeable that edges appear sharper in the original image and Wiener im age in certain regions compared to the MFA restored image. This implies that MFA has blurred the image slightly. How ever since MFA has extensively reduced the noise it is now  possible to apply filters to further enhance image edges  without out amplifying the noise. A standard sharpening  filter h is used and the result is visible in Fig. 3D."}
{"pdf_id": "0705.0828", "content": "To determine the PSF, point sources were placed at vari ous distances away from the collimator. A discrete Gaussian  distribution was then fitted to the acquired point source.  Vertical and horizontal line sources were imaged using  capillary tubes to verify the point sources' distributions and  to verify the radial symmetry of the blur. Fig. 5 shows how  the point source is convolved with a line and then compared"}
{"pdf_id": "0705.0828", "content": "to the acquired line source. Ignoring ends, the two lines  suffered only small differences with an RMSE of 5.5% that  may be attributed to the noise. The process is repeated with  the vertical line resulting in a RMSE of 4.8% which implies  approximate radial symmetry. Radial symmetry and the  fitted Gaussian PSF were verified at numerous distances.  Fig. 6 shows the Standard Deviation of the resulting fitted  PSFs, in which the PSFs display regional linearity. A linear  trend line may be fitted and used to predict approximate  PSFs at different distances from the collimator."}
{"pdf_id": "0705.0828", "content": "With current processing technology the computational  time required to run this image enhancing MFA algorithm is  no longer significant. Although not all the criteria for image  enhancement are present in NM images, enhancement of individual or multiple planes of interest is possible. Sharpening filters are utilized as a post-MFA enhancement tech nique and provide good results. We thus conlude that MFA  holds promise as a supplementary pre-filter diagnostic tool  for the enhancement of NM images."}
{"pdf_id": "0705.0828", "content": "The authors would like to thank Prof. Vangu of the Department of Nuclear Medicine at Wits University for providing the research facilities required in this study. In par ticular, the authors would like to thank Mr. Sibusiso Jozela  of the Medical Physics Department, for all his time spent  acquiring the experimental data, and Mr. Nico van der  Merwe, also of the Medical Physics department for all his  input. We look forward to working with these departments  to further develop this study."}
{"pdf_id": "0705.0952", "content": "Variations in face images due to viewpoint, illumination  and expression changes have been proven to be highly  complex and nonlinear in nature [5] and it has been observed  that variations between face images of the same person due to  illumination and pose are almost always greater than image  variations between the different persons [14]. From a  classification viewpoint linear approaches, which only  describe information based on second order statistics [15], are  therefore said to be suboptimal in terms of accurate data  representation. Complete pattern variation is said to be"}
{"pdf_id": "0705.0952", "content": "In classifier fusion, the outputs of individual classifiers  are combined by a second classifier according to a  pre-defined combination rule. Classifier combination can  essentially be implemented at three levels [19]: Fusion at the  a) Feature Extraction level b) Confidence or Matching score  level and c) Decision level.  The use of classifier fusion has produced many  combination techniques over the years. One popular approach  has been the idea of bagging [20], which manipulates the  training-data with sub-sampling. Another common algorithm,  boosting [21], also manipulates the training data, but with  emphasis on the training of samples that are difficult to  classify"}
{"pdf_id": "0705.0952", "content": "phase, where the comparative assessment will be based on the  combinatorial results of three successive tools. Firstly the  binomial cumulative probability of correct class assignment  will be presented in traditional tabular format. This will be  followed by the FERET testing protocol using Cumulative  Match Scores (CMS) curves also known as rank score [24]  and will offer intuitive insight into which algorithm  performance throughout the rank spectrum. Finally statistical  measures are also applied in the form of McNemar's  Hypothesis Protocol [25] that offers the practical insight  pertaining to what point does the difference in performance   results actually become significant.  ..."}
{"pdf_id": "0705.0952", "content": "level of fusion for this particular application. The similarity  measures from the relevant metric of each algorithm will be  taken as inputs to the combinational classifier. Normalisation  of both the differing metric measures are performed  employing the MinMax scheme, resulting in a common range  of [0 100].  In combining the different metric measures, the weighted  sum rule is selected as the fusion rule. Despite its simplicity,  the sum rule often outperforms other combination schemes  and because of its linear model it is proven to be more tolerant  to noise signals, unlike the product rule that severely  magnifies any noise contributions. The combined matching  score will be calculated as follows:"}
{"pdf_id": "0705.0952", "content": "If  LDA, for example, performs well in the categories of  Expression and Time Delay, it will obtain two fifths or 40%  of the weighting; similarly if FA1 outperformed the other  algorithms in both occlusion categories it will also receive  40% of the weighting; with the remaining 20% going to the  algorithm performing best in the category of illumination"}
{"pdf_id": "0705.0952", "content": "The inter-class assessments, rank-1 results, were carried  out in much the same fashion as the intra-class tests were, the  CMS curves were used as the primary tool for obtaining an  intuitive indication as to which class performed better and this  was confirmed, regionally clarified or nullified by the  findings of McNemar's evaluation"}
{"pdf_id": "0705.0952", "content": "An overview of the ICA class shows that in the categories of  expression, illumination and time delay, there is no  significant statistical difference between any of the  architectures and the choice of employing either the InfoMax  or FastICA implementations does not affect the overall  performance rankings"}
{"pdf_id": "0705.0952", "content": "In selecting the best  metric combination for ICA, the Cosine measure was without  a doubt the best distance measure in all categories  In performing the Inter-class assessments the results were  as follows:  1) Expression: LDA and ICA came out as the top classes,  but only being superior to PCA at rank-1; other than that there  was no statistical difference between any of the classes"}
{"pdf_id": "0705.0952", "content": "2) Illumination: LDA and ICA both claim statistical  superiority over PCA for the first 7 ranks; ICA however,  outperforms LDA for the first 3 ranks leading one to the  conclusion that ICA is the best class to apply for the task of  illumination changes"}
{"pdf_id": "0705.0952", "content": "In summing up the class results, while it is true that the  specific nature of the task may greatly influence the  performance level of any algorithm, on average one could  confidently recommend that the class of ICA is perhaps the  most flexible and widely adaptable subspace methodology"}
{"pdf_id": "0705.0952", "content": "In the class of ICA, in the categories of  Expression, Illumination and Time delay, it was observed that  there were no statistical differences between any of the  variants, however FA2 (Cosine) did seem intuitively better in  the category of Expression and FA1 (Cosine) did come out  very strong in the categories of Illumination and Time delay;  also in the occlusion categories FA1 was clearly the superior  algorithm"}
{"pdf_id": "0705.0952", "content": "perform better, but only by a tiny magnitude.  Comparing the Hybrid weighting approaches, although  very different, both methods performed very well, with  method 1 finding superior claim in the categories of  Expression and Illumination and method 2 being the better  performer in the Occlusion categories. Both performed  equally well in the category of Time Delay. Statistically there  is no significant difference between the results of either  approach.  Turning to McNemar's analyses, the categorical results were  as follows:  1) Expression: Statistically there is absolutely no  significant difference between the Hybrid results and any of"}
{"pdf_id": "0705.0952", "content": "Although the proposed approach only  explores one aspect of hybrid synthesis and the results are not  statistically superior to the best categorical constituent  algorithms, the framework has been made scalable so that  future investigations can easily incorporate and improve other  face recognition modules in the quest to realise a truly"}
{"pdf_id": "0705.0952", "content": "This research investigation presented a rather rare  comparative  study  of  three  of  the most  popular  appearance-based face recognition projection classes, PCA,  LDA and ICA along with the four most widely accepted  similarity measures of City Block (L1), Euclidean (L2),  Cosine and the Mahalanobis metrics"}
{"pdf_id": "0705.0952", "content": "Although comparisons  between these classes can become fairly complex given the  different task natures, the algorithm architectures and the  distance metrics that must be taken into account, an important  aspect of this study was the completely equal working  conditions that were provided in order to facilitate fair and  proper comparative levels of evaluation"}
{"pdf_id": "0705.0952", "content": "This work significantly contributes to prior literary  findings, either by verifying previous results, offering further  insight into why certain conclusions were made or by  providing a better understanding as to why certain claims  should be disputed and under which conditions they may hold  true"}
{"pdf_id": "0705.0952", "content": "By firstly exploring previous literature with respect to  each other and secondly by relating the important findings of  this paper to previous works one is able to meet the primary  objective in providing an amateur, in the field of face  recognition, with a good understanding of publicly available  subspace techniques"}
{"pdf_id": "0705.0969", "content": "Water demand forecasting can be  regarded as a regression problem because the water time  series has non-linear nature and hence the output of the  predicting model has to be a real value depicting the  amount of water that will be needed on a specified date"}
{"pdf_id": "0705.0969", "content": "layer perceptron has three layers of units taking values in  the range (0 to 1). Each layer is nourished with the  previous layers, and hence it is also called a Jump  Connection Network (JCN) [14]. MLPs can have any  number of weighted connections, but networks with only  two weighted connections are very much capable of  approximating just about any functional mapping [15].  The MLP is mathematically represented by:"}
{"pdf_id": "0705.0969", "content": "B) Model Initialization  This section deals with the issues of the number of  model inputs. A short investigation had to be carried out  and this was done from the ANN perspective. Initially the  model is given a total of two inputs, followed by three,  four, five and six inputs. A five input network reflects the  least amount of training error and hence is adopted. The"}
{"pdf_id": "0705.0969", "content": "first four inputs are the previous water demand figures  representing four consecutive days, and the fifth input is  the annual population figure. A sample of the results from  the model input development procedure is reflected in  table IV below. This sample shows the results obtained  from MLP architecture making use of the linear scaled  conjugate gradient optimization algorithm.  TABLE IV  A SAMPLE OF THE RESULTS USED TO DECIDE ON  THE NUMBER OF MODEL INPUTS  Inputs  Training Error"}
{"pdf_id": "0705.0969", "content": "It is evident from table V above that the model with the  most optimum approximation is the one with a linear  kernel function. This is due to the fact that it has 100%  accuracy, and 3.94% validation error. It is therefore  regarded as the Support Vector Genius (SVG).  C) Determination of the ANG  The ANN experiment has two architectures to  investigate, and in turn, these architectures have many  different activation functions. For the sake of simplicity,  the experiments of the two architectures are separated and  the results are compared."}
{"pdf_id": "0705.0969", "content": "AZ1  Linear  9  SCG  AZ2  Linear  10  SCG  AZ3  Linear  9  Conjgrad  AZ4  Linear  10  Conjgrad  AZ5  Linear  9  Quasinew  AZ6  Linear  10  Quasinew  AZ7  Logistic  9  SCG  AZ8  Logistic  10  SCG  AZ9  Logistic  9  Conjgrad  AZ10  Logistic  10  Conjgrad  AZ11  Logistic  9  Quasinew  AZ12  Logistic  10  Quasinew"}
{"pdf_id": "0705.0969", "content": "Both AZ2 and AZ11 have an accuracy of 99%.  However AZ2 has a validation error that is less than that  of AZ11. This therefore implies that the MLP ANN with  the most suitable functional mapping is AZ2. AZ2 is a  network with a linear output activation function, ten  hidden  units  and  the  scaled  conjugate  gradient  optimization algorithm.  TABLE VII  THE RESULTS OBTAINED FROM THE DIFFERENT  MLP CONFIGURATIONS"}
{"pdf_id": "0705.0969", "content": "AZ4  10%  87%  156.828s  AZ5  63%  0%  73.594s  AZ6  35%  7%  20.875s  AZ7  15%  73%  96.703s  AZ8  6%  97%  20.281s  AZ9  9%  93%  90.781s  AZ10  18%  59%  154.984s  AZ11  7%  99%  76.515s  AZ12  9%  96%  146.968s"}
{"pdf_id": "0705.0969", "content": "Table IX shows ANN configurations with 100%  accuracy. These are AX3, AX4, AX5 and AX6. In order to  select the most optimum one, the validation error is  observed to select the smallest. Both AX4 and AX6 have  the same smallest validation error. In order to select the  most optimum one, the error obtained during training is  observed.  AX4 Training Error = 2.4651%  AX6 Training Error = 2.4272%  TABLE IX  THE RESULTS OBTAINED FROM THE RBF  VALIDATION FOR THE DIFFERENT ACTIVATION  FUNCTIONS"}
{"pdf_id": "0705.1013", "content": "1. INTRODUCTION  Collaborative tagging systems are online communities that allow  users to assign terms from an uncontrolled vocabulary (i.e., tags)  to items of interest. This simple tagging feature proves to be a  powerful mechanism for personal knowledge management (e.g.,  in systems like CiteULike [3]) and content sharing (e.g., in"}
{"pdf_id": "0705.1013", "content": "Although collaborative tagging is attracting increasing attention  from both industry and academia, there are few studies that assess  the characteristics of communities of users who share and tag  content. In particular, little research has been done on the  potential benefits of tracking usage patterns in collaborative  tagging communities. Moreover, recent investigations have shown  that, as the user population grows, the efficiency of information  retrieval based on user generated tags tends to decrease [2]."}
{"pdf_id": "0705.1013", "content": "2. RELATED WORK  Two types of techniques, implicit and explicit, are traditionally  used to elicit user preferences in the Web context [1][6][15].  Explicit techniques are based on direct input from a user with  respect to her preferences and interests (e.g., page rating scales,  item reviews, categories of interest). Implicit techniques infer a  definition of user interests from her activity, e.g., using client-side  or service-side mechanisms such as browser plug-ins, client"}
{"pdf_id": "0705.1013", "content": "In a tagging community context, the tags themselves can be  interpreted as explicit metadata added by each user. Additionally,  observed tagging activity including the volume and frequency  with which items are added, the number of tagged items, or tag  vocabulary size can be harnessed to extract implicit information."}
{"pdf_id": "0705.1013", "content": "Due to the youth of collaborative tagging systems, relatively little  work has been done on tracking usage and exploring  contextualized user attention in these communities. However,  several studies present techniques and models for collecting and  managing user attention metadata in the wider web context  without exploring tagging features [1][6][15]. These techniques  include post processing of usage logs, tracking user input (e.g.  search terms) and eliciting explicit user preferences. Other  investigations are concerned with methods to use contextualized  attention to improve web search [1][15]."}
{"pdf_id": "0705.1013", "content": "Other authors follow different approaches to investigate the  characteristics of tagging systems. Schimtz [10][11] studies structural properties of del.icio.us and Bibsonomy, uses a tri partite hypergraph representation, and adapts the small-world  pattern definitions to this representation. Cattuto et al. [12] model  usage behavior via unipartite projections from a tripartite graph.  Our approach differs from these studies in terms of scale and in  the use of dynamic metrics to define shared user interest: we  define metrics that scale as the community grows and/or user  activity increases (Section 6)."}
{"pdf_id": "0705.1013", "content": "By analyzing del.icio.us, Chi and Mytkoswicz [2] find that the  efficiency of social tagging decreases as the communities grow:  that is, tags are becoming less and less descriptive and  consequently it becomes harder to find a particular item using  them. Simultaneously, it becomes harder to find tags that  efficiently mark an item for future retrieval. These results indicate  that, to facilitate browsing through tagging systems, it is  increasingly important to take into account user attention in terms  of observed tagging activity."}
{"pdf_id": "0705.1013", "content": "Niwa et al. [17] propose a recommendation system based on the  affinity between users and tags, and on the explicit site  preferences expressed by the user. Our study differs from this  work as we use implicit user profiles and propose the use of  entropy as a metric to characterize their effectiveness."}
{"pdf_id": "0705.1013", "content": "Outside the academic area, a number of projects explore the use of  implicitly-gathered user information. We mention Google's  initiative to explore users' past search history to refine the results  provided by the Page Rank [8][9]. Commercial interest in  contextualized user attention highlights that tracking user  attention and characterizing collective online behavior is not only  an intriguing research topic, but also a potentially attractive  business opportunity."}
{"pdf_id": "0705.1013", "content": "3. BACKGROUND  A collaborative tagging community allows users to tag items via a  web site. Users interact with the website by searching for items,  adding new items to the community, or tagging existent items.  The tagging action performed by a user is generally referred as a  tag assignment."}
{"pdf_id": "0705.1013", "content": "For example, in CiteULike and Bibsonomy, each user has a  library, i.e., a set of links to scientific publications and books.  Each item in the library is associated with a set of terms (tags)  assigned by users. It is important to highlight that, in both  CiteULike and Bibsonomy, the process of assigning tags to items  is collaborative, in the sense that all users can inspect other users'  libraries and assigned tags. User can thus repeat tags used by  others to mark a particular item. This is unlike other communities  (e.g., Flickr) where each user has a fine-grained access control to  define who has permissions to see the content and apply tags to it."}
{"pdf_id": "0705.1013", "content": "While posting an item, a user can mark it with terms (i.e., tags)  that can be used for future retrieval. The collaborative nature of  tagging relies on the fact that users potentially share interests and  use similar items and tags. Thus, while the tagging activity of one  user may be self-centered the set of tags used may facilitate the  job of other users in finding content of interest."}
{"pdf_id": "0705.1013", "content": "The data sets analyzed in this article were provided by the  administrators of the respective web sites. Thus, the data  represents a global snapshot of each system within the period  determined by the timestamps in the traces we have obtained  (Table 1). It is important to point out that the Bibsonomy data set  has timestamps starting at 1995, which we considered a bug.  Moreover, Bibsonomy has two separate datasets, scientific  literature and URL bookmarks. We concentrated our analysis on  the scientific literature part of the data."}
{"pdf_id": "0705.1013", "content": "In the original CiteULike data set, the most popular tag is \"bibtex import\" while the second most popular tag is \"no-tag\",  automatically assigned when a user does not assign any tag to a  new item. The popularity of these two tags indicates that a large  part of users use CiteULike as a tool to convert their list of  citations to BibTex format, and that users tend not to tag items at  the time they post a new item to their individual libraries. Clearly,  this is relevant information for system designers who might want  to invest effort in improving the features of most interest."}
{"pdf_id": "0705.1013", "content": "Consequently, for the analysis that follows, we have the \"robot\"  user (i.e., a user with 3,000 items tagged within 5 minutes) and  users who used only the tags bibtex-import and/or no-tag. The  total number of users removed from CiteULike represents  approximately 14% of the original data set, while the users  removed from Bibsonomy are around 0.6% of the original data  set. Table 1 summarizes the characteristics of each data set after  the data cleaning operation."}
{"pdf_id": "0705.1013", "content": "5. TAGGING ACTIVITY  To gain an understanding on the usage patterns in these two  communities, we start by evaluating the activity levels along  several metrics: the number of items per user, number of tagging  assignments performed, and number of tags used. The question  answered in this section is the following:"}
{"pdf_id": "0705.1013", "content": "We aim to quantify the volume of user interaction with the  system, either by adding new content to the community, or by  tagging an existing item. Intuitively, one would expect that a few  users are very active while the majority rarely interacts with the  community."}
{"pdf_id": "0705.1013", "content": "A second metric for tagging activity is the size of user libraries.  Figure 2 plots user library size for users ranked in decreasing  order according to the size of their libraries for CiteULike and  Bibsonomy, respectively. This shows the size of the set of items a  particular user pays attention to. The results confirm that the users"}
{"pdf_id": "0705.1013", "content": "A second finding is that the tagging activity (i.e., number of  tagging assignments) and library size per user are strongly  correlated for both communities (with R2 above 0.97) while the  correlations between the tagging activity and the vocabulary size  is strong for CiteULike (R2 = 0.99), but weaker for Bibsonomy  (R2 = 0.67)."}
{"pdf_id": "0705.1013", "content": "to collaborative tagging is the use of Hoerl function to describe  the distribution of bio-diversity across a geographic region  [22][24]. Considering each user's library a region in a  collaborative tagging community, one may draw a comparison  between the potential diversity found in the users' library  regarding the number of items in it, and the bio-diversity  distribution across geographic regions."}
{"pdf_id": "0705.1013", "content": "Although a Hoerl function is a good fit for the activity  distributions, this does not directly imply that diversity of user  libraries or vocabularies represents a phenomenon which is  similar  to  those  presented  by  studies  on biodiversity.  Nevertheless, the Hoerl function does provide a good model for  collaborative tagging activity and it can be useful to study user  diversity in collaborative tagging systems in the future."}
{"pdf_id": "0705.1013", "content": "To summarize: in the communities we study, the intensity of user  activity is distributed over multiple orders of magnitude, it is well  modeled using the Hoerl function and, unlike in other  communities, there is a strong correlation in activity in terms of  items set and vocabulary sizes."}
{"pdf_id": "0705.1013", "content": "6. EVALUATING USER SIMILARITY  While the analysis above is important for an overall usage profile  evaluation of each community, it provides little information about  user interests. Assessing the commonality in user interests is  important for identifying user groups that may form around  content of common interest. Thus, a natural set of questions that  we aim to answer in this section are:"}
{"pdf_id": "0705.1013", "content": "To address these questions, we define the interest-sharing graph  after the intuition of data-sharing graphs introduced by Iamnitchi  et al. [27]. An interest-sharing graph captures the commonality in  user interest for an entire user population: Intuitively, users are  connected in the interest-sharing graph if they focus on the same  subset of items and/or speak similar language (i.e., share a subset  of tags)."}
{"pdf_id": "0705.1013", "content": "More formally, consider a graph G = (U, E) where nodes are users  and edges represent the existence of shared interests or activity  similarity between users. The rest of this study explores three  possible definitions for user interest or activity similarity. All  these definitions employ a threshold t for the percentage of items  or tags shared between two users:"}
{"pdf_id": "0705.1013", "content": "3) Unlike the User-Item definition in Equation 2 above, the  Directed User-Item considers two users' interests similar if  the ratio between the intersection of their item libraries and  the size of one user library is larger than a threshold t. The  idea is to explore the role played by users with large libraries  via the introduction of direction to the edges in the graph."}
{"pdf_id": "0705.1013", "content": "In our analysis of real tag assignment traces from the two tagging  communities, even with low values for the sharing ratio threshold  t, the final graph contains a large number of isolated nodes.  Indeed, by setting the threshold as low as one single item (i.e.,  two users are connected if they share at least one item); we find  that, in CiteULike, 2,672 users (44.87%) are not connected to any  other user. This suggests that a large population of users has  individual preferences."}
{"pdf_id": "0705.1013", "content": "Figure 4 presents, for the three similarity metrics defined above,  the number of connected components for both CiteULike and  Bibsonomy, for thresholds t varying from 1% to 99%. These  results show that regardless of the graph definition the number of  connected components follow a similar trend as the threshold  increases (Note that we exclude isolated nodes from this count of  connected graph components)."}
{"pdf_id": "0705.1013", "content": "The plots in Figure 4 show that the number of connected  components increases up to a certain value of our similarity  threshold. After a certain value of t, the number of connected  components in the graph starts decreasing, since more and more  connected components will contain only one node and will thus  be excluded. The critical threshold value is different for each user  similarity definition."}
{"pdf_id": "0705.1013", "content": "The initial increase in the number of connected components can  be explained by the fact that, as the threshold increases, large  components split to form new islands. Since these islands form  naturally based on user similarity this result is encouraging since  it offers the potential to cluster users according to their interests.  As t continues to increase the definition of similarity becomes too  strict and leads to more and more isolated nodes."}
{"pdf_id": "0705.1013", "content": "All the similarity definitions above generally divide the original  graph into one giant component, several tiny components, and a  large number of isolated nodes. Figure 5 presents the total number  of nodes in the components with at least two nodes and the  number of nodes in the largest connected component for  thresholds varying from 1% to 99% for the three similarity  measures defined above."}
{"pdf_id": "0705.1013", "content": "The results presented in this section demonstrate that using a  similarity metric and the resulting interest-sharing graph it is  possible to segment the user population according to manifested  interest. Based on this intuition, we conjecture that it is possible  to build tag/item recommendation mechanisms that exploit usage  patterns, i.e., the shared interests among users. The next section  offers a preliminary analysis of this hypothesis."}
{"pdf_id": "0705.1013", "content": "7. IMPROVING NAVIGABILITY  Chi and Mytcowicz [2] report that navigability, defined as users'  ability to find relevant content, decreases as a tagging community  grows. More precisely, Chi and Mytcowicz imply that the  decrease in navigability is due to an increase in diversity in the set  of items, users, and tags."}
{"pdf_id": "0705.1013", "content": "In practical terms, in a collaborative tagging community, the  increase in entropy of an item set means that the user needs to  filter out more items to find the one she is interested in. Similarly,  high entropy makes it harder to find a tag that describes an item  well. Conversely, lower entropy makes it potentially easier for a  user to reach an item of interest. Thus, the question to be  answered in this section is the following:"}
{"pdf_id": "0705.1013", "content": "Our two-part answer is briefly presented below and detailed in the  rest of this section. First, we demonstrate that the interest-sharing  graph can be used to reduce the entropy perceived by users. To  this end we define a user's neighborhood as its set of neighbors in  the sharing graph and show that this construction can be used to  present users with an item set with low entropy."}
{"pdf_id": "0705.1013", "content": "Second, we offer preliminary results that suggest that this  segmentation of the user population based on neighborhoods in  the interest-sharing graph has a good predictive power: the items  consumed by a user's neighbors predict well the future  consumption pattern of that user. Thus, this offers a path to build  recommendations systems based on the interest-sharing graph."}
{"pdf_id": "0705.1013", "content": "To support our hypothesis that the interest-sharing graph is a good  basis to develop recommendation systems, we analyze how  efficient the neighbor's item set in predicting future user attention  over items. To this end, we evaluate the hit ratio: the proportion  of items a user adds to her library at time T+1 that are already in  her neighbor' libraries at time T."}
{"pdf_id": "0705.1013", "content": "To evaluate the hit ratio, we considered the interest-sharing graph  based on the User-Item similarity metric with 1% sharing ratio  threshold. Preliminary results show that depending on the  granularity considered (that is the length of our forecasting  period: interval between T and T+1) the hit rate is as high as 20%  for one hour granularity and decays to a low of 5% for a  one-month forecast granularity. This indicates that a user's  neighborhood is a possible source of information to predict near  future user attention and its predictive effectiveness decreases for  longer time intervals."}
{"pdf_id": "0705.1013", "content": "First, we analyze the distribution of tagging activity, i.e., the  distribution of the volume of items, tags, and tagging actions  related to each user' activity in the tagging community. We find  that the activity distribution is highly heterogeneous along all  these multiple axes: a few active users contribute with a large  number of tag assignments and maintain a large number of items  and tags, while the majority of users have a modest tagging  activity."}
{"pdf_id": "0705.1013", "content": "1.  Both communities present a large population of isolated  users (zero-degree nodes in the interest-sharing graph). This  indicates that there are a large number of users with unique  preferences. On the other hand, by introducing direction in  the graph of shared interests, it is possible to reduce the  number of isolated nodes. The final main directed connected  component contains approximately twice more nodes than  the undirected one."}
{"pdf_id": "0705.1013", "content": "4.  Finally, we provide preliminary evidence that suggests that  user's activity can be predicted by considering the union of  the item sets of a node's neighbors in the interest sharing  graph. We conjecture that this property can be used to build  efficient, online recommendation systems for tagging  communities."}
{"pdf_id": "0705.1013", "content": "A second intriguing issue to explore is the following How  malicious behavior affects a tagging system and whether it be  automatically detected? Search results that are manipulated by  tagging misbehavior can have an impact on usage in a  collaborative tagging community [13]. Automatic detection of  malicious users is paramount to the long term survival of these  communities."}
{"pdf_id": "0705.1013", "content": "ACKNOWLEDGMENTS  The authors would like to thank Richard Cameron for providing  the CiteULike data set; Christoph Schmitz for providing the  Bibsonomy data set; professor Lee Iverson for insightful  discussions on early stages of this work, and Armin  Bahramshahry, Samer Al Kiswany and Nazareno Andrade for  their valuable comments. The graph analysis was executed in  parallel using OurGrid (http://www.ourgrid.org)."}
{"pdf_id": "0705.1031", "content": "that ensures that the output is as close to the target vector  as possible. This paper implements the autoencoder  neural network as discussed below.  Autoencoder neural networks: Autoencoders, also known as  auto-associative neural networks, are neural networks  trained to recall the input space. Thompson et al [8]"}
{"pdf_id": "0705.1031", "content": "The first step in  approximating the weight parameters of the model is  finding the approximate architecture of the MLP, where  the architecture is characterized by the number of hidden  units, the type of activation function, as well as the  number of input and output variables"}
{"pdf_id": "0705.1031", "content": "1). Create an initial population P , beginning at an initial  generation  g = .0 2). for each population P, evaluate each population  member (chromosome) using the defined fitness  evaluation function possessing the knowledge of the  competition environment.  3). using genetic operators such as inheritance,  mutation  and  crossover,  alter  P(g) to"}
{"pdf_id": "0705.1031", "content": "5. PROPOSED METHOD: ENSEMBLE BASED  TECHNIQUE FOR MISSING DATA  The algorithm proposed here uses an ensemble of neural  networks to perform both classification and regression in  the presence of missing data. Ensemble based approaches  have well been researched and have been found to  improve  classification  performances  in  various  applications [14-15]. The potential of using ensemble  based approach for solving the missing data problem  remains unexplored in both classification and regression  problems. In the proposed method, batch training is  performed whereas testing is done online. Training is  achieved using a number of neural networks, each trained  with a different combination of features. For a condition"}
{"pdf_id": "0705.1031", "content": "shall only consider a maximum of one sensor failure per  instance. Each network was trained with 1200 training  cycles using the scaled conjugate gradient algorithm and  a hyperbolic tangent activation function. All these  training parameters were again empirically determined.  Results: Since testing is done online where one input  arrives at a time, evaluation of performance at each  instance would not give a general view of how the  algorithm works. The work therefore evaluates the  general performance using the following formula only  after N instances have been predicted."}
{"pdf_id": "0705.1110", "content": "In this section we will define balanced patterns. We first discuss several problems and possibilities, and finally give the proper definition. We call the occurrences balanced if between two successive occurrences there is (almost) always the same amount of transactions. The problem with patterns with balanced occurrences is that an itemset may occur less balanced than a superset of this itemset. Patterns occurring with a balanced interval do not have the anti-monotone property, where the subset is either equally good or better than the superset. In the balanced pattern case: the subset is not always more (or equally) balanced than the superset. This value will be used for pruning."}
{"pdf_id": "0705.1110", "content": "For our definition of balanced patterns we first notice that all balanced oc currences (successive and non-successive) should have at least one intermediate distance a minimal number of times. Furthermore if you count the distances between all occurrences then this count is anti-monotone: a superset never hasmore of one particular distance. This is obvious because the number of occur rences will never increase for a superset and as a consequence the count of one particular distance will never increase. This property is also anti-monotone if we limit the distances we count, e.g., we count a distance only if it is smaller than 10 in-between transactions."}
{"pdf_id": "0705.1110", "content": "The definition of balanced patterns should be the following: A pattern is called a balanced pattern if among all occurrence pairs there is a distance that occurs atleast a user-defined number of times (minnumber) and the distance between suc cessive occurrences have maximally a user-defined standard deviation (maxstdev) and minimally a user-defined average (minavg)."}
{"pdf_id": "0705.1110", "content": "partment of Leiden University, as said before. It contains all 1,991 items of the web-pages that were visited, grouped in half-hour blocks, so each of the 1,488 transactions contains the pages visited during one half-hour. Figure 4 shows how the runtime for the website dataset drops fast as minnumber increases. Table 1 shows the count for distances between successive occurrences. It shows that this particular pattern, consisting of the websites of two professors of the same group and the main page, occurs often with a successive distance of 0, 1 or 2. This pattern probably is caused by students having courses from both professors and some of these students access both pages nearly every half an hour."}
{"pdf_id": "0705.1161", "content": "where pi def = P(Xi = 1 | R = y), qi def = P(Xi = 1 | R = n), Xi is an indicator variable for the presence of the ith term, and R is a relevance random variable. Croft and Harper [2] proposed the use of two assumptions to estimate pi and qi in the absence of relevance information. CH-1, which is unobjectionable, simply states that most of the documents in the corpus are not relevant to the query. This allows us to set d qCH def ni"}
{"pdf_id": "0705.1161", "content": "Despite this claim, we show here that there exists a highly intuitive linear estimate that leads to a term weight varying inversely with document frequency.There are two main principles that motivate our new es timate. First, as already stated, any estimate of pi should be positively correlated with ni. The second and key insightis that query terms should have a higher occurrence proba bility within relevant documents than within the document collection as a whole. Thus, if the ith term appears in the query, we should \"lift\" its estimated occurrence probability in relevant documents above ni/N, which is its estimated occurrence probability in general documents. This leads us to the following intuitive estimate, which is reminiscent of \"add-one smoothing\" used in language modeling (more on this below):"}
{"pdf_id": "0705.1209", "content": "Monica Lagazio holds a PhD in Politics and Artificial Intelligence from Nottingham University and  an MA in Politics from the University of London. Before joining the University of Kent at  Canterbury in 2004, she was Lecturer in Politics at the University of the Witwatersrand and  Research Fellow at Yale University. She also held a position of senior consultant in the economic  and financial service of one of the leading global consulting companies in London."}
{"pdf_id": "0705.1244", "content": "continuous parameter (e.g. speed of forward displacement, or turning angle for left and right actions). The proposed symbolic controller has eight outputs with values in [0, 1]: the first four outputs are used to specify which action will be executed, namely action i, with i = Argmax(output(j), j = 1..4). Output i + 4 then gives the associated parameter. From the given action and the associated parameter, the values of the commands for the actuators are computed by some simple hard-coded program."}
{"pdf_id": "0705.1244", "content": "Initial experiments have been performed using the Khepera simulator EOBot, that was developed by the first author from the EvoRobot software provided by S. Nolfi and D. Floreano [13]. EvoRobot was ported on Linux platform using OpenGL graphical library, and interfaced with the EO library [9]. It is hence now possible to use all features of EO in the context of Evolutionary Robotics, e.g."}
{"pdf_id": "0705.1244", "content": "Nevertheless, in order to definitely avoid this loophole, the fitness is modified in such a way that it increases only when the robot moves forward (sum of both motor speeds positive)3. This modification does not alter the ranking of the controllers: the Symbolic Controller still outperforms the Classical Controller. This advantage somehow vanishes when more hidden neurons are added (see Table 1), but the results of the SC exhibit a much smaller variance."}
{"pdf_id": "0705.1244", "content": "Alternatives for the overall architecture will also be looked for. One crucialissue in autonomous robotics is the adaptivity of the controller. Several architec tures have been proposed in that direction (see [13] and references herein) and will be tried, like for instance the idea of auto-teaching networks. Finally, in the longer run, the library approach helps to keep tracks of the behavior of the robot at a level of generality that can be later exploited by some data mining technique. Gathering the Frequent Item Sets in the best evolved controllers can help deriving some brand new macro-actions. The issue will then be to check how useful such macro-actions can be if added to the library."}
{"pdf_id": "0705.1309", "content": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. GECCO 2007 London, England Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$5.00."}
{"pdf_id": "0705.1309", "content": "3 Halting the Growth Process In Multi-cellular developmental systems, the phenotype (the target structure to be designed, on which the fitness can be computed) is built from the genotype (the cell-controller,here a Neural Network) through an iterative process: Start ing from a uniform initial condition (here, the activity of all neurons is set to 0), all cells are synchronously updated, or, more precisely, all neurons of all cells are synchronously updated, in case the neural network is recurrent"}
{"pdf_id": "0705.1309", "content": "and the organism is considered stable when E(t) = E(t +1) during a given number of time steps. Of course, a max imum number of iterations is given, and a genotype that hasn't converged after that time receives a very bad fitness: such genotype has no phenotype, so the fitness cannot even be computed anyway. After such a final stable state for the organism has been reached, it is considered as the phenotype and undergo evaluation."}
{"pdf_id": "0705.1309", "content": "In order to try to discriminate between the modeling er ror and the method error, a fifth model is also run, on the same test cases and with similar experimental conditions than the four developmental approaches described above: the layout is exactly the same (a 2D grid of cells), the sameNEAT parameters are used (to evolve a feedforward neu ral network), and selection proceeds using the same fitness"}
{"pdf_id": "0705.1309", "content": "The model was validated on four instancesof the 'nag' problem, and on 3 out of 4 instances it performed as good as NEAT applied to the equivalent regres sion problem: this is a hint that the modeling error of the developmental approach is not much bigger than that of the Neural Network approach for regression (which is proved to be small, thanks to the Universal Approximator property), and is in any case small compared to the computational error (i"}
{"pdf_id": "0705.1309", "content": "The major (and somewhat unexpected) consequenceof this adaptivity is the tremendous robustness toward perturbations during the growth process: in almost all experi ments, the fixed point that is reached from the initial state used during evolution (all neural activations set to 0) seems to be a global attractor, in the sense that the organism will end up there from any starting point"}
{"pdf_id": "0705.1886", "content": "ABSTRACT This paper presents the principles of ontology-supported and ontology-driven conceptual navigation. Conceptual navigation realizes the independence between resources and links to facilitate interoperability and reusability. An engine builds dynamic links, assembles resources under an argumentative scheme and allows optimization with a possible constraint, such as the user's available time. Among several strategies, two are discussed in detail with examples of applications. On the one hand, conceptual specifications for linking and assembling are embedded in the resource meta-description with the support of the ontology of the domain to facilitate meta-communication. Resources are like agents looking for conceptual acquaintances with intention. On the other hand, the domain ontology and an argumentative ontology drive the linking and assembling strategies."}
{"pdf_id": "0705.1886", "content": "For the hypertext paradigm, the World Wide Web is a network of links between and within documents through which the user navigates using visual invitations (marks) on the documents. Meanwhile, IR search engines use key words and index databases to gather everything that may resemble a user's query. Each approach is very powerful and has proven to be efficient within its own paradigm. Practically, readers combine both. The lexical search is to look for unknown documents on specific topics, and the hypertext approach uses authors' links to complete the coverage of the topic as needed."}
{"pdf_id": "0705.1886", "content": "It is accepted that there is no ideal solution to a complex problem and a coherent paradigm may present limits when considering the complexity and the variety of the users' needs. Let's recall some of the traditional criticisms about hypertext. The readers get lost in hyperspace. The links are predefined by the authors and the author's intention does not necessary match the readers' intentions. There may be other interesting links to other resources that are not given. The narrative construction which is the result of link following may present argumentative pitfalls."}
{"pdf_id": "0705.1886", "content": "As regards the IR paradigm, there are other criticisms. The search engines leave the readers with a list of weighted documents having no other relation than the lexical one. The set of documents is a set of local results and there is no means for managing redundancy, or a lack of information. The order of presentation is often the decreasing order of the weights and there is no narrative construction between documents."}
{"pdf_id": "0705.1886", "content": "Beyond these specific criticisms, both approaches present other common limits. The reader is the one who must decide most of the navigation strategy. This responsability would not be a problem if the readers already knew the content of the documents they are invited to visit. But when the readers have very little idea about the documents, their content and their volume, which is usually the case, they have not enough information to decide what the best strategy is for meeting their goals."}
{"pdf_id": "0705.1886", "content": "Finally, no constraint is handled by the hypertext navigation on the behalf of the users, such as the time they have available to read the documents they access. This consideration has not inspired much research, but practically, this is the sort of constraint that influences quite a lot the readers' strategies."}
{"pdf_id": "0705.1886", "content": "The research project of our team is to define a new approach where an agent uses ontologies to work on the behalf of readers to find relevant documents, select among them the most appropriate, organize them, and establish links between them with a possible argumentative construction. During the work, the agent takes into account  readers'  requirements  and  constraints, particularly the readers' content objectives and their available time constraints."}
{"pdf_id": "0705.1886", "content": "This paper presents the principles of ontology-supported and ontology-driven conceptual navigation. Several possible models of conceptual navigation strategies are introduced. We illustrate two of them with different applications. We analyse the architectural differences and the advantages and disadvantages they bring about. As a conclusion, we show that what is at stake is not only adaptivity to the users' needs, but also interoperability and reusability."}
{"pdf_id": "0705.1886", "content": "•  The system takes charge of the user's profile involving objectives and constraints. •  It automatically builds intentional weighted semantic links between documents or parts of documents. •  It gives roles (affordances, pragmatics) to these links, taking into account the ontology of the domain and an ontology of argumentation. •  It chooses among these links which are the best according to a particular context and a particular reader's intention. •  It assembles the resources using the most appropriate narrative or pedagogic strategy amongst possible strategies. During this computation, it complies with the user's time constraint, or any other economical constraint."}
{"pdf_id": "0705.1886", "content": "particular learner. The courses are composed of pedagogical resources that are available on line. Karina's long range objective is to propose several conceptual navigation strategies, among which the system will choose the best adapted to the learner's needs. For the moment, only the backward conceptual navigation strategy has been implemented. It will be discussed later on. Besides these strategies, Karina still allows for navigation using the traditional methods, i.e. word indexation and hyperlinks. Three main phases in the conceptual navigation process can be distinguished in Karina. These phases are summarized below. The first two phases are discussed in detail in other sections since they are at the core of conceptual navigation."}
{"pdf_id": "0705.1886", "content": "Phase one: document selection and indexation. The first phase is the production or the selection of resources that may be used or reused in the construction of training courses. These resources may have been produced either by a unique author or by different authors. Karina does not speculate on who is in charge of producing/selecting resources or how. The resources are indexed. A DTD (Document Type Definition), written in XML, is used to structure indexing. Help is obtained from indexing tools which propose a vocabulary and semantic constraints derived from an ontology of the domain."}
{"pdf_id": "0705.1886", "content": "Phase two: Dynamic adaptive course building. In order to build courses, Karina needs to know the learner's profile, i.e. the present knowledge, the knowledge objective and the learner's constraints. The main constraint which is considered is time. An engine called Conceptual Evocative Engine is in charge of selecting among the available indexed resources those that can entirely, or most often partly, fulfill the conceptual description of the learner's objectives. When chosen pedagogical material has  prerequisites,  those  prerequisites  become  an intermediate  objective  for  the  engine  (backward conceptual navigation). The result is a list of pedagogicalresources which is ordered according to the objective prerequisite navigation process."}
{"pdf_id": "0705.1886", "content": "The Karina's DTD The Karina's DTD1 is a XML-written document which allows the qualification of complete resources, or parts of resources called \"segments\". The DTD is composed of several \"elements\" which contain most of the necessary information for retrieving a resource on a conceptual and argumentative basis, analysing it and assembling it with other resources [9]. In the following description of the DTD, we only discuss some features that are used for ontology-supported conceptual navigation, and more precisely for conceptual backward navigation :"}
{"pdf_id": "0705.1886", "content": "Karina's Conceptual Language (KCL) This language is defined in the Karina DTD using XML. It formalizes conceptual descriptions of content into a structure called a Conceptual State Vector (CSV) presented in [8]. A CSV is a weighted sum of conceptual assertions. Each assertion is represented by a conceptual graph (CG) [28]."}
{"pdf_id": "0705.1886", "content": "Simplified Conceptual Graphs in Karina. Although Sowa's CGs are very useful to formalize knowledge, they present some drawbacks in the context of Karina. They are not simple to use for a non-specialist. They are not easy to 1 The Karina DTD and the ontology DTD can be freely downloaded at the address:  http:// www.site-eerie.ema.fr/~multimedia"}
{"pdf_id": "0705.1886", "content": "The traditional CG relations like (AGNT) or (OBJ) have disappeared, but they are still implicit taking into account the ontology of the domain as it is explained below. As far as these three simple graphs describe the same situation, they can be merged applying Sowa's operation \"copy\", \"restrict\", \"join\", and \"simplify\" in order to rebuild the initial conceptual graph. To give more details to the situation, we simply need to add new assertions in the set. For example, if we want to enrich the CGi with the assertion that the caterpillar also speaks to Alice, we can add the following conceptual graph :"}
{"pdf_id": "0705.1886", "content": "Conceptual typing with the help of the ontology of the domain. An ontology is \"an axiomatic characterization of the meaning of a logical vocabulary\" [16]. It is modelled as a hierarchy of types and a set of relations beween those concepts which specify which assertions it is possible to make about a world corresponding to the domain. In Karina, semantic correctness and interoperability is supported by an ontology of the domain which is written in KCL. An ontology is stored as a resource specified with a particular DTD written in XML1."}
{"pdf_id": "0705.1886", "content": "Karina's indexing interface makes use of the ontology of the domain to facilitate the indexing process and to prevent any mistakes. It opens up three slots for each Karina conceptual graph to be edited. The slots are constrained according to the ontology used for indexing the document. The first slot stands for the \"source\" of the conceptual graph. It contains the hierarchy of concepts from the ontology. When a concept is chosen, the indexer limits the hierarchical menu in the second slot to the concepts that are related to the source in the set of predicates in the ontology. It is then possible to choose in"}
{"pdf_id": "0705.1886", "content": "Conceptual State Vectors In order to emphasize specific statements, or concepts inside statements, each statement in the set of statements describing a resource is endowed with a weight having a real value between 0 and 1. A justification for this weight has been given in [7]. As a result, a Karina conceptual description of a resource is a Conceptual State Vector (CSV), i.e. a symbolic sum of weighted conceptual graphs."}
{"pdf_id": "0705.1886", "content": "Translation and independant saving All the information entered for qualifying a resource is translated automatically into XML using Karina's DTD. It is a Resource Description (RD) which is stored in an independant file from the resource in order to avoid polluting a possible original meta-description of the resource. This  choice  is  the  result  of  several considerations :"}
{"pdf_id": "0705.1886", "content": "•  A resource can keep its genuine meta-description which has a specific meaning in the original context. •  The argumentative points of view may vary according to different tutors and there should be different RDs according to the different contexts. •  By keeping the resource in its original state, we partly avoid some problems with rights. • Finally, it is easier to scan a separate meta description stored in a database and it takes less space to store it. The meta-description can be local, and the resources distant."}
{"pdf_id": "0705.1886", "content": "Objective update. The first step consists in updating the objective. Karina takes the CSV corresponding to the objective and withdraws those CGs that are present in the learner's initial model. The weights are not taken into account at this stage. This suppression is made with a total match between the slots of the CGs, i.e. when a slot is empty in one CG, and the corresponding slot is not empty in the other CG, the two CGs are considered not to match."}
{"pdf_id": "0705.1886", "content": "Conceptual Proximity computation. In a second step, the engine explores the different RDs and computes a match beween the learner's updated content objective and the conceptual contents of the resources. This process uses a unification algorithm to compute a Conceptual Proximity (CP) between two CSVs. This algorithm has been formally described in [7]."}
{"pdf_id": "0705.1886", "content": "Choice of the best resource. The resource with the highest CP as regards the updated objective is selected. If several resources have the same CP value, Karina selects the one with the lower time value. This choice is justified because the shorter the resource, the more it will be possible to confine the course in the time constraint given by the learner. If two resources have the same duration, one is arbitrarily chosen. The other one is memorized in case the selection needs to be reviewed at the end (backtracking)."}
{"pdf_id": "0705.1886", "content": "Objective and profile updating. Then Karina withdraws the content of the selected resource from the objective and adds this content to the learner's profile. It behaves as if the learner had consulted the resource. It also adds the prerequisites of the resource to the objective. When doing this, it only adds the prerequisites that are not already present in the learner's profile to avoid looking for contents that have already been dealt with by other selected resources or by the learner's initial knowledge. Any selected resource is tagged so that it will not be considered again during the following round of selection."}
{"pdf_id": "0705.1886", "content": "End of selection. The selection process ends when there is no content left in the objective, or if there are no resources matching the objective. The different resource durations are added up. If the result exceeds the learner's time constraint, Karina tracks back to choose the second-best selected resource in the queue which presents a shorter time value to try another path. If there is no path meeting the time constraint, Karina proposes the shortest path."}
{"pdf_id": "0705.1886", "content": "OTHER CONCEPTUAL NAVIGATION STRATEGIES Traditional navigation strategies are still possible since the resources keep their original hyperlinks and the DTD allows the introduction of keywords for IR engines. But what is most interesting is the numerous conceptual navigation strategies that are possible. We present here some of them that we are studying and that are representative of the power of ontology-supported conceptual navigation."}
{"pdf_id": "0705.1886", "content": "Conceptual expansion may be applied in two ways. In the first case, the user may ask \"more\" about a subject when studying a resource, and the evocative engine will look for conceptually related resources. Since this conceptual relation may be attached to several segments of a resource, the expansion process may help to look in detail at different aspects of the content. The second type of conceptual expansion may be used by the application itself when there is a lack of material to build a sufficient delivery within the time constraint. In such a resource starvation context, the conceptual expansion policy allows for the filling up of the gaps. Conceptual expansion opens up many interesting possibilities that we are studying for other multimedia applications."}
{"pdf_id": "0705.1886", "content": "Forward conceptual navigation Conceptual expansion can be used as a whole strategy which replaces backward navigation. A first resource is chosen and through conceptual expansion other resources are selected. In their turn, they may be used for expansion up to the point where the time constraint is reached. This process looks very much like free navigation in a hypertext, with the difference that here it is based on conceptual evocation and not hyperlinks. The risk is to get lost in a set of resources which are not linked through narrative constraints. It needs some conceptual railing."}
{"pdf_id": "0705.1886", "content": "The  conceptual  specification  strategy  and  its application in narrative abstraction The conceptual prerequisites and the conceptual relation constitute conceptual specifications for linking a resource to other resources. The advantage of embedding conceptual navigation specification within the resources is that the resources are independant, self-contained, and also cooperative. It is a first step to seeing resources as cooperative  agents.  The  drawback  is  that  the narrative/pedagogic  strategy  cannot  be  specified independantly from the resources. This drawback can be overcome with a strategy which is based on a conceptual specification of the expected final resource."}
{"pdf_id": "0705.1886", "content": "It consists in building a purely conceptual resource, i. e. an empty resource that only contains conceptual descriptions of segments. The engine goes to the first segment, takes its description as conceptual objectives and looks for resources that match these objectives. Then the engine proceeds to the next segment keeping the time constraint as a parameter for optimization. We have already presented this type of strategy implemented in the Godart project [8] which builds narrative abstraction from a linear narrative. If the application is educational, the conceptual content of a segment must be added to the learner's profile before going on to the next segment in order to avoid as much redundancy as possible. This"}
{"pdf_id": "0705.1886", "content": "In pedagogic applications, this idea hinges on the observation that a table of content of a course looks very much like an ontology of the domain being taught. Titles and subtitles contain keywords that are presented in a hierarchy. Therefore, we can imagine that the ontology can be the basis for a training course when endowed with pedagogical properties. This is what we present in the next application example, Sybil."}
{"pdf_id": "0705.1886", "content": "engine uses the resources' pedagogical roles from the RDs and the pedagogical rules from the pedagogic ontology. For instance, there is a rule which says: \"IF an Explanation and an Example refer to the same topic, THEN the URL of the Explanation must precede the URL of the Example\"."}
{"pdf_id": "0705.1886", "content": "Moreover, if the general exposition strategy is \"Top Down\", the engine will find in the domain ontology that a sonata is composed of four parts: the \"exposition\", the \"development\", the \"recapitulation\", and a \"coda\". These concepts become new goals for the exposition. As one can see, the conceptual navigation is driven by both the ontology of the domain and the pedagogic ontology, along with the RDs which contain the resources' conceptual description and pedagogic roles. The three structures are independant and reusable although there is a certain limit as far as the resources are concerned as we see next."}
{"pdf_id": "0705.1886", "content": "Comparison of the two approaches Both the Karina and the Sybil approaches are domain ontology-supported through indexation. In Karina, the conceptual navigation is the result of the engine strategies and the conceptual specifications embedded in the resources' description. In Sybil, the strategy is driven by the pedagogic ontology and the domain ontology. Both have pedagogic roles embedded in the resource descriptions. In Sybil, the pedagogic role is part of the resource description conceptual graph. In Karina, the element 'prerequisite' is a particular role for other related resources. There is also a specific element in the DTD called \"type_pedagogique\" which can be used to give a role to the resource."}
{"pdf_id": "0705.1886", "content": "The fact that the description of a resource contains the pedagogical role of the resource is very open to criticism because a resource may have several pedagogic roles according to the context. To solve this problem, we are working to have this role driven by the ontology, which means that it will be calculated through the ontology of the domain using the hierarchy property of concepts and relations, and the conceptual operations of the conceptual graph theory. Then the independence between the conceptual navigation strategies and the resources will be stronger, and all the material (ontologies, and resources) more interoperable and reusable."}
{"pdf_id": "0705.1886", "content": "In adaptive hypermedia systems, the aim is to find a compromise between guiding users and letting them browse on their own [4,14,29,32]. These approaches are attempting to find ways of adapting pre-existent hypermedia. They do not aim at the construction of new links and their narrative organization in response to user needs is predefined."}
{"pdf_id": "0705.1886", "content": "The use of metadata to help with information retrieval and to share resources is a well-established practice. It is the basis of search engines such as Yahoo or Alta Vista when using indexes. But the efficacity of this brute force approach for computing similarities beween resources is limited by the biases caused by synonymy and polysemy (see [6] for a good insight into this problem). To avoid this pitfall, there are two possibilities."}
{"pdf_id": "0705.1886", "content": "The first one is to automatically build links under the constraint of an ontology which contains synonyms and relations between words (semantic networks). It is the case of Green [13] who automatically builds similarity links beween resources considering the fact that resources that are about the same thing will tend to use similar (although not necessary the same) words. He makes use of the WordNet database to build synset (sets of synonyms) weight vectors (the counterparts of Karina's conceptual state vectors)."}
{"pdf_id": "0705.1886", "content": "The other possibility is to annotate resources under structural  and  semantical  constraints  to  ensure interoperability [22]. Resource description articulates around complete resources, or parts of resources like in Karina, and makes use of either specific descriptors [2] or descriptors  already  established  as  standards  or recommendations [11,21,17]. The XML (eXtensible Mark-up Language) [3] language allows the description of electronic resources by means of a DTD (Document Type Definition). The use of DTDs for describing Internet resources is a recent yet already well-established practice [19]. [1] proposes a DTD written in XML to describe the content of Audiovideo (AV) archives with meta-data. The"}
{"pdf_id": "0705.1886", "content": "authors also use an ontology to ascertain that several different resources are described with the same vocabulary. Then resource retrieval is based on dynamic linking either by taking an ontology or any resource as a point of entry. As far as only information retrieval is concerned, their approach is close to ours in many ways. We think, however, that the use of conceptual graphs and conceptual state vectors is more fruitful when it comes to building conceptual links. Moreover, our goal is also to build links with narrative commitment, and to comply with constraints, in particular the time constraint."}
{"pdf_id": "0705.1886", "content": "In Karina's approach to conceptual navigation, the time constraint is used in order to prune the space search of related resources and to give a limit to the final delivery. This facility relies on the fact that the initial resources have been indexed with a time value which corresponds to the reading time hypothesized by the person who indexes. But, as [20] puts it, \"reading time is a difficult thing to"}
{"pdf_id": "0705.1886", "content": "ACKNOWLEDGMENTS The Sybil project is sponsored by Digital Equipment, CEC Karlsruhe, Deutschland. The participants are Leidig T., from CEC Karlsruhe, Ranwez S. (main developper), and Crampes M., from Ecole des Mines d'Ales (EMA), France. Karina, was developped under a contract with the French Ministry of Industry. The developpers are"}
{"pdf_id": "0705.1999", "content": "We present a multi-modal action logic with first-order modalities, which con tain terms which can be unified with the terms inside the subsequent formulas and which can be quantified. This makes it possible to handle simultaneously time and states. We discuss applications of this language to action theory where it is possible to express many temporal aspects of actions, as for example, beginning, end, time points, delayed preconditions and results, duration and many others. We present tableaux rules for a decidable fragment of this logic."}
{"pdf_id": "0705.1999", "content": "Most action theories consider actions being specified by their preconditions and their results. The temporal structure of an action system is then defined by the sequence of actions that occur. A world is conceived as a graph of situations where every link from one node to the next node is considered as an action transition. This yields also a temporal structure of the action space, namely sequences of actions can be considered defining sequences of world states. The action occurs instantantly at one moment and its results are true at the \"next\" moment.However, the temporal structure of actions can be much more complex and com plicated."}
{"pdf_id": "0705.1999", "content": "In order to represent complex temporal structures, underlying actions' occurrences,we have developed an action logic which allows to handle both states and time simul tanuously. We want to be able to express, for instance that action a occurs at moment t if conditions p1, ...pn have been true during the intervals i1, ...all preceding t."}
{"pdf_id": "0705.1999", "content": "The soundness proof is easy and the completeness proof goes along the lines of completeness proofs for modal logics by construction of a canonical model. The proof, which can be found in the appendix, bears several modifications according to the specific language which allows to quantify over terms occurring within modal operators."}
{"pdf_id": "0705.1999", "content": "Using Dal , we can modelize temporal aspects of dynamic actions. The modal logic allows to define action operators as modalities [3, 11]. The first order logic is used to formulate actions at a more general level. Here, we show an example where in addition to the relative representation of time by the modal operators, it is possible to express time points by terms."}
{"pdf_id": "0705.1999", "content": "To continue the previous example, the action execution axiom of the move-action is at(t, x, y) [move(t, d, x, y, z)]at(t d, x, z) (and can be instantiated to at(6, T GV, Marseille) [move(6, 3, T GV, Marseille, Paris)]at(9, T GV, Paris)), which means: if x is at y at instance t, then, after moving from y to z, x is at z at instance t+d."}
{"pdf_id": "0705.2011", "content": "Abstract Recurrent neural networks (RNNs) have proved effective at one dimensional sequence learning tasks, such as speech and online handwriting recognition. Some of the properties that make RNNs suitable for such tasks, for example robustnessto input warping, and the ability to access contextual information, are also desir able in multidimensional domains. However, there has so far been no direct wayof applying RNNs to data with more than one spatio-temporal dimension. This pa per introduces multi-dimensional recurrent neural networks (MDRNNs), therebyextending the potential applicability of RNNs to vision, video processing, medi cal imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. Experimental results are provided for two image segmentation tasks."}
{"pdf_id": "0705.2011", "content": "However, multi-dimensional HMMs suffer from two severe drawbacks: (1) the time required to run the Viterbi algorithm, and thereby calculate the optimal state sequences, grows exponentially with the number of data points; (2)the number of transition probabilities, and hence the required memory, grows expo nentially with the data dimensionality"}
{"pdf_id": "0705.2011", "content": "any case the complexity of the algorithm remains linear in the number of data points and the number of parameters, and the number of parameters is independent of the data dimensionality.For a multi-directional MDRNN, the forward and backward passes through an n dimensional sequence can be summarised as follows:"}
{"pdf_id": "0705.2011", "content": "The standard formulation of LSTM is explicitly one-dimensional, since the cell contains a single self connection, whose activation is controlled by a single forget gate. However we can easily extend this to n dimensions by using instead n self connections (one for each of the cell's previous states along every dimension) with n forget gates."}
{"pdf_id": "0705.2011", "content": "We have introduced multi-dimensional recurrent neural networks (MDRNNs), therebyextending the applicabilty of RNNs to n-dimensional data. We have added multidirectional hidden layers that provide the network with access to all contextual in formation, and we have developed a multi-dimensional variant of the Long Short-Term Memory RNN architecture. We have tested MDRNNs on two image segmentation tasks, and found that it was more robust to input warping than a state-of-the-art digit recognition algorithm."}
{"pdf_id": "0705.2106", "content": "Figure 1: Correlations between citations to a journal from Wikipedia and from scientific journals. Kendall's rank correlation (a) and its associated P-value (b) as a function of the number of journals included in the test, e.g., the value at 80 shows the correlation between Wikipedia citations and JCR numbers for the 80 most cited journals from Wikipedia. The number of citations from Wikipedia is compared with three series of numbers from JCR and one derived: The total citations to a journal, its impact factors, the number of articles and the product of the total citations and impact factor."}
{"pdf_id": "0705.2106", "content": "MNRAS  HumImmunol  PHOR  JNeurosci  Gut  JAVMA  NatMed  AnnNeurol  NAR  AmJMed  JClinMicrobio  JMedGenet  JCI  AmJBot  AAC  AnnRevBiochem  GRL  JCO  JVirol  CommACM  AFP  EHP  JAmAcadDerm  BBRC  AngewChemIntEd  AustJBot  Chest  JExpMed  Epilepsia  AJTMH  FEBSL  Chemical Reviews  ArchNeurol  Plant Physiology  JCellBio  Classical and Quantum Gravity  DigDisSci  rag replacements"}
{"pdf_id": "0705.2106", "content": "Figure 2: Comparison between citations from scientific journals and from Wikipedia. Scatter plot with each dot representing the target journal receiving the citations, and with one axis representing the number of citations from Wikipedia and the other the product of two numbers: JCR total citations and impact factor. It indicates the 100 most Wikipedia referenced articles. The plot shows not all journal titles."}
{"pdf_id": "0705.2106", "content": "plate with the database dump for 2 April 2007. The summary statistics for the individual journals with the largest number of inbound citations from Wikipedia showed Nature (787), Science (669) and New England Journal of Medicine (NEJM) (446) on the top (number of citations in parenthesis). A number of astronomy journals received manycitations: The Astrophysical Journal (424), Astronomy & Astrophysics (154), Icarus, In ternational Journal of Solar System Studies (147) and The Astronomical Journal (93). Apart from NEJM other medical journals high on the list included The Lancet (268), JAMA (217), British Medical Journal (187) and Annals of Internal Medicine (104). Some"}
{"pdf_id": "0705.2236", "content": "approximate models of the considered nonlinear system.  Fuzzy rule-based systems with learning ability, also known as neuro-fuzzy networks  [6], will be considered in this work. This system will be referred to as a neuro-fuzzy  system (model) from here onwards. There are two approaches to training neuro-fuzzy  models [7]:"}
{"pdf_id": "0705.2305", "content": "Abstract—The work proposes the application of fuzzy set  theory (FST) to diagnose the condition of high voltage bushings.  The diagnosis uses dissolved gas analysis (DGA) data from  bushings based on IEC60599 and IEEE C57-104 criteria for oil  impregnated paper (OIP) bushings. FST and neural networks  are compared in terms of accuracy and computational efficiency.  Both FST and NN simulations were able to diagnose the  bushings condition with 10% error. By using fuzzy theory, the  maintenance department can classify bushings and know the  extent of degradation in the component."}
{"pdf_id": "0705.2305", "content": "Fuzzy set theory is used to explore the interrelation between  each bushing's identifying attributes, i.e. the dissolved gases  in oil. In dissolved gas analysis (DGA) there is a relation  between consequent failure and the simultaneous presence of  oxygen with a secondary gas such as hydrogen, methane,  ethane, ethylene, acetylene, and carbon monoxide in a  bushing. The presence of combustible gasses in the absence of"}
{"pdf_id": "0705.2305", "content": "A. Identifying Attributes  In this study ten identifying attributes were selected to  develop membership functions. These are concentrations of  hydrogen, oxygen, nitrogen, methane, carbon monoxide,  carbon dioxide, ethylene, ethane, acetylene and total  dissolved combustibles gases. The concentrations are in parts  per million (ppm). IEC60599 and IEEE C57-104 criteria were  used in decision making.  TABLE I  PROPERTIES OF BUSHING OIL  Property  Magnitude"}
{"pdf_id": "0705.2305", "content": "E. Consequence or Decision Table  Based on the rules the bushing is given a risk rating for  which certain maintenance actions must be taken on the plant.  For safe operation of bushings it is recommended that all HR  cases, trip the transformer and remove the bushing from the  transformer. For all MR cases monitor the bushings more  frequently, i.e. reduce the sampling interval by half. All LR  cases operate as normal. From the decision table an  aggregated membership is developed, shown in Equations 34  and 35"}
{"pdf_id": "0705.2305", "content": "FST was applied to ten bushings. The fuzzy rules were  applied to each bushing. For each rule, the truth value of the  consequence is the minimum membership value of the  antecedent. The degrees of membership of the other gases are  shown in Table 4."}
{"pdf_id": "0705.2305", "content": "Once all the rules have been applied to a particular bushing,  and different truth values of each consequence obtained, the  maximum value of each consequence among all the rules that  result in that consequence, is taken as the degree to which that  consequence applies to a given bushing. This eventually gives  rise to an aggregated fuzzy output as shown in Table 5 and  Equation 37."}
{"pdf_id": "0705.2305", "content": "Where  AGDi is the aggregated decision for category i, e.g. group  HR, CARi is the consequence of aggregated rules in a  particular category i, in a certain compartment. i is the number  of categories, in this case the categories are HR, MR and LR.  TABLE V  AGGREGATED OUTPUT FOR BUSHING #200323106"}
{"pdf_id": "0705.2305", "content": "B. Defuzzification  Defuzzification is aimed at converting fuzzy information  into crisp data. The method used for defuzzification in this  case is called the weighted average of maximum values of  membership functions method used by Siler [12] and Majozi  [5]. The method was selected because it is effective and  computationally inexpensive. The result from the application  of this method gives the rank or level of risk of each bushing.  For bushing #200323106 with an aggregated output is shown  in Table 6, the rank is obtained using Equation (38). Figure 2  shows the aggregated membership function from which the  values for Equation (38) are taken."}
{"pdf_id": "0705.2305", "content": "The coefficients appearing in Equation 38 are the levels of  risk of failure corresponding to the maximum values, i.e. 1, of  the respective sets as shown in the conclusion table, for  example a risk of rating of 60 corresponds with the maximum  value of the membership function of set B. In case there is a  flat, as in the set A membership function as well as set C  membership function, an average value of the extreme values  at the maximum is used as a coefficient, e.g. (80+100). Thus  the solution to (38) is shown in (39)."}
{"pdf_id": "0705.2305", "content": "perceptron with 7 hidden neurons, as done previously by  Dhlamini and Marwala [11]. The manual method used an  experienced maintenance operator, who is supposed to be  100% accurate. The results prove that NN and neuro-fuzzy  have similar levels of accuracy (90%). While the purely fuzzy  method showed 100% accuracy, NN are fast and efficient,  taking 1.35s to train and classify the data compared to 30  minutes for the fuzzy set system and the neuro-fuzzy system,  compared to 5 minutes for the manual method of classification  of 10 bushings.  TABLE VI  CLASSIFICATION OF BUSHINGS"}
{"pdf_id": "0705.2310", "content": "interpreting data from dissolve gas-in-oil analysis  (DGA) test. The methods use machine learning  classifiers multi-layer perceptrons (MLP), radial basis  functions (RBF) and support vector machines (SVM).  These methods are compared and the most effective  method is implemented within the on-line framework.  The justification for an on-line implementation is  based on the fact that training data become available  in small batches and that some new conditions only  appear in subsequent data collection stage and  therefore there is a need to update the classifier in an  incremental fashion without compromising on the  classification performance of the previous data."}
{"pdf_id": "0705.2310", "content": "7.1 Dissolve gas analysis (DGA)  DGA is the most commonly used diagnostic  technique for transformers and bushings [4][5]. DGA  is used to detect oil breakdown, moisture presence  and PD activity. Fault gases are produced by  degradation of transformer and bushing oil and solid  insulation such as paper and pressboard, which are all  made of cellulose [6]. The gases produced from the"}
{"pdf_id": "0705.2310", "content": "7.2.2 Radial basis function  RBFs are type feed-forward neural networks  employing a hidden layer of radial units and an output  layer of linear units [10]. In RBF, the distance  between the input vector and output vector determines  the activation function [10]. RBF have their roots in  techniques of performing exact interpolation of a set  of data points in a multi-dimensional space. This  interpolation requires that every input target be  mapped exactly onto corresponding target vector.  Fig.2 shows the architecture of RBF with four input  layer neurons, five hidden layer neurons and two  output layer neurons."}
{"pdf_id": "0705.2310", "content": "8 Proposed frameworks  The proposed frameworks for fault diagnosis are a  two-level implementation. The first level of the  diagnosis identifies if the bushing is faulty or not. If  the bushing is faulty, the second level determines the  types of faults, which are thermal fault, PD faults and  faults caused by an unknown source. Generally, the  procedure of fault diagnosis includes three steps,  extracting feature and data pre-processing, training  the classifiers and identifying transformer fault with  the trained classifiers. Fig.4 shows the block diagram  of the proposed methodology."}
{"pdf_id": "0705.2310", "content": "The table compares the framework in terms of  accuracy, training and testing time. MLP classifier  shows classification accuracy of 98.9%, RBF shows  97.4% and SVM gives 98.5% classification accuracy.  This table shows that there is no significant difference  between SVM and MLP classifiers. Although, RBF  performs worse than MLP and SVM in terms of"}
{"pdf_id": "0705.2310", "content": "classification accuracy, it trains faster while SVM is  computationally most expensive.  Table 2 compares the results of the networks  designed in terms of accuracy, training time and  testing time to classify bushing conditions into  thermal fault, PD faults and faults caused by an  unknown source bushing faults and this is called  second level classification. This table shows that the  MLP classifier gives 98.62% classification accuracy  while RBF and SVM classifier give 81.73% and  96.9%, respectively. In the second level classification,  the MLP classifier performs better than the RBF and  SVM.  Table 2: Comparison of the performance of different  frameworks for second level of fault diagnosis  MLP  RBF  SVM"}
{"pdf_id": "0705.2310", "content": "If the error is greater than 0.5, the current hypothesis  is discarded and the new training and testing data are  selected according to the distribution DT. Otherwise,  if the error is less than 0.5, the normalized error of the  composite hypothesis is computed as:"}
{"pdf_id": "0705.2310", "content": "The error is used in the distribution update rule,  where the weights of the correctly classified instances  are reduced, consequently increasing the weights of  the misclassified instances. This ensures that  instances that were misclassified by the current  hypothesis have a higher probability of being selected  for the subsequent training set. The distribution  update rule is given by"}
{"pdf_id": "0705.2310", "content": "4.2.Confidence measurement  A simple procedure is used to determine the  confidence of the algorithm on its own decision. A  vast majority of hypothesis agreeing on a given  instances can be interpreted as an algorithm having  confidence on the decision. Let us assume that a total  of T hypothesis are generated in k training sessions  for a C-class problem. For any given example, the  final classification class, if the total vote class c  receives is given by [21][22]:"}
{"pdf_id": "0705.2310", "content": "The data of unknown fault were introduced in  training session three. In each training session,  Learn++ was provided with each database and 20  hypotheses were generated. The last row of Table 3  shows that the classifiers performances increase from  60% to 95.3% as new classes were introduced in the  subsequent training datasets. Table 5 shows the  training and testing performance of the algorithm as  new conditions are introduced. Table 3: Performance of Learn++ for first level on line condition monitoring, key: S =databases.  Dataset  S1  S2  S3  S4  S5"}
{"pdf_id": "0705.2310", "content": "Fig.6. Performance of Learn++ on testing data  against the number of databases  The final experiment addressed the problem of  bushing condition monitoring using MLP network  trained using batch learning. This was done to  compare the classification rate of Learn++ with that  of an MLP."}
{"pdf_id": "0705.3360", "content": "This paper overviews the basic principles and recent advances in the emerging field of  Quantum Computation (QC), highlighting its potential application to Artificial Intelligence  (AI). The paper provides a very brief introduction to basic QC issues like quantum registers,  quantum gates and quantum algorithms and then it presents references, ideas and research  guidelines on how QC can be used to deal with some basic AI problems, such as search and  pattern matching, as soon as quantum computers become widely available.  Keywords: Quantum Computation, Artificial Intelligence"}
{"pdf_id": "0705.3360", "content": "Quantum systems are able to simultaneously occupy different quantum states. This is  known as a superposition of states. In fact, the state of Eq.1 for the qubit and the state  of Eq.2 for the quantum register represent superpositions of the basis states over the  same set of qubits. A quantum register can be in a superposition of two or more basis  states (with a maximum of 2n, where n is the number of its qubits). The qubits of the"}
{"pdf_id": "0705.3360", "content": "Quantum systems in superposition or entangled states are said to be coherent. This is  a very fragile condition and can be easily disturbed by interaction with the  environment (which is considered an act of measurement). Such an accidental  disturbance is called decoherence and results to losing information to the  environment. Keeping a quantum register coherent is very difficult, especially if its  size is large."}
{"pdf_id": "0705.3360", "content": "Higher order quantum computation machines can be devised based on quantum  registers: for instance quantum finite state automata can be produced by extending  probabilistic finite-state automata in the quantum domain. Analogous extensions can  be performed for other similar state machines (e.g. quantum cellular automata,  quantum Turing machines, etc) [Gruska (1999)]. Regardless the machine, the"}
{"pdf_id": "0705.3360", "content": "Quantum gates are the basic computation components for QC. They are very different  from gates in classical computation systems. Quantum gates are not circuits with  input and output; they are operators over a quantum register. These operators are  always reversible; most of them originate from reversible computation theory."}
{"pdf_id": "0705.3360", "content": "•  Parallel Computation: Thought not exactly an algorithm, the intrinsic  property of quantum registers to support massively parallel computation is  mentioned due to its use in almost every quantum algorithm. When a  transformation is performed to the contents of a quantum register this affects the whole set of its superimposed values. Reading the outcome is a non deterministic process, but it is possible to maximize the probability to occur"}
{"pdf_id": "0705.3360", "content": "•  Quantum Fourier Transform (QFT): A basic subroutine in many specialized  algorithms concerning factoring prime numbers and simulating actual  quantum systems. QFT is a unitary operation acting on vectors in the Hilbert  space. By altering their phases and probability amplitudes it can reveal  periodicity in functions just like its classical analog [Coppersmith (1994)]."}
{"pdf_id": "0705.3360", "content": "One of the first contributions that QC offers to AI is the production of truly random  numbers. True randomness has been reported to cause measurable performance  improvement to genetic programming and other automatic program induction  methods [Rylander et al. (2001)]. Monte-Carlo, simulated annealing, random walks  and other analogous search methods are expected to benefit from that as well. A truly  random number of N bits can be produced by applying the Hadamard transformation  to a N-qubit quantum register thus producing the superposition of all basis states"}
{"pdf_id": "0705.3360", "content": "However, random search methods in QC indicate a completely different approach  than in classical computation. The quantum analog of a classical random walk on a  graph, i.e. the quantum random walk, even in one dimension is a much more powerful  computational model [Ben-Avraham et al. (2004)]. While the classical random walk  is essentially a Markov process, in a quantum random walk propagation between node  pairs is exponentially faster, thus enabling the solution of NP-complete problems as  well [Childs et al. (2002)]. Moreover, as mentioned by [Shor (2004)], combinations  of quantum random walks with Grover's algorithm have managed to confront  efficiently some real-world problems like database element comparison and dense  graph search [Childs et al. (2003)]."}
{"pdf_id": "0705.3360", "content": "Grover's algorithm [Grover (1997)] and its variations are ideal for efficient content addressable search and information retrieval from large collections of raw data. The  principle of probability amplitude amplification that guides these processes can be  relaxed for approximate pattern matching as well, thus facilitating applications like  face, fingerprint, and voice recognition, corpus search, and data-mining. A quantum  register containing a set of data in superposition can be seen as the quantum analog of  a Hopfield neural network used as an associative memory [Trugenberger (2002)] only  with much greater capacity to store patterns: while the capacity of a n-neuron  Hopfield network approximates to 0.14n patterns, a quantum register of n-qubits can  store 2n binary patterns."}
{"pdf_id": "0705.3360", "content": "Fortunately, for problems  where a previous approach based on genetic algorithms is available, there is a  significant basis for QC as well: the representation of the gene-string can be  transferred to the quantum implementation almost verbatim and the whole gene pool  can be superimposed to a single quantum register"}
{"pdf_id": "0705.3360", "content": "Game theory and decision-making have also been addressed by QC. A new field of  quantum game theory has emerged [Piotrowski & Sladkowski (2004a)] with  promising applications at least to playing market games [Piotrowski & Sladkowski  (2004b)]. The entanglement effect has been exploited to improve behavior in"}
{"pdf_id": "0705.3466", "content": "model must provide institutional and funding agency policies that not only recommend or require open access publication, but also provide funds earmarked for this purpose. It may even be preferable to use libraries and/or some other external infrastructure to pay these costs, so that authors need not worry about new details."}
{"pdf_id": "0705.3466", "content": "[1] http://public.web.cern.ch/press/PressReleases/Releases2006/PR16.06E.html [2] A nice Timeline of the Open Access movement can be found at http://www.earlham.edu/ peters/fos/timeline.htm [3] Note that other definitions exist, and Open Access has wide range of voices. See, for example, http://www.plos.org/oa/definition.html http://www.eprints.org/openaccess/ http://www.earlham.edu/ peters/fos/ [4] http://www.arl.org/stats/arlstat/graphs/2004/monser04.pdf [pdf file] [5] For example: P. Suber, College Research Libraries News, 64 (February 2003) pp. 92-94, 113 [http://www.earlham.edu/ peters/writing/acrl.htm] [6] S. Harnad, et al. Nature Web Focus, Access Debate. http://www.nature.com/nature/focus/accessdebate/21.html [7] http://www.eprints.org/openaccess/self-faq/ [8] Before the electronic era, a similar culture existed around paper preprints, with SPIRES serving as the unifying catalog. [9] SPIRES data"}
{"pdf_id": "0705.3466", "content": "[10] With the exception of volunteer referees, there is no other funding source for most existing peer review. [11] http://prst-ab.aps.org/help/sponsors.html [12] \"Electronic Scientific, Technical, and Medical Journal Publishing and Its Implications:Report of a Symposium\" 2004, National Academies Press [http://www.nap.edu/catalog/10969.html] [13] For example http://www.ein.net/[14] SPIRES data - Over 90% of published, non-conference, particle physics literature in 2005 was theoretical or phenomeno logical. Conferences tend to have more experimental work, but are still theory dominated. [15] http://cdsweb.cern.ch/record/1020110 [16] http://open-access.web.cern.ch/Open-Access/,http://open-access.web.cern.ch/Open-Access/SCOAP3WPReport.pdf and http://indico.cern.ch/conferenceDisplay.py?confId=7168 [17] S. Mele et. al. JHEP12(2006)S01 [cs.DL/0611130]"}
{"pdf_id": "0705.3593", "content": "Abstract. Subtraction of aligned images is a means to assess changes in a wide variety of clinical applications. In this paper we explore the information theoretical origin of Mutual Information (MI), which is based on Shannon's entropy. However, the interpretation of standard MI registration as a communication channel suggests that MI is too restrictivea criterion. In this paper the concept of Mutual Information (MI) is extended to (Nor malized) Focussed Mutual Information (FMI) to incorporate prior knowledge to overcome some shortcomings of MI. We use this to develop new methodologies to successfully address specific registration problems, the follow-up of dental restorations, cephalometry, and the monitoring of implants. Keywords: image registration, registration criteria, information theory, entropy, mutual information, piecewise rigid, prior knowledge, dentistry, cephalometry, implants, digital subtraction radiography."}
{"pdf_id": "0705.3593", "content": "In Section 2 image registration, the alignment of images, is formally defined. Intrinsicregistration methods are introduced in Section 3, joint entropy of images in Section 4. In formation theory [18] is brieny presented in Section 5. In Section 6 mutual informationbased registration is placed in this information theoretical context, and extended to incor porate prior knowledge. In Section 7 we use this extension to develop new methodologies to successfully address specific registration problems, the follow-up of dental restorations, cephalometry, and the monitoring of mandibular implants. The same ideas can be used for registration of 3D images; currently we are developing software and test strategies for hip-, knee-, and shoulder implants. We do not address issues of medical interpretation and diagnosis."}
{"pdf_id": "0705.3593", "content": "In Maintz and Viergever [12] a classification of registration methods is introduced. Theycall a method \"intrinsic\" when it relies only on patient generated image content, and \"ex trinsic\" when objects foreign to the patient are introduced into the scene of which an image is taken to serve as reference to the alignment process. The intrinsic methods are split into landmark based, segmentation based, and voxel/pixel property based registration methods. In landmark based and segmentation based registration corresponding structures are indicated or extracted from reference and test image, to be used pairwise as input for the alignment procedure. A voxel/pixel property based registration criterion is a criterion directly linked to the discrete two-dimensional gray value maps (3.1)."}
{"pdf_id": "0705.3593", "content": "Let us try to understand the requirements that define H. The first requirement is conti nuity: there is no clear reason to introduce \"jumps\". The continuity requirement does not seem to be too restrictive. The second requirement states that if the number of possible outcomes increases, and if all outcomes are equally probable, the uncertainty about the"}
{"pdf_id": "0705.3593", "content": "• Consider the test image to be the transmitted signal. • Take the reference image to be the received signal. • The communication channel is determined by the registration parameters. • Optimizing the mutual information between the signals is equivalent to the design of an optimal communication channel.• Both images are assumed to represent the same scene, and their multi-modal dif ferences are considered a noise generated by the communication channel."}
{"pdf_id": "0705.3593", "content": "In this section, we will introduce methodologies involving FMI and Digital SubtractionRadiography (DSR), tailored to specific clinical applications. Each of the proposed regis tration methods will be a hybrid form between a landmark/segmentation and a pixel/voxel based method. Anatomical structures, present in reference and test image, will be used todefine a probability distribution f on the reference image incorporating the prior knowl edge of the problem. The trace distributions fT of the probability distribution f on the"}
{"pdf_id": "0705.3593", "content": "(1). Find (all) edges in the reference image by: • median filtering to eliminate \"pepper and salt\" noise from the reference image. • computation of the modulus of the gradient. • convolution with a Gaussian kernel. This results in Fig. 4 left. (2). Find a patch that contains the whole restoration: • segmentation using a threshold to select the restoration. • morphological closing and dilation."}
{"pdf_id": "0705.3593", "content": "FMI registration using this focus distribution results in Fig. 5 right, showing a well aligned restoration. One can think of first creating the patch selecting a part of the image containing the restoration, followed by edge detection and convolution. Working in this order we may easily create spurious edges due to the border of the indicator of the patch."}
{"pdf_id": "0705.3593", "content": "As a case study we applied FMI registration to an example of false maxillary prog nathism. A lack of growth of the mandible is corrected by means of a combined surgical and orthodontic treatment, where the mandibular has been advanced. A lateral radiograph is taken before treatment (Fig. 6 left), and a follow up lateral radiograph is taken two years after treatment (Fig. 6 right). The purpose of the images is the evaluation of skeletal stability, and orthodontic treatment."}
{"pdf_id": "0705.3593", "content": "In the aligning process of the lateral radiographs of the skull the input of the practitioner can easily be reduced or removed. The detection of the edges delineating the front and back of the skull can be fully automated and used as the input for the FMI registration of the lateral radiographs. Another line of thought is to use automatically detected landmarks in the reference image as prior knowledge to construct a focus distribution. The automaticdetection of cephalometric anatomical landmarks is promising e.g. [2] and [16]. In combi nation with the reduced need for accuracy of the localization of landmarks in a FMI they can provide the basis for a successful automated FMI registration algorithm."}
{"pdf_id": "0705.3593", "content": "An even more challenging application is the use of registration of lateral images of theskull in treatment planning. Crucial in the decision to start the orthodontic and/or oper ative treatment of an adolescent is the detection of the end-of-puberty growth sprint. Forcharacterizing the growth curve we plan to study the evolution of the registration parame ters, more precise, the scaling needed to adjust consecutive images of the skull."}
{"pdf_id": "0705.3593", "content": "(1). Find (all) edges in the reference image by: • median filtering to eliminate \"pepper and salt\" noise from the reference image. • computation of the modulus of the gradient. • convolution with a Gaussian kernel. This results in an edge distribution focussing all the edges. (2). Find the complement of a patch covering the implant: • segmentation using a threshold to select the implant. • morphological closing and dilation. • creation of an indicator of the complement of the patch covering the implant. (3). Create the focus distribution: • multiply the patch from step (2) and edge distribution produced in step (1)."}
{"pdf_id": "0705.3593", "content": "Only edges corresponding to structures not related to the implant will contribute to the FMI registration. The reason to focus on the bone structure is that it becomes easy to measure the movement of the implants when the bone structure is well aligned. In the case of dental implants the opposite procedure is more appropriate. It is better to register the implant and evaluate the evolution of the surrounding bone tissue. 3D-2D projections will make displacement measurements unreliable."}
{"pdf_id": "0705.3593", "content": "• convolution with a Gaussian kernel. This results in an edge distribution focussing all the edges (Fig. 9 right). (2). Find a patch covering the implant: • segmentation using a threshold (Fig. 10 left). • morphological closing and dilation (Fig. 10 right). (3). Create the focus distribution: • multiply the patch from step (2) and edge distribution produced in step (1)."}
{"pdf_id": "0705.3593", "content": "In this paper we have explored Mutual Information as registration criterion from itsinformation theoretical origin. The parallelism put forward by Collignon [3] between im age registration and the model of a communication channel remains unsatisfactory. The validity of MI cannot be explained from information theory. Hughes and Daubechies [4] identify fundamental properties of MI in the framework of multi-modal image registration, to introduce simpler alternative similarity measures (distance metric between equivalence"}
{"pdf_id": "0705.3593", "content": "implants are simply connected objects in the scene with a maximal radio-opacity consti tute the prior knowledge. Both applications are handled in a fully automated procedure in which the focus is derived from the image representing the modulus of the gradient. In the first case the object of the study is the movement of the implant due to aseptic loosening, which requires focussing on the bone, and therefore, removing the implant from the focus. In the second case the object of the study is the evolution of the bone tissue surrounding an implant and therefore, focus is put on the implant."}
{"pdf_id": "0705.4302", "content": "Applying a cluster algorithm to a dataset results in—fuzzy or crisp—assignments of cases to anonymous clusters. In order to interpret these clusters, we often wish to compare these clusters to other classifications, so some heuristic is needed to match one classification to another. With the advent of resampling and ensemble methods in clustering (Gordon and Vichi, 2001; Dimitriadou et al., 2002; Strehl and Ghosh, 2002), the task of matching cluster solutions has become even more important: we need reliable and scalable matching algorithms that do the task fully automated."}
{"pdf_id": "0705.4302", "content": "Consider, for example, the use of bootstrapping or cross-validation for cluster validation as suggested by many authors (Moreau and Jain, 1987; Jain and Moreau, 1988; Tibshirani et al., 2001; Roth et al., 2002; Ben-Hur et al., 2002; Dudoit and Fridlyand, 2002): many cluster solutions are created and agreement between them is evaluated. Some agreement indices do not need explicit cluster matching (Rand, 1971; Hubert and Arabie, 1985), but others can only be applied after cluster solutions have been matched, for example, Cohen's kappa (1960)."}
{"pdf_id": "0705.4302", "content": "Recently, authors have suggested transfering the idea of bagging (Breiman, 1996) to clustering. Some approaches aggregate cluster centers (Leisch, 1999; Dolnicar and Leisch, 2000; Bakker and Heskes, 2001) or aggregate consensus between pairs of observations (Montiet al., 2003; Dudoit and Fridlyand, 2003, BagClust2 algorithm). Other approaches aggre gate cluster assignments and, therefore, require cluster matching, for example, the crisp"}
{"pdf_id": "0705.4302", "content": "For example, Dimitriadou et al. (2002) suggested a recursive heuristic to approximate trace maximization. It is known that trying all permutations has time complexity O(K!), where K denotes the number of clusters. The Hungarian method improves on this and achieves polynomial time complexity O(K3).Kuhn (1955) published a pencil and pa per version, which was followed by J.R. Munkres' executable version (Munkres, 1957) andextended to non-square matrices by Bourgeois and Lassalle (1971). For a list of further al gorithmic approaches to this so-called linear sum assignment problem or weighted bipartite matching, see Hornik (2005)."}
{"pdf_id": "0705.4302", "content": "However, scalablility is not the only quality aspect of a matching algorithm. An impor tant statistical feature of a matching algorithm is the following: if we match two random partitions, the matching algorithm should not systematically align the two partitions. We now show that the classic trace maximization does not generally possess this feature."}
{"pdf_id": "0705.4302", "content": "In order to cope with unequal cluster sizes, we suggest basing cluster matching on maximizing the trace of sk,l rather than on maximizing the trace of nk,l. And in order to avoid any systematic not based on the data, we add a probabilistic component to the matching algorithm. Consequently we define the truematch algorithm as:"}
{"pdf_id": "0705.4302", "content": "single 100 theoretical values for single group (no cluster) random 50:50 random clustering with 2 equal sized clusters random 99:1 random clustering 2 unequal sized clusters random 50:49:1 random clustering with 3 unequal sized clusters justified 50:50 justified clustering with 2 equal sized cluster justified 50 random 49:1 2 justified clusters, one randomly split unequal sized"}
{"pdf_id": "0705.4566", "content": "For each cavity distribution Dj and mj the number of pairs of equations is equal to the number of variables in the cavity set. Thus, given a covariance matrix A, the diagonals D can be determined with the second equation, and subsequently the average values m can be determined with the first equation. The marginal distributions then follow directly, since all variables are now known. Substituting (11) into (9), we find"}
{"pdf_id": "0705.4566", "content": "i.e. this is the average of variable l on the graph without i, which may be obtained by running BP on the graph without variable i. Thus by running BP on the original graph once and running it on the graph without i, we can calculate v LC by using equation (25) and writing"}
{"pdf_id": "0705.4566", "content": "These equations suggest inverting matrices by calculating correlation matrices on growing graphs might be a useful application. By subsequently attaching new variables to the graph and running BP, one finds the full correlation matrix with N runs of BP, just as with the procedure described in [1], but the cost of the BP runs is halved since the graph is growing along with the BP runs. However, we should not overlook the fact that the equations above introduce large number of additions and multiplications, such that in the end the total computational complexity for inverting a sparse matrix is similar to other well-known methods."}
{"pdf_id": "0705.4566", "content": "Inspired by the above observations regarding the optimization of the marginal moments of the target approximation, one may derive alternative consistency equations as in [7], starting from the expressions for the actual marginals, such that the integrations include full sets of neighboring factors. Once again, we approximate the cavity distributions by Gaussians, and find"}
{"pdf_id": "0705.4566", "content": "interaction matrix with the rest of the model. However, the benefit of full Gaussian EP is that this Gaussian interaction matrix is optimized on the way, albeit at the cost of an inversion at each iteration, while the loop corrected approach desires an estimate of Ai as input, which is not further updated.Thus loop corrections are an alternative for the current type of model only if these inver sions are so costly that approximations of the above form are sensible."}
{"pdf_id": "0705.4606", "content": "This paper is organized as follows. In Section (2) we give a brief review of the state of the art methods more relevant to our setting, while a more extended survey is postponed in thefull paper. In Section (3) we review known properties of the cosine similarity/distance met ric. In Section (4) we show the main theoretical analysis underpinning our weight embedding technique. In Section (5) we describe and compare the algorithm that uses our new weightembedding scheme, and the scheme proposed in [18]. In Section (6) we describe how the out put quality is measured. In Section (7) we give the experimental set up and the experimental results. Conclusions and future work are in Section (8)."}
{"pdf_id": "0705.4606", "content": "There is a vast literature on similarity searching and k-nearest neighbor problems (see extended surveys in [16, 2]). However, much less is known for the case when users are allowed to change the underlying metric dynamically at query time. Besides the work of [18] we mention work by P. Ciaccia and M. Patella [4] discussing which general relations should hold between two metrics A and B, that allow to build a data structure using the first metric (A), but perform searches according to the second one (B). A series of papers by R. Fagin and co-authors [6, 8, 10, 9] deal with the problem of rank score aggregation in a general setting in which items are ranked independently according to several"}
{"pdf_id": "0705.4606", "content": "The discussion in Section (4) shows that the pre-processing can be done independently of the user provided weights and that any distance based clustering scheme can be used in principle. Weights are used to modify directly the input query point and are relevant only for the query procedure. The basic clustering algorithm we use is described in detail in [11]. It is an algorithm based on the further-point-first (FPF) heuristic for the k-center problem that was proposed by [15]. Summarizing, to produce K clusters we start by taking a sample of"}
{"pdf_id": "0705.4606", "content": "A) The CellDec algorithm described in [18] with k-means clustering and weighted cosine dis tance. B) The algorithm proposed in [3] based on random cluster algorithm and weighted cosine distance, christened PODS07 for lack of a better name. C) The algorithm proposed here based on the furthest point first algorithm and weighted cosine distance (referred to as Our)."}
{"pdf_id": "0705.4606", "content": "Fig. 2. Recall of 10 nearest neighbors as a function of query time. Each point in the graph is the average of measurements of all queries for a class of weights and a number of visited clusters. The points in the upper left corner of the graphs corresponding to our algorithm show clear dominance."}
{"pdf_id": "0706.0022", "content": "Currently, the Semantic Web is perceived primarily asa data modeling environment where data is more \"de scriptive\" rather than \"procedural\" in nature [17]. In other words, the triples in G define a model, not the rules by which that model should evolve. This article will explore the more procedural aspects of G. Figure 1presents an taxonomy of the various types of triples con tained in G, where edges have the semantic \"composed of\"."}
{"pdf_id": "0706.0022", "content": "The classic notion of a computation is any process that can be explicitly represented by a formal algorithm. Analgorithm is a sequence of executable, well-defined in structions [19]. This sequence of instructions is executed by some system, or machine.This machine may contain, internal to it, all the requirements necessary to ren"}
{"pdf_id": "0706.0022", "content": "Perhaps the most common model used to represent computing is the Turing machine [20]. In the Turing machine model of computation, M is a machine with a single read/write head and D is a storage medium called a \"tape\" that can be read from and written to by M. A Turing machine can be formalized by the 5-tuple"}
{"pdf_id": "0706.0022", "content": "Imagine having a single physical machine for every computation one required to execute. For instance, onewould have an M to add integers, an M to divide noating points, an M to compare a string of characters, etc.To meet modern computing requirements, an unimag inable number of machines would be required. However, in fact, a single machine does exist for each computing need! Fortunately, these machines need not be physically represented, but instead can be virtually represented in D. This is the concept of the stored program and wasserendipitously discovered by Alan Turing when he de veloped the idea of the universal Turing machine [20]."}
{"pdf_id": "0706.0022", "content": "As demonstrated by Alan Turing, the most primi tive components required for a computing machine are the ability to read and write to a medium and alter itsstates according to its perception of that medium. Similar to the relationship between M and D, it is possi ble to develop a semantic Turing machine that is able to read/write to G and evolve its state behavior accordingly. A semantic Turing machine is denoted S and can be formalized by the 5-tuple"}
{"pdf_id": "0706.0022", "content": "It is no large conceptual leap to actually encode SPARQL queries in RDF and therefore, in G. In fact, the semantic network data structure is an ideal mediumfor many types of information encodings due to its generalized network nature that naturally supports the expression of trees, lists, graphs, tables, etc. The next sub section will discuss such stored programs."}
{"pdf_id": "0706.0300", "content": "problem. The target image represents the destination of the  optimisation. The 4 parameters, namely scale, rotation,  x-translation and y-translation provide a transformation  between the reference image and the target image. The  transformation image represents the reference image, after it  has been transformed with the optimized parameters. Table I  shows a summary of the parameters found using the GA."}
{"pdf_id": "0706.0300", "content": "C. Image Subtraction  After the images all aligned the ventilation and perfusion  images are subtracted. The algorithm subtracts the ventilation  image from the perfusion image, areas with intensity values  less than 0 indicate that there is more ventilation than  perfusion in that specific area. The severity of the defect can  then be quantified by taking a magnitude of pixel intensity in  the subtraction image."}
{"pdf_id": "0706.0300", "content": "D. Feature Extraction  PCA (principle component analysis) was performed on the  images, from 16x16 to 64x64. As the image size gets smaller,  for the same retained variability (VR), the number of required  eigenvectors decreases. Conversely, for the same number of  eigenvectors,  the  retained  variability  increases  by  approximately 10% for every half reduction in image size.  This trend is most likely caused by a certain amount of  variability being lost when reducing the image size."}
{"pdf_id": "0706.0300", "content": "The VR, chosen during the PCA analysis is a parameter which  was varied. A steep increase in training performance is gained  between a VR of 70% and 75%. There also appears to be a  gradual increase in validation performance with increasing  VR. Validation performance also increased with input size."}
{"pdf_id": "0706.0300", "content": "I would like to thank the staff of the Chris Hani Baragwanath  Hospital, Johannesburg General Hospital and the Donald  Gordon Medical Centre for their assistance in obtaining the  imaging data. A special thanks must go to Dr Carlos Liebhabe.  This work was supported by DENEL and the Ledger Project."}
{"pdf_id": "0706.0306", "content": "The prototype of a worknow system for the submission of content to a digital object repository is here presented. It is based entirely on open-source standard components and features a service-oriented architecture. The front-end consists of Java Business Process Management (jBPM), Java Server Faces (JSF), andJava Server Pages (JSP). A Fedora Repository and a mySQL data base manage ment system serve as a back-end. The communication between front-end and back-end uses a SOAP minimal binding stub. We describe the design principles and the construction of the prototype and discuss the possibilities and limitations of worknow creation by administrators. The code of the prototype is open-source and can be retrieved in the project escipub at http://sourceforge.net."}
{"pdf_id": "0706.0306", "content": "This work has been inspired by the eSciDoc project of the Max-Planck-Society [7]. One of the goals of the eSciDoc project is the creation of a publication management service that allows scientific organizations to establish an institutional repository. Generally speaking, the publication process goes like this. Publications, consisting of a set of metadata and a number of content files, are submitted to a digital repository and are made publicly available following the philosophy of open access. Once publications are available they can be retrieved by a so-called persistent identifier. The organization that"}
{"pdf_id": "0706.0306", "content": "The user interface is implemented using Java Server Faces (JSF) (MyFaces cf. http://myfaces.apache.org). JSF is a framework by Sun for the implementation of web appli cations. MyFaces is the first open-source implementation of JSF. JSF is made for processing user interactions. Its interfaces are made of elements having a state. The states of elements and events can be supervised by the JSF-instance. The tag libraries of JSF can be used in Java Server Pages (JSP). JSF runs as a servlet on the Tomcat servlet container."}
{"pdf_id": "0706.0306", "content": "The open-source data base management system MySQL1 is used for JBoss jBPM and the Fedora Repository.For accessing the SOAP-interface, the Apache Axis-library (Apache eXtensible Interac tion System, cf. http://ws.apache.org/axis/) is used. Axis is a SOAP-engine for the construction of web services and clients.XML-documents are constructed and accessed with the Document Object Model (DOM) library of the World Wide Web Consortium (W3C) (cf. http://www.w3.org/DOM/). The component library Apache Tomahawk is an extension of the MyFaces-implementation and is used for making Java Bean attributes persistent (cf. http://myfaces.apache.org/ tomahawk/index.html)."}
{"pdf_id": "0706.0306", "content": "If programs want to use the SOAP-interface of the Fedora server, the generic data types of Fedora must be known in the runtime environment of the client program. To achieve this, there are two possibilities: include all Java classes of the Fedora implementation as source files or a jar-file, or include a minimal binding stub. Such a binding stub contains only those"}
{"pdf_id": "0706.0306", "content": "One of the roles in our submission process is that of the author. He submits new content to the digital object repository. The workspace of the author (home_author.jsp) contains three areas: \"Task-List\", \"Start New Publication Process\", and an overview of all articles of this author in the repository (cf. figure 3). This section describes the mechanisms for addressing Fedora in the context of jBPM and JSF."}
{"pdf_id": "0706.0306", "content": "It has the scope \"Request\" meaning that this bean is initialized for each request. The JbpmContextFilter and the constructor of the HomeAuthorBean ensure that the correct user- and jBPM-context-information is contained in the bean when the method is called by home_author.jsp. Using the class org.jbpm.db.TaskMgmtSession, the function TaskAuthorBean.getTaskInstances can access the method findTaskInstances, which returns all open tasks of an actor, directly: taskMgmtSession.findTaskInstances(userBean.getUserName());"}
{"pdf_id": "0706.0306", "content": "The newly created object of type javax.xml.namespace.QName.QName represents a Qualified Name, which is connected to the namespace-URI of the Fedora-API. Thisqualified name contains the names of the SOAP-operation (\"ingest\"). By using meth ods setTargetEndpointAddress and setUsername the service-endpoint of the Fedora server and the credentials for authentification are set. The call is now finished."}
{"pdf_id": "0706.0306", "content": "the task corresponding to this initial state is created. The AuthenticationFilter, the JbpmContextFilter, and the assignment of the ActorId in the jBPM-context make the new task to be assigned to the right actor and the corresponding task list. The PID is saved in the process context and is therefore available to all process participants as a process variable. To make the process operations persistent, the jBPM-context is saved:"}
{"pdf_id": "0706.0306", "content": "2. The HomeAuthorBean formulates a query to the integration layer by specifying the maximum number of hits (100), the comparison operator to use info.fedora.www.definitions._1._0.types.ComparisonOperator2, the field the query refers to (\"creator\"), and the value to check (the name of the current user). This query is handed over to the FedoraSOAPClient."}
{"pdf_id": "0706.0306", "content": "The result of the query to the integration layer is an object of type info.fedora.www.definitions._1._0.types.FieldSearchResult. This type encapsulates the abstract type\"resultList\", which is of the (concrete) type ArrayOfObjectFields. The attributes of an ObjectFields-object contain DublinCore metadata like \"creator\", \"subject\", and \"description\", and Fedora object proper ties like the PID or the creation date (\"cDate\") [1]."}
{"pdf_id": "0706.0306", "content": "3. The method doQuery of the FedoraSOAPClient transforms the query coming from the HomeAuthorBean into an object of type info.fedora.www.definitions._1._0.types.FieldSearchQuery. A FieldSearchQuery consists mainly of an array of conditions; thus queries with an arbitrary number of conditions can be handled. In this case, we use only one condition. The FieldSearchQuery is handed over to the method findObjects."}
{"pdf_id": "0706.0306", "content": "4. In method findObjects, there is a SOAP call to the Fedora server as described above (section 5.2). But this time, there are Fedora-specific data types that are unknown to the Axis-library. Thus, all Fedora data types of this SOAP-call are introduced to the Axis-client as qualified name objects before the call.invoke-statement by the method call.registerTypeMapping, like for instance the data type FieldSearchResult1:"}
{"pdf_id": "0706.0306", "content": "7. Before HomeAuthorBean passes on the information from the integration layer to the user-interface layer the monolithic FieldSearchResult-object is transformed to a list of ObjectFields. home_author.jsp can access the entries of this list directly. The indexing shows that some of the Dublin Core attributes are arrays. Indeed, the Dublin Core standard has repeatable attributes."}
{"pdf_id": "0706.0306", "content": "the form on task_author.jsp in jBPM-process variables, so that other roles involved in the same process, e. g. the quality assurance, need not get these metadata from Fedora, but can access these process variables directly. After that, the TaskArticleBean saves the metadata in the corresponding Fedora object. The PID for accessing the correct Fedora object can be read from the process variable and be handed over to the FedoraSOAPClient:"}
{"pdf_id": "0706.0306", "content": "The method changeDC of the FedoraSOAPClient can change the metadata. Here, the new Dublin Core-data stream is built as a DOM-document: at first a new DOM-document is created with the necessary Dublin Core-namespace-attributes. Then the DC-metadata are inserted as additional nodes according to the DC-namespace-specification1. Since Fedora creates a DC-data stream for each new object automatically, the FedoraSOAPClient uses the API-M-method modifyDatastreamByValue to save the metadata in Fedora:"}
{"pdf_id": "0706.0306", "content": "Using the method dsExists, the FedoraSOAPClient has the TaskArticleBean find out, if the data stream with the label \"ARTICLE\" exists. This check is necessary because the TaskArticleBean is also used for reworking an existing article. Prior to saving the article in Fedora, the MIME type of the uploaded file in the local Tomcat-root-directory is detected:"}
{"pdf_id": "0706.0306", "content": "Although this work has been motivated by a scientific context, the concepts are general enough to be used by any organization that needs to manage content for internal or external purposes. We have provided a proof of concept for the integration of an open-source digital repository into a state-of-the-art enterprise architecture."}
{"pdf_id": "0706.0465", "content": "Wafer-to-wafer measurement of these characteristics  in a production setting (where typically this  information may be only sparsely available, if at all,  after batch processing runs with numerous wafers  have been completed) would provide important  information to the operator that the process is or is  not producing wafers within acceptable bounds of  product quality"}
{"pdf_id": "0706.0465", "content": "In a flexible manufacturing  environment this is highly dependent upon the  accurate development and subsequent adaptation of  models  which  simulate  process,  wafer,  and equipment relationships and with feedback from in situ sensors are used to predict process trends and  develop control strategies"}
{"pdf_id": "0706.0465", "content": "The etching process is described and specified by  various parameters which may include:  • Line Width  • Oxide Loss  • Etch rate  • Selectivity: relative etch rate of different   materials  • Anisotropy: ratio of vertical to horizontal   etch rates  • Uniformity: refers to variations in etching rate   among runs, among wafers, or across a wafer  • Defect density on the wafer: these arise   due to particulate matter generated   during the etching process; expressed as   number of point defects/cm2"}
{"pdf_id": "0706.0465", "content": "Process Model Representation  The use of sensor measurements for estimating  setpoints and wafer states is based on the premise  that the large number of signals from machine  sensors, from optical emission spectroscopy (OES)  sensors and from RFM sensors is rich in information  about the \"true\" state of the plasma etch processing"}
{"pdf_id": "0706.0465", "content": "Multiple Virtual Sensors Provide Orthogonal Estimates  of Process and Wafer States  Furthermore, if the actual sensors providing the data  to the virtual sensors are completely independent  from one another (such as OES and RFM), then the  use of multiple virtual sensors using orthogonal  (independent) measurements could be used to  provide redundant estimates of wafer states and  setpoints as shown in Figure 4"}
{"pdf_id": "0706.0465", "content": "1) and wafer state characteristics (g). Prediction of  recipe setpoints based upon sensor data provides a  capability for cross-checking that the machine is  maintaining the desired setpoints, and may indicate  sensor drift or failure if virtual sensors agree with  one another but disagree with recipe setpoint values.  Wafer state characteristics such as Line Width  Reduction and Oxide Loss may be estimated on-line  (g model) using these same process sensors  (Machine,  OES,  RFM).  Wafer-to-wafer  measurement of these characteristics in a production  setting (where typically this information may be only  sparsely available, if at all, after batch processing"}
{"pdf_id": "0706.0465", "content": "1 Design Of Experiments (DOEs)  Since one of the goals was to model the plasma etch  process for a wide variety of process conditions and  across a wide range of setpoints (rather than for just  a single recipe), an experimental design was created  to attempt to span the range of setpoints of interests"}
{"pdf_id": "0706.0465", "content": "2 Data Pretreatment  Raw sensor measurements from wafer processing are  recorded every few seconds (exact sampling rates  depend upon the specific sensor system), sometimes  at irregular intervals, and generally the sampling of  these signals is not coordinated with the sampling  times for other sensors connected to the same  machine"}
{"pdf_id": "0706.0465", "content": "pretreatment used for building the f-1 and g models  needs to be mentioned here. OES data was first  pretreated by reducing 2042 spectral lines into 40.  Next,  the  time  series  records  for  sensor  measurements were reduced a to set of vectors of  signal metrics (means, std, etc.) for each wafer  processed. This pretreament not only greatly  simplified the modelling, but also enhanced model  precision through precalculation of a number of  important metrics which turned out to be very useful  for prediction."}
{"pdf_id": "0706.0465", "content": "provided the best f-1 models, while RFM based  models benefited most from TiN region data for all  predictions). Combining sensor data from multiple  etch regions, based on the premise that there might  be a significant amount of complementary data  present at different stages of the etch, yielded worse  not better predictions. From this result it was  decided to focus in this phase of the project on use of  data from etch regions individually (to not combine  them)."}
{"pdf_id": "0706.0465", "content": "Figure 5. Sensor Data Metrics are Divided by Etch Region  2.3 Modelling Techniques Examined  A wide variety of modelling techniques for  implementation of the virtual sensor models were  analyzed. These included the following:  • Multidimensional Linear Regression (MLR)  • Principal Component Regression (PCR)  • Linear Partial Least Squares (PLS)  • Polynomial Regression  • Polynomial Partial Least Squares (PolyPLS)  • Neural Network Partial Least Squares (NNPLS)"}
{"pdf_id": "0706.0465", "content": "In addition to verifying that wafer state parameters  and process setpoints can in fact be modelled using  process sensor data, we sought to determine which  modelling techniques would be most suitable for this  task, which etch region(s) provided the richest  source(s) of information for prediction, how accurate  and how robust would these models be"}
{"pdf_id": "0706.0465", "content": "The purpose of the f-1 virtual sensor model is to use  process state sensor to predict recipe setpoint values.  This is to provide a way of cross-checking the  effective setpoint parameters according to plasma  chamber dynamics with the desired setpoints as  specified by the current recipe. If there is a  mismatch between what the setpoints are and what"}
{"pdf_id": "0706.0465", "content": "the f-1 virtual sensor models are predicting, then it is  possible that the process has drifted from setpoint  and needs to be corrected. It can also indicate that  the sensors and/or actuators regulating setpoints may  be in error due to miscalibration, drift or  malfunction."}
{"pdf_id": "0706.0465", "content": "Linear PLS Model of Top Power from RFM Sensors,  Ox Region  As shown in Figures 6 and 7, it was possible to get  fairly accurate predictive models for the power  parameters, by carefully selecting sensor type and  etch region which resulted in the best model(s)"}
{"pdf_id": "0706.0465", "content": "Since there are no die  location specific variables in the process sensors  (although there is some OES sensor sensitivity to  stripes of die locations, depending upon the  orientations of the OES fiber optic sensors), it was  necessary to build a separate PLS model for each die  position"}
{"pdf_id": "0706.0465", "content": "Comparison of results from using Neural Network  based PLS models to Linear PLS models illustrates a  common result found in this study: that while the  NNPLS models may have the lowest average  prediction error (NNPLS OES Ox models have the  highest prediction accuracy), the NNPLS technique  may also result in some of the worst models  (NNPLS RFM Al models)"}
{"pdf_id": "0706.1137", "content": "This paper describes a system capable of  semi-automatically filling an XML template from free texts in the clinical domain (prac tice guidelines). The XML template includes  semantic information not explicitly encoded in the text (pairs of conditions and ac tions/recommendations). Therefore, there is a need to compute the exact scope of conditions over text sequences expressing the required actions. We present a system devel oped for this task. We show that it yields  good performance when applied to the  analysis of French practice guidelines."}
{"pdf_id": "0706.1137", "content": "As we have previously seen, practice guidelines are  not routinely fully exploited. One reason is that  they are not easily accessible to doctors during  consultation. Moreover, it can be difficult for the  doctor to find relevant pieces of information from  these guides, even if they are not very long. To  overcome these problems, national health agencies  try to promote the electronic distribution of these guidelines (so that a doctor could check recom mendations directly from his computer)."}
{"pdf_id": "0706.1137", "content": "amination processes (e.g. digestive endoscopy).  The data are thus homogeneous, and is about 250 pages long (150,000+ words). Most of these prac tice  guidelines  are  publicly  available  at:  http://www.anaes.fr or http://affsaps.sante  .fr. Similar documents have been published in  English and other languages; the GEM DTD is  language independent."}
{"pdf_id": "0706.1137", "content": "Segmenting a guideline to fill an XML template is a complex process involving several steps. We de scribe here in detail the most important steps  (mainly the way the scope of conditional sequences  is computed), and will only give a brief overview  of the pre-processing stages."}
{"pdf_id": "0706.1137", "content": "The pre-processing stage concerns the analysis of  relevant linguistic cues. These cues vary in nature:  they can be based either on the material structure or  the content of texts. We chose to mainly focus on  task-independent knowledge so that the method is  portable, as far as possible (we took inspiration  from Halliday and Matthiessen's introduction to  functional grammar, 2004). Some of these cues"}
{"pdf_id": "0706.1137", "content": "As for quantifiers, a conditional element may have  a scope (a frame) that extends over several basic  segments. It has been shown by several authors  (Halliday and Matthiessen, 2004; Charolles, 2005)  working on different types of texts that conditions  detached from the sentence have most of the time a scope beyond the current sentence whereas conditions included in a sentence (but not in the begin ning of a sentence) have a scope which is limited to  the current sentence. Accordingly we propose a  two-step strategy: 1) the default segmentation is  done, and 2) a revision process is used to correct  the main errors caused by the default segmentation  (corresponding to the norm)."}
{"pdf_id": "0706.1137", "content": "1. Scope of a heading goes up to the next head ing;  2. Scope of an enumeration's header covers all  the items of the enumeration ;  3. If a conditional sequence is detached (in the  beginning of a paragraph or a sentence), its  scope is the whole paragraph;  4. If the conditional sequence is included in a  sentence, its scope is equal to the current  sentence."}
{"pdf_id": "0706.1137", "content": "Cases 3 and 4 cover 50-80% of all the cases, de pending on the practice guidelines used. However,  this default segmentation is revised and modified  when a linguistic cue is a continuation mark within  the text or when the default segmentation seems to  contradict some cohesion cue."}
{"pdf_id": "0706.1137", "content": "There are two cases which require revising the default segmentation: 1) when a cohesion mark indi cates that the scope is larger than the default unit;  2) when a rupture mark indicates that the scope is  smaller. We only have room for two examples,  which, we hope, give a broad idea of this process.  1) Anaphoric relations are strong cues of text  coherence: they usually indicate the continuation of  a frame after the end of its default boundaries."}
{"pdf_id": "0706.1137", "content": "Finally, an XML output is produced  for the document, corresponding to a candidate GEM version of the document (no XML tags over lap in the output since we produce an instance of  the GEM DTD; all potential remaining conflicts  must have been solved by the supervisor)"}
{"pdf_id": "0706.1290", "content": "Representing and reasoning about qualitative temporal information is an essential part of many artificial intelligence tasks. These tasks appear in such diverse areas as natural language processing, planning, plan recognition, and diagnosis. Allen [1,2] has proposed an interval algebra framework and Vilain and Kautz [34] have proposed a pointalgebra framework for representing such qualitative information. All models that have been pro posed afterwards in the litterature derive from these two frameworks. Placing two intervals on the Timeline, regardless of their length, gives thirteen relations, known as Allen's [2] relations. Vilain [33]"}
{"pdf_id": "0706.1290", "content": "Hence, assigning a letter to each temporalobject, as its identity, and using as many occur rences of this identity as it has points or intervalbounds, it is possible to describe an atomic tempo ral relation between n objects on the timeline, as far as there is no simultaneity, with a word on ann-alphabet (alphabet with n letters)"}
{"pdf_id": "0706.1290", "content": "In order to model explicitly concurrency with words, various tools have been proposed such as event structures or equivalence relations on words i.e. traces. In those theories, it is not possible to model only synchronization. One is able to say that two events can be done at the same time but it is not possible to express that they have to be done at the same time. This is due to the factthat concurrency is modelled inside a deeply sequential framework, hence, synchronization is sim ulated with commutativity. But one has to handle with instant, in the sense of Russell [29]. This is why we introduce the concept of S-alphabet which is a powerset of a usual alphabet."}
{"pdf_id": "0706.1290", "content": "These objects has been revisited and studied fortheir own by Ladkin [20] under the name of nonconvex intervals. Ligozat [22] generalized to se quences of points and/or intervals under the name of generalized intervals.There are 3 situations between two points, 5 between a point and an interval, 13 situations be tween two intervals, 8989 situations between two sequences of three intervals or two sequences of 6 points. Ladkin [20, Theorem1], proved the number of situations between two chains of intervals is at least exponential in the number of intervals. The exact number of situations between a sequence ofp points and a sequence of q points has been pro"}
{"pdf_id": "0706.1290", "content": "It is usual in temporal applications that infor mation arrives from many various sources or a same source can complete the knowledge about a same set of intervals. The usual way to deal with that, when no weight of credibility or plausibility is given, is to intersect all the information. The knowledge among some set of intervals interferes with some other sets of intervals by transitivity: if you know that Marie leaved before your arrival, and you are waiting for Ivan who attempts to see Marie, you can tell him that he has missed her. Vilain and Kautz [34] argued that there are two kinds of problems:"}
{"pdf_id": "0706.1926", "content": "Michele Bezzi Robin Groenevelt  Accenture Technology Park,   Sophia Antipolis, F-06902, France  ABSTRACT  Measuring and modeling human behavior is a very  complex task. In this paper we present our initial thoughts  on modeling and automatic recognition of some human  activities in an office. We argue that to successfully  model human activities, we need to consider both  individual behavior and group dynamics. To demonstrate  these  theoretical  approaches,  we  introduce  an  experimental system for analyzing everyday activity in  our office.  Keywords  Probabilistic data, office activities, information theory;  social networks"}
{"pdf_id": "0706.1926", "content": "INTRODUCTION  People and businesses have a natural interest in studying  human behavior patterns. This can come forth from  security concerns, to offer improved health care of  individuals, to increase and monitor the performance of  people, to understanding how customers behave, to  optimize  organizational  structure,  or  to  improve  communications among groups of people."}
{"pdf_id": "0706.1926", "content": "Individuals are per se complex entities: their actions  depend not only on the sensory context, but also on  various hard-to-measure factors such as past personal  history, attention, attitudes, experiences, and emotions.  To investigate these complex patterns of activity we need  to consider the actual context and the context history. For  example, collecting sensory information for long periods  (e.g. months) we can search for frequent recurrent  patterns of activity (habits), and, accordingly, create a  statistical model of people's daily activities. Deviations  from this baseline may indicate a change from routine  activity. Due to the high variability that characterizes  human behavior, this process generates a huge number of  patterns. Similarly, the redundancy and the complex"}
{"pdf_id": "0706.1926", "content": "hierarchical structure of habitual behavior [1] (a complex  habit may be decomposed into many simpler sub-habits)  also produce a multitude of recurrent patterns. In our  approach we will apply a combination of context specific  knowledge and statistical methods to choose appropriate  models or to select specific features of certain behaviors.  The choice of the temporal and spatial scale plays also an  important role, e.g. decreasing the spatial resolution (large  spatial bins) may help to compensate for the inherent  stochasticity in people movement patterns, but it may lead  to a large information loss, as well. Again, context  knowledge and physical constraints may be used to  choose the appropriate temporal and spatial resolution."}
{"pdf_id": "0706.1926", "content": "An additional source of stochasticity is the presence of  noise at sensor level. Sensor networks producing large  quantities of (often) redundant, but noisy, data. In fact,  although sensor technology is rapidly progressing,  undetected events and false positive are almost always  present in any sensor network. Thus to fully exploit the  data we should be able to handle the intrinsic noisy nature  of sensor data. In our case, data coming from multiple  heterogeneous sensory sources are integrated using a  Bayesian framework [2,3] that combines probabilistic and  knowledge-based approaches."}
{"pdf_id": "0706.1926", "content": "On the positive side, recent advances in sensor  technologies provide us a large amount of data about  human behaviour in every day life. Taking advantage of  these large data sets and sensor redundancy we may  partly compensate for the stochasticity at the sensor and  behavioral level, and improve precision and robustness of  the system. Furthermore, observing real environments for  long periods of time may reveal dynamics that are not  evident from small-scale studies in artificial environments  and for limited durations [4]."}
{"pdf_id": "0706.1926", "content": "Group dynamics, often due to social interactions, are also  highly complex processes. It has been found that  networks of friendships or personal contacts can exhibit  small world [5,6] or scale-free properties [7], i.e., there  are many people with few connections and a few people  with many connections. An important aspect of our study  on behaviour comes forth from human physical  interactions. To estimate this we will focus on the  movement trajectories of people."}
{"pdf_id": "0706.1926", "content": "In this paper we present a system we are developing to  detect and measure various behaviors in everyday office  life. We will briefly describe our experimental  environment and numerical simulations of office life,  after which we will present some preliminary results  related to detecting unusual activities and social  connections. Finally we will discuss some potential issues  when deploying such a system."}
{"pdf_id": "0706.1926", "content": "We have chosen an office environment as a test setting  for various reasons. First of all, a quantitative description  of various office activities may have important practical  applications (e.g. assessing the quality of space  organization in the office, estimating connections  amongst  different  people/departments,  safety  and  security). Secondly, a video-camera infrastructure is  readily available in our location and the data are easily  accessible. Finally, data from the camera systems can be  integrated with, or replaced by, other sensors (ultra wide  band tracking devices, badge readers, finger print readers)  and with data extensively available in electronic form  (calendars, e-mails, log files)."}
{"pdf_id": "0706.1926", "content": "The actual functionality of our system will be determined  using probabilistic tracking data from Accenture labs in  Chicago [2,3]. This modular system provides long term  recordings and probabilistic tracking. Along with real  world data, we are implementing a numerical simulation  of people their movements in an office analogous to the  one used for collecting real world data."}
{"pdf_id": "0706.1926", "content": "Experimental setup  This section describes a probabilistic framework for  identifying and tracking moving objects using multiple  streams of sensory data (a more detailed description can  be found in [2,3]).  The experimental environment is composed of an office  floor at Accenture Technology Labs in Chicago. The  floor is equipped with a network consisting of 30 video  cameras, 90 infrared tag readers, and a biometric station  for fingerprint reading.  The first step is the fusion of this raw-sensor data into a  higher-level description of people's movements inside the  office. People identification and tracking is performed  using a Bayesian network. In short (see [3] for details),"}
{"pdf_id": "0706.1926", "content": "the office space is divided into 50 locations, each of them  the size of a small office. This allows us to remove the  variability of paths inside a room while still maintaining  enough information about people their movements. Each  sensor detects signals of people in its sensory field. For  each person and location the signals are merged together  to build the current probabilistic evidence of finding a  certain person in a specific location, after which this  information is integrated with the current belief of the  system (originated by previous observation). The result is  a sequence of matrices, one for each time step, where the  probability finding a person in each location is reported."}
{"pdf_id": "0706.1926", "content": "In the second step, starting from these matrices, we derive  the most likely paths for each tracked individual; these  data are then analysed to find frequent patterns,  appropriate statistical quantities to describe long term  activities. Extracted recurrent patterns may be later  identified exploiting local semantics (e.g. meetings usually take place in the meeting room) and context knowledge (e.g. matching movement patterns with the  information available from the electronic calendar)."}
{"pdf_id": "0706.1926", "content": "For example, we have measured the time spent in each  location x by each single user across a number of days,  P(x), and for each single day, P(x|day). See Figure 1. The  behavior on a single day is then compared to an average  day, estimating the so-called stimulus specific information  (also called surprise [9]) for each day:"}
{"pdf_id": "0706.1926", "content": "This quantity is large in case of surprising (different from  the average) patterns. The main advantage of this  statistical quantity is that it is additive (i.e. it fulfills the  chain rule, as mutual information, see [9]). This allow us to easily integrate other sources of information (e.g. log files) by simply summing the corresponding specific  information.  We observe a clear peak on day 5, (Fig. 1c) indicating  some unusual behavior on that day."}
{"pdf_id": "0706.1926", "content": "Figure 1. Measuring deviation from routine behavior. (a) Distribution of occupancy time across one week for one person over  different office locations. (b) Distribution of occupancy time for each single day. (c) Surprise as a function of day of the week.  Surprise quantifies the amount of mutual information we gain observing occupancy time distribution for one day (P(x|day)).  Large values indicate surprising---unusual---behavior."}
{"pdf_id": "0706.1926", "content": "(leaders, followers), the existence of groups of interests,  or potential communication gaps (conflicts) among  groups. Using this analysis we may, for instance, assess  the impact of change in the environment on the social  structure, or the effects of team building exercise or  collaboration on the personal contact network."}
{"pdf_id": "0706.1926", "content": "This simple rule may lead to a large number of false  positives and it also it is limited by the range of sensor  network. However, we expect that in the long run and  with a large number of users it may provide a reasonable  first approximation of global structure of the network of  interactions and of its evolution in time. This approach  will be integrated with more standard methods based on  electronic communications to better specify the structure  of the network and to investigate the (possible) different  topologies of electronic and physical social networks."}
{"pdf_id": "0706.1926", "content": "Automatic recognition and prediction of human activities  from sensory observations is a fast growing research  field. Many technical issues are starting to be solved in  laboratory settings, but there remain many technical and  social obstacles for a successful deployment in real life  environments. The great variability of human behavior  even in rather simple activities is the main technical  obstacle for automatic detection, but social aspects are not  less important. Let us briefly discuss a couple of them:"}
{"pdf_id": "0706.1926", "content": "performance) may induce people to behave artificially,  i.e. to behave in a non-natural way to mimic expected  patterns. This is not necessarily negative, for example, if  such a system is used to assess the compliance with some  safety procedures, but it should be taken into account  when analyzing behavioral data. We may expect this bias  to decrease with an increasing user acceptance of  pervasive technologies."}
{"pdf_id": "0706.1926", "content": "In conclusion, we are implementing a system for  automatic analysis of some behaviors in everyday office  life. Although a fully automatic system for recognition of  human activities in real world situations is still far in the  future, focusing on a specific context and exploiting the  large availability of past and present data, we may derive  a quantitative description for some of these activities,  which are useful for practical purpose."}
{"pdf_id": "0706.1926", "content": "ACKNOWLEDGMENTS  We thank Agata Opalach for providing helpful comments  on previous versions of this document. We also thank  Valery Petrushin and Gang Wei for providing tracking  data obtained from Accenture Technology Labs in  Chicago, and Frederick Schlereth for performing the  numerical simulations."}
{"pdf_id": "0706.2797", "content": "Cunningham, H., D. Maynard, K. Bontcheva, et V. Tablan (2002). Gate : A framework and gra phical development environment for robust nlp tools and applications. In 40th Anniversary Meeting of the Association for Computational Linguistics (ACL'02). Irmak, U. et T. Suel (2006). Interactive wrapper generation with minimal user effort. In WWW '06, 15th international conference on World Wide Web, New York, NY, USA. ACM Press."}
{"pdf_id": "0706.2797", "content": "We are concerned by named entities extraction with the final goal of constructing the list of partners found in an activity report. Starting with an initial list of entities, we use a first set of documents to identify syntactic patterns that are then validated in a supervised learning phase on a set of annotated documents to perform a performance test. The complete collection is then explored. This approach comes from the one that is used in data extraction for semi-structured documents (wrappers) and do not need any linguistic ressources neither a large set for training. As our collection of documents evoluate, we hope that the performance of the extraction will become better year after year."}
{"pdf_id": "0706.3639", "content": "This paper is a survey of a large number of informal definitions of \"intel ligence\" that the authors have collected over the years. Naturally, compiling a complete list would be impossible as many definitions of intelligence are buried deep inside articles and books. Nevertheless, the 70-odd definitionspresented here are, to the authors' knowledge, the largest and most well ref erenced collection there is."}
{"pdf_id": "0706.3639", "content": "In this section we present definitions that have been proposed by groups or organ isations. In many cases definitions of intelligence given in encyclopedias have been either contributed by an individual psychologist or quote an earlier definition givenby a psychologist. In these cases we have chosen to attribute the quote to the psy chologist, and have placed it in the next section. In this section we only list those definitions that either cannot be attributed to a specific individuals, or represent a collective definition agreed upon by many individuals. As many dictionaries source their definitions from other dictionaries, we have endeavoured to always list the original source."}
{"pdf_id": "0706.3639", "content": "3. \"It seems to us that in intelligence there is a fundamental faculty, the alteration or the lack of which, is of the utmost importance for practical life. This faculty is judgement, otherwise called good sense, practical sense, initiative, the faculty of adapting ones self to circumstances.\" A. Binet [5]"}
{"pdf_id": "0706.3639", "content": "31. \"The capacity to inhibit an instinctive adjustment, the capacity to redefine the inhibited instinctive adjustment in the light of imaginally experienced trial and error, and the capacity to realise the modified instinctive adjustment in overt behavior to the advantage of the individual as a social animal.\" L. L. Thurstone quoted in [35]"}
{"pdf_id": "0706.3639", "content": "Features such as the ability to learn and adapt, or to understand, are implicit in the above definition as these capacities enable an agent to succeed in a wide range of environments. For a more comprehensive explanation, along with a mathematical formalisation of the above definition, see [22] or our forthcoming journal paper."}
{"pdf_id": "0706.4375", "content": "2 The authors identify scalability  as a critical parameter for two reasons: (1) it has to be able to process large amounts of data,  in order to build and train statistical models for Information Extraction; (2) it has to support  its own use as an online public service"}
{"pdf_id": "0706.4375", "content": "3. A modular and tunable platform  In the development of Ogmios, we focused on tool integration. Our initial goal was to exploit  existing NLP tools rather than developing new ones3 but integrating heterogeneous tools and  nevertheless achieve good performance in document annotation was challenging. Ogmios  platform was designed to test various combinations of annotations in order to identify which  1 http://deri.ie/projects/swan  2 http://sekt.semanticweb.org  3 We developed NLP systems only when no other solution was available. We preferably chose GPL or free licence software  when possible."}
{"pdf_id": "0706.4375", "content": "We assume that input web documents are already downloaded, cleaned, encoded into the  UTF-8 character set, and formatted in XML (Nazarenko et al., 2006). Documents are first  tokenized to define offsets to ensure the homogeneity of the various annotations. Then,  documents are processed through several modules: named entity recognition, word and  sentence segmentation, lemmatization, part-of-speech tagging, term tagging, parsing,  semantic tagging and anaphora resolution.  Although this architecture is quite traditional, few points should be highlighted:"}
{"pdf_id": "0706.4375", "content": "which is used for further reference. The tokens are the basic textual units in the text  processing line. Tokenization serves no other purpose but to provide a starting point  for segmentation. This level of annotation follows the recommendations of the  TC37SC4/TEI workgroup, even if we refer to the character offset rather than pointer  mark-up (TEI element ptr) in the textual signal to mark the token boundaries. To  simplify further processing, we distinguish different types of tokens: alphabetical  tokens, numerical tokens, separating tokens and symbolic tokens."}
{"pdf_id": "0706.4375", "content": "Named Entity tagging  The Named Entity tagging module aims at annotating semantic units, with syntactic and  semantic types. Each text sequence corresponding to a named entity is tagged with a unique  tag corresponding to its semantic value (for example a \"gene\" type for gene names, \"species\"  type for species names, etc.). We use the TagEN Named Entity tagger (Berroyer, 2004),  which is based on a set of linguistic resources and grammars. Named entity tagging has a  direct impact on search performance when the query contains one or two named entities, as  those semantic units are have a high discriminative power."}
{"pdf_id": "0706.4375", "content": "Word and sentence Segmentation  This module identifies sentence and word boundaries. We use simple regular expressions,  based on the algorithm proposed in (Grefenstette & Tapanainen, 1994). Part of the  segmentation has been implicitly performed during the Named Entity tagging to solve some  ambiguities such as the abbreviation dot in the sequence \"B. subtilis\", which could be  understood as a full stop if it were not analyzed beforehand."}
{"pdf_id": "0706.4375", "content": "Morpho-syntactic tagging  This module aims at associating a part of speech (POS) tag to each word. It assumes that the  word and sentence segmentation has been performed. We are using a probabilistic  Part-Of-Speech tagger: TreeTagger (Schmid, 1997). The POS tags are not used as such for IR  but POS tagging facilitates the rest of the linguistic processing."}
{"pdf_id": "0706.4375", "content": "Lemmatization  This module associates its lemma, i.e. its canonical form, to each word. The experiments  presented in (Moreau, 2006) show that this morphological normalization increases the  performance of search engines. If the word cannot be lemmatized (for instance a number or a  foreign word), the information is omitted. This module assumes that word segmentation and  morpho-syntactic information are provided. Even if it is a distinct module, we currently  exploit the TreeTagger output which provides lemma as well as POS tags."}
{"pdf_id": "0706.4375", "content": "Terminology tagging  This module aims at recognizing the domain specific phrases in a document, like gene  expression or spore coat cell. These phrases considered as the most relevant terminological  items. They can be provided through terminological resources such as the Gene Ontology  (GOConsortium, 2001), the MeSH (MeSH) or more widely UMLS (UMLS). They can also be  acquired through corpus analysis (see Figure 1). Providing a given terminology tunes the term"}
{"pdf_id": "0706.4375", "content": "Semantic type tagging and anaphora resolution  The last modules are currently under test and should be integrated in the next release of the  platform. The semantic type tagging associates to the previously identified semantic units tags  referring to ontological concepts. This allows a semantic querying of the document base.  The anaphora resolution module establishes coreference links between the anaphoric pronoun  occurrences and the antecedents they refer to. Even if solving anaphora has a small impact on  the frequency counts and therefore on IE, it increases IE recall: for instance it inhibits Y may  stand for X inhibits Y and must be interpreted as such in a extraction engine dealing with gene  interactions."}
{"pdf_id": "0706.4375", "content": "5. Performance analysis  We carried out an experiment on a collection of 55,329 web documents from the biological  domain. All the documents went through all NLP modules, up to the term tagging (as  mentioned before, the goal is not to parse the whole documents but only some filtered part of  them). A 400,000 named entity list, including species and gene names, and a 375,000 term list,  issued from the MeSH and Gene Ontology have been used."}
{"pdf_id": "0706.4375", "content": "were processed; 4.53 million named entities and 13.9 million domain specific phrases were  identified. Each document contains, on average, 1,913 words, 85 sentences, 82 named entities  and 251 domain specific phrases. 147 documents contained no words at all; they therefore  underwent the tokenization step only. One of our NLP clients processed a 414,995 word  document.  Table 4 shows the average processing time for each document. Each document has been  processed in 37 seconds. Due to the exploited resource, the most time-consuming steps are the  term tagging (56% of the overall processing time) and the named entity recognition (16% of  the overall processing time).  Average time processing  Percentage"}
{"pdf_id": "0706.4375", "content": "The whole document collection, except two documents, has been analysed. Thanks to the  distribution of the processing, the problems occuring on a specific document had no  consequence on the whole process. Clients in charge of the analysis of these documents have  been simply restarted.  The performance we get on this collection show the robustness of the NLP platform, and its  ability to analyse large and heterogeneous collection of documents in a reasonable time. We  have proven the efficiency of the overall process for semantic crawlers and its accuracy for a  precise indexing of web documents."}
{"pdf_id": "0706.4375", "content": "Textual noise  Scientific texts present particularities that we chose to handle in a normalization step prior to  the parsing. First, the segmentation in sentences and words was taken off from the parser and  enriched with named entities recognition and rules specific to the biological domain. We also  delete some extra-textual information that alters parsing quality (such as citations, for  instance)."}
{"pdf_id": "0706.4375", "content": "Corpus and criteria  We used a subset (10 files5) of the MED-TEST corpus but, contrary to the first evaluation  designed for choosing a parser, we wanted to measure the quality of the whole parse and not  only of specific relations.  Table 1 (for the MED-TEST subset) shows the way that out-of-lexicon words (OoL), i.e.  unknown (UW) and guessed (GW) words, are handled by giving the percentage of incorrect"}
{"pdf_id": "0706.4375", "content": "In Table 2, five criteria inform on the parsing time and quality for each sentence : the number  of linkages (NbL), the parsing time (PT) in seconds, the fact that a complete linkage is found  or not (CLF), the number of erroneous links (EL) and the quality of the constituency parse  (CQ). NbW is the average number of words in a sentence which varies with term  simplification. The results are given for each one of the three versions of the parser."}
{"pdf_id": "0706.4375", "content": "Thus, the parser adaptation relies on three methods: the exploitation of a small base of  morphological rules, the modification of the grammar, and an adequate integration that relieve  the parser from all what do not directly deal with structural ambiguity (POS and term tagging,  especially)"}
{"pdf_id": "0707.0701", "content": "In this paper, we study the application of sparse principal component analysis (PCA) toclustering and feature selection problems. Sparse PCA seeks sparse factors, or linear com binations of the data variables, explaining a maximum amount of variance in the data while having only a limited number of nonzero coefficients. PCA is often used as a simple clustering technique and sparse factors allow us here to interpret the clusters in terms of a reduced set of variables. We begin with a brief introduction and motivation on sparse PCA and detail our implementation of the algorithm in d'Aspremont et al. (2005). We then apply these results to some classic clustering and feature selection problems arising in biology."}
{"pdf_id": "0707.0701", "content": "The most expensive numerical step in this algorithm is the computation of the gradient as a matrix exponential and our key numerical contribution here is to show that using onlya partial eigenvalue decomposition of the current iterate can produce a sufficiently precise gradi ent approximation while drastically improving computational efficiency"}
{"pdf_id": "0707.0701", "content": "Here p and q control the degree and precision of the approximation and we set p = q = 6 (we set p = q in practice due to computational issues; see [MVL03]). The approximation is only valid in a small neighborhood of zero, which means that we need to scale down the matrix before"}
{"pdf_id": "0707.0701", "content": "with partial eigenvalue decomposition (DSPCA). The covariance matrix is formed using colon cancer gene expression data detailed in the following section. Table 1 shows running times for DSPCA and Sedumi on for various (small) problem dimensions. DSPCA clearly beats the interiorpoint solver in computational time while achieving comparable precision (measured as the per centage of variance explained by the sparse factor). For reference, we show how much variation is explained by the leading principal component. The decrease in variance using Sedumi and DSPCA represents the cost of sparsity here."}
{"pdf_id": "0707.0701", "content": "In this section, we use our code for sparse PCA (DSPCA), to analyze large sets of gene expression data and we discuss applications of this technique to clustering and feature selection. PCA is very often used as a simple tool for data visualization and clustering (see [SSR06] for a recent analysis), here sparse factors allow us to interpret the low dimensional projection of the data in terms of only a few variables."}
{"pdf_id": "0707.0701", "content": "the performance increase of using partial, rather than full, eigenvalue decompositions should be substantial when only a few eigenvalues are required. In practice there is overhead due to the necessity of testing condition (8) iteratively. Figure 1 depicts the results of these tests on a 3.0 GHz CPU in a loglog plot of runtime (in seconds) versus problem dimension (on the left). We plot the average number of eigenvalues required by condition (8) versus problem dimension (on the right), with dashed lines at plus and minus one standard deviation. We cannot include interior point algorithms in this comparison because memory problems occur for dimensions greater than 50."}
{"pdf_id": "0707.0701", "content": "Figure 3: Clustering: The top two graphs display the results on the colon cancer data set using PCA (left) and DSPCA (right). Normal patients are red circles and cancer patients are blue diamonds. The bottom two graphs display the results on the lymphoma data set using PCA (left) and DSPCA (right). For lymphoma, we denote diffuse large B-cell lymphoma as DLCL (red circles), follicular lymphoma as FL (blue diamonds), and chronic lymphocytic leukaemia as CL (green squares)."}
{"pdf_id": "0707.0701", "content": "clusters derived from PCA and DSPCA numerically using the Rand index. We first cluster the data (after reducing to two dimensions) using K-means clustering, and then use the Rand index to compare the partitions obtained from PCA and DSPCA to the true partitions. The Rand index measures the similarity between two partitions X and Y and is computed as the ratio"}
{"pdf_id": "0707.0701", "content": "For lymphoma, we can also look at another measure of cluster validity. We measure the impact of sparsity on the separation between the true clusters, defined as the distance between the cluster centers. Figure 5 shows how this separation varies with the sparsity of the factors. The lymphoma clusters with 108 genes have a separation of 63, after which separation drops sharply. Notice that the separation of CL and FL is very small to begin with and the sharp decrease in separation is mostly due to CL and FL getting closer to DLCL."}
{"pdf_id": "0707.0704", "content": "on Nesterov's recent work on non-smooth optimization, and give a rigorous complexity analysis with better dependence on problem size than interior point methods. In Section ?? we show that the algorithms we developed for the Gaussian case can also be used to solve an approximate sparse maximum likelihood problem for multivariate binary data, using a log determinant relaxation for the log partition function given by Wainwright and Jordan [2006]. In Section 6, we test our methods on synthetic as well as gene expression and senate voting records data."}
{"pdf_id": "0707.0704", "content": "A related problem, solved by Dahl et al. [2006], is to compute a maximum likelihood es timate of the covariance matrix when the sparsity structure of the inverse is known in advance. This is accomplished by adding constraints to (1) of the form: Xij = 0 for all pairs (i, j) in some specified set. Our constraint set is unbounded as we hope to uncover the sparsity structure automatically, starting with a dense second moment matrix S."}
{"pdf_id": "0707.0704", "content": "We begin by detailing the algorithm. For any symmetric matrix A, let A\\j\\k denote the matrix produced by removing row k and column j. Let Aj denote column j with the diagonal element Ajj removed. The plan is to optimize over one row and column of the variable matrix W at a time, and to repeatedly sweep through all columns until we achieve convergence."}
{"pdf_id": "0707.0704", "content": "Synthetic experiments require that we generate underlying sparse inverse covariance matri ces. To this end, we first randomly choose a diagonal matrix with positive diagonal entries. A given number of nonzeros are inserted in the matrix at random locations symmetrically. Positive definiteness is ensured by adding a multiple of the identity to the matrix if needed. The multiple is chosen to be only as large as necessary for inversion with no errors."}
{"pdf_id": "0707.0704", "content": "In the following experiments, we fixed the problem size p at 30 and generated sparse un derlying inverse covariance matrices as described above. We varied the number of samples n from 10 to 310. For each value of n shown, we ran 30 trials in which we estimated the sparsity pattern of the inverse covariance matrix using the SML, Lasso-OR, and Lasso-AND"}
{"pdf_id": "0707.0704", "content": "Figure (11) closes in on a region of Figure (10), a cluster of genes that is unconnected to the remaining genes in this estimate. According to Gene Ontology [see Consortium, 2000], these genes are associated with iron homeostasis. The probability that a gene has been false included in this cluster is at most 0.05."}
{"pdf_id": "0707.0704", "content": "By applying Theorem 4 we find that all but 339 of the variables are estimated to be inde pendent from the rest. This estimate is less conservative than that obtained in the Hughes case since the ratio of samples to variables is 160 to 500 instead of 253 to 6136."}
{"pdf_id": "0707.0704", "content": "We conclude our numerical experiments by testing our approximate sparse maximum likeli hood estimation method on binary data. The data set consists of US senate voting recordsdata from the 109th congress (2004 - 2006). There are one hundred variables, correspond ing to 100 senators. Each of the 542 samples is bill that was put to a vote. The votes are recorded as -1 for no and 1 for yes."}
{"pdf_id": "0707.0705", "content": "In this section, we focus on finding a good solution to problem (2) using greedy methods. We first present very simple preprocessing solutions with complexity O(n log n) and O(n2). We then recall a simple greedy algorithm with complexity O(n4). Finally, our first contribution in this section is to derive an approximate greedy algorithm that computes a full set of (approximate) solutions for problem (2), with total complexity O(n3)."}
{"pdf_id": "0707.0705", "content": "Section 5 for sparse PCA problems allow us to prove, deterministically, that a finite dimen sional matrix satisfies the restricted isometry condition in (21). Note that Cand`es and Tao(2005) provide a slightly weaker condition than (21) based on restricted orthogonality con ditions and extending the results on sparse PCA to these conditions would increase the maximum S for which perfect recovery holds. In practice however, we will see in Section 7.3 that the relaxations in (9) and d'Aspremont et al. (2007b) do provide very tight upper bounds on sparse eigenvalues of random matrices but solving these semidefinite programs for very large scale instances remains a significant challenge."}
{"pdf_id": "0707.0705", "content": "right shows the mean squared errors when the consistency condition is not satisfied. The two sets of figures do show that the LASSO is consistent only when the consistency condition is satisfied, while the backward greedy algorithm finds the correct pattern if the noise is small enough (Couvreur and Bresler, 2000) even in the LASSO inconsistent case."}
{"pdf_id": "0707.0705", "content": "Figure 3: Backward greedy algorithm and Lasso. We plot the probability of achieved (dot ted line) and provable (solid line) optimality versus noise for greedy selection against Lasso (large dots), for the subset selection problem on a noisy sparse vector. Left: Lasso consistency condition satisfied. Right: consistency condition not satisfied."}
{"pdf_id": "0707.0705", "content": "Figure 5: Upper and lower bound on sparse maximum eigenvalues. We plot the maximum sparse eigenvalue versus cardinality, obtained using exhaustive search (solid line), the approximate greedy (dotted line) and fully greedy (dashed line) algorithms. We also plot the upper bounds obtained by minimizing the gap of a rank one solution (squares), by solving the semidefinite relaxation explicitly (stars) and by solving the DSPCA dual (diamonds). Left: On a matrix F T F with F Gaussian. Right: On a sparse rank one plus noise matrix."}
{"pdf_id": "0707.0705", "content": "of the biological examples that follow), while Gaussian random matrices are harder. Note however, that the duality gap between the semidefinite relaxations and the optimal solution is very small in both cases, while our bounds based on greedy solutions are not as good. This means that solving the relaxations in (9) and d'Aspremont et al. (2007b) could provide very tight upper bounds on sparse eigenvalues of random matrices. However, solving these semidefinite programs for very large values of n remains a significant challenge."}
{"pdf_id": "0707.0808", "content": "We expect that the Astrobiology Phone-cam will allow us to perform field tests more easily, so that we can upgrade the computer vision software in the near future. We intend to use the Astrobiology Phone-cam system instead of the wearable-computer system for much of our future work in the Cyborg Astrobiologist research program."}
{"pdf_id": "0707.0808", "content": "Table 1: List of images and their attributes for the observing run at Anchor Bay, Malta. The capture time indicates the time at which each image was taken, the receive time gives the time at which the image was received by the mail server, and the completion time gives the time at which the images were uploaded on the web-site and hence available to the user."}
{"pdf_id": "0707.1913", "content": "Collaborative work on unstructured or semi-structured documents, such as in literature corpora or source code, often involves agreed upon templates containing metadata. These templates are not consistent across users and over time. Rule-based parsing of these templates is expensive to maintain and tends to fail as new documents are added. Statistical techniques based on frequent occurrences have the potential to identify automatically a large fraction of the templates, thus reducing the burden on the programmers. We investigate the case of the Project GutenbergTM"}
{"pdf_id": "0707.1913", "content": "corpus, where most documents are in ASCII format with preambles and epilogues that are often copied and pasted or manually typed. We show that a statistical approach can solve most cases though some documents require knowledge of English. We also survey various technical solutions that make our approach applicable to large data sets."}
{"pdf_id": "0707.1913", "content": "The Web has encouraged the wide distribution of collaboratively edited collec tions of text documents. An example is Project Gutenberg1 [Pro09] (hereafterPG), the oldest digital library, containing over 20,000 digitized books. Mean while, automated text analysis is becoming more common. In any corpus of unstructured text files, including source code [AG06], we may find that some uninteresting \"boilerplate\" text coexists with interesting text that we wish to process. This problem also exists when trying to \"scrape\" information from Web"}
{"pdf_id": "0707.1913", "content": "Stripping unwanted and often repeated content is a common task. Frequent patterns in text documents have been used for plagiarism detection [SGWG06], for document fingerprinting [SWA03],for removing templates in HTML doc uments [DMG05], and for spam detection [SCKL04]. Template detection in HTML pages has been shown to improve document retrieval [CYL06]. The algorithmics of finding frequent items or patterns has received much attention. For a survey of the stream-based algorithms, see Cormode andMuthukrishnan [CM05b, p. 253]. Finding frequent patterns robustly is pos sible using gap constraints [JBD05]. The specific problem of detecting preamble/epilogue templates in the PG corpus has been tackled by several hand-crafted rule-based systems [Atk04, Bur05, Gru06]."}
{"pdf_id": "0707.1913", "content": "Our solution identifies frequent lines of text in the first and last sections of each file. These frequent lines are recorded in a common data structure. Then, each file is processed and the prevalence of infrequent lines is used to detect a transition from a preamble to the main text, and one from the main text to an epilogue. To motivate this approach, see Fig. 2. It shows the frequencies of the first 300 lines in each of 100 e-books randomly sampled from the first DVD. From it, we see files with long preambles (an older style) as well as those with short preambles (used in recent e-books)."}
{"pdf_id": "0707.1913", "content": "The algorithm's first pass builds a data structure to identify the frequent lines in the corpus. Several data structures are possible, depending whether we require exact results and how much memory we can use. One approach that we do not consider in detail is taking a random sample of the data. If the frequent-item"}
{"pdf_id": "0707.1913", "content": "threshold is low (say K = 5), too small a sample will lead to many new false negatives. However, when K is large, sampling might be used with any of the techniques below. Although we assume that only 600 (pmax + emax) lines are processed per PG e-book file, there may be similar applications where this assumption cannot be made and the entire file must be processed. The impact of removing the assumption on the desired data structure should be considered."}
{"pdf_id": "0707.1913", "content": "To know exactly which lines occur frequently, if we have inadequate main mem ory, an external-memory solution is to sort the lines. Then a pass over the sorted data can record the frequent lines, presumably in main memory. If we build a file F containing just the first and last 300 non-trivial pre-processed lines of each file, the following GNU/Linux pipeline prints a list of under 3,000 frequent lines (occurring 10 times or more) in less than 100 s on our somewhat old server:"}
{"pdf_id": "0707.1913", "content": "68 ***The Project Gutenberg's Etext of Shakespeare's First Folio*** 1034 ***These EBooks Were Prepared By Thousands of Volunteers*** 1415 ***These Etexts Are Prepared By Thousands of Volunteers!*** 126 ***These Etexts Were Prepared By Thousands of Volunteers!*** 5058 ***These eBooks Were Prepared By Thousands of Volunteers!*** 20 ***This file should be named 1rbnh10.txt or 1rbnh10.zip*** 128 (2) Pay a royalty to the Foundation of 20% of the gross 54 (2) Pay a royalty to the Project of 20% of the net 53 [3] Pay a trademark license fee of 20% (twenty percent) of the 8061 [3] Pay a trademark license fee to the Foundation of 20% of the"}
{"pdf_id": "0707.1913", "content": "A large majority of PG e-books can have their preambles and epilogues de tected by a few heuristic tricks. However, there are many exceptions where thetricks fail, and our experience is that they cannot replace the frequent-line ap proach without being significantly more complex and constantly updated. Yet, heuristics can improve processing based on frequent lines. The heuristic rules we consider can be expressed as Java regular expressions."}
{"pdf_id": "0707.1913", "content": "so that it would run faster than our approach: it has no frequent-line data struc ture to maintain and can probably process the corpus in a single pass. However, is it accurate? Figure 10 shows the errors obtained when we inferred where GutenMark detected preambles. In one case, Diary of Samuel Pepys, October 1666, we see an error of more than 1000 lines. Apparently the diary format used did not have headings GutenMark could detect."}
{"pdf_id": "0707.2506", "content": "But the constraints (18) are nonconvex. So, if they are added to MP1-Dec, it would amount to maximizing a linear function under nonconvex, nonlinear constraints, and again we would not have any guarantee of finding the globally optimal solution. We therefore must also linearize these constraints. We shall do this in this step and the next. Suppose that ("}
{"pdf_id": "0707.2506", "content": "In this paper we have introduced a new exact algorithm that for solving finite-horizon Dec-Pomdps. The results from Table 1 show a clear advantage of the MILP algorithms over existing exact algorithm for the longest horizons considered in each problem. We now point out three directions in which this work can be extended."}
{"pdf_id": "0707.2506", "content": "Pompds: Finally, the approach consisting of the use of the sequence-form and mathematical programming could be applied to Pomdps. We have already shown in this paper how a finite-horizon Pomdp can be solved. In conjunction with the dynamic programming approach analogous to the one described above, it may be possible to compute the infinite-horizon discounted value function of a Pomdp."}
{"pdf_id": "0707.2886", "content": "To our view, the core factors that will  lead to a fruitful collaboration between research institutions and publishers can be outlined as  follows:  • Copyright transfer should be left out of any such agreement, so that independently of  the certification and/or dissemination service provided by the publisher, full liability is  left to the author to issue new dissemination formats or variants that he/she feels  necessary to propagate his/her results;  • The institution should have the capacity to mirror the final paper in its own archive"}
{"pdf_id": "0707.2886", "content": "Independently of addresses appearing on printable papers, it is essential to work  towards agreements that would lead, in the long run, to a full compatibility between  metadata in publishers' databases, institutional archives, and consequently commercial  bibliographical databases;  • Last but not least, transparent cost models should allow research institutions or  universities to choose the level of service they may require from publishers, with the  expectation that cost saving can become a natural, and shared trend"}
{"pdf_id": "0707.2886", "content": "These various constraints together with priorities set by researchers themselves within the  Max Planck Society have thus led us to articulate our policy along three main action lines:  • Taking part in multi-organisation consortia working towards global switches from  traditional subscription based models to full open access"}
{"pdf_id": "0707.2886", "content": "This is typically the case  with Copernicus, which, with the support of the European Geoscience Union, offers  probably at present the most transparent and scientifically motivated open access  scheme;  • Avoid the fragmentation of our financial and decisional surrounding by rejecting  paper-based open access scheme in favour of global negotiation with traditional  publishers"}
{"pdf_id": "0707.2886", "content": "As a whole, the policy of us going Gold is not to contribute to the preservation of the existing  publishing ecology, but above all to contribute to make this ecology evolve in the direction  we think would provide better services and at a better price for our scientists"}
{"pdf_id": "0707.2886", "content": "Indeed, this is already an issue that has been put high  on the agenda by several research communities such as astronomers, geneticians or  researchers in the history of science, who have started to develop communities and  infrastructures to provide a wide dissemination of their digital assets"}
{"pdf_id": "0707.2886", "content": "From the point of view of the Max Planck Society, we both contribute to disseminate the  technical experience of communities which have already developed complex environments  for the management and dissemination of data, while offering technical support, through the  MPDL, for newcomers, focusing on generic solutions that may bring more and more  researchers to a better management of their digital production"}
{"pdf_id": "0707.2886", "content": "New Publication Platforms, New Publication Models  Whether Green or Gold the traditional views on open access are based on the assumption that  publication vectors remain unchanged, i.e. in the form of fixed published articles in journals  as resulting from a closed peer-review process. Still, it is probably our duty to see what the  development of new technical means can bring to us and explore new forms of scientific  communication that could be adopted by all or some research communities."}
{"pdf_id": "0707.2886", "content": "Already explored in communities like genomics, where short papers  can be associated to the deposit of a genomic sequence in a database, it appears to be a  necessary environment for disciplines whose core activity is to analyse primary sources or  objects, such as linguistics, archaeology or history"}
{"pdf_id": "0707.2886", "content": "Improving awareness  As one can see from this overview of the various issues at hand, open access is a highly  complex issue, even more, if it is taken for granted independently from the scientific diversity  as observed in the various institutes of the Max Planck Society. Since there is no global OA  solution, we want also to defend the idea that an OA dissemination policy should not be based  on education (or evangelization), but on the capacity to listen to the scientists' needs or  worries with regards to communication of their scientific results. By doing so, we have  already identified that their main expectations rely not so much on OA as a principle, but on"}
{"pdf_id": "0707.2886", "content": "the capacity of the corresponding infrastructures to provide reliable and effective research  environments for preserving and handling their own information. This rather self-interested  view on scientific information has then to be matched against more systemic views on  community or institution interests, so that the idea of open access per se becomes a natural  component of the scientists' ecology."}
{"pdf_id": "0707.2886", "content": "In this respect, endeavours aiming at coordinating activities on publication archives (Driver5),  research data management (Dariah6) or open access communication (OA information  platform7) play an essential role in ensuring a better synergy between institutions, but also  foster the development of new ideas in the field of open access"}
{"pdf_id": "0707.2886", "content": "Acknowledgments  This paper has been written on the basis of numerous discussions that have been held within  the Max Planck Society. I am in particular most grateful to my colleagues in the sInfo steering  committee and Max Planck Digital Library8 for having brought so many complementary ideas  in the debate. It has also benefited from the experience gained in the French research  environment both at CNRS9 and INRIA10."}
{"pdf_id": "0707.3575", "content": "The pilot project CrossRef Search (http://www.crossref.org/crossrefsearch.html) can be seen  as a test and predecessor of Google Scholar. For CrossRef Search Google indexed full-text  databases of a large number of academic publishers such as Blackwell, Nature Publishing  Group, Springer, etc., and academic/professional societies such as the Association for  Computing Machinery, the Institute of Electrical and Electronics Engineers, the Institute of  Physics, etc., displaying the results via a typical Google interface. The CrossRef Search  interface continues to be provided by various CrossRef partners (e.g. at Nature Publishing  Group)."}
{"pdf_id": "0707.3575", "content": "First and foremost, what stands out is that Google Scholar, as previously mentioned, delivers  results restricted to exclusively scientific documents and this constraint has yet to be  consistently implemented by any other search engine. Google Scholar is a freely available  service with a familiar interface similar to Google Web Search. Much of the content indexed  by Google Scholar is stored on publishers' servers where full-text documents can be  downloaded for a fee, but at least the abstracts of the documents found will be displayed at no cost. The Google approach does, however, provide documents from the open access and self archiving areas (compare Swan and Brown, 2005)."}
{"pdf_id": "0707.3575", "content": "Aha, D. W. (1991), Instance based learning algorithms, Machine Learning 6(1), 37 66. D. W. Aha, D. Kibler and M.  K. Albert, Instance-Based  Learning Algorithms.  Machine Learning 6 37-66,  Kluwer Academic Publishers,  1991. Aha, D. W., Kibler, D. &  Albert, M. K. (1990).  Instance-based learning  algorithms. Draft submission  to Machine Learning."}
{"pdf_id": "0707.3575", "content": "Google Scholar is also noteworthy for the fact that it is conceived of as an interdisciplinary  search engine. In contrast to specialty search engines like the CiteSeer system which indexes  freely available computer science literature or RePEc for economic papers, the Google  Scholar approach can be conceived of as a comprehensive science search engine."}
{"pdf_id": "0707.3575", "content": "html) The  relevance statement offered by Google in 2004 has since been shortened to the following:  \"Google Scholar aims to sort articles the way researchers do, weighing the full text of  each article, the author, the publication in which the article appears, and how often the  piece has been cited in other scholarly literature"}
{"pdf_id": "0707.3575", "content": "Figure 2 shows a typical Google Scholar results list. The individual components of a hit will  be discussed in more detail later. Figure 2 illustrates that the availability of a hit can differ.  The two different items depicted in the figure (labeled as book or citation) are not accessible  via hyperlink as they are extracted only from indexed documents."}
{"pdf_id": "0707.3575", "content": "Our study was carried out as an alternative attempt to create a more accurate picture of  Google Scholar' current situation. Compared with the former studies, it utilizes a brute force  approach to give a more macroscopic view on the content indexed by Scholar. Our study uses  brute force in the sense that we gathered a lot of data from Google, and analyzed the data in a  macroscopic fashion. The following study addresses the question: How deep does Google  Scholar dig? The study should make it possible to answer these research questions:"}
{"pdf_id": "0707.3575", "content": "Is Scholar  touching the academic invisible web (compare Lewandowski and Mayr, 2006)?  • Which document types does Google Scholar deliver? Are theses results sufficient for  professional searchers and academic researching? The analyzed data gives indications  about the composition and utility of the results delivered by Scholar: full-text, link  and citation"}
{"pdf_id": "0707.3575", "content": "In August of 2006 five different journal lists were queried and the results returned were  analyzed. In most scientific disciplines journals are the most important forum for scientific  discussion; they can be readily processed and a relatively small amount of journals yields a  representative and evaluable amount of results."}
{"pdf_id": "0707.3575", "content": "o Arts & Humanities Citation Index (AH = 1,149 Titles) contains journals from  the Humanities  o Social Science Citation Index (SSCI = 1,917 Titles) contains international  social science journals3  o Science Citation Index (SCI = 3,780 Titles) contains journals from  Science/Technology and Medicine  • Open Access journals from the Directory of Open Access Journals (DOAJ, see  http://www"}
{"pdf_id": "0707.3575", "content": "• Step 4: Analysis and aggregation of the extracted data. The extracted data was aggregated  using simple counts. We first counted each journal whose title could either be clearly  identified or not. The results which could be matched were ordered according to the four  different types of documents and counted (see Fig. 3). For each result matched to a"}
{"pdf_id": "0707.3575", "content": "In addition to the relevance of a reference users are also interested in the availability of  documents. The best case scenario is when users are directly linked to the full text; less  favorable is when only a citation is displayed with the opportunity to query further via Google  Web Search. The first line determines the type of the record. Certain types of documents are  marked by brackets in front of the actual title to indicate their type."}
{"pdf_id": "0707.3575", "content": "If the record is a link, the main web server is denoted (see 2 in Fig. 3). If there are multiple  sources, these can be reached by clicking the link \"group of xy\" (see (2.1) in Fig. 3). These  links were not included in the analysis; we only analyzed the main link for each linked record."}
{"pdf_id": "0707.3575", "content": "Google Scholar supports phrase search in limited fashion so journals will be searched and  displayed which do not necessarily contain the search term as a phrase. For this reason every  record was individually checked and only counted as a hit when the exact title (see (4) in Fig.  3) was found."}
{"pdf_id": "0707.3575", "content": "Table 3 shows the 25 servers most frequently offering journal articles of the SCI list. The  description column categorizes the type of server. Publisher indicates a commercial server  offered by an academic publisher where there is a fee for full-text downloads; Scientific portal  stands for servers offering free references and full-texts, although they do not always link  directly to the full text in every case. For some there may be more than a single appropriate  description, for example, portal.acm.org is a publisher and scientific portal. Open Access  describes open access servers which deliver full-text free of charge."}
{"pdf_id": "0707.3575", "content": "Our results show that the expanding sector of open access journals (DOAJ list) is  underrepresented among the servers. Something that remains unclear is why journal articles  that are freely available on web servers are not readily listed by Google Scholar even though  they are searchable via the classic Google Web Search. Although Google Scholar claims to  provide \"scholarly articles across the web,\" the ratio of articles from open access journals or  the full-text (eprints, preprints) is comparably low."}
{"pdf_id": "0707.3575", "content": "In comparison with many abstracting and indexing databases, Google Scholar does not offer  the transparency and completeness to be expected from a scientific information resource.  Google Scholar can be helpful as a supplement to retrieval in abstracting and indexing  databases mainly because of its coverage of freely accessible materials."}
{"pdf_id": "0707.3781", "content": "In this article, we study translations between variants of defaults logics such that the extensions of the theories that are the input and the output of the translation are in a bijective correspondence. We assume that a translation can introduce new variables and that the result of translating a theory can either be produced in time polynomial in the size of the theory or its output is of size polynomial in the size of the theory; we restrict to the case in which the original theory has extensions. This study fills a gap between two previous pieces of work, one studying bijective translations among restrictions of default logics, and the other one studying non-bijective translations between default logics variants."}
{"pdf_id": "0707.3781", "content": "All semantics select a set of processes that satisfy two conditions: success and closure. Intuitively, success means that the justifications of the applied defaults are not contradicted; closure means that no other default should be applied. The particular definitions of success and closure depend on the specific semantics; in turn, closure can be defined in terms of applicability of a default. The following are the definitions used by the variants of default logic considered in this paper."}
{"pdf_id": "0707.3781", "content": "The existence or non-existence of polynomial-time trans lations do not give an answer to the question \"is it true that, for every formula in the first semantics, there exists a formula in the second semantics that is equivalent to it and only polynomially larger than it?\" A polysize translation from the first semantics to the second instead provides a positive answer to this question"}
{"pdf_id": "0707.3781", "content": "In this section, we show some bijective faithful reductions that require polynomial time only once given one of the strongest extensions E of the original theory is known. Such translations are polynomial-time given a formula that is equivalent to E; since E is deductive closure of the consequences of some defaults in the theory, a formula of polynomial size that is equivalent to E exists. Since these translations produce a polynomially sized result, they are polynomial-size."}
{"pdf_id": "0707.3781", "content": "The correspondence between the processes of the original and the translated theory is not bijective. Indeed, many processes of the translated theory generate the extension E, while the same extension can be generated by one or few processes in the original theory. Onereason is that more than one constrained process might generate an extension that is var equivalent to E. On the other hand, we can prove that all such processes generate the same extension."}
{"pdf_id": "0707.3781", "content": "Proof. Consider the first default T e RC(d, i) that follows T g RC(D). All defaults between these two are in the form T n RC(d, i) because this process does not contain T s RC(D) and T e RC(d, i) is the first one after T g RC(D). By Lemma 15, the default T e RC(d, i) can be moved immediately after the default T g RC(D). In other words, if there exists a globally successful process in which T e RC(d, i) follows T g RC(D), then the following is also a globally successful process:"}
{"pdf_id": "0707.3781", "content": "These defaults can only be applied if the precondition of the original default is entailed. In particular, if the justification of the original default is contradicted, we have a choice of applying the first or the second default. If the original default is instead applicable, we are forced applying the first default. The fact that the first default can be applied even if the original default cannot will not be a problem, as these processes will be at a later time forced to generate the known extension E. As above, we have the default that generates the known extension, and which can always be applied:"}
{"pdf_id": "0707.4289", "content": "Abstract—In this paper, we employ Probabilistic Neural Net work (PNN) with image and data processing techniques to implement a general purpose automated leaf recognition for plant classification. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the PNN. The PNN is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. Compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation."}
{"pdf_id": "0707.4289", "content": "The leaf image is acquired by scanners or digital cameras. Since we have not found any digitizing device to save the image in a lossless compression format, the image format here is JPEG. All leaf images are in 800 x 600 resolution. There is no restriction on the direction of leaves when photoing. An RGB image is firstly converted into a grayscale image. Eq. 1 is the formula used to convert RGB value of a pixel into its grayscale value."}
{"pdf_id": "0707.4289", "content": "where R, G, B correspond to the color of the pixel, respec tively.The level to convert grayscale into binary image is deter mined according to the RGB histogram. We accumulate the pixel values to color R, G, B respectively for 3000 leaves and divide them by 3000, the number of leaves. The average histogram to RGB of 3000 leaf images is shown as Fig. 2."}
{"pdf_id": "0707.4289", "content": "4) Leaf Area: The value of leaf area is easy to evaluate, just counting the number of pixels of binary value 1 on smoothed leaf image. It is denoted as A.5) Leaf Perimeter: Denoted as P, leaf perimeter is calcu lated by counting the number of pixels consisting leaf margin."}
{"pdf_id": "0707.4289", "content": "where Wi is the vector made of the i-th row of W and bi is the i-th element of bias vector b. 3) Some characteristics of Radial Basis Layer: The i-th element of a equals to 1 if the input p is identical to the i-th row of input weight matrix W. A radial basis neuron with a weight vector close to the input vector p produces a value near 1 and then its output weights in the competitive layer will pass their values to the competitive function which will be discussed later. It is also possible that several elements of a are close to 1 since the input pattern is close to several training patterns."}
{"pdf_id": "0707.4289", "content": "4) Competitive Layer: There is no bias in Competitive Layer. In Competitive Layer, the vector a is firstly multiplied with layer weight matrix M, producing an output vector d. The competitive function, denoted as C in Fig. 5, produces a 1 corresponding to the largest element of d, and 0's elsewhere. The output vector of competitive function is denoted as c. The index of 1 in c is the number of plant that our system can classify. It can be used as the index to look for the scientific name of this plant. The dimension of output vector, K, is 32 in this paper."}
{"pdf_id": "0707.4289", "content": "Since the essential of the competitive function is to output the index of the maximum value in an array, we plan to let our algorithm output not only the index of maximum value, but also the indices of the second greatest value and the third greatest value. It is based on this consideration that the index"}
{"pdf_id": "0707.4289", "content": "This paper introduces a neural network approach for plant leaf recognition. The computer can automatically classify 32 kinds of plants via the leaf images loaded from digital cameras or scanners. PNN is adopted for it has fast speed on training and simple structure. 12 features are extracted and processed by PCA to form the input vector of PNN. Experimental result indicates that our algorithm is workable with an accuracy greater than 90% on 32 kinds of plants. Compared with other methods, this algorithm is fast in execution, efficient in recognition and easy in implementation. Future work is under consideration to improve it."}
{"pdf_id": "0707.4289", "content": "Prof. Xin-Jun Tian, Department of Botany, School of LifeSciences, Nanjing University provided the lab and some ad vises for this research. Yue Zhu, a master student of Department of Botany, School of Life Sciences, Nanjing University, helped us sampling plant leaves. Ang Li and Bing Chen from Institute of Botany, Chinese Academy of Science, provided us some advises on plant taxonomy and searched the scientific name for plants. Shi Chen, a PhD student from School of Agriculture, Pennsylvania State University, initiated another project which inspired us this research.The authors also wish to thank secretary Crystal Hwan Ming Chan, for her assistance to our project."}
{"pdf_id": "0708.0505", "content": "provide a better scalability.In this work we make a preliminary conceptual analysis on the use of meta heuristics for the Haplotype Inference problem. We start introducing the Haplotype Inference problem in Section 2 and then we present two possible local search models for the problem (Section 3) highlighting the possible benefits and drawbacks of each model. Section 4 contains the description of metaheuristic approaches that, in our opinion, could be adequate for Haplotype Inference. In Section 5 we consider the role of constructive techniques in the hybridization with metaheuristics and, finally, in Section 6 we discuss our proposals and outline future developments."}
{"pdf_id": "0708.0505", "content": "It is possible to define a graph that express the compatibility between genotypes, so as to avoid unnecessary checks in the determination of the resolvents.2 Let us build the graph G = (G, E), in which the set of vertices coincides with the set of the genotypes; in the graph, a pair of genotypes g1, g2 are connected by an edge whether they are compatible, i.e., one or more common haplotypes can resolve both of them. For example, the genotypes (2210) and (1220) are compatible, whereas genotypes (2210) and (1102) are not compatible. The formal definition of this property is as follows."}
{"pdf_id": "0708.0505", "content": "Observe that the set of compatible genotypes of a haplotype can contain only mutually compatible genotypes (i.e., they form a clique in the compatibility graph). Another interesting observation is the following. Due to the resolution definition, when one of the two haplotypes composing the pair, say h, has been selected, then the other haplotype can be directly inferred from h and the genotype g thanks to the resolution conditions."}
{"pdf_id": "0708.0505", "content": "We start our conceptual analysis of metaheuristic approaches for Haplotype Inference with the basic building blocks of local search methods. Indeed, in order to apply this class of methods to a given problem we need to specify three entities, namely the search space, the cost function and the neighborhood relation, that constitute the so-called local search model of the problem."}
{"pdf_id": "0708.0505", "content": "The second approach for tackling the Haplotype Inference problem defines a search strategy that tries to minimize |H| and resolve all the genotypes at the same time. In such a case, it is possible that some genotypes are not resolved during search, therefore also states which are infeasible w.r.t. the original problem formulations are explored during search. We will illustrate two possible strategies for implementing metaheuristics based on this problem formulation."}
{"pdf_id": "0708.0505", "content": "We have presented a feasibility study on the application of metaheuristics to the Haplotype Inference problem. The main purpose of this work was to point out critical design issues about the problem in order to guide future developments and to foster further research on metaheuristic approaches to this problem. Indeed, we believe that the Haplotype Inference problem could become a relevant problem subject of application of metaheuristic techniques. However, besides the relevance of the Haplotype Inference problem itself, this preliminary analysis has posed some"}
{"pdf_id": "0708.0505", "content": "To the best of our knowledge, there have been no attempts to exploit structural properties of the problem which can be deduced from compatibility graphs, or other problem representations. In this section, we present a reduction procedure that starts from a set of haplotypes in the complete representation and tries to reduce its cardinality by exploiting compatibility properties of the instance. Other heuristics based on graph representation of the problem are subject of ongoing work."}
{"pdf_id": "0708.0694", "content": "This has led to the development of specialized part-of-speech (POS) tag sets (such as SPECIALIST [28]), POS taggers (such as MedPost [33]), ontologies [11], text processors (such as MedLEE [15]), and full IE systems, such as GENIES [16], MedScan [29], MeKE [4], Arizona Relation Parser [10], and GIS [5]"}
{"pdf_id": "0708.0694", "content": "systems or modifying existing systems were time consuming [20]. Although work by Grover [17] suggested that native generic tools may be used for biological text, a recent review had highlighted successful uses of a generic text processing system, MontyLingua [14, 23], for a number of purposes [22]. For example, MontyLingua has been used to process published economics papers for concept extraction [35]. The need to modify generic text processors had not been formally examined and the question of whether an un-modified, generic text processor can be used in biological text analysis with comparable performance, remains to be assessed."}
{"pdf_id": "0708.0694", "content": "[23], in a two-layered generalization-specialization architecture [29] where the generalization layer processes biological text into an intermediate knowledge representation for the specialization layer to extract genic or entity-entity interactions. This system demonstrated 86.1% precision using Learning Logic in Languages 2005 evaluation data [9], 88.1% and 90.7% precisions in extracting protein-protein binding and activation interactions respectively. Our results were comparable to previous work which modified generic text processing systems which reported precision ranging from 53% [24] to 84% [5], suggesting this modification may not improve the efficiency of information retrieval."}
{"pdf_id": "0708.0694", "content": "We have developed a biological text mining system, known as Muscorian, for mining protein-protein inter-relationships in the form of subject-relation-object (for example, protein X bind protein Y) assertions. Muscorian is implemented as a 3-module sequential system of entity normalization, text analysis, and protein-protein binding finding, as shown in Figure 1. It is available for academic and non-profit users through http://ib-dwb.sf.net/Muscorian.html."}
{"pdf_id": "0708.0694", "content": "accuracy and consistency. The dictionary was assembled as follows: firstly, a set of 25000 abstracts from PubMed was used to interrogate Stanford University's BioNLP server [3] to obtain a list of long forms with its abbreviations and a calculated score. Secondly, only results with the score of more than 0.88 were retained as it is an inflection point of ROC graph [3], which is a good balance between obtaining the most information while reducing curation efforts. Lastly, the set of long form and its abbreviations was manually curated with the help of domain experts."}
{"pdf_id": "0708.0694", "content": "Entity normalized abstracts were then analyzed textually by an un-modified text processing engine, MontyLingua [14], where they were tokenized, part-of-speechtagged, chunked, stemmed and processed into a set of assertions in the form of 3element subject-verb-object(s) (SVO) tuple, or more generally, subject-relation object(s) tuple. Therefore, a sequential pattern of words which formed an abstract was transformed through a series of pattern recognition into a set of structurally-definable assertions."}
{"pdf_id": "0708.0694", "content": "sentences had to be separated into individual sentences. This is done by regular expression recognition of sentence delimiters, such as full-stop, ellipse, exclamation mark and question mark, at the end of a word (regular expression: ([?!]+|[.][.]+)$) with an exception of acronyms. Acronyms, which are commonly represented with a full-stop, for example \"Dr.\", are not denoted as the end of a sentence and were generally prevented by an enumeration of common acronyms."}
{"pdf_id": "0708.0694", "content": "English sentence can be grammatically constructed with virtually unlimited words and unlimited ideas) was collapsed into a sequence of part-of-speech tags, in this case, Penn TreeBank Tag Set [25], with only about 40 tags. Therefore, tagging reduced the large number of English words to about 40 \"words\" or tags."}
{"pdf_id": "0708.0694", "content": "phase, where the verb phrase may be reduced into more noun phrases, verbs, and verb phrases. More precisely, the English language is an example of subject-verb-object typology structure, which accounts for 75% of all languages in the world [7]. Thisconcept of English sentence structure is used to process a tagged sentence into higher order structures of phrases by a process of chunking, which is a precursor to the extraction of semantic relationships of nouns into SVO structure. Using only the sequence of tags, chunking was performed as a recursive 4-step process: protecting"}
{"pdf_id": "0708.0694", "content": "verbs, recognition of noun phrases, unprotecting verbs and recognition of verb phrases. Firstly, verb tags (VBD, VBG and VBN) were protected by suffixing the tags. The main purpose was to prevent interference in recognizing noun phrases. Secondly, noun phrases were recognized by the following regular expression pattern of tags:"}
{"pdf_id": "0708.0694", "content": "Firstly, each word was matched against a set of rules for specific stemming. For example, the rule \"dehydrogenised verb dehydrogenate\" defines that if the word \"dehydrogenised\" was tagged as a verb (VBD, VBG and VBN tags), it would be stemmed into \"dehydrogenate\". Similarly, the words \"binds\", \"binding\" and \"bounded\" were stemmed to \"bind\". Secondly, irregular words which could not be stemmed by removal of prefixes and suffixes, such as \"calves\" and \"cervices\", were stemmed by a pre-defined dictionary. Lastly, stemming was done by simple removal of prefixes or suffixes from the word based on a list of common prefixes or suffixes. For example, \"regards\" and \"regarding\" were both stemmed into \"regard\"."}
{"pdf_id": "0708.0694", "content": "The protein-protein binding finder module is a data miner for protein-protein binding interaction assertions from the entire set of subject-relation-object (SVO) assertions from the text analysis process using apriori knowledge. That is, the set of proteins of interest must be known, in contrast to an attempt to uncover new protein entities, and their binding relationships with other protein entities, that were not known to the researcher."}
{"pdf_id": "0708.0694", "content": "direction, making it a vector quality. However, this requirement was not biologically significant to protein-protein binding interactions, which is scalar. For example, \"X binds to Y\" and \"Y binds to X\" have no biological difference. Hence, this requirement of directionality was eliminated and the precision and recall was 86.1% and 30.7% respectively."}
{"pdf_id": "0708.0694", "content": "A large scale mining of protein-protein binding interactions was carried out using all of the PubMed abstracts on mouse (about 860000 abstracts), which were obtained using \"mouse\" as the keyword for searches, with a predefined set of about 3500 abbreviated protein entities as the list of proteins of interest (available from http://cvs.sourceforge.net/viewcvs.py/ib-dwb/muscorian-data/protein_accession.csv? rev=1.2&view=markup). In this experiment, the primary aim was to apply Muscorian to large data set and the secondary aim was to look for multiple occurrences of the same interactions as multiple occurrences might greatly improve precision"}
{"pdf_id": "0708.0694", "content": "with respect to mining protein-protein binding interactions is 82%, which means that every binding assertion has an 18% likelihood of not having a corresponding representation in the published abstracts. However, if 2 abstracts yielded the same binding assertion, the probability of both being wrong was reduced to 3.2% (0.182), and the corresponding probability that at least one of the 2 assertions was correctly represented was 96.8% (1-0.182). The more times the same assertion was extracted from multiple sources text (abstracts), the higher the possibility that the mined interaction was represented at least once in the set of abstracts. For example, if 5 abstracts yielded the same assertion, the possibility that at least one of the 5 assertions was correctly represented would be 99.98% (1-0.185)."}
{"pdf_id": "0708.0694", "content": "protein-protein binding finder module as described in Section 3.3 previously. The only difference was that raw assertion output from MontyLingua was filtered for activation-related assertions, instead of binding-related assertions, before analysis for the presence of protein names in both subject and object nouns from a pre-defined list of proteins of interest. For example, by modifying the Protein-Protein Binding Finding module to look for the verb 'activate' instead of 'bind', it can then be used for mining protein-protein activation interactions. A trial was done for insulin activation and a subgraph is illustrated in Figure 4 below."}
{"pdf_id": "0708.0694", "content": "receptor binds to IL-10 promoter through IRF and IRAK-1, which is an important insulin receptor signalling pathway. In addition, our data shows insulin activates CREB via Raf-1, MEK-1 and MAPK, which is consistent with the MAP kinase pathway. Combining these data (Figures 2 and 4) indicated that insulin activates CREB via MAP kinase pathway, and CREB binds to cpg15 promoter in the nucleus. A simple keyword search on PubMed, using the term \"cpg15 and insulin\" (done on 30th of April, 2007), did not yield any results, suggesting that the effects of insulin on cpg15, also known as neuritin [2], had not been studied thoroughly. This might also suggest limited knowledge shared between insulin investigators and cpg15"}
{"pdf_id": "0708.0694", "content": "investigators as suggested by Don Swanson in his classical paper describing the links between fish oil and Raynaud's syndrome [34]. Neuritin is a relatively new research area with less than 20 papers published (as of 30th of April, 2007) and had been implicated as a lead for neural network re-establishment [18], suggesting potential collaborations between endocrinologists and neurologists."}
{"pdf_id": "0708.0694", "content": "For example, 30% recall essentially means a loss of 70% of the information; however, if the same information (in this case, protein interactions) were mentioned in 3 or more abstracts, there is still a reasonable chance to believe that information from at least 1 of the 3 or more abstracts will be extracted"}
{"pdf_id": "0708.0694", "content": "activation interactions between entities was performed by domain experts comparing the assertions with their source abstracts. Both approaches gave similar precision measures and are consistent with the evaluation using LLL05 test set. The ANOVA test demonstrated that there was no significant differences between these three precision measures. Taken together, these evaluations strongly suggested that Muscorian performed with precisions between 86-90% for genic (gene-protein and"}
{"pdf_id": "0708.0741", "content": "The Web has become a global tool for sharing informa tion. It can be represented as a huge graph which consists of billions of hypertext web pages connected by hyperlinks pointing from one web page to another [4, 11]. Each web page is part of a larger web site, which is loosely defined as a group of web pages whose URL addresses use the same domain name, such as cs.ucl.ac.uk and ieee.org."}
{"pdf_id": "0708.0741", "content": "We brieny review and define the following topological properties, which are grouped into three orders according to the scope of information required to compute them [12].These are (i) the 1st-order properties, e.g. degree distribu tion, (ii) the 2nd-order properties, e.g. degree correlationand rich-club connectivity, and (iii) the 3rd-order proper ties, e.g. triangle coefficient and clustering coefficient."}
{"pdf_id": "0708.0741", "content": "The most studied topological property for large networks isthe degree distribution P(k), which is defined as the proba bility that a randomly selected node has degree k. A random graph [7] is characterised by a Poisson degree distributionwhere the distribution peaks at the network's average de gree. It has been reported that a number of networks [2] follow a power-law degree distribution,"}
{"pdf_id": "0708.0741", "content": "A more widely studied 3rd-order property is the clustering coefficient C, which is defined as the ratio of actual links among a node's neighbours to the maximal possible number of links they can share [23]. The clustering coefficient of a node can be given as a function of a node's degree and its triangle coefficient,"}
{"pdf_id": "0708.0741", "content": "WT10g is a mega dataset of the Web proposed by the annual international Text REtrieval Conference (TRECs, http://trec.nist.gov). WT10g is constructed from more than 320 gigabytes of archived data containing1.7M web pages and hyperlinks between them. It is re ported that WT10g retains properties of the larger Web [21] and has been used as a data resource for research on Web retrieval and modelling. We randomly sampled 10 subsets of WT10g, each of which contains 50,000 web pages and links between those pages. In this paper we use the average properties of the 10 WT10g subsets as an approximation of the Web's link structure."}
{"pdf_id": "0708.0741", "content": "The Internet topology at the autonomous systems (AS) level has been extensively studied in recent years [18, 25, 13, 12]. On the AS Internet, nodes represent Internet service providers and links represent connections between them. Inthis paper we use the AS Internet dataset ITDK0304 col lected by CAIDA [1]."}
{"pdf_id": "0708.0741", "content": "Figure 4b shows that the citation network and the AS Inter net are typical disassortative networks where knn decreases monotonically with k. The BA model is an example of a neutral network where knn does not change with k. For the average of the web sites, and the Web, knn first increases and then decreases with k, and peaks at k = 30 and k = 15 respectively. For large degrees, the average knn of the web sites is significantly larger than all other networks."}
{"pdf_id": "0708.0741", "content": "Figure 4e shows that, in general, all the networks exhibita positive correlation between triangle coefficient and de gree. This is because the larger the degree of a node, the more neighbours a node has, and thus the higher the chance of forming triangles. As discussed in Section 4.1.2, all theweb sites exhibit a very similar relationship between trian gle coefficient and degree, that is well characterised by theaverage over all the web sites. The average correlation be tween triangle coefficient and degree of the web sites can be closely fitted by a function given as"}
{"pdf_id": "0708.1150", "content": "project at the Research Library of the Los Alamos NationalLaboratory aims at developing metrics for assessing scholarly communication artifacts (e.g. articles, journals, confer ence proceedings, etc.)and agents (e.g. authors, institu tions, publishers, repositories, etc.) on the basis of scholarly usage. In order to do this, the MESUR project makes use of a representative collection of bibliographic, citation and usage data. This data is collected from a wide variety ofsources including academic publishers, secondary publish ers, institutional linking servers, etc. Expectations are that the collected data will eventually encompass tens of millions of bibliographic records, hundreds of millions of citations,"}
{"pdf_id": "0708.1150", "content": "source identified by URIb, where URIa and URIb are nodes and http://xmlns.com/foaf/0.1/#knows is a directed labeled edge (see Figure 2). The meaning of knows is fully defined by the URI http://xmlns.com/foaf/0.1/. Theunion of instantiated FOAF triples is a FOAF semantic network. Current platforms for storing and querying such se mantic networks are called triple stores. Many open sourceand proprietary triple stores currently exist. Various querying languages exist as well [13]. The role of the query lan guage is to provide the interface to access the data contained in the triple store. This is analogous to the relationships"}
{"pdf_id": "0708.1150", "content": "In the above query, the ?x variable is bound to any node that is the domain of a triple with an associated predicate of http://xmlns.com/foaf/0.1/#knows and a range of http://homepages.vub.ac.be/#cgershen. Thus, the above query returns all people who know vub:cgershen (i.e. Carlos Gershenson). The ontology plays a significant role in many aspects of a semantic network. Figure 3 demonstrates the role of the ontology in determining which real world data is harvested,how that data is represented inside of the triple store (se mantic network), and finally, what queries and inferences are possible to execute."}
{"pdf_id": "0708.1150", "content": "3. SCHOLARLY ONTOLOGIES In general, an ontology's classes, their relationships, andinferences are determined according to what is being mod eled, for what problems that model is trying to solve, and how that model's classes can be instantiated according to real world data.Thus, there were three primary require ments to the development of the MESUR ontology:"}
{"pdf_id": "0708.1150", "content": "5. LEVERAGING RELATIONAL DATABASE TECHNOLOGYThe MESUR project makes use of a triple store to rep resent and access its collected data. While the triple store is still a maturing technology, it provides many advantagesover the relational database model. For one, the network based representation supports the use of network analysis algorithms. For the purposes of the MESUR project, a network-based approach to data analysis will play a majorrole in quantifying the value of the scholarly artifacts con tained within it. Other benefits that are found with triple"}
{"pdf_id": "0708.1150", "content": "The two tables demonstrate how bibliographic and usage data can be easily represented in a relational database. From the relational database representation, a RDF N-Triple6 data file can be generated. One such solution for this relational database to triple store mapping is the D2R mapper [24]. However, note that not all data in the relational database is exported to this intermediate format. Instead, only those properties that promote triple store scalability and usage research were included. Thus, article titles, journal issues"}
{"pdf_id": "0708.1150", "content": "6. THE MESUR ONTOLOGY The MESUR ontology is currently at version 2007-01 athttp://www.mesur.org/schemas/2007-01/mesur (abbreviated mesur). Full HTML documentation of the ontology can be found at the namespace URI. The following sections will describe how bibliographic and usage data is mod eled to meet the requirements of understanding large-scaleusage behavior, while at the same time promoting scalabil ity."}
{"pdf_id": "0708.1150", "content": "a particular Context. However, as will be demonstrated, direct relationships can be inferred. All inferred properties are denoted by the \"(i)\" notation in the following UML classdiagrams. All inferred properties are supernuous relation ships since there is no loss of information by excluding theirinstantiation (the information is contained in other relation ships). The algorithms for inferring them will be discussed in their respective Context subsection. Currently, all the MESUR classes are specifications or generalizations of other classes. No holonymy/meronymy(composite) class definitions are used at this stage of the ontology's development. Figure 6 presents the complete taxon omy of the MESUR ontology. This diagram primarily serves as a reference. Each class will be discussed in the following sections."}
{"pdf_id": "0708.1150", "content": "In general, Document objects are those artifacts that are written, used, and published by Agents. Thus, a Document can be a specific article, a book, or some grouping such as a Journal, conference Proceedings, or an EditedBook. There are two Document subclasses to denote whether theDocument is a collection (Group) or an individually written work (Unit). A Journal and Proceedings is an ab stract concept of a collection of volumes/issues.An edition to a proceedings or journal is associated with its ab stract Group by the partOf property. The authoredBy, containedIn, publishedBy, and contains properties can be inferred from the Publishes context. Also, the usedBy property can be inferred from the Uses context."}
{"pdf_id": "0708.1150", "content": "6.4 The Context Classes As previously stated, all properties from the Agent and Document classes that are marked by the \"(i)\" notation are inferred properties. These properties can be automatically generated by inference algorithms and thus, are not required for insertion into the triple store. What this means is that inherent in the triple store is the data necessary to infersuch relationships. Depending on the time (e.g. query com plexity) and space (e.g. disk space allocation) constraints,"}
{"pdf_id": "0708.1150", "content": "the inclusion of these inferred properties is determined. At any time, these properties can be inserted or removed from the triple store.The various inferred properties are de termined from their respective Context objects.Therefore, the MESUR owl:ObjectProperty taxonomy pro vides two types of object properties: ContextProperty and InferredProperty (see Figure 9)."}
{"pdf_id": "0708.1150", "content": "A Context class is an N-ary operator much like an rdf:Bag.Current triple store technology expresses tertiary relation ships. That means that only three resources are related by a semantic network edge (i.e. a subject URI, predicateURI, and object URI). However, many real-world relation ships are the product of multiple interacting objects. It isthe role of the various Context classes to provide relation ships for more than three URIs. The Context classes are represented in Figure 10."}
{"pdf_id": "0708.1150", "content": "6.4.1 The Publishes Context A Publishes event states, in words, that a particular bibliographic data provider has acknowledged that a set of authors have authored a unit that was published in a group by some publisher at a particular point in time. A Publishes object relates a single bibliographic data provider, Agent authors, a Unit, an Agent publisher, a Group, anda publication ISO-8601 date time literal8. Figure 11 rep resents a Publishes context and the inferable properties(dashed edges) of the various associated artifacts. All in ferred properties have a respective inverse relationship. Notethat both PreprintArticle and Book publishing are rep resented with OWL restrictions (i.e. they are not published in a Group). The details of these restrictions can be found in the actual ontology definition."}
{"pdf_id": "0708.1150", "content": "6.4.2 The Uses Context The Uses context denotes a single usage event where an Agent uses a Document at a particular point in time. The Uses context is diagrammed in Figure 12. Like thePublishes context, the Uses context is an N-ary con struct. Depending on the usage provider, a session identifier and access type is recorded. A session identifier denotes the user's login session. An access type denotes, for example, whether the used Document had its abstract viewed or was fully downloaded."}
{"pdf_id": "0708.1150", "content": "SELECT ?a ?b ? c WHERE ?x r d f : type mesur : Uses ?x mesur : hasDocument ?a ?a r d f : type mesur : A r t i c l e ?x mesur : hasUser ?b ?y r d f : type mesur : Publishes ?y mesur : hasUnit ?a ?y mesur : hasGroup ? c"}
{"pdf_id": "0708.1150", "content": "Given Unit to Unit citations, the Citation weight between any two Groups can be inferred. The following ex ample SPARQL query generates the Citation object for citations from 2007 articles in the Journal of Informetrics (ISSN: 1751-1577) to 2005-2006 articles in Scientometrics (ISSN: 0138-9130). Assume that the URI of the journals are their ISSN numbers, the date time is represented as a year instead of the lengthy ISO-8601 representation, and the COUNT command is analogous to the SQL COUNT command (i.e. returns the number of elements returned by the variable binding)."}
{"pdf_id": "0708.1150", "content": "SELECT ?a ?b WHERE ?x r d f : type mesur : A f f i l i a t i o n ?x mesur : h a s A f f i l i a t o r ?a ?x mesur : h a s A f f i l i a t e e ?b"}
{"pdf_id": "0708.1150", "content": "6.4.5 The Metric Context The primary objective of the MESUR project is to studythe relationship between usage-based value metrics (e.g. Us age Impact Factor [5]) and citation-based value metrics (e.g. ISI Impact Factor [15] and the Y-Factor [25]). The Metriccontext allows for the explicit representation of such met rics. The Metric context has both the NumericMetric and NominalMetric subclasses. Figure 16 diagrams the 2007 ImpactFactor numeric metric context for a Group.Note that the Context hierarchy in Figure 10 does not rep resent the set of Metrics explored by the MESUR project. This taxonomy will be presented in a future publication."}
{"pdf_id": "0708.1150", "content": "The 2007 Usage Impact Factor for the JCDL Proceedings can be calculated by using the following SPARQL queries and INSERT commands. The 2007 Usage Impact Factor for the JCDL is defined as the number of usage events in 2007 that pertain to articles published in the JCDL proceedings in either 2005 or 2006 normalized by the total number of articles published by the JCDL in 2005 and 2006 [5]."}
{"pdf_id": "0708.1150", "content": "As demonstrated, the presented metrics can be easily calculated using simple SPARQL queries. However, more com plex metrics, such as those that are recursive in definition, can be computed using other semantic network algorithms. For example, the eigenvector-based Y-Factor [25] can be computed in semantic networks using the grammar-based random walker framework presented in [26].The objec tive of the MESUR project is to understand the space of such metrics and their application to valuing artifacts in the"}
{"pdf_id": "0708.1527", "content": "Abstract. This is to present work on modifying the Aleph ILP system so that it evaluates the hypothesised clauses in parallel by distributing the data-set among the nodes of a parallel or distributed machine.The paper brieny discusses MPI, the interface used to access message passing libraries for parallel computers and clusters. It then proceeds to describe an extension of YAP Prolog with an MPI interface and an implementation of data-parallel clause evaluation for Aleph through this interface. The paper concludes by testing the data-parallel Aleph on artificially constructed data-sets."}
{"pdf_id": "0708.1527", "content": "where MPI_Send() would dispatch count bytes from memory location message to the node of rank dest. To receive the message, the recipient must issue an MPI_Recv() specifying: the maximum number of bytes to accept and where to place them; the source node's rank or MPI_ANY_SOURCE; the message's type and tag (or MPI_ANY_TYPE and MPI_ANY_TAG, respectively); and the memory location where the status of the transfer should be stored. This last MPI_Status structure includes information such as the actual message length, type and tag."}
{"pdf_id": "0708.1527", "content": "changes have been made to either the abstract machine implementation or the internal database mechanism. Just like MPI itself is not a parallelising compiler but only a message-passing mechanism, a Prolog interface to MPI only providesthe infrastructure for passing messages between the nodes of a parallel computa tion. The interface is implemented as an additional foreign library and the onlychanges made within the existing Yap code were are at the initialisation rou tine, where the mpi_* predicates are declared and the MPI-related command-line arguments extracted and stored so that they can be used by mpi_open/3."}
{"pdf_id": "0708.1527", "content": "have the predicate fail if the argument fails to unify against the term that has been received, but that would have been misleading: once the source and tag arguments match, the message will be extracted from the message queue and only then unified with Data. Since there is no way to push messages back into the head of the queue, the only reasonable design choice is to always accept a message if the tag and source match, in other words require that the first argument of mpi_receive/3 is an unbound variable. To make this point clearer, consider the two variations of the code of Figure 3"}
{"pdf_id": "0708.1527", "content": "The (correct) code to the left accepts any term (assuming the sender and tag match) and then performs the necessary checks, whereas the code to the right incorrectly assumes that because the sent message cannot be unified with the msg(file1,Text) term it expects, it will not be extracted from the queue and a second attempt to receive it can be made"}
{"pdf_id": "0708.1527", "content": "Aleph [7] is an ILP system written in Prolog. It implements (among others) the Progol algorithm [4, 5], a sequential-cover ILP algorithm. The Prolog interface to MPI libraries described above, is used to extend Aleph 3 so that it evaluates in parallel the hypothesised clauses it builds during the search for a good clause. The predicates within Aleph that were mostly innuenced were those pertaining to loading the example files (since the examples had to be distributed among the processes) and the those implementing the example-proving mechanism itself."}
{"pdf_id": "0708.1527", "content": "2. When activated with any non-zero rank value, induce/1 goes into the work ers' loop that issues a broadcast, acts upon prove requests as soon as they get broadcast, uses mpi_send/3 to transmit back to the master the list of successful examples, and returns to waiting for the next broadcast."}
{"pdf_id": "0708.1527", "content": "The second assumption might not be always satisfied, since it is the case thatin modern workstation clusters it is the delay of establishing a connection be tween nodes that is responsible for the transmission costs, rather than the low bandwidth of the network. The prove_cache/8 predicate is the entry point to the example-proving mechanism: it first checks to see if a given clause has already been proven (andcached), and if yes returns the already calculated and cached coverage, other wise it tries to prove the examples with this clause and returns (and caches) the results."}
{"pdf_id": "0708.1527", "content": "It should, then, be noted that the computation expense discussed above cannot be treated by data-parallelism, since most of the time is consumed in con structing candidate clauses and traversing the search space, rather than the bottleneck being the large amount of data against which each hypothesis needs to be tested"}
{"pdf_id": "0708.2303", "content": "Abstract. We argue for a compositional semantics grounded in a strongly typed  ontology that reflects our commonsense view of the world and the way we talk  about it in ordinary language. Assuming the existence of such a structure, we  show that the semantics of various natural language phenomena may become  nearly trivial."}
{"pdf_id": "0708.2303", "content": "We begin by making a case for a semantics that is grounded in a strongly typed  ontological structure that is isomorphic to our commonsense view of reality. In doing  so, our ontological commitments will initially be minimal. In particular, we assume  the existence of a subsumption hierarchy of a number of general categories such as  animal, substance, entity, artifact, event, etc., and where the fact that  an object of type human is also an entity, for example, is expressed as"}
{"pdf_id": "0708.2303", "content": "From the standpoint of commonsense, the reference to a book review should  imply the existence of a book, whereas the reference to a book proposal should  be considered to be a reference to a proposal of some book, a book that might not  (yet) actually exist. That is,"}
{"pdf_id": "0708.2303", "content": "2 Interestingly, type unification and the embedding of ontological types into our semantics seems also  promising in providing an explanation for the notion of metonymy in natural language. While we cannot  get into this issue here in much details, we will simply consider the following example by way of  illustration, where R is some salient relationship between a human and a hamSandwich:"}
{"pdf_id": "0708.2303", "content": "That is, we have assumed that it always makes sense to speak of a human that  attended or cancelled some event, where to attend an event is to have an existing  event; and where the object of a cancellation is an event that does not (anymore, if it  ever did) exist3. Consider now the following:"}
{"pdf_id": "0708.2303", "content": "That is, we are assuming that it always makes sense to speak of a human that painted  some painting, and of some human that found some entity. Consider now the  interpretation in (22), where it was assumed that Large is a property that applies to (or  makes sense of) objects that are of type physical."}
{"pdf_id": "0708.2303", "content": "Note that what we now have is a quantified variable, e, that is supposed to be an  object of type elephant, an object that is described by a property, where it is  considered to be an object of type physical, and an object that is in a relation in  which it is considered to be a painting"}
{"pdf_id": "0708.2303", "content": "There are two pairs of type unifications  that must now occur, namely ( elephant painting and ( elephant physical ,  where, if we recall the type unification definition given in (2), the former would result  in making the reference to e abstract and in the introduction of a new variable of type  painting"}
{"pdf_id": "0708.2303", "content": "Note that this analysis itself seems to shed some light on the nature of the ontological  categories under consideration. For example, (31) seems to be an instance of a more  generic template that can adequately represent the compositional meaning of a  number of similar nominal compounds, as illustrated in (a) below."}
{"pdf_id": "0708.2303", "content": "The general strategy we are advocating can therefore be summarized as follows: (i)  we can start our semantic analysis by assuming a set of ontological categories that are  embedded in the appropriate properties and relations (based on our use of ordinary  language); (ii) further semantic analysis of some non-trivial phenomena (such as  nominal compounds, intensional verbs, metonymy, etc.) should help us put some  structure on the ontological categories assumed in step (i); and (iii) this additional  structure is then iteratively used to repeat the entire process until, presumably, the  nature of the ontological structure that seems to be implicit in everything we say on  ordinary language is well understood."}
{"pdf_id": "0708.2303", "content": "Although we could not, for lack of space, fully demonstrate  the utility of our approach, recent results we have obtained suggest an adequate  treatment of a number of phenomena, such as the semantics of nominal compounds,  lexical ambiguity, and the resolution of quantifier scope ambiguities, to name a few"}
{"pdf_id": "0708.2432", "content": "We state an elementary inequality for the structure from motion problem for mcameras and n points. This structure from motion inequality relates space dimen sion, camera parameter dimension, the number of cameras and number points andglobal symmetry properties and provides a rigorous criterion for which reconstruc tion is not possible with probability 1. Mathematically the inequality is based on Frobenius theorem which is a geometric incarnation of the fundamental theorem of linear algebra. The paper also provides a general mathematical formalism for the structure from motion problem. It includes the situation the points can move while the camera takes the pictures."}
{"pdf_id": "0708.2432", "content": "A basic question is to find the minimal number of cameras for a given point set or the minimal number of points for a given number of cameras so that we have alocally unique reconstruction. This motivates to look for explicit inversion formu las for the structure from motion map F as well as the exploration of ambiguities: camera-point configurations which have the same image data."}
{"pdf_id": "0708.2432", "content": "How many points are needed to reconstruct both the points and the cameras up to a global symmetry transformation? This question depends on the dimension and the camera model. Assume we are in d dimensions, have n points and m cameras and that the camera has f internal individual parameters and h global parameters and that a g-dimensional group of symmetries acts on the global configuration space without changing the pictures."}
{"pdf_id": "0708.2432", "content": "Let's take the case of m = 2 and m = 3 cameras and see what the dimension inequality predicts if the manifold of all camera parameters matches dimension-wise the manifold of all possible camera point configurations. We can use the dimensioninequality to count the number of points needed for various cameras in two dimen sions. First to the stereo case with m = 2 cameras."}
{"pdf_id": "0708.2432", "content": "The dimension formula only tells hat happens generically. For example, if the camera-point configurations are contained in one single plane, the larger 2D numbers apply. Even so the dimensional analysis shows that two points should be enough in space, we need three points if the situation is coplanar and noncolinearity conditions are needed to eliminate all ambiguities. We will see with counter examples that these results are sharp. The dimension formula gives a region in the (n, m) plane, where the structure from motion problem can not have a unique solution. We call these regions forbidden region of the structure from motion problem."}
{"pdf_id": "0708.2432", "content": "We quickly look at an example of a camera, where the retinal surface is not a hypersurface. The camera Q is given by a line S in space. The map Q is the orthographic projection of a point P onto S = S(Q). How many points do we need for a reconstruction with 3 cameras? We have d = 3 and s = 1. Because a line in space is determined by a point and a direction, the dimension f of the camera manifold is f = 3. The global symmetry group consists of Euclidean transformations, which gives g = 6. The structure from motion inequality tells 3n + 3m = nm + 6 ."}
{"pdf_id": "0708.2432", "content": "If points can move, we still have nm equations and a global g dimensional sym metry group but now 3nk +3mf unknown parameters. The dimension formula still applies. But now, the dimension of the space N is d(k + 1). The point space M is larger and the retinal plane S has a much lower dimension than M. Let's formulate it as a lemma:"}
{"pdf_id": "0708.2432", "content": "We need at least m = 5 cameras to allow a reconstruction. The inequality assures us that with 4 pictures, a unique reconstruction is impossible. For m = 5 cameras, we need at least n = 11 points. For m = 6 cameras, we need at least n = 7 points. If we observe a swarm of 11 points with 5 camera frames, we expect a reconstruction of the moving points and the cameras."}
{"pdf_id": "0708.2438", "content": "Both in the plane and in space, we invert the nonlinear Ullman transformation for 3 points and 3 orthographic cameras. While Ullman's theorem assures a unique reconstruction modulo a renection for 3 cameras and 4 points, we find a locally unique reconstruction for 3 cameras and 3 points. Explicit reconstruction formulas allow to decide whether picture data of three cameras seeing three points can be realized as a point-camera configuration."}
{"pdf_id": "0708.2438", "content": "Ullman's theorem in computer vision is a prototype of a structure from motion result. Given m planes in space and n points for which we know the orthogonal projections of the points on the planes, we want to recover the planes and the points. The problem can also be formulated as follows: given a fixed orthographic camera,and a point configuration which undergoes a rigid transformation. Taking m pic tures of this rigid n-body motion, how do we reconstruct the body as well as its motion? Ullman's theorem is often cited as follows: \"For rigid transformations, a unique metrical reconstruction is known to be possible from three orthographic views of four points\" [11]."}
{"pdf_id": "0708.2438", "content": "While 3 points in general position can be reconstructed from 2 orthographic projections, if the image planes are known, one needs 3 views to recover also the camera parameters. While Ullman's theorem states four points, three points are enough for a locally unique reconstruction. Actually, already Ullman's proof demonstrated this. We produce algebraic inversion formulas in this paper. Ullman's transformation is a nonlinear polynomial map which computer algebra systems is unable to invert. Ullman's proof idea is to reconstruct the intersection lines of theplanes first, computer algebra systems produce complicated solution formulas be cause quartic polynomial equations have to be solved. Fortunately, it is possible to"}
{"pdf_id": "0708.2438", "content": "The two-dimensional Ullman problem is interesting by itself. The algebra is simpler than in three dimensions but it is still not completely trivial. The two dimensional situation plays an important role in the 3 dimensional problem because the three dimensional situation reduces to it if the three planes have coplanar normal vectors. Let's first reformulate the two-dimensional Ullman theorem in a similar fashion as Ullman did. A more detailed reformulation can be found at the end of this section."}
{"pdf_id": "0708.2438", "content": "Figure 2 The setup for the structure of motion problem with three orthographic cameras and three points in two dimensions. One point is at the origin, one camera is the x-axis. The problem is to find the y coordinates of the two points as well as the two camera angles from the scalar projections onto the lines."}
{"pdf_id": "0708.2438", "content": "Proof. With the first point P1 at the origin (0, 0), the translational symmetry of the problem is fixed. Because cameras can be translated without changing the pictures, we can assume that all camera planes go through the origin (0, 0). By having the first camera as the x-axis, the rotational symmetry of the problem is fixed. We are left with 6 unknowns, the y-coordinates of the two points (xi, yi) and the directions"}
{"pdf_id": "0708.2438", "content": "Figure 8 The setup for the structure of motion problem with three orthographic cameras and three points in three dimensions. One point is at the origin, one camera is the xy-plane. The problem is to find the z-coordinates of the two points as well as the three Euler angles for each cameras from the projections onto the planes."}
{"pdf_id": "0708.2438", "content": "Because Ullman stated his theorem with 4 points and this result is cited so widely [4, 1, 5, 3, 9, 2, 6, 10], we give more details to the proof of Ullman for 3 points. The only reason to add a 4'th point is to reduce the number of ambiguities from typically 64 to 2. We will give explicit solution formulas which provide an explicit reconstruction with in the case of 3 points. One could write down explicit algebraic expressions for the inverse."}
{"pdf_id": "0708.2438", "content": "Proof.Again we chose a coordinate system so that one of the cameras is the xy plane with the standard basis q0, p0. One of the three points P1 = O is fixed at the origin. The problem is to find two orthonormal frames pj, qj in space spanning two planes S1 and S2 through the origin and two points P2, P3 from the projection data"}
{"pdf_id": "0708.2438", "content": "On page 194 in the book [11], there are only 4 equations needed, not 5 as stated there to solve for the intersection lines of the planes. With 5 equations the number of ambiguities is reduced. Actually, the Ullman equations with 4 equations havefinitely many additional solutions which do not correspond to point-camera config urations. They can be detected by checking what projections they produce."}
{"pdf_id": "0708.2438", "content": "If the normals to the cameras are coplanar, the problem reduces to a two dimensional problem by turning the coordinate system so that the intersection line is the z-axes. This situation is what Ullman calls the degenerate case. After finding the intersection line, we are directly reduced to the two-dimensional Ullman problem."}
{"pdf_id": "0708.2438", "content": "The fact that there are solutions to the Ullman equation which do not lead to intersection lines of photographic planes could have been an additional reason for Ullman to add a 4'th point. Adding a 4'th point reduces the number of solutionsfrom 64 to 2 if the four points are noncoplanar but it makes most randomly cho sen projection data unreconstructable. With three points, there is an open and algebraically defined set for which a reconstruction is not possible and and open algebraically defined set on which the reconstruction is possible and locally unique. The boundary of these two sets is the image of the set det(F) = 0."}
{"pdf_id": "0708.2438", "content": "We have studied the structure from motion problem for spherical cameras in detail in the paper [7] and shown for example that for three cameras and three points in the plane a unique reconstruction is possible if both the camera and point sets are not collinear and the 6 points are not in the union of two lines"}
{"pdf_id": "0708.2442", "content": "The field of image reconstruction is part of computer vision and also related to photogrammetry [23], where the focus is on accurate measurements. In the motion picture industry, reconstructions are used for 3D scanning purposes or to render computer generated images CGI. Most scanning and CGI methods often work with known camera positions or additional objects are added to calibrate the cameras with additional geometric objects. As mentioned above, the problem iscalled simultaneous localization and mapping problem in the robotics liter ature and is also known as concurrent mapping and localization."}
{"pdf_id": "0708.2442", "content": "We know from daily experience that we can work out the shape and position of the visible objects as well as our own position and direction while walking through our surroundings. Objects closer to us move faster on the retinal surface, objects far away do less. It is an interesting problem how much and by which way we can use this information to reconstruct our position and surroundings [11, 25]. Even with moving objects, we can estimate precisely the position and speed of objects. For example, we are able to predict the trajectory of a ball thrown to us and catch it."}
{"pdf_id": "0708.2442", "content": "The mathematical problem of reconstructing of our surroundings from obser vations can be considered as one of the oldest tasks in science at all because it is part of an ancient astronomical quest: the problem of finding the positions and motion of the planets when observing their motion on the sky. The earth is theomni-directional camera moving through space. The task is to compute the posi tions of the planets and sun as well as the path of the earth which is the camera. This historical case illustrates the struggle with the structure from motion problem:"}
{"pdf_id": "0708.2442", "content": "An other seed of interest in the problem is the two dimensional problem of nautical surveying. A ship which does not know its position but its orientationmeasures the angles between various points it can see. It makes several observa tions and observes cost points. The task is to draw a map of the coast as well as to reconstruct the position of the ship. [1]."}
{"pdf_id": "0708.2442", "content": "In practice, an omni-directional camera can be considered oriented if an arrow of gravity and the north direction vector are both known. A robot on earth with a spherical camera is oriented if it has a compass built in. It could also orient itself with some reference points at infinity. We discuss in a later section how one can recover the orientation from the camera frames."}
{"pdf_id": "0708.2442", "content": "We now solve the reconstruction problem for oriented omni-directional cameras in the plane. This two-dimensional reconstruction will be an integral part of the general three-dimensional reconstruction for oriented omni-directional cameras. It turns out that for the omni-directional inverse problem with oriented cameras, the uniqueness of the reconstruction in space is already determined by the uniqueness in the plane, because if the first two coordinates of all points are known, then the height coordinate is determined uniquely by the slopes up to a global translation. How many points and cameras do we need?"}
{"pdf_id": "0708.2442", "content": "Figure 1 The forbidden region in the (n, m) plane for oriented omni-directional cameras. In the plane, (m, n) = (3, 3) is a border line case. In space, (m, n) = (2, 2) is a border line case. For (m, n) outside the forbidden region, the reconstruction problem is over-determined."}
{"pdf_id": "0708.2442", "content": "It is important to know when the reconstruction is unique and if the system is overdetermined, when the least square solution is unique. In a borderline case, the matrix A is a square matrix and uniqueness is equivalent to the invertibility of A. In the overdetermined case, we have a linear system Ax = b. There is a unique least square solution if and only if the matrix A has a trivial kernel."}
{"pdf_id": "0708.2442", "content": "For ambiguous configurations, the solution space to the reconstruction is a linear space of positive dimension. Examples of an ambiguous configuration are collinear configurations, where all points as well as the camera path lie on one line. In that case, the points seen on the image frames are constant. One can not reconstruct the points nor the camera positions."}
{"pdf_id": "0708.2442", "content": "Theorem 4.1 (Structure from motion for omni cameras in the plane I) If both the camera positions as well and the point positions are not collinear and the union of camera and point positions are not contained in the union of two lines, then the camera pictures uniquely determine the circular camera positions together with the point locations up to a scale and a translation."}
{"pdf_id": "0708.2442", "content": "Even so the actual reconstruction is a problem in linear algebra, this elementary result is of pure planimetric nature: we have two non-collinear point sets P, Q whose union is not in the union of two lines, then the angles between points in P and Q determine the points P, Q up to scale and translation"}
{"pdf_id": "0708.2442", "content": "Proof. a) If C is not on the line PQ, we know two angles and the length of one side of the triangle PQC. Similarly for the other lines QR, PR. Because the intersection of the three lines is empty, every point C is determined. b) Part b) has the same proof. Just switch P, Q, R and A, B, C."}
{"pdf_id": "0708.2442", "content": "Remark: Alternatively, we could have fixed the coordinates x2 = 1 of thesecond point P2 instead of the distance. In that case, we additionally have the pos sibility that the point P2 deforms on the line x = x2 = 1. But then, every camera must deform on the line x = x1 = 0. This violates the non-collinearity assumption for the cameras."}
{"pdf_id": "0708.2442", "content": "For points Pi = (xi, yi, zi) and camera positions Qj = (aj, bj, cj) in space, the full system of equations for the unknown coordinates is nonlinear. However, we have already solved the problem in the plane and all we need to deal with is another system of linear equations for the third coordinates zi and cj."}
{"pdf_id": "0708.2442", "content": "Theorem 5.1 The reconstruction of the scene and camera positions in three-dimensional space has a unique solution if both the xy-projections of the point configurations as well as the xy-projection of the camera configurations are not collinear and the union of point and camera projections are not contained in the union of two lines."}
{"pdf_id": "0708.2442", "content": "Remarks. 1) There is nothing special about taking the xy-plane to reduce the dimenson from 3 to 2. We can adjust the orientation of the cameras arbitrarily. So, if 3 points are not collinear in space and three camera positions in space are not collinear and thecamera-point set is not contained in the union of two lines, then a unique recon struction is possible. Also, if four points define a tetrahedron of positive volume and three camera positions are not on a line, then a unique reconstruction is possible."}
{"pdf_id": "0708.2442", "content": "Assume we take threepictures of three points and if the camera orientation is identical for all three pic tures, then we can reconstruct the point and the camera positions up to a scale and translation, if both points and cameras are not collinear and the point camera set is not contained in the union of two lines"}
{"pdf_id": "0708.2442", "content": "Figure 12 Two orientedomni directional cameras and two points in the plane. The angles between camerasand points do not determine the config uration. Arbitrary many points can be added. In three dimensions however, two points P, Q and two cameras A, B allow a reconstruction because the directions PA, PB, QA, QB of the tetrahedron sides determines theshape of the tetrahedron up to a dila tion and a Euclidean transformation. The 4 points A, B, C, D need to be non-coplanar."}
{"pdf_id": "0708.2442", "content": "The reconstruction needs more work in this case, but the problem remains lin ear if we make a Taylor expansion of each point path. Again the reconstruction is ambiguous if we do not fix one body because the entire scene as well as the camera could move with constant speed and provide alternative solutions. This ambiguity is removed by assuming one point in the scene to have zero velocity."}
{"pdf_id": "0708.2442", "content": "With moving bodies, there can be even more situations, where the motion can not be reconstructed: take an example with arbitrarily many points, but where two points P1(t), P2(t) form a line with the camera position r(t) at all times. In that case, we are not able to determine the distance between these two points because the points are on top of each other on the movie."}
{"pdf_id": "0708.2442", "content": "Remarks. 1) The situation with variable camera orientation could be put into the framework of the moving bodies. This has the advantage that the system of equations is still linear. The disadvantage is an explosion of the number of unknown variables. 2) A further refinement of the algorithm to first filter out points which are further away and only average the mean motion of those points. A rough filter is to discard points which move with large velocity. See [12] for a Bayesian approach. See also [32]."}
{"pdf_id": "0708.2974", "content": "The fuzzy vault is an algorithm for hiding a secret string S in such a way that a user who is in possession of some additional information T can easily recover S, while an intruder should face computationally infeasible problems in order to achieve this goal. The information T can be fuzzy, in the sense that the secret S is"}
{"pdf_id": "0708.2974", "content": "2.1. A brute force attack. If Victor intercepts a vault V = V(k, t, r, Fq), but has no additional information about the location of minutiae or some of their statistics, he may still try to recover S by brute force trials. For this he needs to find k points"}
{"pdf_id": "0708.2974", "content": "This requires the equivalent of r/K Lagrange interpolations. If no point is found, then discard T . 3. If T was not discarded, search for a further point which verifies (2). This step is met with probability 1/q. If a point is found, add it to T ; otherwise discard T . 4. Proceed until a break condition is encountered (no more points on the graph of g(X)) or D points have been found in T , and thus g(X) = f(X) with high probability. Adding up the numbers of operations required by the steps 1.-4., with weights given by the probabilities of occurrence, one finds:"}
{"pdf_id": "0708.2974", "content": "4.1. Using more fingers. We have shown that the parameters r, t, k, allowing to control the security factor, are naturally bounded by image size, variance of minutiae location and average number of reliable minutiae. They cannot thus be modified beyond certain bounds and it is likely that this bounds have been well established in [CKL]. It lays thus at hand to propose using for instance the imprints of two fingers rather then only one, for creating the vault. This leads practically to a squaring of the security factor."}
{"pdf_id": "0708.2974", "content": "4.4. The alternative of cryptographic security. These observations lead to the question: is the use of one - way functions and template hiding an intrinsicsecurity constraint, or just one in many conceivable approaches to securing biomet ric authentication? The second is the case, and it is perfectly feasible to construct a secure biometric authentication system based on the mechanisms used by state of the art certification authorities. The mechanisms are standard and have been"}
{"pdf_id": "0708.4170", "content": "This article presents a technique for proving problems hard for classes of the polynomial hierarchy or for PSPACE. The rationale of this technique is that some problem restrictions are able to simulate existential or universal quantifiers. If thisis the case, reductions from Quantified Boolean Formulae (QBF) to these restric tions can be transformed into reductions from QBFs having one more quantifier in the front. This means that a proof of hardness of a problem at level n in the polynomial hierarchy can be split into n separate proofs, which may be simpler than a proof directly showing a reduction from a class of QBFs to the considered problem."}
{"pdf_id": "0708.4311", "content": "The more recent some event, the harder it is to judge its long-term significance. But this biased author thinks that the most important thing that happened recently in AI is the begin of a transition from a heuristics-dominated science (e.g., [24]) to a real formal science. Let us elaborate on this topic."}
{"pdf_id": "0708.4311", "content": "But the new millennium's formal point of view is actually taking this step into account in a very general way, through the first mathematical theory of universal embedded AI, combining \"old\" theoretical computerscience and \"ancient\" probability theory to derive optimal behavior for embedded, em bodied rational agents living in unknown but learnable environments"}
{"pdf_id": "0708.4311", "content": "It is possible to come up with theoretically optimal ways of improving the predic tive world model of a curious robotic agent [28], extending earlier ideas on how to implement artificial curiosity [25]: The rewards of an optimal reinforcement learner are the predictor's improvements on the observation history so far"}
{"pdf_id": "0708.4311", "content": "puter whose original software includes axioms describing the hardware and the originalsoftware (this is possible without circularity) plus whatever is known about the (proba bilistic) environment plus some formal goal in form of an arbitrary user-defined utilityfunction, e.g., cumulative future expected reward in a sequence of optimization tasks  see equation (1). The original software also includes a proof searcher which uses theaxioms (and possibly an online variant of Levin's universal search [15]) to systemati cally make pairs (\"proof\", \"program\") until it finds a proof that a rewrite of the original software through \"program\" will increase utility. The machine can be designed such that each self-rewrite is necessarily globally optimal in the sense of the utility function, even those rewrites that destroy the proof searcher [29]."}
{"pdf_id": "0708.4311", "content": "Which are today's practically most promising extensions of traditional machine learning? Since virtually all realistic sensory inputs of robots and other cognitive systems are sequential by nature, the future of machine learning and AI in general depends on progress in in sequence processing as opposed to the traditional processing of stationary input patterns"}
{"pdf_id": "0708.4311", "content": "Most traditional methods for learning time series and mappings from sequencesto sequences, however, are based on simple time windows: one of the numerous feed forward ML techniques such as feedforward neural nets (NN) [1] or support vector machines [38] is used to map a restricted, fixed time window of sequential input valuesto desired target values"}
{"pdf_id": "0708.4311", "content": "through a focus on reducing search spaces by co-evolving the comparatively small weight vectors of individual recurrent neurons [7]. Such RNNs can learn to create memories of important events, solving numerous RL / optimization tasks unsolvable by traditional RL methods [6, 7]. They are among the most promising methods for practical program learning, and currently being applied to the control of sophisticated robots such as the walking biped of TU Munich [16]."}
{"pdf_id": "0708.4311", "content": "Truly nontrivial predictions are those that most will not believe until they come true. We will mostly restrict ourselves to trivial predictions like those above and refrain from too much speculation in form of nontrivial ones. However, we may have a look at previous unexpected scientific breakthroughs and try to discern a pattern, a pattern that may not allow us to precisely predict the details of the next revolution but at least its timing."}
{"pdf_id": "0708.4311", "content": "across Asia from Korea all the way to Germany. Chinese neets and later also European vessels start exploring the world. Gun powder and guns invented in China. Rennaissance and Western bookprint (often called the most innuential invention of the past 1000 years) and subsequent Reformation in Europe. Begin of the Scientific Revolution"}
{"pdf_id": "0709.0116", "content": "How best to quantify the information of an object, whether naturalor artifact, is a problem of wide interest. A related problem is the com putability of an object. We present practical examples of a new way toaddress this problem. By giving an appropriate representation to our ob jects, based on a hierarchical coding of information, we exemplify how itis remarkably easy to compute complex objects. Our algorithmic com plexity is related to the length of the class of objects, rather than to the length of the object."}
{"pdf_id": "0709.0116", "content": "In section 4 we use a simple case study of a set of concepts, and show how each is computed or generated from others among these concepts, and/or a superset of nouns. This study is complemented by the analysis of texts or documents. In dealing with faces and with texts, we have carefully selected a range of case studies to exemplify a new approach to computability, in the sense of generation of an object and, related to this, the inherent complexity of an object. In summarizing and concluding, sections 5 and 6 provide further discussion on our approach."}
{"pdf_id": "0709.0116", "content": "the rank orders as 1 = most frequent term, 2 = next most frequent term, and so on, through to the least frequent term. Where terms are ex aequo, we use lexicographical order. Then we replace the text with the ranks of terms. So we have a particular, numerical (integer) encoding of the text as a whole. For convenience we ignore punctuation and whitespace although we could well consider these. In general we ignore upper and lower case. We do not use stemming or other processing."}
{"pdf_id": "0709.0116", "content": "• Finally it is likely that wordk is not in the word set that we are examining. We adopt an easy solution to how we represent wordk through its rank, r(wordk). Firstly, wordk can be from a superset of the word set beinganalyzed; and we allow multiples of our top rank to help with this repre sentation. Figures, to be discussed now (Figures 6 and 7), will exemplify this."}
{"pdf_id": "0709.0522", "content": "Until very recently, the most commonly used conditioning rule for belief revision was the one proposed by Shafer [2] and referred here as Shafer's Conditioning Rule (SCR). The SCR consists in combining the prior bba m(.) with a specific bba focused on A with Dempster's rule of combination for transferring the connicting mass to non-empty sets in order to provide the revised bba. In other words, the conditioning by a proposition A, is obtained by SCR as follows :"}
{"pdf_id": "0709.0522", "content": "All other qualitative masses take the value L0. Such prior suggests normally/rationally to bomb in priority the zone C since it is the one carrying the higher belief on the location of enemies. But for some unknown reasons (military, political or whatever) let's assume that the headquarter has finally decided to bomb D first. Let's examine how will be revised the prior qm(.) with QBCR1 and QBCR2 in such situation for the two cases:"}
{"pdf_id": "0709.0522", "content": "We assume that the military headquarter has decided to bomb in priority region D because there was a high qualitative belief on the presence of enemies in D according to the prior qbba qm(.). But after bombing and verification, it turns out that the enemies were not in D (same scenario as for example 2). Let's examine the results of the conditioning by the rules QBCR1 and QBCR2 for the cases 1 and 2:"}
{"pdf_id": "0709.0522", "content": "The results obtained by QBCR1 and QBCR2 are again coherent with rational human reasoning since after bombing zone D we get, in such case, a higher belief in finding enemies in C than in A which is normal due to the prior information we had before bombing D and taking into account the constraint of the model."}
{"pdf_id": "0709.0522", "content": "In this paper, we have designed two Qualitative Belief Conditioning Rules in order to revise qualitative basic belief assignments and we presented some examples to show how they work. QBCR1 is more prudent than QBCR2 because the revision of the belief is done in a less specific transfer than for QBCR2. We use it"}
{"pdf_id": "0709.0522", "content": "when we are less confident in the source. While QBCR2 is more optimistic and refined; we use it when we are more confident in the source. Of course, the qualitative conditioning process is less precise than its quantitative counterparts because it is based on a rough approximation, as it normally happens when working with linguistic labels. Such qualitative methods present however some interests for manipulating information and beliefs expressed in natural language by human experts and can be helpful for high-level decision support systems."}
{"pdf_id": "0709.0674", "content": "Figure 2 provides another example: a butterny and a vase with a nower. The image to the left can be specified by very few bits of information; it can be constructed through a very simple procedure or algorithm based on fractal circle patterns [15]. People who understand this algorithm tend to appreciate the drawing more than those who do not. They realize how simple it is. This is not an immediate, all-or-nothing, binary process"}
{"pdf_id": "0709.0674", "content": "though. Since the typical human visual system has a lot of experience with circles, most people quickly notice that the curves somehow fit together in a regular way. But few are able to immediately state the precise geometric principles underlying the drawing. This pattern, however, is learnable from the right-hand side of Figure 2. The conscious or subconscious discovery process leading from a longer to a shorter description of the data, or from less to more compression, or from less to more subjectively perceived beauty, yields reward depending on the first derivative of subjective beauty."}
{"pdf_id": "0709.0674", "content": "So we conceptually separate the goal (explaining / compressing the history) from themeans of achieving the goal. Once the goal is formally specified in terms of an algo rithm for computing curiosity rewards, let the controller's reinforcement learning (RL) mechanism figure out how to translate such rewards into action sequences that allow the given compressor improvement algorithm to find and exploit previously unknown types of compressibility."}
{"pdf_id": "0709.0674", "content": "The previous Section A.2 only discussed measures of compressor performance, but not of performance improvement, which is the essential issue in our curiosity-oriented context. To repeat the point made above: The important thing are the improvements of the compressor, not its compression performance per se. Our curiosity reward in response to the compressor's progress (due to some application-dependent compressor improvement algorithm) between times t and t + 1 should be"}
{"pdf_id": "0709.0674", "content": "3. Let some (application-dependent) compressor improvement algorithm (such asa learning algorithm for an adaptive neural network predictor) use hold to ob tain a hopefully better compressor pnew (such as a neural net with the same sizebut improved prediction capability and therefore improved compression perfor mance). Although this may take many time steps, pnew may not be optimal, due to limitations of the learning algorithm, e.g., local maxima."}
{"pdf_id": "0709.0674", "content": "Obviously this asynchronuous scheme may cause long temporal delays between con troller actions and corresponding curiosity rewards. This may impose a heavy burdenon the controller's RL algorithm whose task is to assign credit to past actions (to in form the controller about beginnings of compressor evaluation processes etc., we may augment its input by unique representations of such events). Nevertheless, there are RL algorithms for this purpose which are theoretically optimal in various senses, to be discussed next."}
{"pdf_id": "0709.0674", "content": "The expected consequences are: at time t the controller will do the best to select anaction y(t) that starts an action sequence expected to create observations yielding max imal expected compression progress up to the expected death T , taking into accunt the limitations of both the compressor and the compressor improvement algorithm"}
{"pdf_id": "0709.0896", "content": "Kurtz, et al (2005a) investigated three possible  causes for the effect: Early Access (EA), arXiv  deposited papers are cited more because they are  available several months before the journal  versions; Quality Bias (QB), either the best  researchers tend to use arXiv, or researchers tend  to post their best papers; and Open Access (OA),  by being available for free on the internet more  people are able to read the arXiv deposited papers,  thus they are more cited"}
{"pdf_id": "0709.0896", "content": "astrophysics. They were unable to find any OA  effect. They explained this by suggesting that in a  well funded field like astrophysics essentially  everyone who is in a position to write research  articles has full access to the literature.  Using different methodologies Moed (2007)  studied the literature of solid state physics and  came  to  very  similar conclusions.  The"}
{"pdf_id": "0709.0896", "content": "The most obvious effect  (Henneken, et al 2006b) is that arXiv deposited papers are cited at about twice the rate of non deposited papers; next we see that the 1998 arXiv  deposited papers have their peak citation rate  earlier than the 1997 deposited papers, part of a  long term trend shown by Brody, et al"}
{"pdf_id": "0709.1099", "content": "Vehicle localization on a map has two meanings in the  literature in this domain. In many works, [2], [3], [4] and  [5] it refers to the projection of the absolute position  estimate onto a segment of the road network stored in the  database. In this case, the vehicle is localized when the  curvilinear abscissa along the segment are known from"}
{"pdf_id": "0709.1099", "content": "2.1 Localization and heading estimation by  combining odometry and GPS  Consider a car-like vehicle with front-wheel drive. The  mobile frame is chosen with its origin M attached to the  center of the rear axle. The x-axis is aligned with the  longitudinal axis of the car (see Figure 2)."}
{"pdf_id": "0709.1099", "content": "Where (xcarto, ycarto) is the orthogonal projection onto  each segments and capcarto is the segment heading.  To represent the error of the cartographical observation  in the SKF formalism, we choose a Gaussian distribution  of the uncertainty zone all around the segment. So this  error can be represented with an ellipsoid which encloses  the road (we choose to use an ellipsoid because it is just  the available model). This ellipsoid has its semi-major  axis in the length of the segment and its semi-minor axis  equals to the width of the road [8] (see Figure 4).   Segment"}
{"pdf_id": "0709.1099", "content": "The GPS position measurement provides the GPS  observation (xgps, ygps). The GPS measurement error can  be provided also and in real time using the Standard  National Marine Electronics Association (NMEA)  sentence \"GPGST\" given by the Trimble AgGPS132  receiver which has been used in the experiments.  Therefore, the GPS noise is not stationary. The non  stationary of the GPS measurements noise affect the  observation model. With each measurement provided, the"}
{"pdf_id": "0709.1099", "content": "For each candidate segment one can build a  cartographical observation given by projection of  odometric  estimation  onto  the  segments.  The  cartographical observations and/or GPS observation are  used to update variables Xk and Sk. A result of Bayesian  inference is a probability of each candidate segment. The  synoptic of this algorithm is given by Figure 7.  Let us use a specific case study to illustrate the  method. In Figure 8, the vehicle is traveling on the road  represented by the segments 1 and 2. Estimation errors  and digital map errors oblige the selection of the segment"}
{"pdf_id": "0709.1099", "content": "used for 1.5Km. One can remark that in spite of the long  GPS mask, the vehicle location is matched correctly. As  matter of fact, the final estimated positions stay close to  the GPS points. In Figure 9, we only presented the most  probable SKF estimation of the pose."}
{"pdf_id": "0709.1099", "content": "In Figure 11, GPS was not available after the  intersection. One can see that the method manage two  hypotheses for seven steps then wrong hypothesis was  eliminated. We can remark that, the good segment always  presents the most important probability computed by the  SKF inference."}
{"pdf_id": "0709.1167", "content": "The benefit of RDF, and perhaps what is not generally appreciated, is that with RDF it is possible to represent anything in relation to anything by any type of qualified relationship. In many cases, this generality can lead to an uncontrolled soup of relationships; however, thanks to ontology languages such as RDFS and OWL, it is possible to formally constrain the topological features of an RDF network and thus, subsets of the larger Semantic Web."}
{"pdf_id": "0709.1167", "content": "other organization denoted X, it is inferred that that X is an rdf:type of lanl:Institution. While this is not intuitive for those familiar with constraint-based database schemas, such inferencing of new relationships is the norm in the RDFS and OWL world.Beyond the previously presented RDFS constructs, OWL has one pri mary construct that is used repeatedly: owl:Restriction4. Example owl:Restrictions include, but are note limited to, owl:maxCardinality, owl:minCardinality, owl:cardinality, owl:hasValue, etc. With OWL, it is possible to state that a lanl:Human can work for no more than 1 lanl:Institution. In such cases, the owl:maxCardinality restriction would be specified on the lanl:worksFor predicate. If there exist the triples"}
{"pdf_id": "0709.1167", "content": "propriety and open-source triple-store providers. The most popular pro prietary solutions include AllegroGraph7, Oracle RDF Spatial8 and the OWLIM semantic repository9. The most popular open-source solution is Open Sesame10. The primary interface to a triple-store is SPARQL [7]. SPARQL is analogous to the relational database query language SQL. However, SPARQL is perhaps more similar to the query model employed by logic languages such as Prolog. The example query"}
{"pdf_id": "0709.1167", "content": "The previous query would require a complex joining of tables in therelational database model to yield the same information. Unlike the relational database index, the triple-store index is optimized for such seman tic network queries (i.e. multi-relational queries). The triple-store a useful tool for storing, querying, and manipulating an RDF network."}
{"pdf_id": "0709.1167", "content": "The above code defines the class lanl:Human. Any instance of lanl:Human can have either 0 or 1 lanl:worksFor relationships (i.e. owl:maxCardinalityof 1). Furthermore, when the method lanl:quit is executed, it will de stroy any lanl:worksFor triple from that lanl:Human instance to the provided lanl:Institution x. Fhat is a virtual machine encoded in an RDF network and processes Fhat triple-code. This means that a Fhat's program counter, operand stack, variable frames, etc., are RDF sub-netwoks. Figure 3 denotes a Fhat processor (A) processing Neno triple-code (B) and other RDF data (C)."}
{"pdf_id": "0709.1167", "content": "This article presented a review of the standards and technologies associated with the Semantic Web that can be used for complex systems mod eling. The World Wide Web provides a common, standardized substrate whereby researchers can easily publish and distribute documents (e.g. web pages, scholarly articles, etc.). Now with the Semantic Web, researchers can easily publish and distribute models and processes (e.g. data sets, algorithms, computing machines, etc.)."}
{"pdf_id": "0709.1701", "content": "Qualitative methods for reasoning under uncertainty have gained more and more attention by Information Fusion community, especially by the researchers and system designers working in the development of modernmulti-source systems for defense, robotics and so on. This is because traditional methods based only on quanti tative representation and analysis are not able to completely satisfy adequately the need of the development ofscience and technology integrating at higher fusion levels human beliefs and reports in complex systems. There fore qualitative knowledge representation becomes more and more important and necessary in next generations of (semi) intelligent automatic and autonomous systems."}
{"pdf_id": "0709.1701", "content": "This paper is organized as follows: In section 2, we remind brieny the basics of DSmT. In section 3 we present and justify in details the q-operators, in order to get ready for introducing new enriched qualitative-enriched (qe) operators in sections 5. In section 6, we illustrate through very simple examples how these operators can be used for combining enriched qualitative beliefs. Concluding remarks are then given in 7."}
{"pdf_id": "0709.1701", "content": "Justification of b): when we divide say L4/L1 in the above example, we get 0.8/0.2 = 4, but no label is corresponding to number 4 which is not even in the interval [0, 1], hence in the division as an internal operator we need to get as response a label, so in our example we approximate it to Lmax = L5, which is a very rough approximation! So, depending on the fusion combination rules, it might better to consider the qualitative division as an external operator, which gives us the exact result."}
{"pdf_id": "0709.1701", "content": "The above qualitative operators are logical, justified due to the isomorphism between the set of linguistic equidistant labels and a set of equidistant numbers in the interval [0, 1]. These qualitative operators are built exactly on the track of their corresponding numerical operators, so they are more mathematical than the ad-hoc definition of qualitative operators proposed so far in the literature. They are similar to the PCR5 combination numerical rule with respect to other fusion combination numerical rules based on the conjunctive rule. But moving to the enriched label qualitative operators the accuracy decreases."}
{"pdf_id": "0709.1701", "content": "Remark about doing multi-operations on labels: When working with labels, no matter how many opera tions we have, the best (most accurate) result is obtained if we do only one approximation, and that one should be just at the very end. For example, if we have to compute terms like LiLjLk/(Lp + Lq) as for qPCR5 (see example in section 6), we compute all operations as defined above, but without any approximations (i.e. not even calculating the integer part of indexes, neither replacing by n + 1 if the intermediate results is bigger than n + 1), so:"}
{"pdf_id": "0709.1701", "content": "From these very simple qualitative operators, it is thus possible to extend directly the DSmH fusion rule for combining qualitative basic belief assignments by replacing classical addition and multiplication operators on numbers with those for linguistic labels in DSmH formula. In a similar way, it is also possible to extend PCR5 formula as shown with detailed examples in [14] and in section 6 of this paper. In the next section, we propose new qualitative-enriched (qe) operators for dealing with enriched linguistic labels which mix the linguistic value with either quantitative/numerical supporting degree or qualitative supporting degree as well. The direct qualitative discounting (or reinforcement) is motivated by the fact that in general human experts provide more easily qualitative values than quantitative values when analyzing complex situations."}
{"pdf_id": "0709.1701", "content": "In this paper, both quantitative enrichments and qualitative enrichments of linguistic labels are considered and unified through same general qe-operators. The quantitative enrichment is based directly on the percentage of discounting (or reinforcement) of any linguistic label. This is what we call a Type 1 of enriched labels. The qualitative enrichment comes from the idea of direct qualitative discounting (or reinforcement) and constitutes the Type 2 of enriched labels."}
{"pdf_id": "0709.1701", "content": "These qe-operators with numerical confidence degrees are consistent with the classical qualitative operators when ei = ej = 1 since c = 1 and Li(1) = Li for all i, and the qe-operators with qualitative confidence degrees are also consistent with the classical qualitative operators when ei = ej = O (this is letter \"O\", not zero, hence the neutral qualitative confidence degree) since c = O (neutral)."}
{"pdf_id": "0709.1701", "content": "a) qm1(A)qm2(B) = L1(0.3)L2(0.7) = L0(0.3) is redistributed back to A and B proportionally with respect to their corresponding qualitative masses put in this partial connict, i.e. proportionally with respect to L1(0.3) and L2(0.7). But, since L0(0.3) is the null qualitative label (equivalent to zero for numerical masses), both A and B get L0 with the minimum confidence, i.e. L0(0.3)."}
{"pdf_id": "0709.1701", "content": "With the recent development of qualitative methods for reasoning under uncertainty developed in Artificial Intelligence, more and more experts and scholars have great interest on qualitative information fusion, especially those working in the development of modern multi-source systems for defense, robot navigation, mapping, localization and path planning and so on"}
{"pdf_id": "0709.1771", "content": "where the index j runs over those of Xj that is among the k nearest neighbors of Xi. with nearest neighbors determined by some metric d(Xi, Xj). This minimization problem has a non-trivial solution since it is usually assumed that the dimension of Xi is much bigger than k.To generalize LLE, we first assume that the data come in with two com ponents Xi = (Yi, Zi) (think of Yi as grid position, and Zi as intensity value). Now we can minimize the following:"}
{"pdf_id": "0709.1771", "content": "the index j still runs over k nearest neighbors of Xi. but now with nearest neigh bors determined by some metric d(Yi, Yj) depending on the other component of X. If dimension of Xi is small compared to k (as in the case of an image), we must add regularization term to make the problem well-posed. And we will recover the discrete counterpart of (2) after ignoring the convexity constraint(5)."}
{"pdf_id": "0709.1771", "content": "In this work, we proposed a new algorithm for single-image super-resolution problem using variational method. Instead of working on the image space as in the previous work utilizing variational method, we use variational formulation to estimate the local structure of an image. The resulting adaptive filter renects both local pixel variance and global image information. The experimental result shows some advantage of our method over some previous approaches. A futureresearch direction might be to explore other applications of the variational es timation of the local image structure."}
{"pdf_id": "0709.2065", "content": "12 \"There had been a short conflict, and the end of this internal struggle was that the idea which had been appeared before  consciousness as the vehicle of this irreconcilable wish fell a victim to repression, was pushed out of consciousness with all its  attached memories and was forgotten"}
{"pdf_id": "0709.2065", "content": "role of sources of the resistance force which does not permit reappearance of hidden forbidden wishes, desires and wild  impulses which were repressed.  We note again that blocking thresholds depends on thinking processors. Thus the same individual can have the normal  threshold for one thinking block and abnormal degree of blocking for another thinking block."}
{"pdf_id": "0709.2065", "content": "him; but there was some force that prevented them from becoming conscious and compelled  them to remain unconscious. The existence of this force could be assumed with  certainty...\", Freud, 1962b  15 The feeling of pleasure is approached at the moment of realization. The strength of this feeling is determined  by the magnitude of the interest-measure."}
{"pdf_id": "0709.2065", "content": "Our aims are similar of those formulated for humanoid robots, see e.g. Brooks et al., 1981a,b, 1999, 2002. However,  we jump directly to high level psyche (without to create e.g. the visual representation of reality). The idea of Luc  Steels to create a robot culture via societies of self-educating robots, Manuel, 2003, is also very attractive for us. It is  clear that real humanoid psyche (including complexes and symptoms) could be created only in society of interacting  Psychots and people. Moreover, such AI-societies of Psychots can be used for modeling psychoanalytic problems and  development of new methodologies of treatment of such problems."}
{"pdf_id": "0709.2506", "content": "Abstract: Data collection often results in records that have missing values or variables. This investigation  compares 3 different data imputation models and identifies their merits by using accuracy measures.  Autoencoder Neural Networks, Principal components and Support Vector regression are used for  prediction and combined with a genetic algorithm to then impute missing variables. The use of PCA  improves the overall performance of the autoencoder network while the use of support vector regression  shows promising potential for future investigation. Accuracies of up to 97.4 % on imputation of some of  the variables were achieved."}
{"pdf_id": "0709.2506", "content": "Data imputation using Auto  Encoder Neural Networks as a regression model has been  carried out by Abdella and Marwala (Mussa et al, 2005) and  others (Leke et al, 2005) (Nelwamondo et al, 2007a) while  other variations are available in literature including  Expectation Maximisation (Nelwamondo et al, 2007a),  Rough Sets (Crossingham et al, 2005) (Nelwamondo et al,  2007b), Decision Trees (Barcena et al, 2002)"}
{"pdf_id": "0709.2506", "content": "Auto Encoder Networks comes with the price of  computational complexity and a time trade-off as a  disadvantage that is mostly cited for the use of other methods  (Nelwamondo et al, 2007b), . The advantage of using Auto  Encoder Networks it the high level of accuracy. The data  used in this investigation is HIV demographic data collected  from ante-natal clinics from around South Africa."}
{"pdf_id": "0709.2506", "content": "This report focuses on investigating the use of different  regression methods that offer a glance into the data  imputation world. The report first gives a background into  missing data, neural networks and the other regression  methods used. Secondly the data set to be used is introduced  and explained. The methodology is given and then carried  through. By the end of the report the results are given and  then discussed."}
{"pdf_id": "0709.2506", "content": "Data collection forms the backbone of most projects and  applications. To accurately use the data all information  required must be available. Data collections suffer from  missing values/data variables. This for example can be in the  form of unfilled fields in a survey or data entry mistakes.  Simply removing all entries concerned with the missing value  is not always the best solution. There are three different types  of missing data mechanisms as discussed by Little and Rubin  (Little et al, 2000)."}
{"pdf_id": "0709.2506", "content": "Methods are needed to impute the missing data. There are  numerous ways that have been used to impute missing data.  The approach taken in this investigation is to use regression  methods to find the inter-relationships between the data and  then use the regression methods to verify the approximations  that are made. The next subsections discuss the different  regression methods used."}
{"pdf_id": "0709.2506", "content": "This has two layers of weights which connect the input layer  to the output layer. The middle of the network is made up of  a hidden layer. This layer can be made up of a different  number of hidden nodes. This number has to be optimised so  that the network can model systems better (Krose et al,  1996). An increase in hidden nodes translates into an increase  in the complexity of the system. The output and the hidden  nodes also have activation functions (Bishop, 1995). The  general equation of a MLP neural network is shown below  (1):"}
{"pdf_id": "0709.2506", "content": "ji inner kj outer (1)  The activation function (Fouter) chosen for the project was  linear. The inner activation (Finner) function chosen was the  hyperbolic tangent function (tanh). This served to increase  accuracy in regression (Krose et al, 1996). This function  produced the best results during training. Thus the relation  becomes (2):"}
{"pdf_id": "0709.2506", "content": "PC (6)  Here D' is the retransformed data. If all of the principal  components are used from the covariance matrix then D =  D'. The transformed data (D) can be used in conjunction with  the ANN to increase the efficiency of the ANN by reducing  its complexity (number of training cycles). These results from  the property of the PCA extracting linear relationships  between the data variables, thus the ANN only needs to  extract the non linear relationships. This then results in less  training cycles that are needed. Thus ANNs can be built more  efficiently. Fig. 3 illustrates this concept. The PCA function  in Netlab was used for the investigation 0."}
{"pdf_id": "0709.2506", "content": "Genetic algorithms are defined as population based models  that use selection and recombination operators to generate  new sample points in search space (Whitley, 1994). Genetic  algorithms are primarily used for optimisation as they can  find values for variables that will achieve a target. In this  investigation the genetic algorithm is used to find the input  into regression model that will result in the most accurate  missing data value. Genetic algorithm use is good for non  linear functions and applications, thus the use in this  investigation. The overview of the procedure of genetic  algorithm is the same as that of natural selection."}
{"pdf_id": "0709.2506", "content": "The data that is used for this investigation is HIV data from  antenatal clinics from around South Africa. It was collected  by the department of health in the year 2000. The data  contains multiple input fields that result from a survey. The  information is in a number of different formats resulting from  the survey. For example the provinces, region and race are  strings. The age, gravidity, parity etc. are integers. Thus  conversions are needed. The strings were converted to  integers by using a lookup table e.g. there are only 9  provinces so 1 was substituted for Gauteng etc."}
{"pdf_id": "0709.2506", "content": "Data collected from surveys and other data collection  methods normally have outliers. These are normally removed  from the data set. In this investigation data sets that had  outliers had only the outlier removed and the data set was  then classified as incomplete. This then means that the data  can still be used in the final survey results if the missing  values are imputed. The data with missing values was not  used for the training of the computational methods. The data  variables and their ranges are shown below in Table 1."}
{"pdf_id": "0709.2506", "content": "The pre-processed data resulted in a reduction of training  data. This was 12750 processed data sets from around 16500  original records in the survey data. To use the data for  training it needs to be normalised. This ensures that the all  data variables can be used in training. If the data is not  normalised, some of the data variables with larger variances  will influence the result more than others. E.g. if we use  WTREV and Age Group data only the age data will be  influential as it has large values. Thus all of the data is"}
{"pdf_id": "0709.2506", "content": "The approach taken for the project is to use the regression  methods with an optimisation technique. The optimisation  technique chosen was the Genetic algorithm. Fig. 4 illustrates  the manner in which the regression methods and the  optimisation technique will be used to impute data"}
{"pdf_id": "0709.2506", "content": "The training data was first used to extract the principal  components. After the extraction the training data was  multiplied with the principal components and the resulting  data was used to train a new ANN. This was then labelled a  PCA-ANN. Two PCA-ANNs were trained. One PCA-ANN  had no compression and was just a transform; the other"}
{"pdf_id": "0709.2506", "content": "PCANN compressed the data from 11 dimensions to 10. The  number of hidden nodes and training cycles were optimised  as in the previous subsection. The number of hidden nodes  for the PCA-ANN-11 was 10 and for the PCA-ANN-10 were  9. The inner and outer activation functions were as for the  ANN above. Validation was also carried out with an unseen  data set. This also ensures that the ANN is trained well and  not over trained."}
{"pdf_id": "0709.2506", "content": "The Genetic Algorithm was setup with 50 initial population  and 50 generation cycles. As mentioned earlier the GA uses  simple crossover, geometric selection and non uniform  mutation. This produced the best results and was used for  every model so as to serve for correct comparisons."}
{"pdf_id": "0709.2506", "content": ") / (10)  x is the correct value data and y is the imputed data. n is the  number of records in the data. The mean square error is  calculated after the imputation by the GA. This is before  de-normalisation and rounding. Thus does not carry over any  rounding errors."}
{"pdf_id": "0709.2506", "content": "Prediction within year is used as a useful and easy to  understand measure of accuracy. This for example would be  expressed as 80% accuracy within 1 year for age data. This  means for age data the values that are found are 80% accurate  within a tolerance of 1 year. This measure is used mainly for  the some of the regression data."}
{"pdf_id": "0709.2506", "content": "The results indicate that the autoencoder network genetic  algorithm architecture seems to perform well in the HIV  classification and as well all the others except the education  level. The high estimation accuracies are on par with  previous research. The education level seems to be the weak  point."}
{"pdf_id": "0709.2506", "content": "The  PCANNGA  architecture  was  run  with  two  configurations. The first configuration had no compression  thus is named PCANNGA11 indicating the transformation  from 11 inputs to 11 outputs. The second configuration has a  compression of 1 value thus is named PCANNGA-10,  indicating the compression and transformation from 11 inputs  to 10 inputs. The results of the test are shown below in Table  3."}
{"pdf_id": "0709.2506", "content": "The results for PCANNGA-11 indicate good estimation for  all the variables except education level. PCANNGA-10  performs poorly on Age and Age Gap while having good  results in the other variables. This results from the loss of  information during the compression. This then impacts on the  regression ability of the network resulting in poor imputation  accuracy for some of the variables."}
{"pdf_id": "0709.2506", "content": "The SVRGA imputation model took a long time to run. Due  to the inefficiencies of running a computational such as this  on MATLAB, the simulations were slow. Nonetheless the  imputations did run and did return all required results. The  results from the SVRGA are tabulated below in Table 4."}
{"pdf_id": "0709.2506", "content": "For the comparison of results, the previous accuracies as well  as the mean square error of each method will be analysed.  This will give an indication of how the errors in the  imputation affect the accuracy as well as which model  produces the best results. The average mean square errors of  the imputation methods are shown in Table 5"}
{"pdf_id": "0709.2506", "content": "In the mean square errors a smaller value is desirable. It can  be seen from Table 5 that in HIV classification the SVRGA  performed the worst as it had the highest error but in the  education level it performed the best as it has the lowest  error. The following figure, Fig. 6, is a graph of the average  mean square error of the imputation models"}
{"pdf_id": "0709.2506", "content": "From Fig. 6 it can be seen that the SVRGA has the smallest  average mean square error (if HIV classification is not  included) from the rest of the methods. This indicates that the  SVRGA functioned well on regression parameters and poorly  on the classification of HIV. The following graph in Fig. 7.  makes this clear. The ANNGA performs the best with an  average accuracy of 68.5 % while the rest of the models fell  behind and the SVRGA has the lowest average accuracy of  22 %. In Education level accuracy the SVRGA performed"}
{"pdf_id": "0709.2506", "content": "From the comparison of all of the imputation models it can  be seen that the PCANN11 performs better even though it has  a worse HIV classification. The SVRGA only makes good  ground on the education level and thus cannot be considered  superior to the PCANN11"}
{"pdf_id": "0709.2506", "content": "Due to time constraints the support vector regression could  not be investigated further. This is due to the fact that the  simulations of the SVRGA were very slow. SVR though is  still a viable solution if an optimised c++ or other  programming language toolbox is used instead of a  MATLAB toolbox, the speed of computation will increase.  Thus it is suggested that more research and investigation be  done on the SVR. There have been cases were the SVR has"}
{"pdf_id": "0709.2506", "content": "A hybrid approach of using the ANNGA and SVRGA or  PCANNGA11 and SVRGA together is also a viable future  investigation area. This could not be implemented in the  investigation due to time. It is expected that this would  increase the performance of the neural network based  methods in imputing the education level while assisting the  SVRGA in imputing the HIV classification."}
{"pdf_id": "0709.2506", "content": "An investigation into the data only for classification for the  classification parameters such as HIV can yield better results.  This comes at the price of loss of generalisation. Leke and  Marwala (Leke et al, 2005) investigated a classification based  problem of HIV classification only. This cannot be directly  used with data imputation without then resulting in high  complexity hybrid networks with models only dealing with  missing data that is classification based and then other  models dealing with regression based missing data."}
{"pdf_id": "0709.3974", "content": "The paper proceeds as follows. The next section summarizes definitions and facts about CAs and the density task, including previous results obtained inbuilding CAs for the task. A description of fitness landscapes and their sta tistical analysis follows. This is followed by a detailed analysis of the majority problem fitness landscape. Next we identify and analyze a particular subspaceof the problem search space called the Olympus. Finally, we present our con clusions and hints to further works and open questions."}
{"pdf_id": "0709.3974", "content": "In general, the size of the search space does not allow to consider all the possible individuals, when trying to draw a fitness cloud. Thus, we need to use samples to estimate it. We prefer to sample the space according to a distribution that gives more weight to \"important\" values in the space, for instance those at a higher fitness level. This is also the case of any biased searcher such as an evolutionary algorithm, simulated annealing and other heuristics, and thus this kind of sampling process more closely simulates the way in which the program space would be traversed by a searcher. So, we use the Metropolis-Hastings technique [35] to sample the search space."}
{"pdf_id": "0709.3974", "content": "0.76 is a neighboring solution of solution find by Mitchell (see tab 2). We try to explore the NN by strictly increasing the Hamming distance from the starting solution at each step of the walk. The neutral walk stops when there is no neutral step that increases distance. The maximum length of walk is thus 128. On average, the length of neutral walks on NN0.5 is 108.2 and 33.1 on NN0.76. The diameter (see section 3.3.2) of NN0.5 should probably be larger than the one of NN0.76."}
{"pdf_id": "0709.3974", "content": "Figure 6 shows the distribution of neutral degree collected along all neutralwalks. The distribution is close to normal for NN0.76. For NN0.5 the distribu tion is skewed and approximately bimodal with a strong peak around 100 and a small peak around 32. The average of neutral degree on NN0.5 is 91.6 and standard deviation is 16.6; on NN0.76, the average is 32.7 and the standarddeviation is 9.2. The neutral degree for NN0.5 is very high : 71.6 % of neigh bors are neutral neighbors. For NN0.76, there is 25.5 % of neutral neighbors. It can be compared to the average neutral degree overall neutral NKq-landscape with N = 64, K = 2 and q = 2 which is 33.3 % [41]."}
{"pdf_id": "0709.3974", "content": "In this section, we study the spatial distribution of the six blok. Table 4 gives the Hamming distance between these local optima. All the distances are lower than 64 which is the distance between two random solutions. Local optima do not seem to be randomly distributed over the landscape. Some are nearby, for instance GLK and Davis rules, or GLK and Coe2 rules. But Das and GLK rules, or Coe1 and Das rules are far away from each other."}
{"pdf_id": "0709.3974", "content": "Fig. 9. Centroid C of the six blok. The squares give the frequency of 1 over the six blok as function of bits position. The right column gives the number of bits of C from the 128 which have the same frequency of 1 indicated by the ordinate in the ordinate (left column)."}
{"pdf_id": "0709.3974", "content": "Altenberg defined evolvability as the ability to produce fitter variants [43]. The idea is to analyze the variation in fitness between one solution and its neighbors. Evolvability is said positive if neighbor solutions are fitter than the solution and negative otherwise. In this section, we define the evolvability horizon (EH) as the sequence of solutions, ordered by fitness values, which can be reached with one bitnip from the given solution. We obtain a graph with fitness values in ordinates and the corresponding neighbors in abscissa sorted by fitnesses (see figure 10)."}
{"pdf_id": "0709.3974", "content": "Figure 10 shows the evolvability horizon of the blok. There is no neighbor with a better fitness value than the initial rule; so, all the best known rules are local optima. The fitness landscape has two important neutral networks at fitness 0 (NN0) and fitness 0.5 (NN0.5) (see section 4.3). No local optimum is nearby NN0; but a large part of neighbors of local optima (around 25% on average) are in NN0.5. As a consequence a neutral local search on NN0.5 can potentially find a portal toward the blok."}
{"pdf_id": "0709.3974", "content": "For each EH, there is an abscissa r from which the fitness value is roughly linear. Let fr be this fitness value, f128 the fitness of the less sensible bit, and m the slope of the curve between abscissa r and 128. Thus, the smaller m and r, the better the neighbors. On the contrary, higher slope and r values mean that the neighbor fitness values decay faster. For example evolvability is slightly negative from GLK, as it has a low slope and a small r. At the opposite, the Coe2 rule has a high slope ; this optimum is thus isolated and evolvability is strongly negative. We can imagine the space \"view from GLK\" natter than the one from Coe2."}
{"pdf_id": "0709.3974", "content": "The neutral degree of 103 solutions randomly chosen in Olympus is depicted in figure 14-b. Two important NN are located around fitnesses 0 and 0.5 where the neutral degree is over 80. On average the neutral degree is 51.7. For comparison, the average neutral degree for NKq landscapes with N = 64,"}
{"pdf_id": "0709.3974", "content": "In this section we analyze the correlation structure of the Olympus landscape using the Box-Jenkins method (see section 3.3.4). The starting solution of each random walk is randomly chosen on the Olympus. At each step one random bit is nipped such that the solution belongs to the Olympus and the fitness is computed over a new sample of ICs of size 104. Random walks have length 104 and the approximated two-standard-error bound used in the Box-Jenkins"}
{"pdf_id": "0709.3974", "content": "slope, it seems easy for a local search heuristic to reach fitness values close to 0.6. A comparison of this fitness cloud with the one shown in figure 5 (where the whole fitness landscape was considered, and not only the Olympus) is illuminating: if the whole fitness landscape is considered, then it is \"hard\" to find solutions with fitness up to 0.5 ; on the other hand, if only solutions belonging to the Olympus are considered, the problem becomes much easier : it is now \"easy\" to access to solutions with fitness greater than 0.5."}
{"pdf_id": "0709.3974", "content": "Performance Each GA run lasts 103 generations and 50 independent runs were performed. For each run, we have performed post-processing. At each generation, the best individuals are evaluated on new sample of 104 ICs and the average distance between all pairs of individuals is computed. Best and average performances with standard deviation are reported in table 8. We also computed the percentage of runs which are able to reach a given fitness level and the average number of generations to reach this threshold (see figure 19)."}
{"pdf_id": "0709.4010", "content": "Abstract. This paper introduces the concept of fitness cloud as an alternative way to visualize and analyze search spaces than given by the geographic notion of fitness landscape. It is argued that the fitnesscloud concept overcomes several deficiencies of the landscape repre sentation. Our analysis is based on the correlation between fitness ofsolutions and fitnesses of nearest solutions according to some neigh boring. We focus on the behavior of local search heuristics, such as hill climber, on the well-known NK fitness landscape. In both cases the fitness vs. fitness correlation is shown to be related to the epistatic parameter K."}
{"pdf_id": "0709.4010", "content": "The search space is the set of bit-string of length N = 25. Twostrings are neighbors if their Hamming distance is one. All experi ments are led on the same instance of NK-landscape with K = 20. Datas are collected from an exhaustive enumeration of the search space3. Practically two fitness values are taken as equal if they both stand in the same interval of size 0.002."}
{"pdf_id": "0709.4010", "content": "We draw scatterplot, the so-called whole fitness cloud including, foreach string of the search space, all the points in the hamming neigh borhood (see fig.1). As the density of points on the scatterplot gives little information on dispersion, a standard deviation is plotted on both side of the mean curve."}
{"pdf_id": "0709.4015", "content": "sentence boundaries and, thus, include several sentences. In other words, sequences of  conditions and recommandations correspond to discourse structures.  Discourse processing requires the recognition of heterogeneous linguistic features  (especially, the granularity of relevant features may vary according to text genre [9]).  Following these observations, we made a study based on a representative corpus and  automatic text mining techniques, in order to semi-automatically discover relevant  linguistic features for the task and infer the rules necessary to accurately structure the  practice guidelines."}
{"pdf_id": "0709.4015", "content": "The paper is organized as follow: first, we present the task and some previous approaches  (section 2). We then describe the rules for text structuring (section 3) and the method used  to infer them. We finish with the presentation of some results (section 4), before the  conclusion."}
{"pdf_id": "0709.4015", "content": "Several attempts have already been made to improve the use of practice guidelines. For  example, knowledge-based diagnostic aids can be derived from them [3]. GEM is an  intermediate document model, between pure text (paper practice guidelines) and  knowledge-based models like GLIF [4]. GEM is thus an elegant solution, independent  from any theory or formalisms, but compliant with other frameworks. Previous attempts to  automate the translation process between the text and GEM are based on the analysis of  isolated sentences and do not compute the exact scope of conditional segments [5]."}
{"pdf_id": "0709.4015", "content": "We evaluated the approach on a corpus that has not been used for training. The evaluation  of basic segmentation gives the following results: .92 P&R1 for conditional segments and  .97 for recommendation segments. The scope of conditions is recognized with accuracy  above .7. This result is encouraging, especially considering the large number of parameters  involved in discourse processing. In most of successful cases the scope of a condition is  recognized by the default rule (default segmentation, see section 3)."}
{"pdf_id": "0709.4015", "content": "We have presented in this paper a system capable of performing automatic segmentation of  clinical practice guidelines. Our aim was to automatically fill an XML DTD from textual  input. The system is able to process complex discourse structures and to compute the scope  of conditional segments spanning several propositions or sentences. Moreover, our system  is the first one capable of resolving the scope of conditions over several recommendations."}
{"pdf_id": "0709.4669", "content": "Abstract. Similarity search is an important problem in information retrieval.  This similarity is based on a distance. Symbolic representation of time series  has attracted many researchers recently, since it reduces the dimensionality of  these high dimensional data objects. We propose a new distance metric that is  applied to symbolic data objects and we test it on time series data bases in a  classification task. We compare it to other distances that are well known in the  literature for symbolic data objects. We also prove, mathematically, that our  distance is metric."}
{"pdf_id": "0709.4669", "content": "Among data compression techniques, symbolic representation is an idea that seemed  to have potentially interesting pros, in that by using it we can benefit from the wealth  of text-retrieval algorithms and techniques. However, the first papers presented were  mainly ad hoc. In addition, they didn't present a technique to support Euclidean  queries. There were also other questions concerning the discretization and the size of  the alphabet [10].  But symbolic representation is receiving more and more attention. New distance  measures mainly adapted to this kind of representation have been proposed. Also  there have been many papers that suggest methods to discretize the data. For all these  reasons, symbolic representation seems very promising."}
{"pdf_id": "0709.4669", "content": "Different variations of  this distance were proposed later like the edit distance on real sequence (EDR) [4],  and the edit distance with real penalty (EDRP) [4]  The edit distance has a main drawback, in that it penalizes all change operations in the  same way, without taking into account the character that is used in the change  operation"}
{"pdf_id": "0709.4669", "content": "The edit distance was presented mainly to apply on spelling errors. But because of the  conventional keyboard arrangement, the probability that an \"A\" be mistyped as \"S\" is  not the same as mistyping \"A\" as \"P\", for instance (on an English keyboard), but yet,  the edit distance doesn't take these different possibilities into consideration."}
{"pdf_id": "0709.4669", "content": "somehow large. Third, if we try to use multiresolution techniques on the symbolic  representation, then we will have to define a table for each resolution. Another serious  problem arises in this case; merging two characters in text processing is not intuitional  at all. So there's no clear way on how the \"new\" characters (those of a different  resolution) can be related to the old ones.  In this paper, we present a new distance metric for symbolically represented data. It  has a few advantages; one of them is dealing with the above problems in a natural  way (no need to define a cost function for the change operation, no need to redefine it  for different resolutions)"}
{"pdf_id": "0709.4669", "content": "Given two strings  ,..., sm S = s and  ,..., nr R = r r . Their longest common  subsequence (abbreviated as LCSS) is the longest common subsequence to both of  them. This subsequence doesn't have to be consecutive, but it has to have the same  order in both strings."}
{"pdf_id": "0709.4669", "content": "the same as  ED S1 S2 )  But we notice that  NC S S 7.  This means that one change operation used a character that is more \"familiar\" to the  two strings in the first case than in the second case, in other words, S is closer to  S"}
{"pdf_id": "0709.4669", "content": "than  S . However, the edit distance couldn't recognize this, since the edit distance  was the same in both cases.  We will see later that this concept of \"familiarity\" can be extended to consider not  only NC but the frequency of sequences too.  N.B. We chose an example of strings of identical lengths since we were only  discussing the change operation"}
{"pdf_id": "0709.4669", "content": "(Revisiting the example presented in section 4.1)  We define the form of a string is a vector as follows:  ,..., nf Form S  ( n is the size of the alphabet, in our example it's 26, the  English alphabet)  0,1 ,... ,0 1 0, ..] [ ,2 0,....., ,0 1, 1 0, ,.., ,0 ( 1) M N Form S"}
{"pdf_id": "0709.4669", "content": "each of these strings less similar to  S than  S is. We also see from this case that the  position at which this unfamiliar character was changed didn't affect the EED.  iii- If we continue this process and change the characters in position 4 in  S or in"}
{"pdf_id": "0709.4669", "content": "position 1 in  S with that same unfamiliar character x (in both cases we obtain  S ).  In both of these cases we substitute a familiar character ( a in the first case and n in  the second case) with an unfamiliar character x so there should be loss of similarity  compared with  S and  S .  By calculating the EED we see that:  EED S S , which is what we expected.  We see that the EED was not the same in the above cases, while the ED was always  the same."}
{"pdf_id": "0709.4669", "content": "series and n is the length of the second time series, or  O n2 if the two time series are  of the same lengths. The complexity is high. However, we have to take into  consideration that EED is a universal distance that can be applied to all symbolic  represented data objects, where other distance measures are not applicable.  In order to make EED scale well when applied to time series, we can find a symbolic  representation method that can allow high compression of the time series, with  acceptable accuracy."}
{"pdf_id": "0709.4669", "content": "SAX, in simple words,  consists of three steps;  1-Reducing the dimensionality of the time series by using PAA (After normalizing the  times series)  2-Discretization the PAA to get a discrete representation of the times series(Using  breakpoints)  3-Using a distance measure defined by the authors  To test EED we proceeded in the same way for steps 1 and 2 above to get a symbolic  representation of time series, then in step 3 we compared EED with ED and the  distance measure defined in SAX"}
{"pdf_id": "0709.4669", "content": "The tests were aimed at comparing three main methods; the edit distance (ED) (we  tested it for comparison reasons), our method; the extended edit distance (EED), and  SAX . It's very important to point out that ED is mainly a method that is applied to  textual data, what we did to test it on time series was to use the symbolic  representation suggested in SAX, then we applied the ED to these symbolic  representation obtained (the same thing we did to test EED). Anyway, SAX is a  method that is designed directly to be used on time series, so it's a very competitive  method."}
{"pdf_id": "0709.4669", "content": "So the  datasets chosen are; FaceAll, SwedishLeaf, wafer, ECG200, Adiac, Beef, OliveOil (7  datasets)  It's important to mention here that even though the optimization process on the  training set is actually a generalization of the optimization process of the first  experiment (where the alphabet size was between 3 and 10), this second experiment is  completely independent on the first one, since the parameters that optimize the"}
{"pdf_id": "0709.4669", "content": "training set of a certain dataset don't necessarily give the smallest error for the testing  set. In fact, the error may even increase when using a wider range of alphabet size.  In order to study the impact of using a wider range of alphabet size, we calculate, on  the train data, the mean and standard deviation of the error for the datasets in  question, for an alphabet size varying in [3, 10 ] (Table. 3)  Table 3  1-NN  Euclidean  Distance"}
{"pdf_id": "0709.4669", "content": "Now, in order to study the error for the new range, we proceed in the same way we  did for the first experiment, that is; we optimize the parameters on the training sets for  the datasets in question, but this time for alphabet size that varies between 3 and 20,  then we use these parameters on the testing sets of these databases, we get the  following results (Table. 4)"}
{"pdf_id": "0709.4669", "content": "The main advantage of the EED over the two other methods is that it can be extended  to take into account not only the frequency of characters, but also the frequency of  segments, so it can be applied to different resolutions, which is something we're  working on.  Another possible future work is using the EED in anomaly detection in time series  data mining, by representing the motif symbolically and applying the EED by taking  the frequency of the motif rather than the frequency of characters"}
{"pdf_id": "0709.4669", "content": "In this paper we presented a new distance metric applied to strings. The main feature  of this distance is that it considers the frequency of characters, which is something  other distance measures don't consider.  We tested this distance metric on a time series classification task, and we compared it  to two other distances , and we showed that our distance gave better results, even  when compared to a method (SAX) that is designed mainly for symbolically  represented time series.."}
{"pdf_id": "0710.0013", "content": "locally within the graph. Using a multiscale representation of the model allows information to propagate through coarse scales, which improves the rate of convergence to global equilibrium. Also, in discrete problems, such multiscale representations can help to avoid local minima. In the context of our convex LR approach, we expect this to translate into a reduction of the duality gap to obtain the optimal MAP estimate in a larger class of problems."}
{"pdf_id": "0710.0013", "content": "[15] L. Ruschendorf. Convergence of the iterative proportional fitting procedure. Annals Stat., 23, 1995. [16] V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. IEEE Trans. Pattern Analysis and Machine Intelligence, January 2005. [17] D. Malioutov, J. Johnson, and A. Willsky. Walk-sums and belief propagation in Gaussian graphical models. J. Machine Learning Research, 7, October 2006. [18] V. Chandrasekaran, J. Johnson, and A. Willsky. Estimation in Gaussian graphical models using tractable subgraphs: a walk-sum analysis. IEEE Trans. Signal Processing, to appear. [19] B. Gidas. A renormalization group approach to image processing problems. IEEE Trans. Pattern Analysis and Machine Intelligence, 11, February 1989. [20] U. Trottenberg, C. Oosterlee, and A. Schuller. Multigrid. Academic Press, 2001."}
{"pdf_id": "0710.0043", "content": "Here we introduce another globally rigid graph which has the advantage of having a smaller maximal clique size. Although the graph is not chordal, we will show that exact inference is tractable and that we will indeed benefit from the decrease in the maximal clique size. As a result we will be able to obtain optimality guarantees like those from [1]. Our graph is constructed using Algorithm 1."}
{"pdf_id": "0710.0043", "content": "This algorithm will produce a graph like the one shown in Figure 2. We will denote by G the set of graphs that can be generated by Algorithm 1. G = (V, E) will denote a generic graph in G. In order to present our results we need to start with the definition of a globally rigid graph:"}
{"pdf_id": "0710.0043", "content": "So our statements are really about graph embeddings in R2, but for simplicity of presentation we will simply refer to these embeddings as \"graphs\". This means that there are no degrees of freedom for the absent edges in the graph: they must all have specified and fixed lengths. To proceed we need a simple definition and some simple technical lemmas."}
{"pdf_id": "0710.0043", "content": "We now draw on results first obtained by Weiss [8], andconfirmed elsewhere [9]. There it is shown that, for graphi cal models with a single cycle, belief propagation converges to the optimal MAP assignment, although the computed marginals may be incorrect. Note that for our purposes, this is precisely what is needed: we are after the most likelyjoint realization of the set of random variables, which cor responds to the best match between the template and the scene point patterns. Max-product belief propagation [10] in a cycle graph like the one shown in Figure 3 amounts to computing the following messages, iteratively:"}
{"pdf_id": "0710.0169", "content": "Abstract: The classification of metrics and algorithms search for related terms via WordNet, Roget's  Thesaurus, and Wikipedia was extended to include adapted HITS algorithm. Evaluation experiments on  Information Content and adapted HITS algorithm are described. The test collection of Russian word pairs  with human-assigned similarity judgments is proposed."}
{"pdf_id": "0710.0169", "content": "http://www.ii.uam.es/~ealfon/pubs/2005-awic.pdf[Shi2005]. Shi Z., Gu B., Popowich F., Sarkar A. Synonym-based expansion and boosting based re-ranking: a two-phase approach for genomic information retrieval. Simon Fraser  University, 2005. http://trec.nist.gov/pubs/trec14/t14_proceedings.html [Strube2006]. Strube M., Ponzetto S. WikiRelate! Computing semantic relatedness using  Wikipedia. In Proceedings of the 21st National Conference on Artificial Intelligence  (AAAI 06). Boston, Mass., July 16-20, 2006. [to appear] http://www.eml"}
{"pdf_id": "0710.0243", "content": "In this paper, we use belief-propagation techniques to de velop fast algorithms for image inpainting. Unlike traditional gradient-based approaches, which may require many iterationsto converge, our techniques achieve competitive results af ter only a few iterations.On the other hand, while belief propagation techniques are often unable to deal with high-order models due to the explosion in the size of messages, we avoidthis problem by approximating our high-order prior model us ing a Gaussian mixture. By using such an approximation, weare able to inpaint images quickly while at the same time re taining good visual results."}
{"pdf_id": "0710.0243", "content": "To avoid the above problems, image restoration is typically performed using gradient-ascent, thereby eliminating the need to deal with many discrete gray-levels, and avoiding expensive sampling [16]. While gradient-based approaches are generally considered to be fast, they may still require several thousand iterations in order to converge, and even then will converge only to a local optimum."}
{"pdf_id": "0710.0243", "content": "2Although this final step may appear to make the running time of our solu tion linear in the number of gray-levels, it should be noted that this step needs to be performed only once, after the final iteration. It should also be noted that this estimate only requires us to measure the response of a one-dimensional Gaussian, which is inexpensive. More sophisticated mode-finding techniques exist [5], which we considered to be unnecessary in this case. Finally, note that this step is not required when our mixture contains only a single Gaussian, in which case we simply select the mean."}
{"pdf_id": "0710.0243", "content": "Unfortunately, it proved very difficult to compare the execution times of our model with existing gradient-ascent techniques. For example, the inpainting algorithm used in [16] computesthe gradient for all pixels using a 2-dimensional matrix convolution over the entire image, and then selects only the re gion corresponding to the inpainting mask. While this results in very fast performance when a reasonable proportion of an image is being inpainted, it results in very slow performance when the inpainting region is very sparse (as is often the case with scratches). It is easy to produce results which favor either algorithm, but such a comparison will likely be unfair. To make explicit this difficulty, consider the images in figure"}
{"pdf_id": "0710.0243", "content": "In this paper, we have developed a model for inpainting images quickly using belief-propagation. While image inpaint ing has previously been performed using low-order models by belief-propagation, and high-order models by gradient-ascent, we have presented new methods which manage to exploit the benefits of both, while avoiding their shortcomings. We have shown these algorithms to give satisfactory visual results and to be faster than existing gradient-based techniques, even in spite of our high-level implementation."}
{"pdf_id": "0710.0736", "content": "Our computations are solved by a multigrid algorithm which falls into the category of SuccessiveSubspace Corrections (see Xu [30], [31]). This was successfully applied to the vector-valued Allen Cahn equation in Kornhuber, [17], Kornhuber and Krause [18], [19] with a small variation (see Kornhuber and Krause [15]). In section II we brieny introduce and summarise previous directly relevant work leading up to section II-C, in which we formally introduce our own formulation and show how the minimisation of our functional leads to the desired system of PDEs; in section III we discretise the system and introduce the numerical method of solution, and in section IV we present a few practical aspects of implementation together with examples."}
{"pdf_id": "0710.0736", "content": "B. A phase-field formulation The Allen-Cahn PDE was introduced in [1] to model the domain coarsening occurring after a phase transition. It follows the evolution of a function u(x) known as the order parameter, which smoothly varies between the values of 0 and 1 across an interface1 to represent which parts of the"}
{"pdf_id": "0710.0736", "content": "the quantity c representing the average of I in u, in other words being a measure of the oscillation of the data over the support of u. In order to achieve a simultaneous segmentation of I into arbitrarily many pieces, we refer to the vector-valued formulation of the Allen-Cahn system was introduced in Garcke, Nestler and Stoth"}
{"pdf_id": "0710.0736", "content": "with some appropriate time discretisation to follow. The inequality is due to the multi-valued nature of the subgradient at the boundaries of GN; each iteration in the numerical method is performed as though (16) were a strict equality, and if the result lies outside the acceptable space, then it is projected appropriately, as described in section III-C."}
{"pdf_id": "0710.0736", "content": "Each method is associated with its own advantages, disadvantages, and computational costs. It is worth noting that the errors associated with each one decrease with each mesh refinement. The former can be thought of as projection by node and the latter as projection by simplex; examples are shown in figure 3."}
{"pdf_id": "0710.0736", "content": "Further, because each component has values not identical to 0 or 1, notably at each interface, it is useful to round all values to either extremum, in such a way that only one component is equal to 1 and all others are 0 at any given point; in this way, segmented regions are defined more precisely"}
{"pdf_id": "0710.1962", "content": "The idea for this note arose1 during the \"Web Information Retrieval and Linear Algebra Algorithms\" held at Schloss Dagstuhl in February 2007. Many brilliant people working on either side (numerical analysis and web search) had a chance to meet and talk for one week about mathematical and practical aspects of linear methods for ranking, and in particular (not surprisingly) PageRank and HITS.2"}
{"pdf_id": "0710.1962", "content": "These considerations bring us to the point of this note. The problem of computing PageRank is interesting from a practical viewpoint only if the size of the matrix is large and if the type of the matrix is a web graph. What do we mean by \"large\"? Currently, search engines claim to index a number of pages in the order of 1010. We cannot expect, as scientists, to replicate exactly"}
{"pdf_id": "0710.1962", "content": "There is an interesting phenomenon going on: some typical properties (e.g., high compressibility) arise in our examples only beyond a certain size (about 10 million nodes). People invoking the \"fractal nature\" of the web as an excuse to use small samples should thus be very careful (the .eu snapshot, for instance, is not a very good candidate)."}
{"pdf_id": "0710.1962", "content": "Note that I am not suggesting that all web graphs should look the same, or that we should set up some standards to define a web graph: there is a healthy diversityof structure in the real world due to culture, wealth, and available tools (content management systems, for instance, have steadily increased the average outdegree of the web in the last 5 years). But there are criteria, based on common sense and experience, that should delimit what we use in our experiments if we want to derive sensible conclusions, and the Stanford matrix largely falls short of such criteria."}
{"pdf_id": "0710.2037", "content": "Abstract—In this paper, we firstly modify a parameter in affinity propagation (AP) to improve its convergence ability, and then, we apply it to vector quantization (VQ) codebook designproblem. In order to improve the quality of the resulted code book, we combine the improved AP (IAP) with the conventional LBG algorithm to generate an effective algorithm call IAP-LBG. According to the experimental results, the proposed method not only improves its convergence abilities but also is capable of providing higher-quality codebooks than conventional LBG method."}
{"pdf_id": "0710.2037", "content": "A generalized algorithm was proposed by Linde, Buzo, and Gray (LBG) [4]. It is the most popular codebook design method. LBG iteratively applies two optimality conditions (nearest neighbor condition and centroid condition) to generate a codebook. However, it suffers from local optimality and is sensitive to the initial solution. If the initial solution is poor, the resulted codebook's quality will probably be poor, and as a result it will be difficult to produce a high-quality image."}
{"pdf_id": "0710.2037", "content": "Recently, a powerful algorithm called Affinity Propagation (AP) for unsupervised clustering was proposed by Frey and Dueck [5] . In AP algorithm, each point in a set is viewd as a node in a network. AP is based on message passing along edges of the network, following the idea of belief propagation [6] [7]. AP takes input real-value similarities s(n, m) which indicate how well the data point m is suited to be the"}
{"pdf_id": "0710.2037", "content": "cluster centroid to data point n, and then, two kinds of real value messages \"responsibility\" r(n, m) and \"availability\" a(n, m) are exchanged among data points until a high-qulity set of cluster centroids and corresponding clusters gradually emerges [5]. Breiny, there are two significant advantages of AP: one is its high-quality clustering capabilty; the other is its computational efficiency, especially for large data sets [8]. However, in AP, for self-similarity is the same for each point, all data points are simultaneously considered as potential clustering centroids. Actually, this feature brings a drawback for AP, since it will be more difficult to converge."}
{"pdf_id": "0710.2037", "content": "s(m, m) indicates that data points with larger values are more likely to be chosen as clustering centroids. The number of the final examplars is innuenced by the value of s(m, m). In the conventional AP, all data points are simultaneously considered as potential examplars so the authors set all s(m, m) to be the same value [5]."}
{"pdf_id": "0710.2037", "content": "For point n, the value of that maximizes a(n, m) + r(n, m) either identifies point n as an exemplar if m = n, or identifies the data point that is the exemplar for point n [5]. The message-passing procedure may be terminated after a fixed number of iterations, after changes in the messages fall below a thereshold, or after the local decisions stay constant for some number of iterations."}
{"pdf_id": "0710.2037", "content": "Since in the conventional AP, the authors consider that alldata points can be equally suitable as exemplars, they set self similarities of each point to be the same. However, we propose a different view of s(m, m). We argue that the self-similarity of each point should vary according to the similarities between this point and the others. A point may \"love\" to take itself as a exemplar more if it \"knows\" there are more other points choosing it to be a exemplar. We call this rule network-support similarities which, in this paper, is denoted as ns(m, m):"}
{"pdf_id": "0710.2037", "content": "We consider that the point whose ns(m, m) is larger would be more appropriate to be an examplar. Because the cluster shape is regular in VQ codebook design, there is only one centroid for each cluster. As to a point, when more points support it to be a centroid, it should prefer to choosing itself as a centroid than other points. In order to get the very number ofcodewords, we set a tuning parameter called ratio of network support similarities rs. And we find that the codeword number decreases monotonously with rs."}
{"pdf_id": "0710.2037", "content": "Comparisons measured by PSNR (dB) on genarating code books for the five different images are compared among the four methods. Results are shown in Table 1 and Table 2. The codebooks used in Table 1 are generated from the training sets accordingly, and the codebook used in Table 2 is generated from the training set of the \"peppers\". From Table 1, we can see that IAP-LBG method can improve the PSNR of the generated codebook by 0.62 dB compared with conventional AP, and 0.95 dB compared with conventional LBG averagely. From Table 2 we can see that IAP-LBG algorithm can improve the PSNR by 0.18 compared with conventional AP, and 0.28 compared with conventional LBG averagely. In a word, the proposed algorithm in this paper is really effective."}
{"pdf_id": "0710.2231", "content": "This architecture, which is described in greater detail in [17], con tains prior knowledge in that it uses tying of weights within the neural net to extract low-level features from the input that are invariant with respect to the position within the image, and only in later layers of the neural net the position information is used"}
{"pdf_id": "0710.2231", "content": "Shape Context [3] 210, 448, 583, 692, 717, 948, 1034, 1113, 1227, 1248, 1300, 1320, 1531, 1682, 1710, 1791, 1879, 1902, 2041, 2074, 2099, 2131, 2183, 2238, 2448, 2463, 2583, 2598, 2655, 2772, 2940, 3063, 3074, 3251, 3423, 3476, 3559, 3822, 3851, 4094, 4164, 4202, 4370, 4498, 4506, 4663, 4732, 4762, 5736, 5938, 6555, 6572, 6577, 6598, 6884, 8066, 8280, 8317, 8528, 9506, 9643, 9730, 9851"}
{"pdf_id": "0710.2231", "content": "SVM [9] 448, 583, 660, 675, 727, 948, 1015, 1113, 1227, 1233, 1248, 1300, 1320, 1531, 1550, 1682, 1710, 1791, 1902, 2036, 2071, 2099, 2131, 2136, 2183, 2294, 2489, 2655, 2928, 2940, 2954, 3031, 3074, 3226, 3423, 3521, 3535, 3559, 3605, 3763, 3870, 3986, 4079, 4762, 4824, 5938, 6577, 6598, 6784, 8326, 8409, 9665, 9730, 9750, 9793, 9851"}
{"pdf_id": "0710.2231", "content": "IDM [15] 446, 448, 552, 717, 727, 948, 1015, 1113, 1243, 1682, 1879, 1902, 2110, 2131, 2183, 2344, 2463, 2524, 2598, 2649, 2940, 3226, 3423, 3442, 3559, 3602, 3768, 3809, 3986, 4054, 4164, 4177, 4202, 4285, 4290, 4762, 5655, 5736, 5938, 6167, 6884, 7217, 8317, 8377, 8409, 8528, 9010, 9506, 9531, 9643, 9680, 9730, 9793, 9851"}
{"pdf_id": "0710.2611", "content": "The two noise terms are here linearly dependent by accident. This is a consequence of too small dimensionality of our binary strings (four bits, whereas in realistic cases Kanerva suggested 104 bit strings). This is the price we pay for simplicity of the example. Decoding the name involves two steps. First"}
{"pdf_id": "0710.3185", "content": "EIT images treated by the fuzzy model were compared with the hypertonic saline injection method and CT-scan images, presenting good results in both qualitative (the image obtained by the model was very similar to that of the CT-scan) and quantitative (the ROC curve provided an area equal to 0"}
{"pdf_id": "0710.3185", "content": "Recently, fuzzy set theory has been used to deal with uncertainties present in health sciences and the results are very promising. It's aplicability covers a wide range of subjects, from epidemiological studies to diagnosing system development [4-7]. Our implementation of the EIT image treatment system employs the method of Mamdani and comprises software modules grouped in three steps: EIT raw data acquisition and image generation step, fuzzy modeling step and image segmentation step (Figure 1)."}
{"pdf_id": "0710.3185", "content": "Each EIT image is formed by a matrix containing 32x32 pixels. The fuzzy modeled image was obtained by running the model once for each pixel, requiring 1024 runs to form one modeled image. All fuzzy linguistic models developed for this study applied the Mamdani inference procedure and the center of area defuzzification method, and were based on expert experience in EIT chest image analysis. 1) Heart fuzzy model: The fuzzy linguistic model for the heart has three antecedent variables in its propositions: normalized perfusion amplitude, normalized time delay (TD) and pixel position, all of them were derived from ECG gated images; and one consequent variable: the possibility that the pixel carries the heart information (heart possibility). The pixel position is derived from"}
{"pdf_id": "0710.3185", "content": "The fuzzy models, as previously described and depicted in Figure 1, were run for each of the seven EIT raw data sets acquired in the present experiment, totalizing seven lung perfusion images and seven lung ventilation images. For evaluation purposes, it was generated two representative images: median lung perfusion image and median lung ventilation image, both resultants from the pixel-by-pixel median of the seven images, respectively."}
{"pdf_id": "0710.3185", "content": "For evaluation purposes and in order to partition the modeled images in regions of practical interests, a segmented image was generated. The method used for segmentation was the threshold of the modeled images. The images were submitted to threshold values, generating two images, one representing the lung perfusion map and the other representing the lung ventilation map. This methodology consists in a defuzzification procedure of the two fuzzy lung images, in a theoretical point of view. A total lung map was generated as the classical union of the two previous ones."}
{"pdf_id": "0710.3185", "content": "Two variables were calculated: a) sensibility, defined as the number of pixels that belonged at the same time to the lung perfusion map and the reference image, divided by the number of pixels in the reference image; b) specificity, defined as the number of pixels that, at the same time, did not belong either to the perfusion map or to the reference image, divided by the total number of pixels that did not belong to the reference image"}
{"pdf_id": "0710.3185", "content": "V. CONCLUSIONS The method for EIT image fuzzy modeling presented in this study provided very good resultswhen compared with the reference methods. Besides an anatomic image similar to CT-scan, sepa rating heart and lung also provided a segmented image in which the mapping of the ventilation and perfusion pulmonary functions were observed. The model provided new lung structure delineation based on pulmonary functions not available before in the original EIT images. These achievements could serve as the base for development of an EIT based clinical tool for the diagnosis of some critical diseases commonly prevalent in the critical care units."}
{"pdf_id": "0710.3561", "content": "Abstract. A method for the construction of approximate analytical expressions for the stationary marginal densities of general stochastic search processes is proposed. By the marginal densities, regions of the search space that with high probability contain the global optima can be readily defined. The density estimation procedure involves a controlled number of linear operations, with a computational cost per iteration that grows linearly with problem size."}
{"pdf_id": "0710.3561", "content": "where the brackets represent the average over the iterations of the density estimation procedure. Previous preliminary applications of the density estimation method on the generation of suitable populations of initial points for optimization algorithms can be found in [16]. In the next section the capabilities of the proposed algorithm for the construction of reliable probabilistic bounds is tested on several benchmark unconstrained examples and in a family of well known constrained NP-hard problems."}
{"pdf_id": "0710.3561", "content": "Two measures written in terms of normalized distances are presented in the examples of Figures 3, 4 and 5: i) The distance between the global optimum and the point in which the density is maximum. ii) The length of the 95% probability interval around the point of maximum probability."}
{"pdf_id": "0710.4231", "content": "Abstract: This paper addresses a method to analyze the covert social network  foundation hidden behind the terrorism disaster. It is to solve a node discovery  problem, which means to discover a node, which functions relevantly in a  social network, but escaped from monitoring on the presence and mutual  relationship of nodes. The method aims at integrating the expert investigator's  prior understanding, insight on the terrorists' social network nature derived  from the complex graph theory, and computational data processing. The social  network responsible for the 9/11 attack in 2001 is used to execute simulation  experiment to evaluate the performance of the method."}
{"pdf_id": "0710.4231", "content": "Biographical notes: Yoshiharu Maeno received the B.S. and M.S. degrees in  physics from the University of Tokyo, Tokyo, Japan. He is currently working  toward the degree at the Tsukuba University, Tokyo. He is with NEC  Corporation. His research interests lie in non-linear phenomena, complex  networks, social interactions, human cognition, and innovation. He is a member  of the IEEE (Systems Man & Cybernetics, Computational Intelligence,  Computer, and Technology Management Societies), APS, and INSNA. He  received the Young Researchers' Award from the IEICE in 1999."}
{"pdf_id": "0710.4231", "content": "Yukio Ohsawa received the Ph.D. degree in communication and information  engineering from the University of Tokyo, Tokyo, Japan. He was with the  Graduate School of Business Sciences, Tsukuba University, Tokyo. In 2005, he  joined the School of Engineering, University of Tokyo, where he is currently an  Associate Professor. He initiated the research area of chance discovery as well  as a series of international meetings (conference sessions and workshops) on  chance discovery, e.g., the fall symposium of the American Association of  Artificial Intelligence (2001). He co-edited books on chance discovery  published by Springer-Verlag and Advanced Knowledge International, and also"}
{"pdf_id": "0710.4231", "content": "special issues of journals such as New Generation Computing. Since 2003, his  activity as Director of the Chance Discovery Consortium Japan has linked  researchers in cognitive science, information sciences, and business sciences,  and business people to chance discovery. It also led to the introduction of these  techniques to researchers in Japan, the U.S., the U.K., China, Taiwan, R.O.C.,  etc."}
{"pdf_id": "0710.4231", "content": "Figure 2 Interactive process from the intelligence, surveillance and prior knowledge of the expert  investigators toward the hypothesis on the latent structure. The computational data  processing in the dashed grey box visualizes the observed records on communication in  the form of eq.(1). It consists of clustering using the prior knowledge, and ranking of  suspicious inter-cluster relationships which originates in the unobserved person. The  expert explores the difference between the visualized social network diagram and the  prior understanding, which is the basis to invent a hypothesis."}
{"pdf_id": "0710.4231", "content": "( ) B s . (2)  At first, the all persons appearing in the observed records bi in eq.(1) are grouped into  clusters cj. The number of clusters |c| depends on the prior knowledge. Mutually close  persons form a cluster. The measure of closeness between a pair of persons is evaluated  by Jaccard's coefficient. It is defined by eq.(3). The function F(pi) is the occurrence  frequency of a person pi in the records. The closeness means activeness of the  communication if the record is a set of the persons appearing together in the emails,  conversations, or meetings. Jaccard's coefficient is used widely in link discovery, web  mining, or text processing."}
{"pdf_id": "0710.4231", "content": ". (3)  Here, we employ the k-medoids clustering algorithm (Hastie, 2001). It is an EM  (expectation-maximization) algorithm similar to the k-means algorithm for numerical  data. A medoid  ( j ) pmed c  locates most centrally within a cluster cj. It corresponds to the"}
{"pdf_id": "0710.4231", "content": "center of gravity in the k-means algorithm. The modoid persons are selected at random  initially. The other |p|-|c| persons are classified into the clusters whose medoids is the  closest. A new medoid is selected within an individual cluster so that the sum of  Jaccard's coefficients between the modoid and persons in the cluster can be maximal  (M(cj) defined by eq.(4)). This is repeated until the medoids converge."}
{"pdf_id": "0710.4231", "content": "We briefly review the social network responsible for the 9/11 attack in 2001 (Krebs,  2002). The study provides us with an insight on the covert social network foundation  behind the terrorism disaster. The social network is also used in the simulation is section  4. (Krebs, 2002) and (Morselli, 2007) studied the social network consisting of the 19  hijackers boarding on the 4 crashed airplanes (AA11, AA77, AA175, and UA93) and the  revealed 18 conspirators. The network is shown in figures 3 and 4. Figure 3 shows the  hijackers. Figure 4 includes the conspirators."}
{"pdf_id": "0710.4231", "content": "structure. It is in agreement with the observation that the Al Qaeda network is a flexible  tie-up of isolated cliques (Popp, 2006). Note that a bridge is an essential component to  make clusters rendezvous to form a social network. The absence of hubs overcomes the  drawbacks of a scale-free network, where the hubs result in vulnerability to attacks  (Albert, 2000) and easy exposure by the efficient search over the network (Adamic,  2001)."}
{"pdf_id": "0710.4231", "content": "In information retrieval, precision and recall are used as evaluation criteria. Precision  p is the fraction of relevant data among the all data returned by search. The relevant data  here is the records where the covert conspirator has been deleted in the second step.  Recall r is the fraction of the all relevant data that is returned by the search among the all  relevant data. They are defined by eq(11). and eq.(12)."}
{"pdf_id": "0710.4231", "content": "rd . (14)  Performance of the algorithm is evaluated with the test data under several conditions.  Figure 5 shows precision and recall to retrieve the records where a covert conspirator,  Mustafa A. Al-Hisawi, has been hidden. Mustafa A. Al-Hisawi was a big financial  sponsor to the hijackers, as mentioned in section 1.The number of clusters is |c|=4. The  probability of communication transmission is t=0.8. The horizontal axis is the ratio of the"}
{"pdf_id": "0710.4231", "content": "number of retrieved basket data to the number of the whole basket data ( mret |/ b | ).  The records retrieved as top 10% ranking are correct. The algorithm outputs correct  information. The ranking function Isd(bi) seems to show a little better performance than  Iav(bi). Isd(bi) is employed in the following study. Precision is 100% when the top 10%  of the baskets are retrieved. The algorithm works fine. Precision is 0.45 when the all  baskets are retrieved. The problem here includes many correct answers. It is not so  difficult because the network is small. (Maeno, 2006) studies the performance for a  network consisting of 400 nodes"}
{"pdf_id": "0710.4231", "content": "Figure 5 Precision p and recall r to retrieve the records where a covert conspirator, Mustafa A. Al Hisawi, has been hidden: (a) p using Iav(bi), (b) r using Iav(bi), (c) p using Isd(bi), (d) r  using Isd(bi), (e) p using Itp(bi), and (f) r using Itp(bi). The number of clusters is |c|=4.  The probability of communication transmission is t=0.8. The horizontal axis is the ratio  of the number of retrieved basket data to the number of the whole basket data (mret/|b|)."}
{"pdf_id": "0710.4231", "content": "Figure 6 shows precision and recall at |c|=2, 4, 8, and t=0.8. The value of |c| depends  on the prior knowledge of the social network structure. The case where |c|=4 is a  reasonable choice, based on the knowledge that 4 airplanes were hijacked. It actually  shows the best performance. With the wrong prior knowledge, |c|=2, the performance  degrades. Performance degradation at |c|=8 is small because the practical number of  groups including conspirators may be close to, but a little larger than 4."}
{"pdf_id": "0710.4231", "content": "Figure 6 Precision p and recall r to retrieve the records where a covert conspirator, Mustafa A. Al Hisawi, has been hidden: (a) p at |c|=2, (b) r at |c|=2, (c) p at |c|=4, (d) r at |c|=4, (e) p at  |c|=8, and (f) r at |c|=8. The simulation condition is that t=0.8, and Isd(bi) is used."}
{"pdf_id": "0710.4231", "content": "Figure 7 shows F value gain at |c|=4, and t=1.0, 0.8, 0.6, 0.4. At t=1.0, 0.8, the  performance is stable (the curve is smooth). At t=1.0, the gain is small because the  increasing input information and longer reach communication make the problem easy. At  t=0.6, the performance begins to be unstable (the curve begins to fluctuate). At t=0.4, the  algorithm fails to work because the input information is too poor to extract inter-cluster  relationship."}
{"pdf_id": "0710.4231", "content": "Figure 8 F value gain to retrieve the records where a covert conspirator has been hidden. The covert  conspirator is (a) Mustafa A. Al-Hisawi, (b) Lotfi Raissi, (c) Rayed M. Abdullah, (d)  Ramzi B. Al-Shibh, (e) Said Bahaji, (f) Osama Awadallah, and (g) Raed Hijazi. The  simulation condition is that |c|=4, t=0.8, and Isd(bi) is used."}
{"pdf_id": "0710.4231", "content": "Figure 9 shows F value gain to retrieve the records where a covert conspirator, Raed  Hijazi, has been hidden. Iav(bi) and Itp(bi) are employed again as in Figure 5. Itp(bi)  shows better performance although it is still a little unstable and may not be sufficient for  a practical use. The performance may be improved by focusing on the relationship  between 2 clusters, rather than between the all clusters."}
{"pdf_id": "0710.4231", "content": "A social network diagram is drawn from the observed records according to the  process in figure 2. The unobserved person in a suspicious record is drawn as a red node.  The red node and the gateway persons  pgtw bi c j  are connected with red links."}
{"pdf_id": "0710.4231", "content": "(Klerks, 2002) points out that criminal organizations  tend to be strings of inter-linked small groups that lack a central leader, but to coordinate  their activities along logistic trails and through bonds of friends, and that hypothesis can  be built by paying attention to remarkable white spots and hard-to-fill positions in a  network"}
{"pdf_id": "0710.4231", "content": "In this paper, we demonstrate the proposed method to analyze the covert social  network foundation hidden behind the terrorism disaster. The method integrates the  expert investigator's prior understanding, insight on the terrorists' social network nature  derived from the complex graph theory, and computational data processing. It is effective  to discover a node, which functions relevantly in a social network, but escaped from  monitoring on the presence and mutual relationship of nodes. Precision, recall, and F  value characteristics of the algorithm are evaluated in the simulation experiment using the  social network responsible for the 9/11 attack in 2001."}
{"pdf_id": "0710.4734", "content": "neural network, fuzzy and genetic algorithm) to further  manipulate these sets of multiple trip point values and tests  based on semiconductor test equipments, Our experimental  results demonstrate an excellent design parameter variation  analysis in device characterization phase, as well as detection  of a set of worst case tests that can provoke the worst case  variation, while traditional approach was not capable of  detecting them"}
{"pdf_id": "0710.4734", "content": "In contrast, the  methodology for characterization is a kind of closed loop test;  that is, a test repeated many times within a specific timing  edge varied with a range, looking for the pass/fail point of an  associated parameter, and this is called trip point as shown in  figure 1"}
{"pdf_id": "0710.4734", "content": "under all admissible conditions. It is practically impossible to determine the true worst case test manually using a deterministic method. This finally leads to the major technical challenges: How to select a set of worst case tests that can provoke the worst case variation against specification? How  can we automate this process intelligently? This paper solves the problem efficiently using computational intelligence techniques with industrial ATE."}
{"pdf_id": "0710.4734", "content": "2. Contribution  Example: Binary Search for Trip Point End point Comparing to the traditional device characterization concepts [1-7] [15-16], our work has the following contributions [11]: Device Fail Region Test 2  We propose multiple characterization trip point concept instead of conventional single trip point method. Test 1 Trip Point We develop a search method: search until trip point technique, to reduce the repetition of measurement during  characterization  phase.  This method ultimately speeds up the searching time of worst case test in characterization process."}
{"pdf_id": "0710.4734", "content": "Worst Case Trip Point Variation We use neural network (NN) to learn from a set of input tests and their corresponding characterization trip points via ATE. In addition, we propose to use fuzzy set theory to encode the characterization trip  point information. In operation phase, neural network will perform a classification task to identify the worst case test. Finally, this set of pre-selected worst case tests will be further optimized by genetic algorithm (GA) based on the fitness of the trip point value obtained from the ATE. Final set of worst case tests can be re-simulated or analyzed in detail with ATE (e.g. wafer probing analysis) to localize the design weakness efficiently."}
{"pdf_id": "0710.4734", "content": "For the procedure in figure 2, we use the random test generator based on [9-10], combined with a device characterization algorithm such as binary search or successive approximation. In order to pin-point the potential worst case test sequences more precisely, we define small test sequences in between 100 to 1000 vector cycles for each characterization"}
{"pdf_id": "0710.4734", "content": "are properly designed. Therefore, it is not necessary to search through the whole \"generous range\" for multiple repetitions of trip point measurement that would cause a very lengthy process, since CR(IT) is much larger than SF(IT) as shown in figure 3. In addition, In case of unexpected drift of design performance vs target specification due to unexpected design weaknesses provoked by a set of worst case tests, our proposal is flexible enough to detect the drift while keeping smallest effort of searching for the trip point value based on RTP. This ultimately leads to huge savings of measurement time and guaranteed automatic convergence, keeping the test time as low as possible."}
{"pdf_id": "0710.4734", "content": "Today, what is missing in typical device characterization concepts with industrial ATE is that the test system is not designed to perform the worst case device characterization. Instead ATE is used to detect the trip point as accurate as possible based on a set of pre-defined patterns. A pre-defined test is based on deterministic way of testing the circuit. It does not for sure emulate the worst case application condition, and this ultimately leads to potential application failures, even if the circuit has passed all deterministic characterization tests. On the other hand, it would be a huge work if we try to analyze all different combinations of test sequences and specifications. To solve this limitation, we change the major objective of"}
{"pdf_id": "0710.4734", "content": "device characterization, focusing only on how to accurately detect the worst case test that can provoke the worst case performance vs. specification variation, while keeping the time of measurement as low as possible using the techniques proposed in section 2 and 3. In addition, we combine computational intelligence techniques with industrial ATE to perform learning of device characterization and the worst case test classification task. To implement this concept, we re-configure our previous work [9][10] to use it in semiconductor device characterization. The completed device characterization learning and optimization scheme can be described as follows in figures 4 and 5."}
{"pdf_id": "0710.4734", "content": "(4) The confidence in the classification is determined by averaging the mean error for each network (i.e. consistency check). After that, NN will continue learning with iterative network learnability and generalization check [12-14] until learning and generalization error is small enough; otherwise go back to (1). (5) At the end of NN learning, a NN weight file is generated. This file will be used in classification task of worst case test based on only software computation without measurement in optimization phase as in figure 5.  Random Test Generator: T (N=number of tests)"}
{"pdf_id": "0710.4734", "content": "(1) To measure how confident the neural net is in  its classification, we propose to use the NN voting machine algorithm, such that multiple NNs are trained on different subsets of the training input tests, then vote in parallel on unknown input tests. Thus, the first step is presenting a random test to ATE and neural network modules continuously."}
{"pdf_id": "0710.4734", "content": "(1) A number of GA test populations are  initialized by a set of sub-optimal tests selected by fuzzy-neural network test generator based on its previous learning experience (NN weight file). (2) Detect the first reference trip point RTP using equation (2), and search for the subsequent trip point using equation (3) or (4) depending on the search parameter conditions."}
{"pdf_id": "0710.4734", "content": "(4) GA optimization process continues until GA fitness value can not improve anymore. Then go to (1) and a brand new population will start GA again. This process will continue until either it  reaches the maximum optimization steps or the worst case is detected based on worst case ratio  theorem. At last, final worst case tests are generated and stored in the database."}
{"pdf_id": "0710.4975", "content": "The computational burden of the method remainslight as the number of nodes and surveillance logs in creases. The method is expected to work generally for clustered networks but moderately even if the network topological and stochastic mechanism to generate the surveillance logs is not understood well. The method works without the knowledge about the hub-and-spoke model; the parametric form with rjk and fj in Section 3. The result, however, can not be very accurate because of the heuristic nature. A statistical inference methodwhich requires heavy computational burden, but out puts more accurate results is presented next."}
{"pdf_id": "0710.4975", "content": "In the performance evaluation in Section 6, a few assumptions are made for simplicity. The probability fj does not depend on the nodes (fj = 1/M). The value of the probability rjk is either 1 when a link is present between nodes, or 1 otherwise. It means thatthe number of the possible collaborative activity patterns is bounded. The innuence transmission is sym metrically bi-directional; rjk = rkj."}
{"pdf_id": "0710.4975", "content": "I illustrate how the method aids the investigators inachieving the long-term target of the non-routine re sponses to the terrorism attacks. Let's assume that the investigators have surveillance logs of the members of the global mujahedin organization except Osama bin Laden by the time of the attack. Osama bin Laden"}
{"pdf_id": "0710.5547", "content": "warp path and are parallel to the main diagonal, will  keep a similarity degree; the closer to the main  diagonal the bigger would be their similarity.  Our technique has been tested with Time Series, [3]  obtaining the expected results, similar subsequence  detection using an automatic no supervised algorithm  and make no features extraction.  2.3. Source Code Transform  Now we explain the representation transform that  we applied to the source codes in order to obtain its  sequence representation (1)."}
{"pdf_id": "0710.5547", "content": "2.4. Results  The data set contains C# source codes from  programming classes of the National Polytechnique  Institute. These codes were modified by : reemplazing  variable names, data types, alter the instruction  sequence order, for mention some of them. By making  these systematic modifications we obtained a reference  data set, which are similar to each source code from  the original data set. The input source codes to our  method are free of syntax errors. On figure 5 and 6, we  show some of the experiments using a first level  representation (operators category)."}
{"pdf_id": "0711.0694", "content": "show how to deduce error bounds involving the (more standard) Lp and max norms. Since the span seminorm can be zero for non zero (constant) vectors, there is no relation that would enable us to derive error bounds in span seminorm from a Lp or a max norm. Bounding an error with the span seminorm is in this sense stronger and this constitutes our motivation for using it."}
{"pdf_id": "0711.0694", "content": "Given an MDP, standard algorithmic solutions for computing an optimal value/policy (which dates back to the 1950s, see for instance (Puterman, 1994) and the references therein) are Value Iteration and Policy Iteration. The rest of this section describes both of these algorithms with some of the relevant properties for the subject of this paper."}
{"pdf_id": "0711.0694", "content": "• interestingly, we shall provide all our results using the span seminorms we have in troduced at the beginning of the paper, and using the relations between this span semi-norms and the standard Lp norms (Equation 1), it can be seen that our results are in this respect slightly stronger than all the previously described results."}
{"pdf_id": "0711.0694", "content": "When the policy or the value converges The performance bounds with respect to the approximation error can be improved if we know or observe that the value or the policy converges. Note that the former condition implies the latter (while the opposite is not true: the policy may converge while the value still oscillates). Indeed, we have the following Corollary."}
{"pdf_id": "0711.0694", "content": "These bounds, proved in Appendix E, unify and extend those presented for Approximate Value Iteration (Corollary 5 page 7) and Approximate Policy Iteration (Corollary 9 page 10), in the similar situation where the policy or the value converges. It is interesting to notice that in the weaker situation where only the policy converges, the constant decreases from"}
{"pdf_id": "0711.0694", "content": "where S is the set of wall configurations, P is the set of pieces, A(p) is the set of translation rotation pairs that can be applied to a piece p, r(s, p, a) and succ(s, p, a) are respectively the number of lines removed and the (deterministic) next wall configuration if one puts a piece p on the wall s in translation-orientation a. The only function that satisfies the above Equation gives, for each wall configuration s, the average best score that can be achieved from s. If we know this function, a one step look-ahead strategy (that is a greedy policy) performs optimally."}
{"pdf_id": "0711.0784", "content": "I. Present and explain, i- the theoretical presence of biovielectrolumines cence via ny's vision, ii- the biovielectroluminescence phenomenon under laboratorial conditions via at least one prototype relative to a ny andits associated engineering modules, iii- pre/post-motion frame expecta tions on patterns of motion via biovielectroluminescence technology, e.g., a mountable visual + imaging unit on a man's head."}
{"pdf_id": "0711.0784", "content": "The author thanks G. E. Goodwin, External Examiner of Leeds Metropolitan University, M. Dickinson, Former Senior Lecturer of Mathematics, University of Lincoln, for their written character reference support on behalf of the author's personalized scientific activities. It is highly appreciated for Dr. H. Alipour et al., on their moral support on the author's research-based endeavours."}
{"pdf_id": "0711.1466", "content": "Abstract An empty spot refers to an empty hard-to-fill space which can be found in the records of the social interaction, and is the clue to the persons in the underlying social network who do not appear in the records. This contribution addresses a problem to predict relevant empty spots in social interaction. Homogeneous and inhomogeneous networks are studied as a model underlying the social interaction. A heuristic predictor function method is presented as a new method to address the problem. Simulation experiment is demonstrated over a homogeneous network. A test data set in the form of market baskets is generated from the simulated communication. Precision to predict the empty spots is calculated to demonstrate the performance of the presented method."}
{"pdf_id": "0711.1466", "content": "• An organization can be modeled as a social network which underlies below the socialinteraction. Nodes are persons. Links are relationship such as friendship, business partnership, chain of command etc. The links can be undirectional, unidirectional, or bidirec tional. Variety of network topologies are known. A scale-free network[3] and a small-world network[19] were studied mathematically in detail. The topology of the real networks are diverse. The topologies of contemporary inter-working terrorists, self-organizing on-line community, and purposefully organized business team do not resemble."}
{"pdf_id": "0711.1466", "content": "• The empty spot in the social interaction is the main topic of this contribution. It refers to an empty hard-to-fill space, which can exist in the observed records of the social interaction, and is the potential clue to the persons in the underlying social network who do not appear in the records. Such hidden persons are the origin of the empty spot in a nutshell."}
{"pdf_id": "0711.1466", "content": "In this contribution, the problem we address is to discover relevant empty spots in a complex social interaction. We propose a heuristic predictor function method to predict the relevant empty spots and the hidden persons from communication records. The method is presented in detail in 4 after studying the related works in 2 and the network models (homogeneous and inhomogeneous network) in 3. Simulation experiment is demonstrated in 5. A test data set is generated in the form of market baskets as the simulated communication records over a homogeneous network. Precision to discover the empty spots is calculated to evaluate the performance of the method for three trial cases. Concluding remarks are presented in 6."}
{"pdf_id": "0711.1466", "content": "The output from the method is a clue on empty spots generated by the predictor function. More specifically, our aim is to identify the basket bi which is related to the empty spots (or the underlying hidden persons) the most likely. The core of our method is, therefore, to design a predictor function W(bi|D) to evaluate the likeliness of the individual baskets bi. The basket bi evaluated as the most likely should include the hidden node nx, and arise from the links rxj between the node nx and a gateway node nj. The gateway node is the observed node which is a neighbor of the hidden node."}
{"pdf_id": "0711.1466", "content": "At first, the nodes in the observation are clustered into groups based on the inter-node distance. The distance (or closeness) are defined according to the co-occurrence frequency between the nodes. Occurrence frequency of a node F(ni) is defined by Equation (5) using a Boolean function B(s) for a proposition s in Equation (6)."}
{"pdf_id": "0711.1466", "content": "Then, the predictor function W(bi|D) in Equation (10) is used to evaluate the likeliness of the individual baskets bi as a candidate which should have included empty spots. The empty spots arise from the hidden participants to the basket, which is the origin of attraction in the empty spots among clusters. The baskets ranked more highly are retrieved by the baskets."}
{"pdf_id": "0711.1466", "content": "We study how precisely the heuristic predictor function method extracts information on the empty spots from the test data set generated as the observed communication records. Communication is a typical social interaction. The homogeneous social network in Figure 3 is employed as a model for the communication patterns among 995 persons. We use precision as a measure of the performance. In information retrieval, precision has been used as evaluation criteria, which is the fraction of the amount of relevant data to the amount of the all data returned by search (the data ranked highly by the heuristic predictor function)."}
{"pdf_id": "0711.1814", "content": "The two readings of ontology describedabove are indeed related each other, but in order to solve the terminological im passe the word conceptualization is used to refer to the philosophical reading as appear in the following definition, based on (Gruber 1993): An ontology is a formal explicit specification of a shared conceptualization for a domain of interest"}
{"pdf_id": "0711.1814", "content": "Ontology Engineering, notably its DL-based approach, is playing a relevant role in the definition of the Semantic Web. The Semantic Web is the vision of the World Wide Web enriched by machine-processable information which supports the user in his tasks (Berners-Lee et al. 2001). The architecture of the Semantic Web is shown in Figure 1. It consists of several layers, each of which is equipped with an ad-hoc mark-up language. In particular, the design of the mark-up language for the"}
{"pdf_id": "0711.1814", "content": "The relational part of AL-log allows one to define Datalog3 programs enriched with constraints of the form s : C where s is either a constant or a variable, and C is an ALC-concept. Note that the usage of concepts as typing constraints applies only to variables and constants that already appear in the clause. The symbol & separates constraints from Datalog atoms in a clause."}
{"pdf_id": "0711.1814", "content": "In ILP the key mechanism is generalization intended as a search process through a partially ordered space of hypotheses (Mitchell 1982). The definition of a generality relation for constrained Datalog clauses can disregard neither the peculiarities ofAL-log nor the methodological apparatus of ILP. Therefore we rely on the reason ing mechanisms made available by AL-log knowledge bases and propose to adapt Buntine's generalized subsumption (Buntine 1988) to our framework as follows."}
{"pdf_id": "0711.1814", "content": "The former consists of using internalised heuristics to organize the observations into categories whereas the latter consists in determining a concept (that is, anintensional description) for each extensionally defined subset discovered by cluster ing. We propose a pattern-based approach for the former (see Section 4.2) and a bias-based approach for the latter (see Section 4.3). In particular, the clustering approach is pattern-based because it relies on the aforementioned commonalities between Clustering and Frequent Pattern Discovery. Descriptive tasks fit the ILPsetting of characteristic induction (De Raedt and Dehaspe 1997). A distinguish ing feature of this form of induction is the density of solution space. The setting of learning from interpretations has been shown to be a promising way of dealing with such spaces (Blockeel et al. 1999)."}
{"pdf_id": "0711.1814", "content": "organized in the DAG GCIA (see Figure 3). They are numbered according to the chronological order of insertion in GCIA and annotated with information of the generation step. From a qualitative point of view, concepts C-223310 and C-5333 well characterize Middle East countries. Armenia (ARM), as opposite to Iran (IR), does not fall in these concepts. It rather belongs to the weaker characterizationsC-3233 and C-4333. This suggests that our procedure performs a 'sensible' cluster ing. Indeed Armenia is a well-known borderline case for the geo-political concept of Middle East, though the Armenian is usually listed among Middle Eastern ethnic"}
{"pdf_id": "0711.1814", "content": "groups. Modern experts tend nowadays to consider it as part of Europe, therefore out of Middle East. But in 1996 the on-line CIA World Fact Book still considered Armenia as part of Asia.When the m.s.d. criterion is adopted (see Figure 4), the intensions for the con cepts C-2233, C-3233, C-8256, C-2333 and C-3333 change as follows:"}
{"pdf_id": "0711.1814", "content": "Building rules on top of ontologies for the Semantic Web is a task that can beautomated by applying Machine Learning algorithms to data expressed with hy brid formalisms combining DLs and Horn clauses. Learning in DL-based hybridlanguages has very recently attracted attention in the ILP community. In (Rou veirol and Ventos 2000) the chosen language is Carin-ALN, therefore examplecoverage and subsumption between two hypotheses are based on the existential en tailment algorithm of Carin (Levy and Rousset 1998). Following (Rouveirol andVentos 2000), Kietz studies the learnability of Carin-ALN, thus providing a pre processing method which enables ILP systems to learn Carin-ALN rules (Kietz"}
{"pdf_id": "0711.2832", "content": "ABSTRACT. In the first design stage, image reference plays a double role of means of formulation and resolution of problems. In our approach, we consider image reference as a support of creation activity to generate ideas and we propose a tool for navigation in references by image in order to assist daylight ambience design. Within this paper, we present, in a first part, the semantic indexation method to be used for the indexation of our image database. In a second part we propose a synthetic analysis of various modes of referential navigation in order to propose a tool implementing all or a part of these modes."}
{"pdf_id": "0711.2867", "content": "We analyze linkage strategies for a set I of webpages for which the webmaster wants to maximize the sum of Google's PageRank scores.The webmaster can only choose the hyperlinks starting from the web pages of I and has no control on the hyperlinks from other webpages.We provide an optimal linkage strategy under some reasonable assump tions."}
{"pdf_id": "0711.2867", "content": "we introduce some notations. In Section 3, we develop tools for analysing the PageRank of a set of pages I. Then we come to the main part of this paper: in Section 4 we provide the optimal linkage strategy for a set of nodes. In Section 5, we give some extensions and variants of the main theorems. We end this paper with some concluding remarks."}
{"pdf_id": "0711.2867", "content": "We will firstly determine the shape of an optimal external outlink struc ture Eout(I), when the internal link structure EI is given, in Theorem 10.Then, given the external outlink structure Eout(I) we will determine the pos sible optimal internal link structure EI in Theorem 11. Finally, we will put both results together in Theorem 12 in order to get the general shape of an optimal linkage strategy for a set I when Ein(I) and E"}
{"pdf_id": "0711.2867", "content": "Finally, combining the optimal outlink structure and the optimal internal link structure described in Theorems 10 and 11, we find the optimal linkage strategy for a set of webpages. Let us note that, since we have here control on both EI and Eout(I), there are no more cases of several final classes or several leaking nodes to consider. For an example of optimal link structure, see Figure 1."}
{"pdf_id": "0711.2867", "content": "The optimal outlink structure for a single webpage has already been given by Avrachenkov and Litvak in [2]. Their result becomes a particular case of Theorem 12. Note that in the case of a single node, the possible choices for Eout(I) can be found a priori by considering the basic absorbing graph, since V = V0."}
{"pdf_id": "0711.2909", "content": "In this game each strategy Ci is strictly dominated by Ni, so the game can be solved by either reducing it in two steps (by removing in each step one Ci strategy) or in one step (by removing both Ci strategies) to a game in which each player i has exactly one strategy, Ni"}
{"pdf_id": "0711.2909", "content": "Indeed, in each step the removed element is strictly dominated in the considered CP-net. So using the iterated elimination of strictly dominated elements we reduced the original CP-net to one in which each variable has a singleton domain and consequently found a unique optimal outcome of the original CP-net N. Finally, the following result shows that the introduced reduction relation on CP-nets is complete for acyclic CP-nets."}
{"pdf_id": "0711.2909", "content": "The above example shows that graphical games with parametrized preferences can be used to provide a natural qualitative analysis of some problems studied in social networks. Expressing the process of selecting a technology using games with parametrized preferences, Nash equilibria and elim ination of never best responses is more natural than using CP-nets. On theother hand we arrived at the relevant result about adoption of a single tech nology by searching for an analogue of Theorem 4 about acyclic CP-nets."}
{"pdf_id": "0711.2909", "content": "There are other ways to relate CSPs and games so that the CSP solutions and the Nash equilibria coincide. This is what is done in [10], where a mapping from the strategic games to CSPs is defined. Notice that our mapping goes in the opposite direction and it is not the reverse of the one in [10]. In fact, the mapping in [10] is not reversible."}
{"pdf_id": "0711.2917", "content": "Abstract Wikipedia is a useful source of knowledge that has many applications in language processing and knowledge representation. The Wikipedia category graph can be compared with the class hierarchy in an ontology; it has some characteristics in common as well as some differences. In this paper, we present our approach for answering entity ranking queries from the Wikipedia. In particular, we explore how to make use of Wikipedia categories to improve entity ranking effectiveness. Our experiments show that using categories of example entities works significantly better than using loosely defined target categories."}
{"pdf_id": "0711.2917", "content": "The objective of entity extraction is to identify named entities from plain text and tag each and every occurrence; whereas the objective of entity ranking is to search for entities in a semi-structured collection and to get back a list of the relevant entity names as answers to a query (with possibly a page or some description associated with each entity)"}
{"pdf_id": "0711.2917", "content": "France, belonging to categories such as \"European Countries\" and \"Republics\".There are two tasks in the INEX 2007 entity rank ing track: a task where the category of the expected entity answers is provided; and a task where a few (two or three) of the expected entity answers are provided. The inclusion of target categories (in the first task) and example entities (in the second task) makes these quite different tasks from the task of full-text retrieval, and the combination of the query and example entities (in the second task) makes it a task quite different from the task addressed by an application such as Google Sets1"}
{"pdf_id": "0711.2917", "content": "where only entity examples are provided. In this paper, we present our approach to entity ranking that augments the initial full-text information retrieval approach with information based on hypertext links and Wikipedia categories. In our previous work we have shown the benefits of using categories in entity ranking compared to full-text retrieval [19]. Here we particularly focus on how best to use the Wikipedia category information to improve entity ranking."}
{"pdf_id": "0711.2917", "content": "The traditional entity extraction problem lies in the ability to extract named entities from plain text using natural language processing techniques or statistical methods and intensive training from large collections. Benchmarks for evaluation of entity extraction have been performed for the Message Understanding Conference (MUC) [17] and for the Automatic Content Extraction (ACE) program [11]."}
{"pdf_id": "0711.2917", "content": "McNamee and Mayfield [14] developed a system for entity extraction based on training on a large set of very low level textual patterns found in tokens. Their main objective was to identify entities in multilingual texts and classify them into one of four classes (location, person, organisation, or \"others\"). Cucerzan and Yarowsky [6] describe and evaluate a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns. It achieves competitive performance when trained on a very short labelled name list."}
{"pdf_id": "0711.2917", "content": "Other approaches for entity extraction are based on the use of external resources, such as an ontology or a dictionary. Popov et al. [16] use a populated ontologyfor entity extraction, while Cohen and Sarawagi [4] ex ploit a dictionary for named entity extraction. Tenieret al. [18] use an ontology for automatic semantic an notation of web pages. Their system first identifies the syntactic structure that characterises an entity in a page. It then uses subsumption to identify the more specificconcept for this entity, combined with reasoning ex ploiting the formal structure of the ontology."}
{"pdf_id": "0711.2917", "content": "These measures are mostly renexive and symmetric [9] and take into account the distance (in the path) between the concepts, the depth from the root of the ontology and the common ancestor of the concepts, and the density of concepts on the paths between the concepts and from the root of the ontology [2]"}
{"pdf_id": "0711.2917", "content": "Fissaha Adafre et al. [10] form entity neighbourhoods for every entity, which are based on clustering of similar Wikipedia pages using a combination of extracts from text content and following both incoming and outgoing page links. These entity neighbourhoods are then used as the basis for retrieval for the two entity ranking tasks.Our approach is similar in that it uses XML struc tural patterns (links) rather than textual ones to identify potential entities. It also relies on the co-location of entity names with some of the entity examples (when provided). However, we also make use of the category hierarchy to better match the result entities with the expected class of the entities to retrieve."}
{"pdf_id": "0711.2917", "content": "description provides a natural language description of the information need, and the narrative provides a detailed explanation of what makes an entity answer relevant. In addition to these fields, the entities field provides a few of the expected entity answers for the topic (task 2), while the categories field provides the category of the expected entity answers (task 1)."}
{"pdf_id": "0711.2917", "content": "As Wikipedia is fast growing and evolving it is not pos sible to use the actual online Wikipedia for experiments, and so there is a need to use a stable collection to do evaluation experiments that can be compared over time.Denoyer and Gallinari [8] have developed an XML based corpus based on a snapshot of the Wikipedia, which has been used by various INEX tracks in 2006 and 2007. It differs from the real Wikipedia in some respects (size, document format, category tables), but it is a very realistic approximation."}
{"pdf_id": "0711.2917", "content": "Wikipedia also offers categories that authors can associate with Wikipedia pages. There are 113,483 cate gories in the INEX Wikipedia XML collection, which are organised in a graph of categories. Each page can be associated with many categories (2.28 as an average). Wikipedia categories have unique names (e.g. \"France\", \"European Countries\"). New categories can also be created by authors, although they have to"}
{"pdf_id": "0711.2917", "content": "The target categories will be generally very broad, so it is to be expected that the answer entities wouldnot generally belong to these broad categories. Accordingly, we defined several extensions of the set of cate gories, both for the target categories and the categories attached to answer entities. The extensions are based on using sub-categoriesand parent categories in the graph of Wikipedia cat egories. We define catd(C) as the set containing the target category and its sub-categories (one level down) and catu(t) as the set containing the categories attached"}
{"pdf_id": "0711.2917", "content": "We also experiment with two alternative approaches: by sending the title of the topic T as a query to the search engine (denoted as Tcat(C)); and by sending both the title of the topic T and the category names C as a query to the search engine (denoted as TCcat(C))"}
{"pdf_id": "0711.2917", "content": "In task 2, the categories attached to entity examples are likely to correspond to very specific categories, just like those attached to the answer entities. We define a similarity function that computes the ratio of common categories between the set of categories attached to an answer entity page cat(t) and the set of the union of the categories attached to entity examples cat(E):"}
{"pdf_id": "0711.2917", "content": "Our approach to identifying and ranking entities com bines: (1) the full-text similarity of the entity page with the query; (2) the similarity of the page's categorieswith the target categories or the categories of the en tity examples; and (3) the links to a page from the top ranked pages returned by a search engine for the query."}
{"pdf_id": "0711.2917", "content": "search engine, applying our entity ranking algorithms, and finally returning a ranked list of entities. We use Zettair2 as our choice for a full-text search engine. Zettair is a full-text information retrieval system developed by RMIT, which returns pages ranked by their similarity score to the query. We used the Okapi BM25 similarity measure that has proved to work well on the INEX 2006 Wikipedia test collection [1]. Our approach involves the following modules:"}
{"pdf_id": "0711.2917", "content": "together with the information about the paths of the links (XML paths). The assumption is that a good entity page is a page that is referred to by a page answering the query; this is an adaptation of the Google PageRank [3] and HITS [13] algorithms to the problem of entity ranking."}
{"pdf_id": "0711.2917", "content": "• The linkrank module calculates a weight for a page based (among other things) on the number of links to this page (see 6.2). The assumption is that a good entity page is a page that is referred to fromcontexts with many occurrences of the entity ex amples. A coarse context would be the full pagethat contains the entity examples. Smaller and bet ter contexts may be elements such as paragraphs, lists, or tables [15]."}
{"pdf_id": "0711.2917", "content": "• The category similarity module calculates a weight for a page based on the similarity of the page categories with that of the target categories or the categories attached to the entity examples (see 6.3). The assumption is that a good entity page is a page associated with a category close to the target categories or categories of the entity examples."}
{"pdf_id": "0711.2917", "content": "to the query. We carried out some experiments with different values of N and found that N=20 was an acceptable compromise between performance and discovering more potentially good entities. We use a very basic linkrank function that, for an answer entity page t that is pointed to by a page p, takes into account the Zettair score of the referring page z(p), and the number of reference links to the answer entity page #links(p, t):"}
{"pdf_id": "0711.2917", "content": "where f(x) = x (when there is no reference link to the answer entity page, f(x) = 0). The linkrank function can be implemented in a variety of ways; for task 2 where entity examples are provided, we have also experimented by weighting pages containing a number of entity examples, or by exploiting the locality of links around the entity examples [15]. This more complex implementation of the linkrank function is outside the scope of this paper."}
{"pdf_id": "0711.2917", "content": "We chose 27 topics that we considered were of an \"entity ranking\" nature, where for each page that had been assessed as containing relevant information, we reassessed whether or not it was an entity answer, and whether it loosely belonged to a category of entities we had loosely identified as being the target of the topic"}
{"pdf_id": "0711.2917", "content": "We use mean average precision (MAP) as our primary method of evaluation, but also report results using several alternative information retrieval measures: mean of P[5] and P[10] (mean precision at top 5 or 10 entities returned), and mean R-precision (R-precision for a topic is the P[R], where R is the number of entities that have been judged relevant for the topic)"}
{"pdf_id": "0711.2917", "content": "For this task we carried out three separate investiga tions. First, we wanted to investigate the effectivenessof our category similarity module when varying the ex tensions of the set of categories attached to both thetarget categories and the answer entities. We also investigated the impact that this variation had on the ef fectiveness when the two different category indexes are"}
{"pdf_id": "0711.3128", "content": "A wrapper is a tool that extracts information (entities or values) from a document, or a set of documents, with a purpose of reusinginformation in another system. A lot of research has been carried out in this field by the database community, mostly in relation to querying heterogeneous databases [1, 16, 24, 28]. More re cently, wrappers have also been built to extract information from web pages with different applications in mind, such as productcomparison, reuse of information in virtual documents, or build"}
{"pdf_id": "0711.3128", "content": "Recent research in named entity extraction has developed approaches that are not language dependant and do not require lots of linguistic knowledge. McNamee and Mayfield [20] developed a system for entity extraction based on training on a large set of very low leveltextual patterns found in tokens. Their main objective was to iden tify entities in multilingual texts and classify them into one of four classes (location, person, organisation, or \"others\"). Cucerzan andYarowsky [9] describe and evaluate a language-independent boot strapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns. It achieves competitive performance when trained on a very short labelled name list."}
{"pdf_id": "0711.3128", "content": "Other approaches for entity extraction are based on the use of exter nal resources, such as an ontology or a dictionary. Popov et al. [23] use a populated ontology for entity extraction, while Cohen andSarawagi [7] exploit a dictionary for named entity extraction. Te nier et al. [27] use an ontology for automatic semantic annotation of web pages. Their system firstly identifies the syntactic structure that characterises an entity in a page, and then uses subsumption to identify the more specific concept to be associated with this entity."}
{"pdf_id": "0711.3128", "content": "However, unlike PageRank where the page scores are calculated independently of the query by using the complete webgraph, in HITS the calculation of hub and authority scores is query dependent; here, the so-called neighbourhood graph includes not only the set of top-ranked pages for the query, but it also includes the set of pages that either point to or are pointed to by these pages"}
{"pdf_id": "0711.3128", "content": "We use the idea behind PageRank and HITS in our system; how ever, instead of counting every possible link referring to an entitypage in the collection (as with PageRank), or building a neigh bourhood graph (as with HITS), we only consider pages that are pointed to by a selected number of top-ranked pages for the query"}
{"pdf_id": "0711.3128", "content": "3. INEX ENTITY RANKING TRACK The INEX Entity ranking track was proposed as a new track in 2006, but will only start in 2007. It will use the Wikipedia XML document collection (described in the next section) that has been used by various INEX tracks in 2006 [19]. Two tasks are planned for the INEX Entity ranking track in 2007 [12]:"}
{"pdf_id": "0711.3128", "content": "Figure 1 shows an example INEX entity ranking topic; the titlefield contains the query terms, the description provides a natu ral language summary of the information need, and the narrative provides an explanation of what makes an entity answer relevant. In addition, the entities field provides a few of the expected entity answers for the topic (task 2), while the categories field provides the category of the expected entity answers (task 1)."}
{"pdf_id": "0711.3128", "content": "4. THE INEX WIKIPEDIA CORPUS Wikipedia is a well known web-based, multilingual, free content encyclopedia written collaboratively by contributors from around the world. As it is fast growing and evolving it is not possible to use the actual online Wikipedia for experiments. Denoyer and Gallinari [13] have developed an XML-based corpus founded on a snapshot of the Wikipedia, which has been used by various INEX tracks in 2006. It differs from the real Wikipedia in some respects (size, document format, category tables), but it is a very realistic approximation. Specifically, the INEX Wikipedia XML documentcorpus retains the main characteristics of the online version, al though they have been implemented through XML tags instead of"}
{"pdf_id": "0711.3128", "content": "4.1 Entities in Wikipedia In Wikipedia, an entity is generally associated with an article (a Wikipedia page) describing this entity. Nearly everything can be seen as an entity with an associated page, including countries, famous people, organisations, places to visit, and so forth. The entities have a name (the name of the corresponding page) and a unique ID in the collection. When mentioning such an entity in a new Wikipedia article, authors are encouraged to link at least the first occurrence of the entity name to the page describing this entity. This is an important feature as it allows to easily locate potential entities, which is a major issue in entity extraction from plain text. Consider the following extract from the Euro page."}
{"pdf_id": "0711.3128", "content": "All the underlined words (hypertext links that are usually highlighted in another colour by the browser) can be seen as occur rences of entities that are each linked to their corresponding pages.In this extract, there are 18 entity references of which 15 are coun try names; these countries are all \"European Union member states\", which brings us to the notion of category in Wikipedia."}
{"pdf_id": "0711.3128", "content": "4.2 Categories in Wikipedia Wikipedia also offers categories that authors can associate with Wikipedia pages. New categories can also be created by authors, although they have to follow Wikipedia recommendations in bothcreating new categories and associating them with pages. For ex ample, the Spain page is associated with the following categories:\"Spain\", \"European Union member states\", \"Spanish-speaking countries\", \"Constitutional monarchies\" (and some other Wikipedia ad ministrative categories). There are 113,483 categories in the INEXWikipedia XML collection, which are organised in a graph of cate gories. Each page can be associated with many categories (2.28 as"}
{"pdf_id": "0711.3128", "content": "• a category may have many sub-categories and parent cate gories;• some categories have many associated pages (i.e. large ex tension), while others have smaller extension; • a page that belongs to a given category extension generally does not belong to its ancestors' extension;• the sub-category relation is not always a subsumption rela tionship; and • there are cycles in the category graph."}
{"pdf_id": "0711.3128", "content": "• answers the query (or a query extended with the examples), • is associated with a category close to the categories of the entity examples (we use a similarity function between the categories of a page and the categories of the examples),• is pointed to by a page answering the query (this is an adap tation of the HITS [15] algorithm to the problem of entity ranking; we refer to it as a linkrank algorithm), and • is pointed to by contexts with many occurrences of the entity examples. We currently use the full page as the context when calculating the scores in our linkrank algorithm. Smaller contexts such as paragraphs, lists, or tables have been used successfully by others [18]."}
{"pdf_id": "0711.3128", "content": "We have built a system based on the above principles, where candidate pages are ranked by combining three different scores: alinkrank score, a category score, and the initial search engine sim ilarity score. We use Zettair,2 a full-text search engine developed by RMIT University, which returns pages ranked by their similarity score to the query. We use the Okapi BM25 similarity measure as it was effective on the INEX Wikipedia collection [2].Our system involves several modules for processing a query, submitting it to the search engine, applying our entity ranking algo rithms, and finally returning a ranked list of entities, including:"}
{"pdf_id": "0711.3128", "content": "The overall process for entity ranking is shown in Figure 2. Thearchitecture provides a general framework for evaluating entity rank ing which allows for replacing some modules by more advancedmodules, or by providing a more efficient implementation of a mod ule. It also uses an evaluation module (not shown in the figure) toassist in tuning the system by varying the parameters and to glob ally evaluate the entity ranking approach."}
{"pdf_id": "0711.3128", "content": "We have implemented a very basic linkrank function that, for a target entity page t, takes into account the Zettair score of the referring page z(pr), the number of distinct entity examples in the referring page #ent(pr), and the number of reference links to the target page #links(pr, t):"}
{"pdf_id": "0711.3128", "content": "where rel(i) = 1 if the ith article in the ranked list was judged as a relevant entity, 0 otherwise. Average precision is calculated as the average of P[r] for each relevant entity retrieved (that is at natural recall levels); if a system does not retrieve a particular relevant entity, then the precision for that entity is assumed to be zero. MAP is the mean value of the average precisions over all the topics in the training (or test) data set. We also report on several alternative measures: mean of P[1], P[5], P[10] (mean precision at top 1, 5 or 10 entities returned), mean R-precision (R-precision for a topic is the P[R], where R is the number of entities that have been judged relevant for the topic)."}
{"pdf_id": "0711.3235", "content": "We consider how an agent should update her uncertainty when it is represented by a set P of probability distributions and the agent observes that a random variable X takes onvalue x, given that the agent makes decisions using the minimax criterion, perhaps the best studied and most commonly-used criterion in the literature"}
{"pdf_id": "0711.3235", "content": "In the second game, the bookie gets to choose the distribution after the value of X is observed. Again, in this game, the Nash equilibrium leads to the use of minimax, but now conditioning is the right thing to do. If P is a singleton, the two games coincide (since there is only one choice the bookie can make, and the agent knows what it is). Not surprisingly, conditioning is the appropriate thing to do in this case. The moral of this analysis is that, when uncertainty is characterized by a set of distributions, if the agent is making decision using the minimax criterion, then the right decision depends on the game being played. The agent must consider if she is trying to protect"}
{"pdf_id": "0711.3235", "content": "Such loss functions arise quite naturally. For example, in a medical setting, we can take Y to consist of the possible diseases and X to consist of symptoms. The set A consists of possible courses of treatment that a doctor can choose. The doctor's loss function depends only on the patient's disease and the course of treatment, not on the symptoms. But, in general, the doctor's choice of treatment depends on the symptoms observed."}
{"pdf_id": "0711.3235", "content": "an adversary gets to choose a distribution from the set P.3 But this does not completely specify the game. We must also specify when the adversary makes the choice. We consider two times that the adversary can choose: the first is before the agents observes the value of X , and the second is after. We formalize this as two different games, where we take the \"adversary\" to be a bookie. We call the first game the P-game. It is defined as follows:"}
{"pdf_id": "0711.3235", "content": "This is a zero-sum game; the agent's loss is the bookie's gain. In this game, the agent's strategy is a decision rule, that is, a function that gives a distribution over actions for each observed value of X. The bookie's strategy is a distribution over distributions in P. We now consider a second interpretation of P, characterized by a different game that gives the bookie more power. Rather than choosing the distribution before observing the value of X, the bookie gets to choose the distribution after observing the value. We call this the P-X-game."}
{"pdf_id": "0711.3419", "content": "3.1. Translating Facts  SWORIER uses a syntax different from that typically found in previous work. For  example, Volz et al. (2003) would produce the translation of Table 2d, instead of the translation  in Table 2a. But we note that the syntax used by Volz et al. (2003) cannot represent \"every class  that smith is a member of\" with X(smith), because most Prolog implementations disallow  predicate variables. In contrast, by making the class names and property names be arguments  instead of predicates, SWORIER has the flexibility to generalize on them with, for example,  ismemberof(smith, X).  Table 2. Translations  a. ismemberof(smith, sniper).  haspropertywith(smith, hasCombatIntent, friendlyIntent)."}
{"pdf_id": "0711.3419", "content": "3.2. General Rules  The General Rules are meant to capture the semantics of the primitives in OWL. For  example, the rules in Table 3a enforce the transitivity of subclass. Note that there are two  different predicates: issubclassof and isSubClassOf. One predicate would be  insufficient, because Table 3b has left recursion, resulting in an infinite loop.  Table 3. The Transitive Closure of Subclass  a.  isSubClassOf(C, D) :- issubclassof(C, D).  isSubClassOf(C, E) :- issubclassof(C, D), isSubClassOf(D, E)."}
{"pdf_id": "0711.3419", "content": "2 Any predicates that are not used for input or output are written in an underscore case, such as  is_sub_class_of_but_not_equal_to. Also, for some predicates, there are two sources of  recursion, requiring three cases of the predicate. An example of this is the member relation, for which the  three cases are ismemberof, is_member_of, and isMemberOf."}
{"pdf_id": "0711.3419", "content": "4.3. Complementary and Disjoint Classes  Volz et al. (2003) claimed that \"OWL features the complementOf primitive, which  cannot be implemented in Horn Logics due to the fact, that there may be no negation in the  head...\" With the introduction of the logicNot predicate, this is no longer a problem. We can  handle the complementary classes as well as the disjoint classes with the rules in Table 6.  Table 6. Complementary and Disjoint Classes"}
{"pdf_id": "0711.3419", "content": "However, although it may not be possible to solve this problem in general, because we  are limiting our analysis to OWL, there are a finite number of predicates with which that variable  can be instantiated, and this set of predicates does not require any knowledge of the particular  ontologies or rules that are provided by the developer"}
{"pdf_id": "0711.3419", "content": "4.5.Enumerated Classes  \"The owl:oneOf primitive can be partially supported.\" (Volz et al, 2003) This  primitive, which corresponds to our Prolog predicate, isset, defines a class, C, extensionally by  providing a set of all and only the individuals in the class, a0, ..., an. For example, Table 8a  declares  that  there  are  exactly  three  members  of  the  class  combatIntent:  friendlyIntent, hostileIntent, and unknownIntent.  Table 8. Enumerated Class  a.  isset(combatIntent, [friendlyIntent, hostileIntent, unknownIntent])."}
{"pdf_id": "0711.3419", "content": "4.8. Cardinality In OWL, there are three cardinality primitives: (1) minCardinality, (2) max Cardinality, and (3) cardinality. Each of these primitives takes three arguments: a  class, a property, and a number. The primitives' meanings are that each individual in the given  class participates in the given property with (1) at least, (2) at most, or (3) exactly the given  number of unique individuals.  Table 11. Cardinality Rules"}
{"pdf_id": "0711.3419", "content": "We propose changing the subclass transitive closure rules (Table 3a) into the rules in  Table 13b. The idea is to stop the cycle when it reaches the beginning again, which occurs when  the two parameters of isSubClassOf are equal. For this purpose, we create a new predicate  is_sub_class_of_but_not_equal_to that includes all of the subclass relations, except  for the reflexive ones. (The first rule catches them.) Note that we use the technique discussed in  Section 4.9, by including isclass predicates to insure that the variables are bound before  running any not tests on them.  Table 13. Cyclic Hierarchies"}
{"pdf_id": "0711.3419", "content": "4.11. Anonymous Classes  OWL can define classes called anonymous classes without actually naming them. Table  14a has an example of an anonymous class, and Table 14b has our suggestion of how to translate  it. An anonymous class, unnamedClass(hasCombatIntent, friendly-Intent), is  generated like anonymous individuals that were presented in Section 4.7.  Table 14. Anonymous Classes and Properties"}
{"pdf_id": "0711.3419", "content": "The time efficiency that is required depends on the application. For our military task,  once a mission begins, the system's responses must be very fast. If it takes more than a few  seconds to answer a query at run time, the system is effectively useless. However, before the  mission begins, more time is generally available for knowledge compilation. Still, this offline  processing would usually need to be done in hours, not days."}
{"pdf_id": "0711.3419", "content": "6.1. Extensionalization  In order to make the system tractable at run time, we implemented an offline technique to  speed up the program. We modified SWORIER to extensionalize all of the facts that can be  derived from the input (that a user might want to query on), converting the program from an  intensional form to an extensional form. Figure 5 shows the modified system design."}
{"pdf_id": "0711.3419", "content": "This preprocessing technique enabled the system to work much faster, as shown in Table  15b. However, it still required 25.2 minutes to incorporate the same two dynamic changes as in  the previous test, and to answer the two queries took 58 minutes. This is still unacceptably slow.  In addition, the offline extensionalization process caused the AMZI Prolog application to crash,  as shown in Table 17a. We presume that the computer ran out of memory.  Table 17. Extensionalization Time (offline)  Avoiding Reanalysis Code Minimization Extensionalization"}
{"pdf_id": "0711.3419", "content": "6.2. Avoiding Reanalysis  In the process of extensionalizing the code, it was very common to test a term several  times with the same arguments. This unnecessary processing can be very slow. For example,  given the code in Table 18, the system must test isSubClassOf(convoy,  theaterobject) at least twice: Once when searching for all of the true isSubClassOf  terms, and again when trying to prove isMemberOf(convoy1, theaterobject).  Table 18. Reevaluating a Term  ismemberof(convoy1, convoy).  issubclassof(convoy, militaryunit).  issubclassof(militaryunit, theaterobject).  isSubClassOf(C, D) :- issubclassof(C, D).  isSubClassOf(C, E) :- issubclassof(C, D), isSubClassOf(D, E).  isMemberOf(I, C) :- ismemberof(I, C).  isMemberOf(I, D) :- isSubClassOf(C, D), isMemberOf(I, C)."}
{"pdf_id": "0711.3419", "content": "The proof of isSubClassOf(convoy, theaterobject) takes five steps.3 In  general, a very slow test may be run several times. To avoid the reevaluation of a term, each time  3  1. isSubClassOf(convoy, theaterobject) :-  issubclassof(convoy, theaterobject). (FAILS)  2. isSubClassOf(convoy, theaterobject) :-  issubclassof(convoy, D),  isSubClassOf(D, theaterobject).  3. issubclassof(convoy, militaryunit).  4. isSubClassOf(militaryunit, theaterobject) :-"}
{"pdf_id": "0711.3419", "content": "an isSubClassOf term is tested, that term is asserted as a success or failure. Then, the next  time the term needs to be tested, the answer is found in the new assertion, so it is not necessary to  run the full test again.  Table 19. The Code Minimization Algorithm"}
{"pdf_id": "0711.3419", "content": "Efficiency problems have been addressed through 1) extensionalization, which is a  tabling method that converts a set of rules and facts into a set of facts, 2) avoiding reanalysis,  which saves results the first time they are determined to avoid running the same costly evaluation  again, and 3) code minimization, which deletes rules that are unnecessary, for both offline and  online processing"}
{"pdf_id": "0711.3419", "content": "Rector, Alan, Drummond, Nick, Horridge, Matthew, Rogers, Jeremy, Knublauch, Holger,  Stevens, Robert, Wang, Hai & Woe, Chris (2004), \"OWL Pizzas: Practical Experience of  Teaching OWL-DL: Common Errors & Common Patterns\", 14th International Conference  on Knowledge Engineering and Knowledge Management (EKAW), Whittlebury Hall, UK  [Online at http://www"}
{"pdf_id": "0711.3419", "content": "Samuel, Ken, Obrst, Leo, Stoutenburg, Suzette, Fox, Karen, Franklin, Paul, Johnson, Adrian,  Laskey, Ken, Nichols, Deborah, Lopez, Steve & Peterson, Jason (2006), \"Applying Prolog to  Semantic Web Ontologies & Rules: Moving Toward Description Logic Programs\",  Proceedings of the International Workshop on Applications of Logic Programming in the  Semantic Web and Semantic Web Services, International Conference on Logic Programming,  August 16, 2006"}
{"pdf_id": "0711.3419", "content": "Berkeley, Technical report no. UCB/CSD 90/600, U. C. Berkeley Computer Science  Division. Also: Fast Logic Program Execution, Intellect Books.  Van Roy, Peter & Despain, Alvin M. (1992), \"High-Performance Logic Programming with the  Aquarius Prolog Compiler\", IEEE Computer, 25(1):54-68.  Van Roy, Peter (1994), \"The Wonder Years of Sequential Prolog Implementation\", Journal of Logic Programming, 19:385-441. [Online at ftp://ftp.digital.com/pub/DEC/PRL/research reports/PRL-RR-36.ps.Z, accessed 12 Sep 2007].  Raphael Volz (2004), Web Ontology Reasoning with Logic Databases, PhD thesis, AIFB,  University of Karlsruhe. Volz, Raphael, Decker, Stefan & Oberle, Daniel (2003), \"Bubo - Implementing OWL in Rule Based Systems\", http://www.daml.org/listarchive/joint-committee/att-1254/01-bubo.pdf  [Accessed 12 Sep 2007]."}
{"pdf_id": "0711.3964", "content": "Let us remark that beside the refinement process of the reputations and the outlier detection given by our procedure, other applications can take advantage of these data. For example, [2] want to remove spammers to improve collaborativefiltering. Similarly in [4], they propose a framework to take into account the dif ferent qualities of ratings for collaborative filtering. Hence they weight each rating according to its reliability, these weights can be those obtained by the iterative filtering we described."}
{"pdf_id": "0711.3964", "content": "In the sequel, we first explain in section 2 how the reputation vector for the objects and the weights for the evaluation are built. Moreover, we develop the algorithm Reputation that calculates these values, and we explain its interpretation and its properties of convergence. Then in section 3, our experiments test the robustness of our method against attackers and show several iterations on graphics. Finally in section 4, we point out possible extensions and experiments for our method."}
{"pdf_id": "0711.3964", "content": "Our experiment concerns a data set2 of 100,000 evaluations given by 943 users on 1682 movies and raging from 1 to 5. Each user has rated at least 20 movies. In order to simulate the robustness of the algorithm Reputation, two types of behavior are analyzed in the sequel: first, raters that give random evaluations, and second, spammers that try to improve the reputation of their preferred item."}
{"pdf_id": "0711.3964", "content": "Figure 3 illustrates this perturbation due to the addition of random raters. The reputations are better preserved when using Reputation. It turns out that thereputations given by Reputation take less into account the random users. More over, one iteration of the algorithm gives poor information to trust the raters, it is indeed useful to wait until convergence, as seen in Figure 4."}
{"pdf_id": "0711.4142", "content": "The Data Sets  We evaluate two online tagging communities: CiteULike  and Connotea. They are designed as personal content  management tools with collaborative features such as  tagging and comments.  The data sets consist of all tagging activity since the  creation of each community, more than two years of user  activity for each. We obtained the CiteULike dataset  directly from www.CiteULike.org website which provides  logs of past tagging activity. For Connotea, we built a  crawler that leverage Connotea's API to collect all data  available since December 2004.  CiteULike  Connotea"}
{"pdf_id": "0711.4142", "content": "Table 1: The data sets evaluated.  Table 1 presents the characteristics of the data sets  analyzed. It is worth highlighting that we only have access  to traces of explicit content use (i.e., tag assignments and  item postings). An entry in the activity trace means that a  user assigned a particular tag to one item, at a particular  timestamp. The analysis of implicit content usage traces  (i.e., browse and download activity) is left as future work."}
{"pdf_id": "0711.4142", "content": "Assessing Collaboration Levels  We define two metrics to evaluate the level of  collaboration in a community: content reuse and shared  user interest.  • Content reuse refers to the percentage of activity in a  community that involves existing rather than new  content. In a highly dynamic community, where users  often add content, harnessing collective action is"}
{"pdf_id": "0711.4142", "content": "Table 2: A summary of daily item and tag reuse, and user  activity in absolute values followed by percetages between  brackets.  In summary we find that, both communities present the  following major characteristics: (1) consistently low levels  of item reuse, (2) high levels of tag reuse, and (3) most"}
{"pdf_id": "0711.4142", "content": "level of tag reuse results in users that are tagging  overlapping sets of items and/or use overlapping sets of  tags.  To this end, this section formalizes the notion of shared  interest between a pair of users and presents an evaluation  of the level of shared interest in CiteULike (we are still  analyzing Connotea dataset). In particular, the analysis of  the level of shared interest consists of two parts: first, in  this section, the characteristics of the pair-wise interest  sharing relation among users; the next section the structure  of interest sharing at the community level as displayed by  the interest-sharing graph."}
{"pdf_id": "0711.4142", "content": "Discussion  So far, this paper introduced two metrics (content reuse  and shared interest level) to estimate the level of user  collaboration in online tagging communities and presented  evidence to support our claim that the level of  collaboration in tagging communities is lower than  generally assumed in the literature"}
{"pdf_id": "0711.4142", "content": "This is  a view long held by experts (Grudin 1994) (Golder and  Huberman 2007) (Iverson 2007), and our study offers  quantitative data to support this view: Collaboration does  not always naturally emerge, and the current popularity of  existing collaborative tagging sites is a result of their  ability to cater to the demands of individual users rather  than a direct consequence of their ability to aggregate  social knowledge"}
{"pdf_id": "0711.4142", "content": "large share of users with non-overlapping interests is likely  to limit the efficiency of such algorithms, since there is no  information that can be extracted to infer the reputation of  these users based on the link structure. Additionally, the  low level of content reuse implies that, for a large number  of items, no reputation data can be inferred as they are  recently added. A potential solution that may be worth  investigating is to augment the reputation extraction  algorithms based on explicit content sharing combined  with implicit usage patterns such as browsing histories."}
{"pdf_id": "0711.4388", "content": "Abstract— The main contribution of this paper is to design anInformation Retrieval (IR) technique based on Algorithmic Information Theory (using the Normalized Compression Distance NCD), statistical techniques (outliers), and novel organization of data base structure. The paper shows how they can be integrated to retrieve information from generic databases using long (text-based) queries. Two important problems are analyzed in the paper. On the one hand, how to detect \"false positives\" when the distance among the documents is very low and there is actual similarity. On the other hand, we propose a way to structure a document database which similarities distance estimation depends on the length of the selected text. Finally, the experimental evaluations that have been carried out to study previous problems are shown."}
{"pdf_id": "0711.4388", "content": "The Kolmogorov Complexity of a text can be used to char acterize the minimal amount of information needed to codify that particular text, regardless of any probability consideration. The Kolmogorov Complexity K(x) of a string x, which is the size of the shortest program able to output x in a universal Turing machine, is an incomputable problem too (due to the Halting problem), the most usual (upper bound) estimation is based on data compression: the size of a compressed version of a document x, which we will denote by C(x) may be used as an estimation of K(x)."}
{"pdf_id": "0711.4388", "content": "The variable length of the user query will be handle as our previous files, so any user query will be processed into elemental units from 1Kb to NKb, if the size of the user query is greater than N KB, it will be processed into NKb blocks (as any other file)"}
{"pdf_id": "0711.4388", "content": "If we consider a file like a sequence of characters (i.e. string) we can divide it into blocks of approximately 1024 bytes, 2048 bytes, etc, until the complete division of the file. These blocks build the elemental units of a particular file, that finally are indexed and stored in the corresponding database. However, the results, in the retrieval process, of this structure organization could not work so well at it would be expected. The problem is newly related with the base technique used(compression) to look for a particular document. Any compres sor is an algorithm designed to detect several similarities, or"}
{"pdf_id": "0711.4388", "content": "This search engine uses a set of graphical inter faces to allow: preprocessing a set of document repositories and store them into our database organization; deploy these databases in the search engine; calculate the NCD for each stored document; show the set of documents found from a particular user query (with the NCD distance for each block); show the documents found, and highlight those blocks (inside a particular document) with the best similarity"}
{"pdf_id": "0711.4388", "content": "Figure 4 depicts a representative query result of the above described kind of experiments. We also depict the ROC curve of a random binary classifier for the sake of comparison. Results above the random curve represent positive evidence of information retrieval, and the faster the curve separates from the random curve, the better the search engine performs.In a second step we remove the abstract from every docu ment of the database, and we repeat the previous queries. The true positive and false positive consideration is unchanged. A representative result is depicted in Figure 5."}
{"pdf_id": "0711.4388", "content": "In the final step, we choose 20 documents which scientific classification subject coincides with one or more subjects of the documents in the database. This is done using the SpringerLink search engine (www.springerlink.com). We then select 5 fragments from each document, and use each of them as a query to the database. True positive results are those documents whose subject coincides with the query subject, and false positive are those which do not. A representative result of single query is shown in figure 6."}
{"pdf_id": "0712.0131", "content": "I describe an approach to similarity motivated by Bayesian methods. This yields a similarity function that is learnable using a standard Bayesian methods. The relationship of the approach to variable kernel and variable metric methods isdiscussed. The approach is related to variable kernel Ex perimental results on character recognition and 3D object recognition are presented."}
{"pdf_id": "0712.0131", "content": ", [15, 8, 10, 18, 3]) have proposed using similarityfunctions other than the Euclidean distance in nearest neigh bor classification, and give on-line or off-line procedures forcomputing such similarity functions1 Another recent devel opment is the increased demand in applications for soundways of determining the \"similarity\" of two objects in areas like 3D visual object recognition, biometric identifica tion, case based reasoning, and information retrieval (e"}
{"pdf_id": "0712.0131", "content": "1They are often referred to as \"adaptive similarity metrics\", but they do not satisfy the metric axioms and to avoid confusion, we refer to them here as \"similarity functions\". 2 Without loss of generality, we consider minimization of the expected loss under a zero-one loss function only in this paper."}
{"pdf_id": "0712.0131", "content": "us a prescription for constructing a nearest neighbor classifier for many kinds of classification problems that is guar anteed to achieve the Bayes optimal error rate.Of course, not all classification problems have unam biguous exemplars; an analysis of such cases goes beyond the scope of this paper, and it is probably not necessary for real-world applications. For actual applications, we can use methods of machine learning for estimating the statistical similarity function and then pick a set of exemplars thatempirically minimizes misclassification rate in a way anal ogous to other nearest neighbor methods."}
{"pdf_id": "0712.0131", "content": "were selected from a separate test set and classified like the training vectors (however, misclassified feature vectors were not added during the set of prototypes). As a control, the same training and testing process was carried out using Euclidean distance. The results of these experiments are shown in Table 1. They show a 2.7-fold improvement of using statistical similarity over Euclidean distance."}
{"pdf_id": "0712.0131", "content": "In a second set of experiments, the statistical similarityfunction was trained not on randomly selected pairs of fea ture vectors, but only on pairs of feature vectors from thesame writer. This means that the statistical similarity func tion characterizes the variability for individual writers. For testing, feature vectors from 200 writers not in the training set were used. For each writer, the first instance of eachcharacter was used as a prototype, resulting in 10 prototypes per writer. These prototypes were then used to clas sify the remaining samples from the same writer. These results are shown in Table 2. The results show a 4.4-fold improvement of statistical similarity over Euclidean nearest neighbor methods."}
{"pdf_id": "0712.0131", "content": "Because of the projection involved in the imaging trans form, there is potentially an infinity of models that couldhave given rise to a given image B. For example, all mod els that differ only by their placement of vertices along the optical axis after rigid body transformation and the addition of noise are indistinguishable from their images."}
{"pdf_id": "0712.0131", "content": "Table 3: Experiments evaluating MLP-based statisticalsimilarity relative to view based recognition using 2D similarity. Error rates (in percent) achieved by MLP-based sta tistical view similarity models relative to error rates based on Euclidean distance (equivalent to 2D similarity in the case of location features).In all experiments, the training set consisted of 200 clips consisting each of five ver tices. The test set consisted of 10000 previously unseen clips drawn from the same distribution. The structure of the network is given as \"(n:m:r)\", where n is the number of inputs, m the number of hidden units, and r the number of outputs."}
{"pdf_id": "0712.0131", "content": "A second set of experiments compared the performance of statistical similarity with the performance of Euclidean nearest methods on a 3D generalization problem in visual object recognition. This example is interesting because it lacks a class structure; as shown in [2], it is impossible to partition a set of 3D models into non-overlapping sets of"}
{"pdf_id": "0712.0136", "content": "They also do not easily explain how an observer can transferhis skill at recognizing existing objects to generaliz ing from single or multiple views of novel objects; toexplain such transfer, a variety of additional meth ods have been explored in the literature, includingthe use of object classes or categories, the acquisi tion and use of object parts, or the adaptation and sharing of features or feature hierarchies"}
{"pdf_id": "0712.0136", "content": "(and we will do so for two such methods), the for mulation in terms of view generalization functionsmakes it easy to apply any of a wide variety of stan dard statistical models and classifiers to the problem of generalization to novel objects. In this paper, I will first express Bayes-optimal 3D object recognition in terms of training and target views and prior distributions on object models and viewpoints. Then, I will describe the statistical basis of learning view generalization functions. Finally, I will demonstrate, both on the standard \"paperclip\" model and on the COIL-100 database, that learning view generalization functions is feasible."}
{"pdf_id": "0712.0136", "content": "Therefore, applying Equation 4 together with Equation 1 results in Bayes-optimal 3D model-based recog nition from 2D training views. Now that we have derived the Bayes-optimal 3D object recognition, let us look at some approachesthat have been proposed in the literature for solv ing the 3D object recognition problem and how they relate to Bayes optimal recognition."}
{"pdf_id": "0712.0136", "content": "Model Priors. One of the important properties of the view generalization function is that it does notdepend on the specific models the observer has ac quired in his model base. Rather, it depends on the prior distribution of models from which the actual models encountered by the system are drawn."}
{"pdf_id": "0712.0136", "content": "But this means that if we look at log P(Bi|T), it is a blurred version of the training view, with with hij as a spatially varying blurring kernel.Blurring, with or without spatially variable kernels, has been proposed as a means of generalization in computer vision by a number of previous au thors. In a recent result, [2] derives non-uniform blurring for 2D geometric matching problems, the"}
{"pdf_id": "0712.0136", "content": "\"geometric blur\" of an object. The results sketchedin this section make the connection between nonuniform geometric blurring and first order approx imations to the single view generalization function, g(B, T) = P(B|T).This connection lets us determine more precisely how we should compute geometric blurring, what approximations it involves com pared to the Bayes-optimal solution, and how we canimprove those approximations to higher-order statis tical models. Let us note also that there is nothing special about the representation in terms of featuremaps; had we chosen to represent views as collections of feature coordinates, a first order approxima tion would have turned into error distributions on the location of each model feature."}
{"pdf_id": "0712.0136", "content": "Experiments.Let us look now at how view simi larity functions can be learned in an the case of 3D paperclips. As in the previous section, we consider the single view generalization problem and apply it tothe problem of paperclip recognition. During a train ing phase, the experiments used a collection of 200paperclips, generated according to the procedure de scribed in the previous section. The procedure used"}
{"pdf_id": "0712.0136", "content": "These results show a substantial improvement of view-similarity functions over 2D similarity on single view generalization to novel objects. Note that manytraditional recognition methods, like linear combi nations of views or model-based recognition, cannot even be applied to this case because the observer is only given a single training view for each novel object."}
{"pdf_id": "0712.0136", "content": "3Of course, even better performance can be achieved byhardcoding additional prior knowledge about shape and object similarity into the recognition method (e.g., [1]). Achiev ing competitive performance with such methods would eitherrequire encoding additional prior knowledge about shape sim ilarity in the numerical model of the view similarity function, or simply using a much larger training set to allow the observer to learn those regularities directly."}
{"pdf_id": "0712.0137", "content": "evidence combination schemes, while others allow for the learning or adaptation of either or both. One of the most restrictive forms of view-based3D object recognition requires that, in order to per form recognition, each stored view is compared with atarget view using only a fixed, non-invariant similarity measure. After performing those similarity mea surements, the observer is then permitted to perform some kind of \"combination of evidence\" on them. Intheir papers on human 3D generalization [6][5] re fer to such an observer as an observer using a strong view-approximation method:"}
{"pdf_id": "0712.0137", "content": "\"For example, assume that an object is rep resented by two independent views. The task is to decide whether a novel view belongsto the object. The strong version of view approximation maintains that in order to recognize a novel view, a similarity measure is calculated independently between this viewand each of the two stored views [...]. Recog nition is a function of these measurements.The simplest function is the nearest neigh bor scheme, where a match is based on the closest view in memory.A more sophis ticated scheme is the Bayes classifier that combines the evidence over the collection of views optimally.\" [5]"}
{"pdf_id": "0712.0137", "content": "In this paper, I demonstrate that that is not the case: given the correct Bayesian combination of the individualview similarity values, a strongly two-dimensional ob server can achieve the same Bayes-optimal error rateas an observer that can access all the coordinate mea surements of the target and training views and uses explicit 3D models internally"}
{"pdf_id": "0712.0137", "content": "G disappear. Appendix B contains such a similarity measure. The reason for using Euclidean distance in thesederivations is that it is, at the same time, an intu itive similarity measure for similarity of 2D views andthat the proof of Lemma 1 is fairly easy. The rota tional invariance, for example, can be eliminated bychoosing a slightly more complicated similarity func tion S(V, T ) ="}
{"pdf_id": "0712.0137", "content": "Note on Model Acquisition. The reader should recognize that the \"reconstruction\" of coordinatesfrom similarity measurements is a completely sepa rate computation from the acquisition of 3D models from 2D views (e.g., [7]). The reconstruction above is concerned with the recovery of 2k-dimensional vectors from internally computed similarity valuesamong 2k-dimensional vectors. In 3D model acqui sition from 2D views, we attempt to combine views of an object, possibly subject to sensor noise, into aconsistent model. 3D model acquisition could be car"}
{"pdf_id": "0712.0137", "content": "In the previous sections, we have seen that strongly view-based observers can perform Bayes-optimal 3D object recognition. We also showed that strongly view-based observers can perform model acquisition as well as any 3D model-based recognition system.In both cases, the reason was that the set of similar ity measurements S(V, T) is essentially equivalent to complete knowledge of all the training views and the"}
{"pdf_id": "0712.0451", "content": "The generation of meaningless \"words\" matching certain statistical and/or linguistic criteria is frequently needed for experimental purposes in Psycholinguistics. Such stimuli receive the name of pseudowords or nonwords in the Cognitive Neuroscience literature. The process for building nonwords sometimes has to be based on linguistic units such as syllables or morphemes, resulting in a numerical explosion of combinations when the size of the nonwords is increased. In this paper, a reactive tabu search scheme is proposed to generate nonwords of variable size. The approach builds pseudowords by using a modified Metaheuristic algorithm based on a local search procedure enhanced by a feedback-based scheme. Experimental results show that the new algorithm is a practical and effective tool for nonword generation."}
{"pdf_id": "0712.0451", "content": "In the last few years there has been a great deal of cognitive neuroscience research into how language is processed, acquired, comprehended and produced by the human brain [1][2]. Two major tools in this research area  are  computational  models  and  laboratory experiments in which language features are manipulated. Computational models try to simulate how language information is processed, while psycholinguistics experiments record behavioral responses such as reaction  times,  or  the  electrophysiological  or haemodynamic responses of human subjects to specific linguistic stimuli. Thus, the experiments test the predictions of the computational models with the aim of understanding the representation and processing of language components in the human brain."}
{"pdf_id": "0712.0451", "content": "In order to empirically test hypotheses and models, cognitive neuroscience researchers have frequently faced the problem of generating appropriate linguistic stimuli for their experiments. This involves, in some cases, searching for words with well-defined statistical and/or linguistic properties (e.g., words within specific ranges of printed frequency, syllable frequency, number of neighbors and so forth), and/or nonwords (i.e, stimuli that resemble a word but are not part of the words of a particular language; for instance, \"pint\" is an English word, but \"pont\" is not) also with special properties. It is"}
{"pdf_id": "0712.0451", "content": "The rest of this paper is organized as follows: In the next section, the problem we address is presented. To emphasize the characteristics of the problem a brief analysis of complexity is made, reviewing some aspects of combinatorial optimization. Section 3 is a formal description of the problem and the approach proposed: The adaptation of a Reactive Tabu Search scheme to a combinatorial search task. Section 4 focusses on the application of the proposed scheme to a specific case study. The most important parts of the algorithm are sketched in this section. The experimental results are covered in section 5, with some implementation and performance details. Finally, section 6 provides a summary of the present study and some concluding remarks."}
{"pdf_id": "0712.0451", "content": "approach could be prohibitive in many cases. A promising way to solve this problem is to adapt a combinatorial optimization algorithm to a merely combinatorial search task. Metaheuristic algorithms offer a good alternative in this line. Here, a Reactive Tabu Search (henceforth RS) scheme is considered in the following discussion."}
{"pdf_id": "0712.0451", "content": "Limited cycles and confinements in limited portions of the search space are discouraged by the reactive mechanisms defined by the algorithm that modify the discrete dynamical system defined by the trajectory. The reaction is based on the past history of the search and it causes possible changes of T(t) or the activation of a"}
{"pdf_id": "0712.0451", "content": "When the reaction that modifies T(t) is not sufficient to guarantee that the trajectory is not confined in a limited portion of the search space, the search dynamics enter a phase of random walk specified by the function diversify_search. Specifically, when this phase begins the memory structure is cleaned, although Rave and T(t)"}
{"pdf_id": "0712.0451", "content": "A word w2 is said to be an orthographic neighbor of word w1 if and only if w2 can be obtained simply by changing one of the letters of w2. For instance, the word \"cable\" is an orthographic neighbor of \"table\". Similarly, \"used\" is an orthographic neighbor of \"uses\". Thus, given a generic word the process of computing its orthographic neighbors consists in the generation of all the possible permutations, using the target language alphabet, changing only one character at a time of the"}
{"pdf_id": "0712.0451", "content": "The process of neighborhood generation can be stated as follows. From the current configuration point v an elementary move is performed by replacing one of the components of vector v, that is, v(i) by a value obtained from a randomly generated set of points which are bounded by the cardinality of the word unit employed. This procedure is repeated in turn for each of the vector dimensions and using all the values contained in the random set."}
{"pdf_id": "0712.0451", "content": "fact, the simplest form of an iterated local search scheme [12] [13] . We adapted the above-mentioned algorithm to account for the combinatorial search task., denoting the modified algorithm as Combinatorial Iterated Local Search (CILS hereafter). In particular, it is based on the repeated generation of random configurations that are used as starting points for a local search algorithm. The pseudocode of the algorithm is shown in figure 5."}
{"pdf_id": "0712.0451", "content": "The local search procedure simply generates a neighborhood of the current solution v by using the algorithm presented in the previous subsection. Thus, a more reliable measure of quality can be obtained when comparing both algorithms. Afterwards, the points of the neighborhood are evaluated using the functions described in section 4. The set of points that accomplish the optimality criterion (C1 = 1) are inserted into the data structure D."}
{"pdf_id": "0712.0451", "content": "The algorithms were written in JAVA and compiled and tested using the JDK1.3.1. A major advantage of using an object-oriented language like JAVA is the flexibility it provides for re-use existing code and rapid prototyping capabilities. In this sense, nonword generation, as we have stated before, is subject to very difficult and changing criteria that depend on the particularities of the experiment or the application context. Therefore, the fact of using an object-oriented language permits the templatization of the nonword generation criterion by simply redefining certain steps of the algorithm (eg: simply by subclassing and re-implementation of a class method) without changing the algorithm structure."}
{"pdf_id": "0712.0451", "content": "The simulation results show that the CRS scheme outperforms CILS in all of the problem instances, although this is accomplished through a slight increase in the computation time. In addition, the running times for the orthographic neighbors problem (table 3) are one order of magnitude bigger than for the bigrams frequency problem due to the higher computational load introduced by this task. In general, the computational cost per iteration is greater in the CRS scheme than in CILS, nevertheless, this is not always the case as it depends on how often the algorithm enters into a diversification phase and also on its length."}
{"pdf_id": "0712.0451", "content": "In this paper we have investigated the application of a meta-heuristic algorithm suitable for combinatorial optimization problems in a merely combinatorial search problem. Throughout this paper we have referred to the concept of combinatorial search as the problem of finding the highest amount of solutions matching a certain 0-1 criterion over a vast combinatorial space."}
{"pdf_id": "0712.0451", "content": "We have presented a formal description of the problem in terms of its application context. Specifically, within the Cognitive Neuroscience research field. We have also shown how to adapt the Reactive Search framework of algorithms to address a combinatorial search problem. In addition to the changes shown for the basic RS functions, several successive steps must also be performed in this regard:"}
{"pdf_id": "0712.0451", "content": "The experimental results clearly show the algorithm is in fact able to generate nonwords of any size and subject to any criteria, since the proposed encoding scheme is universal. The abilities of this model suggest the applicability of the proposed methodology to other domains. Although further research must be carried out, one of the important conclusions of this work is that the reaction and feedback mechanisms introduced by this model offers a good alternative to classic random generation techniques that cannot cope adequately with a combinatorial search. Furthermore, they cannot offer general solutions to combinatorial search problems. Another interesting feature of the algorithm is its robustness against problem dimensionality."}
{"pdf_id": "0712.0499", "content": "• We experimentally evaluate these query rewriting techniques, using an actual click graph from Yahoo!, and a set of queries extracted from Yahoo! logs. We evaluate the resulting rewrites using several metrics. One of the comparisons we perform involves manual evaluation of query-rewrite pairs by members of Yahoo!'s Editorial Evaluation Team. Our results show that we can significantly increase the number of useful rewrites over those produced by SimRank and by another basic technique."}
{"pdf_id": "0712.0499", "content": "Simrank [5] is a method for computing object similarities, applicable in any domain with object-to-object relationships, that measures similarity of the structural context in which objects occur, based on their relationships with other objects. Specifically, in the case where there are two types of objects, bipartite Simrank is an iterative"}
{"pdf_id": "0712.0499", "content": "Random walks behind Simrank The intuition behind the similarity scores that Simrank defines is based on a \"random surfers\" model. According to this, a Simrank score sim(a, b) measures how soon two random surfers are expected to meet at the same node if they started at nodes a, b and randomly walked the graph. The transition probabilities of this random walk are uniform, which means that (assuming C1 = C2 = 1) if a has n out-neighbors, with the same probability 1/n the random surfer will move to one of these out-neighbors."}
{"pdf_id": "0712.0499", "content": "Let us look at the similarity scores that Simrank computes for the pairs \"camera\" - \"digital camera\" and \"pc\" - \"camera\" from the graphs of Figure 4. Table 3 tabulates these scores for the first 7 iterations. As we can see sim(\"camera\", \"digital camera\") is always less than sim(\"pc\", \"camera\") although we observe that sim(\"camera\", \"digital camera\") increases as we include more iterations. In fact, we can prove that sim(\"camera\", \"digital camera\") becomes eventually equal to sim(\"pc\", \"camera\") as we include more iterations. We can actually prove the following two Theorems for the similarity scores that Simrank computes in complete bipartite graphs (refer to Appendix A for the proofs)."}
{"pdf_id": "0712.0499", "content": "The intuition behind choosing such a function is as follows. We want the evidence score evidence(a,b) to be an increasing function of the common neighbors between a and b. In addition we want the evidence scores to get closer to one as the common neighbors increase. Thus, another reasonable choice would be the following:"}
{"pdf_id": "0712.0499", "content": "Weighted Simrank In the previous sections we ignored the information contained in the edges of a click graph and we tried to derive similarity scores for query pairs by just using the click graph's structure. In this section, we focus on weighted click graphs. We explore ways to derive query-query similarity scores that (i) are consistent with the graph's weights and (ii) utilize the edge weights in the computation of similarity scores."}
{"pdf_id": "0712.0499", "content": "The judgment scores are solely based on the evaluator's knowledge, and not on the contents of the click graph. Our second evaluation method addresses the question of whether our methods made the \"right\" decision based on the evidence found in the click graph. The basic idea is to remove certain edges from the click graph and to see if using the remaining data our schemes can still make useful inferences related to the missing data."}
{"pdf_id": "0712.0499", "content": "(i) Precision/recall: We consider two IR tasks. Firstly, we interpret the rewrites with scores 1-2 as relevant queries and the rewrites with scores 3-4 as irrelevant queries. Secondly, we interpret as relevant query rewrites only the ones with score 1 and the rest as irrelevant. Thus, we can define the precision/recall of"}
{"pdf_id": "0712.0499", "content": "10.1 Query Coverage Figure 8 illustrates the percentage of queries from the 120 queries sample that Pearson and Simrank provide rewrites for. Simrank provides rewrites almost for all queries (98%) when Pearson gives rewrites only for the 41% of the queries. This can be considered as expected, since Pearson can only measure similarity between two queries if they share a common ad, whereas Simrank takes into account the whole graph structure and does not require something similar. Also notice, that evidence-based Simrank further improves the coverage to 99%."}
{"pdf_id": "0712.0499", "content": "10.3 Rewriting Depth Figure 11 compares the rewriting depth of Pearson and the variations of Simrank. Note that our two enhanced schemes can provide the full 5 rewrites for over 85% of the queries. As mentioned earlier, the more rewrites we can generate, the more options the back-end will have for finding ads with active bids."}
{"pdf_id": "0712.0499", "content": "10.4 Desirability prediction Figure 12 provides the results of our experiments for identifying the correct order of query rewrites as described in Section 9.3. Simple Simrank and evidence-based Simrank manage to predict successfully the desirable rewrite for 27 out of the 50 queries (54%). Note that both methods do not exploit the graph weights in the similarity computations and rely only on the graph structure. Weighted Simrank predicts correctly the desirable rewrite for 46 queries (92%)."}
{"pdf_id": "0712.0836", "content": "We have employed evolutionary computation techniques developed by Sapin et al 13,14,15,16 for evolving cellular automata which support mobile localizations (gliders). We used an evolutionary algorithm that incorporates aspects of natural selection or survival of the fittest. It maintains a population of structures (usuallyinitially generated at random) that evolves according to rules of selection, recombination, mutation, and survival, referred to as genetic operators. A shared 'envi ronment' is used to determine the fitness or performance of each individual in the"}
{"pdf_id": "0712.0836", "content": "see that, in most cases, development of an automaton from initial random configurations leads to disorderly looking configurations (even if the patch of initial stimu lation was small enough). This is because gliders inhabit such spaces in abundance, they interact one with another, produce more gliders in result of their interaction, and populations of swarming gliders look like quasi-chaotic patterns for naked eyes (Fig. 3)."}
{"pdf_id": "0712.0836", "content": "Fig. 4.Isolines representation for glider likehood matrices. Number of states of reactant A in creases from top left corner to bottom left, number of states of reactant B increases from top left corner to top right one. In each case there is a single elevation. Approximate locations of elevations are F S 00, F A 11, F B 11, and F # 22."}
{"pdf_id": "0712.0836", "content": "A typical scenario of how the system (1) behaves in a well-stirred reactor is shown in Fig. 5. We have confirmed in the computational experiments that the reaction scheme developed represents an oscillatory chemical system, where concentration of substrate is significantly higher than concentrations of reactants A and B. This indeed conforms with the nature of spreading localizations and pulsating behavior"}
{"pdf_id": "0712.0932", "content": "KEY WORDS  Mirroring Neural Network, non-linear dimensionality  reduction, characteristic vector, adalines, classification.  1. Introduction  This paper proposes a pattern recognition algorithm using  a new neural network architecture called Mirroring Neural  Network. This paper uses facial patterns as an example, to  explain mirroring neural network architecture and  illustrate its performance. Facial pattern recognition can  be broadly classified into two techniques viz., manually  specifying the facial features and automatically extracting  the features. This paper deals with the second technique in  which  neural  network  recognizes  face  patterns"}
{"pdf_id": "0712.0932", "content": "If these networks are connected and a framework  or architecture is made such that a pattern is fed as input  to all these networks and this architecture gives output  from the network which successfully mirrors the pattern,  then such an architecture could be a possible data structure  for simulated memory"}
{"pdf_id": "0712.0932", "content": "and 25 adalines in the last layer. The pattern is  reconstructed at the output with its original dimension of  25 units from this signature. The input patterns with 25  dimensions can thus be represented with the 3 code units  of the 3rd hidden layer (least dimensional layer). We have  tried various architectures with varying hidden layer  dimensions. After considerable experimentation, we found  that a network having one hidden layer and an output  layer is a suitable choice for our pattern. The degree of  reduction of the input pattern plays an important role  while  reconstructing  input  pattern  from  reduced"}
{"pdf_id": "0712.0932", "content": "dimension vector and so, the number of units in the least  dimensional hidden layer must be chosen after careful  experimentation. After trying different dimensions of the  hidden layers by trail & error method, and checking the  neural network's performance, we found that 40 units at  the hidden layer gave the most accurate results. We  designed our mirroring neural network with 676 inputs to  40 hidden (code) units and 676 output units (676-40-676).  The inputs to the network were 26X26 grayscale images."}
{"pdf_id": "0712.0932", "content": "biasoj = bias term of the jth node in   the output layer  Wojk = kth weight of jth node in the   output layer  Adalinehk = output of kth node in the   hidden layer  Adalineoj = output of jth node in the   output layer"}
{"pdf_id": "0712.0932", "content": "While training the back propagating Mirroring Neural  Network we have used the usual gradient descent [10] to  minimize the mean squared error between the input and its  reconstruction at the output. The activation function and  variable learning rate parameter [11] reduce out-of-range  values and help in faster convergence of the network. The  learning rate parameter was incremented by 10% at the  hidden layer compared to the output layer. The mirroring  neural  network,  with  learning  rate  rescaling  in"}
{"pdf_id": "0712.0932", "content": "Conclusions and future work  The architecture described in this paper is a simple  approach for object recognition which is applicable to  various image categories like faces, furniture, flowers,  trees, etc and was tested for the same with slight changes  in the network architecture w"}
{"pdf_id": "0712.0932", "content": "References   [1] C. Garcia & M. Delakis, Convolutional face finder: A  neural architecture for fast and robust face detection, IEEE  Trans. Pattern Anal. Mach. Intell., 26(11), Nov. 2004,  1408-1423.  [2] M. -H. Yang, D. Kriegman & N. Ahuja, Detecting  faces in images: A survey, IEEE Trans. Pattern Anal.  Mach. Intell., 24(1), Jan. 2002, 34-58.  [3] M. D. Ganis, C. L. Wilson & J. L. Blue, Neural  Network-based systems for handprint OCR applications,  IEEE Trans. Image Process., 7(8), Aug. 1998, 1097-1112.  [4] Son Lam Phung & Abdesselam Bouzerdoum, A  Pyramidal  Neural  Network  For  Visual  Pattern"}
{"pdf_id": "0712.1097", "content": "As mentioned in the previous section, one of the major drawbacks of the PBO model for MAXSAT is the large number of blocking variables that must be considered. The ability to reduce the number of required blocking variables is expected to improve significantly the ability of SAT/PBO based solvers for tackling instances of MAXSAT. Moreover, any solution to the MAXSAT problem will be unable to satisfy clauses that must be part of an unsatisfiable subformula. Consequently, one approach for reducing the number"}
{"pdf_id": "0712.1097", "content": "A proof of correctness of algorithm msu1 is given in [6]. However, [6] does not ad dress important properties of the algorithm, including the number of blocking variablesthat must be used in the worst case, or the worst-case number of iterations of the algo rithm. This section establishes some of these properties. In what follows, n denotes the number of variables and m denotes the number of clauses."}
{"pdf_id": "0712.1097", "content": "formula. For the AtMost 1 constraint, the BDD-based encoding of a cardinality con straint is linear in n [5]. For the results in Section 6, the most significant performance gains are obtained from using a BDD-based encoding for AtMost 1 constraints, using Tseitin's encoding and Plaisted&Greenbaum's polarity optimizations. One final remark is that Fu&Malik's algorithm will also work if only AtMost 1 constraints are used instead of Equals 1 constraints. This allows saving one (possibly quite large) clause in each iteration of the algorithm."}
{"pdf_id": "0712.1097", "content": "This section proposes a new alternative algorithm for MAXSAT. Compared to the algo rithms described in the previous sections, msu1 and msu2, the new algorithm guarantees that at most 1 blocking variable is associated with each clause. As a result, the worst case number of blocking variables that can be used is m. Moreover, during a first phase, the new algorithm extracts identified cores, whereas in a second phase the algorithm"}
{"pdf_id": "0712.1097", "content": "1. Bounded model checking sintances from IBM [31]. The problem instances were restricted to unsatisfiable instances, up to 35 computation steps, for a total of 252. 2. Instances from the parametrized pipelined-processor verification problem [19]. The problem instances were restricted to the smallest 58 instances. 3. Verification of out-of-order microprocessors, from UCLID [13]. 31 unsatisfiable instances were considered. 4. Circuit testing instances [11]. 228 unsatisfiable instances were considered. 5. Automotive product configuration [27]. 84 unsatisfiable instances were considered."}
{"pdf_id": "0712.1097", "content": "The MAXSAT solvers considered were the following: the best performing solver in the MAXSAT 2007 evaluation [1], maxsatz [16,17], a PBO formulation of the MAXSAT problem solved with minisat+, one of the best performing PBO solvers [5, 20], an implementation of the algorithm based on identification of unsatisfiable cores (msu1) [6], msu1 with the improvements proposed in Section 4 (msu2), and the new MAXSAT algorithm described in Section 5 (msu3)"}
{"pdf_id": "0712.1097", "content": "Recent work has shown that MAXSAT has a number of significant practical applica tions [25]. However, current state of the art MAXSAT solvers are ineffective on most problem instances obtained from practical applications. This paper focus on solving MAXSAT problem instances obtained form practicalapplications, and conducts a detailed analysis of MAXSAT algorithms based on unsat"}
{"pdf_id": "0712.1182", "content": "Arguments in subjective logic are subjective opin ions about propositions. The opinion space is a subset of the belief function space used in Dempster-Shafer belief theory.The term be lief will be used interchangeably with opinions throughout this paper.A binomial opinion applies to a single proposition, and can be rep resented as a Beta distribution. A multinomial opinion applies to a collection of propositions,and can be represented as a Dirichlet distribution. Through the correspondence between opin ions and Beta/Dirichlet distributions, subjective logic provides an algebra for these functions."}
{"pdf_id": "0712.1182", "content": "The two types of fusion defined for subjective logic are cumulative fusion and averaging fusion[4]. Situations that can be modelled with the cu mulative operator are for example when fusingbeliefs of two observers who have assessed sepa rate and independent evidence, such as when they have observed the outcomes of a given process over two separate non-overlapping time periods.Situations that can be modelled with the averag ing operator are for example when fusing beliefsof two observers who have assessed the same ev idence and possibly interpreted it differently."}
{"pdf_id": "0712.1182", "content": "quires the already fused belief and one of its contributing belief components as input, and will pro duce the remaining contributing belief componentas output. Fission is basically the opposite of fu sion, and the formal expressions for fission can be derived by rearranging the expressions for fusion. This will be described in the following sections."}
{"pdf_id": "0712.1182", "content": "b: belief that the proposition is true d: disbelief that the proposition is true (i.e. the belief that the proposition is false) u: uncertainty about the probability of x (i.e. the amount of uncommitted belief) a: base rate of x (i.e. probability of x in the absence of belief)"}
{"pdf_id": "0712.1182", "content": "The expression of Eq.(3) is equivalent to the pig nistic probability in traditional belief function theory [10], and is based on the principle that the belief mass assigned to the whole frame is split equally among the singletons of the frame. In Eq.(3) the base rate ax must be interpreted in the sense that the relative proportion of singletons contained in x is equal to ax."}
{"pdf_id": "0712.1182", "content": "Bayesian belief networks represent models of conditional relationships between propositions of interest. Subjective logic provides operators forconditional deduction [8] and conditional abduc tion [9] which allows reasoning to take place in either direction along a conditional edge. Fig.4 shows a simple Bayesian belief network where x and y are parent evidence nodes and z is the child node."}
{"pdf_id": "0712.1182", "content": "Belief revision based on the fission operator can be useful in case a very certain opinion about z has been determined from other sources, and it is in connict with the opinion derived through the Bayesian network. In that case, the reasoning canbe applied in the inverse direction using the fis sion operator to revise the opinions about x and y or about the conditional relationships z|x and z|y."}
{"pdf_id": "0712.1182", "content": "Opinion ownership in the form of a superscript to the opinions is not expressed in this example. It can be assumed that the analyst derives input opinion values as a function of evidence collectedfrom different sources. The origin of the opinions are therefore implicitly represented as the ev idence sources in this model."}
{"pdf_id": "0712.1182", "content": "The principle of belief fusion is used in numerousapplications. The opposite principle of belief fis sion is less commonly used. However, there aresituations where fission can be useful. In this paper we have described the fission operators cor responding to cumulative and averaging fusion insubjective logic. The derivation of the fission op erators are based on rearranging the expressions for the corresponding fusion operators."}
{"pdf_id": "0712.1529", "content": "Finally, note the clear distinction between ontological concepts (such as human), which Cocchiarella (2001) calls first-intension con cepts, and logical (or second-intension) concepts, such as thief(x).  That is, what ontologically exist are objects of type human, not  thieves, and thief is a mere property that we have come to use to  talk of objects of type human4. Moreover, logical concepts such as  thief are assumed to be defined by virtue of some logical expression,  such as"}
{"pdf_id": "0712.1529", "content": "What this suggests, and correctly so, in our opinion, is that in our  effort to understand the complex and intimate relationship between  ordinary language and everyday commonsense knowledge, one could,  as also suggested in (Bateman, 1995), \"use language as a tool for  uncovering the semiotic ontology of commonsense\" since ordinary  language is the best known theory we have of everyday knowledge"}
{"pdf_id": "0712.1529", "content": "To avoid this seeming circularity (in wanting this ontological  structure that would trivialize semantics; while at the same time  suggesting that semantic analysis should itself be used as a guide to  uncovering this ontological structure), we suggested here performing  semantic analysis from the ground up, assuming a minimal (almost a  trivial and basic) ontology, in the hope of building up the ontology as  we go guided by the results of the semantic analysis"}
{"pdf_id": "0712.1529", "content": ", Lenat, & Guha (1990); Guarino (1995); and Sowa  (1995)), but would instead be discovered from what is in fact  implicitly assumed in our use of language in everyday discourse; (ii)  the semantics of several natural language phenomena should as a  result become trivial, since the semantic analysis was itself the source  of the underlying knowledge structures (in a sense, the semantics  would have been done before we even started!) Throughout this paper we have tried to demonstrate that a num ber of challenges in the semantics of natural language can be easily  tackled if semantics is grounded in a strongly-typed ontology that  reflects our commonsense view of the world and the way we talk about it in ordinary language"}
{"pdf_id": "0712.1529", "content": "Our ultimate goal, however, is the sys tematic discovery of this ontological structure, and, as also argued in Saba (2007), it is the systematic investigation of how ordinary language is used in everyday discourse that will help us discover (as op posed to invent) the ontological structure that seems to underlie all  what we say in our everyday discourse"}
{"pdf_id": "0712.1916", "content": "Figure 2. The relationship between the JIF and the PoP h-index (based on all citations accruing to  journal publications during 2000-2007). The filled point near the top of the figure is Forest  Ecology and Management; Agricultural and Forest Meteorology is at the top right. Journals not  recognised by Thomson Scientific are shown with a zero JIF, and are omitted from the calculation  of the trend line (trend based on 43 journals)."}
{"pdf_id": "0712.1916", "content": "Superficial examination of Table 1 may lead to the suggestion that AFM publishes  relatively few papers all of which are high-quality, reflecting a high editorial standard, and in  turn, credit to any author who has a paper accepted for publication (which is what the RQF seeks  to achieve)"}
{"pdf_id": "0712.1916", "content": "de Vries et al  Guariguata, Ostertag  Marcot et al  Swank et al  Schoenholtz et al  Ripple, Beschta  Gardiner, Quine  Tiedemann et al  Vesterdal et al  Griffis et al  Liski et al  Knoepp et al  Bowman et al  Fule et al  Ketterings et al  Emborg et al  Pretzsch et al  Kavvadias et al  Yanai et al"}
{"pdf_id": "0712.1916", "content": "Tables 2 and 3, and Figure 3 suggest that AFM and FEM are similar in many regards, but Figure  2 highlights the large discrepancy between the JIF and the h-index for these two journals. The  total number of citations reported in Table 2 may shed some light on this difference. AFM  appears to service a specialised audience that is more visible to Thomson Scientific than to  Google Scholar. In contrast, FEM is cited in a substantial number of non-academic publications"}
{"pdf_id": "0712.1916", "content": "Academic publications (including theses 10%)  15  Journals not listed by WoS (mostly refereed)  12  Government publications  12  Books  6  Conferences proceedings and presentations  3  Publications by NGOs and associations  3  Consultants reports and other commercial documents  1  Total  100"}
{"pdf_id": "0712.2063", "content": "tant and fast developing part of mathematics, the object of study of asymptotic geometric analysis, see [16, 15, 9] and references therein. Features of a dataset X are functions on X that in some sense respect the intrinsic structure of X. In the presence of a metric, they are usually understood to be 1-Lipschitz, or non-expanding, functions f, that is, having the property"}
{"pdf_id": "0712.2389", "content": "Abstract. We describe decomposition during search (DDS), an integra tion of And/Or tree search into propagation-based constraint solvers. The presented search algorithm dynamically decomposes sub-problems of a constraint satisfaction problem into independent partial problems, avoiding redundant work. The paper discusses how DDS interacts with key features that make propagation-based solvers successful: constraint propagation, especially for global constraints, and dynamic search heuristics.We have implemented DDS for the Gecode constraint programming li brary. Two applications, solution counting in graph coloring and protein structure prediction, exemplify the benefits of DDS in practice."}
{"pdf_id": "0712.2389", "content": "Overview. The paper starts with a presentation of the notations and concepts that are used throughout the later sections. In Sec. 3, we brieny recapitulate And/Or search, and then present, on a high level of abstraction, decomposition during search (DDS), our integration of And/Or search into a propagation-based constraint solver. Sec. 4 deals with the interaction of DDS with propagation and search heuristics. Section 5 discusses how global constraints interact with DDS, focusing on decomposition strategies for some important representatives. On a lower level of abstraction, Sec. 6 sketches the concrete implementation of DDS using the Gecode C++ constraint programming library. With the help"}
{"pdf_id": "0712.2389", "content": "of values for x and y. Then x and y may still be independent, but the constraintgraph shows a hyperedge connecting the two variables, so that x and y will al ways end up in the same connected component. In the following section, we will see how propagation-based solvers can deal with this."}
{"pdf_id": "0712.2389", "content": "One of the key features of modern constraint solvers is the use of global con straints to strengthen propagation. Therefore, a search algorithm has to support global constraints in order to be practically useful in such systems. We describe the problems global constraints pose for DDS, and how to tackle them."}
{"pdf_id": "0712.2389", "content": "Our implementation of DDS extends Gecode, a C++ constraint programming li brary. In this section, we give an overview of relevant technical details of Gecode, and discuss the four main additions to Gecode that enable DDS: access to the constraint graph, decomposing global constraints, integrating Decompose into the search heuristic, and specialized search engines. The additions to Gecode comprise only 2500 lines (5%) of C++code and enable the use of DDS in any CSP modeled in Gecode. DDS will be available as part of the next release of Gecode."}
{"pdf_id": "0712.2389", "content": "1. Full source code enables changes to the available propagators. 2. The renection capabilities allow access to the constraint graph. 3. Search is based on recomputation and copying, which significantly eases the implementation of specialized branchings and search engines. 4. It provides good performance, so that benchmarks give meaningful results."}
{"pdf_id": "0712.2389", "content": "In most CP systems, the constraint graph is implicit in the data structures for variables and propagators. Gecode, e.g., maintains a list of propagators, and each propagator has access to the variables it depends on.For DDS, a more explicit representation is needed that supports the com putation of connected components. We can thus either maintain an additional, explicit constraint graph during propagation and search, or extract the graphfrom the implicit information each time we need it. For the prototype implemen tation, we chose the latter approach. We make use of Gecode's renection API,which allows to iterate over all propagators and their variables. Through renec tion, we construct a graph using data structures from the boost graph library [6], which also provides the algorithm that computes connected components."}
{"pdf_id": "0712.2389", "content": "CPSP uses a database of pre-calculated point sets, called H-cores, that rep resent possible optimal distributions of H-monomers. By that, the optimization problem is reduced to a satisfaction problem for a given H-core, if H-variables are restricted to these positions. For optimal H-cores, the solutions of the CSP are optimal structures. Thus, for counting all optimal structures, one iterates through the optimal cores."}
{"pdf_id": "0712.2389", "content": "Results. The average ratio results are given in Tab. 2. There, the enormous search tree reduction with an average factor of 11 and 25 respectively is shown.The reduction using DDS compared to DFS leads to much less propagations (3 to 5-fold). This and the slightly less fails result in a runtime speedup of 3-/4-fold using the same variable selection heuristics for both search strategies. Here, the immense possibilities of DDS even without advanced constraint-graph specific heuristics are demonstrated. This also shows the rising advantage of DDS over DFS for increasing problem sizes (with higher solution numbers)."}
{"pdf_id": "0712.2449", "content": "Therefore, two methods, which are derived from scientometrics and network analysis, will be  implemented with the objective to re-rank result sets by the following structural properties:  the ranking of the results by core journals (so-called Bradfordizing) and ranking by centrality  of authors in co-authorship networks"}
{"pdf_id": "0712.2449", "content": "Findings - The methods, which will be implemented, focus on the query and on the result  side of a search and are designed to positively influence each other. Conceptually they will  improve the search quality and guarantee that the most relevant documents in result sets will  be ranked higher."}
{"pdf_id": "0712.2449", "content": "Semantic mappings could support distributed search in several ways. First and foremost, they  should enable seamless search in databases with different subject metadata systems.  Additionally, they can serve as tools for vocabulary expansion in general since they present a  vocabulary network of equivalent, broader, narrower and related term relationships. Thirdly,  this vocabulary network of semantic mappings can also be used for query expansion and  reformulation.  The following chapter introduces the concept of a search term recommender. This tool is an  aid for query reformulation and reconstruction that has been adapted from human search"}
{"pdf_id": "0712.2449", "content": "The advantage of suggesting controlled vocabulary terms as search terms is that  these terms have been systematically assigned to the documents so that there is a high  probability of relevant and precise retrieval results if these terms are used instead of whatever  natural language keywords the searcher happens to think of"}
{"pdf_id": "0712.2449", "content": "In one implementation, a likelihood ratio statistic is used to measure the association between  the natural language terms from the collection and the controlled vocabulary terms to predict  which of the controlled vocabulary terms best mirror the topic represented by the searcher's  search terms (Plaunt/Norgard, 1998; Gey et al"}
{"pdf_id": "0712.2449", "content": "Several approaches seem possible: a pivot  controlled vocabulary, from which terms are suggested and mappings approached; a general suggestion pattern, which clusters similar concepts from several vocabularies; or a domain specific approach, whereby terms and vocabularies are chosen according to the subject of  interest for the searcher"}
{"pdf_id": "0712.2449", "content": "Bradford Law as a general law in informetrics can be applied to all scientific disciplines and  especially in a multi-database scenario in combination with semantic treatment of  heterogeneity as described before. Bradfordizing (White, 1981) is an information science  application of Bradford Law of Scattering which sorts/re-ranks a result set according to the  identified core journals for a query. The journals for a search are ranked by the frequency of  their listing in the result set (number of articles for a journal title). If a search result is  bradfordized, articles of core journals are ranked ahead of the journals which contain an  average number or only few articles on a topic. This method is interesting in the context of"}
{"pdf_id": "0712.2449", "content": "Integration  Beyond an isolated use, a combination of the approaches is promising to yield much higher  innovation potential. In our model, the following scenarios are supported (e.g. combining  Bradfordizing with Author Centrality as in figure 4).  The user is provided with publications which are associated with both central authors as well  as core journals. From a technical point of view, the following variants are suitable which  may yield different results:"}
{"pdf_id": "0712.2449", "content": "• The \"intersection\" variant: core journals and central authors are first evaluated  independently from one another on the basis of the whole result set. Publications that  satisfy both relevance criteria (they appear in a core journal and their authors are  central) are determined in a second step (see figure 4)."}
{"pdf_id": "0712.2923", "content": "The LULU operators for sequences are extended to multi-dimensional ar rays via the morphological concept of connection in a way which preserves their essential properties, e.g. they are separators and form a four element fully ordered semi-group. The power of the operators is demonstrated by deriving a total variation preserving discrete pulse decomposition of images."}
{"pdf_id": "0712.2923", "content": "Let us recall that, according to the well known theorem of Matheron [10],in general, two ordered morphological operators generate a six element semi group which is only partially ordered. The power of the LULU operators as separators is further demonstrated by their Total Variation Preservation property. Let BV (Z) be the set of sequences with bounded variation, that is,"}
{"pdf_id": "0712.2923", "content": "We should remark that in the one dimensional setting, the sequences with out local maximum sets or local minimum sets of size less than or equal ton are exactly the so-called n-monotone sequences. Hence Corollary 13 gener alizes the respective results in the LULU theory of sequences, [13, Theorem 3.3]."}
{"pdf_id": "0712.2923", "content": "to be closed under composition is the equality in Theorem 15. Now one can easily derive the rest of the formulas for the compositions of the operators in this set. The composition table is indeed as given in Table 1. Furthermore, Theorem 15 implies the total order on the set (22) as in (4). Indeed, we have"}
{"pdf_id": "0712.2923", "content": "in the analysis of images. Since the information in an image is in the con trast, the total variation of the luminosity function is an important measure of the quantity of this information. Image recovery and noise removal via total variation minimization are discussed in [3] and [16]. It should be noted that there are several definition of total variation of functions of multi-dimensionalargument (Arzel variation, Vitali variation, Pierpont variation, Hardy varia tion, etc.). In the applications cited above the total variation is the L1 norm of a vector norm of the gradient of the function. Here we consider a discrete analogue of this concept."}
{"pdf_id": "0712.2923", "content": "As mentioned in the introduction, the LULU operators for sequences aretotal variation preserving. We show here that their two-dimensional counter parts considered in this section have the same property with respect to the total variation as given in Definition 18. Let us denote by BV (Z2) the set of all functions of bounded variation in A(Z2). Clearly, all functions of finite support are in BV (Z2). In particular, the luminosity functions of images are in BV (Z2). The total variation given in Definition 18 is a semi-norm on BV (Z2). In particular, this implies that"}
{"pdf_id": "0712.2923", "content": "3 are of size less than or equal to 20 and only about 2% have size greater than 100. Hence by removing the pulse of small support we remove large portion of any impulsive noise. Figure 5 gives in the same format the pulse distribution of the image on Figure 4. A large portion of the pulses has small support but, unlike Figure 3, we have also significant number of pulses with relatively larger support. Partial reconstruction of the image by using pulses of selected sizes is given on Figure 6. We can consider (a) as removing of impulsive noise, (b) as extraction of small features and (c) as extraction of large features."}
{"pdf_id": "0712.3147", "content": "This paper presents experiments on common knowledge logic, conducted with the help of the proof assistant Coq. The main feature of common knowledge logic is the eponymous modality that says that a group of agents shares a knowledge about a certain proposition in a inductive way. This modality is specified by using a fixpoint approach. Furthermore, from these experiments, we discuss and compare the structure of theorems that can be proved in specific theories that use common knowledge logic. Those structures manifests the interplay between the theory (as implemented in the proof assistant Coq) and the metatheory."}
{"pdf_id": "0712.3147", "content": "Now let us suppose that we have a group G of agents. The knowledge of a fact can be shared by the group G, i. e., \"each agent in G knows \". We write EG() and the meaning of EG is easily axiomatized by the equivalence given in Figure 2 which can also be seen as the definition of EG; it is called shared knowledge. In common knowledge logic, there is another modality, called common knowledge which is much stronger than shared knowledge. It is also associated with a group G of agents and is written CG. Given , CG() is the least solution of the equation"}
{"pdf_id": "0712.3147", "content": "which says that there are two white hats. Notice that this is stated in a weak form, indeed it is only when Bob and Carol wear white hats that one can deduce that Alice wears a red hat. Moreover there are three concepts which say that each agent sees the hat of the other agents and therefore knows the color of the hat."}
{"pdf_id": "0712.3147", "content": "The father of the kid who organized the party asked the children to come around him in a circle for the kids to see each other and he tells them that there is at least one child who has mud on his face so that they clearly all hear him"}
{"pdf_id": "0712.3147", "content": "In other words, if the fact that there is at least p muddy children is a common knowledge and all the children know that there is not exactly p muddy children, then the fact that there is at least p + 1 muddy children is a common knowledge. Together with the first statement of Father:"}
{"pdf_id": "0712.3147", "content": "This statement is here to translate what children see after Father has asked the muddy ones to step forward and none did. They all know that there is at least p muddy children and they all know that there is not exactly p muddy children otherwise those with muddy face would have stepped forward, but now each one knows that all the others know that there is not exactly p muddy children."}
{"pdf_id": "0712.3147", "content": "A logic L, the object logic or the object theory, is said to be deeply embedded in another logic M, the meta-theory, or in a proof assistant if one considers the logic M to be this of the proof assistant, if all the constituents of the logic L are made objects of the logic M and all the connectors and the rules of L are defined inside the logic M"}
{"pdf_id": "0712.3298", "content": "This work has been supported in part by National Institutes of Health grants R01 LM008106 \"Representingand Acquiring Knowledge of Genome Regulation\" and U54 DA021519 \"National center for integrative bioin formatics,\" as well as by grants IDM 0329043 \"Probabilistic and link-based Methods for Exploiting Very Large Textual Repositories,\" DHB 0527513 \"The Dynamics of Political Representation and Political Rhetoric,\" 0534323 \"Collaborative Research: BlogoCenter - Infrastructure for Collecting, Mining and Accessing Blogs,\" and 0527513 \"The Dynamics of Political Representation and Political Rhetoric,\" from the National Science Foundation"}
{"pdf_id": "0712.3298", "content": "Much can be done using Clairlib on its own. Some of the things that Clairlib can do are listed below, in separate lists indicating whether that functionality comes from within a particular distribution of Clairlib, or is made available through Clairlib interfaces, but actually is imported from another source, such as a CPAN module, or external software."}
{"pdf_id": "0712.3298", "content": "This guide explains how to install both Clairlib distributions, Clairlib-Core and Clairlib-Ext. To install Clairlib core, follow the instructions in the section immediately below. To install Clairlib-Ext, first follow the instructions for installing Clairlib-Core, then follow those for Clairlib-Ext itself. Clairlib-Ext requires an installed version of Clairlib-Core in order to run; it is not a stand-alone distribution."}
{"pdf_id": "0712.3298", "content": "If you have not yet configured the CPAN installer, then you'll have to do so this one time. If you do not knowthe answer to any of the questions asked, simply hit enter, and the default options will likely suit your environ ment adequately. However, when asked about parameter options for the perl Makefile.PL command, users without root permissions or who otherwise wish to install Perl libraries within their personal $HOME directory structure should enter the suggested path when prompted:"}
{"pdf_id": "0712.3298", "content": "# For Clairlib-core users: # 1. Edit the value assigned to $CLAIRLIB_HOME and give it the value of the path to your installation. # 2. Edit the value assigned to $MEAD_HOME and give it the value that points to your installation of MEAD. # 3. Edit the value assigned to $EMAIL and give it an appropriate value."}
{"pdf_id": "0712.3298", "content": "The Clairlib-Ext distribution contains optional extensions to Clairlib-Core as well as functionality that depends on other software. The sections below explain how to configure different functionalities of Clairlib-Ext. As each is independent of the rest, you may configure as many or as few as you wish. Section VI provides instructions for the installation and testing of the Clairlib-ext modules itself."}
{"pdf_id": "0712.3298", "content": "This tutorial will walk you through downloading files, creating a corpus from them, creating a network from the corpus, and extracting information along the way. We'll be using utilities included in the Clairlib package to do the work. Before beginning, install the clairlib package. To do so, follow the instructions at:"}
{"pdf_id": "0712.3298", "content": "sentences_to_docs.pl -i \\ $CLAIRLIB/corpora/news-sample/lexrank-sample.txt \\ -o lexrank-sample directory_to_corpus.pl -c lexrank-sample -b produced \\ -d lexrank-sample index_corpus.pl -c lexrank-sample -b produced corpus_to_cos.pl -c lexrank-sample -b produced \\ -o lexrank-sample.cos cos_to_histograms.pl -i lexrank-sample.cos cos_to_cosplots.pl -i lexrank-sample.cos cos_to_stats.pl --graphs -i lexrank-sample.cos \\ -o lexrank-sample.stats print_network_stats.pl --triangles -i lexrank-sample-0.26.graph stats2matlab.pl -i lexrank-sample.stats -o lexrank-sample.m network_growth.pl -c lexrank-sample -b produced stats2matlab.pl -i lexrank-sample.wordmodel.stats \\ -o lexrank-sample-wordmodel.m"}
{"pdf_id": "0712.3298", "content": "make_synth_collection.pl --policy zipfian --alpha 1 -o synth \\ -d synth_out -c lexrank-sample -b produced --size 11 --verbose link_synthetic_collection.pl -n synth -b produced -c synth \\ -d synth_out -l erdos -p 0.2 index_corpus.pl -c synth -b produced corpus_to_cos.pl -c synth -b produced -o synth.cos cos_to_histograms.pl -i synth.cos cos_to_cosplots.pl -i synth.cos cos_to_stats.pl -i synth.cos -o synth.stats --graphs --all -v stats2matlab.pl -i synth.stats -o synth.m network_growth.pl -c synth -b produced stats2matlab.pl -i synth.wordmodel.stats -o synth-wordmodel.m"}
{"pdf_id": "0712.3298", "content": "Clairlib makes analyzing relationships beween documents very simple. Generally, for simplicity, documents should be loaded as a cluster, then converted to a network, but documents can be added directly to a network.Creating a Cluster: Documents can be added individually or loaded collectively into a cluster. To add doc uments individually, the insert function is provided, taking the id and the document, in that order. It is not a"}
{"pdf_id": "0712.3298", "content": "Once IDF values have been computed, they can be accessed by creating an Idf object. In the constructor, root dir and corpusname parameters should be supplied that match the CorpusDownload parameters, along with a stemmed parameter depending on whether stemmed or unstemmed values are desired (1 and 0 respectively). To get the IDF for a word, then, use the method getIdfForWord, supplying the desired word. A Tf object is created with the same parameters passed to the constructor. The function getFreq returns the number of times a word appears in the corpus, getNumDocsWithWord returns the number of documents it appears in, and getDocs returns the array of documents it appears in."}
{"pdf_id": "0712.3298", "content": "This applies only to users of Clairlib-ext! The WebSearch module is used to perform Google searches. A key must be obtained from Google in order to do this. Follow the instructions in the section \"Installing the Clair Library\" to obtain a key and have the WebSearch module use it. Once the key has been obtained and the appropriate variables are set, use the googleGet method to obtain a list of results to a Google query. The following code gets the top 20 results to a search for the \"University of Michigan,\" and then prints the results to the screen."}
{"pdf_id": "0712.3298", "content": "The parse function runs a file through the Charniak parser. The result of parsing will be returned from the function as a string, and may optionally be written to a file by specifying an output file. Note that a file must be correctly formatted to be parsed. See the previous section, \"Preparing a File for the Charniak Parser\" for more information."}
{"pdf_id": "0712.3298", "content": "# Print out usage message sub usage print \"usage: $0 --input input_file --output output_dir [--words word_limit]\"; print \" --input input_file\"; print \" Name of the input file\"; print \" --output output_dir\"; print \" Name of the output directory.\"; print \" --words word_limit\"; print \" Number of words to include in each file. Defaults to 500.\"; print \"\"; print \"example: $0 --input file.txt --output ./corpus --words 1000\"; exit;"}
{"pdf_id": "0712.3298", "content": "# Print out usage message sub usage print \"usage: $0 -c corpus_name -o output_file [-b base_dir]\"; print \" -c corpus_name\"; print \" Name of the corpus\"; print \" -b base_dir\"; print \" Base directory filename. The corpus is loaded from here\"; print \" -o output_file\"; print \" Name of file to write network to\"; print \" -s,--sample n\"; print \" Take a sample of size n from the documents\"; print \"\";"}
{"pdf_id": "0712.3298", "content": "my $out_filename = \"$hist_prefix\".\"-cosine-hist\"; print OUT \"loglog(x(:,1), x(:,2));\"; print OUT \"title(['Number of pairs per cosine in $hist_prefix']);\"; print OUT \"xlabel('Cosine Value');\"; print OUT \"ylabel('Number of pairs');\"; # Change label font sizes print OUT \"h = get(gca, 'title');\"; print OUT \"set(h, 'FontSize', 16);\"; print OUT \"h = get(gca, 'xlabel');\"; print OUT \"set(h, 'FontSize', 16);\"; print OUT \"h = get(gca, 'ylabel');\"; print OUT \"set(h, 'FontSize', 16);\";"}
{"pdf_id": "0712.3298", "content": "$out_filename = $hist_prefix . \"-cosine-cumulative\"; print OUT2 \"];\"; print OUT2 \"loglog(x(:,1), x(:,2));\"; print OUT2 \"title(['Number of pairs per cosine in $hist_prefix']);\"; print OUT2 \"xlabel('Cosine Threshold Value');\"; print OUT2 \"ylabel('Number of pairs w/cosine less than or equal to threshold');\"; # Change label font sizes print OUT2 \"h = get(gca, 'title');\"; print OUT2 \"set(h, 'FontSize', 16);\"; print OUT2 \"h = get(gca, 'xlabel');\";"}
{"pdf_id": "0712.3298", "content": "# Print out usage message sub usage print \"usage: $0 --input input_file [--output output_file] [--start start] [--end end] [--step step]\"; print \" --input input_file\"; print \" Name of the input graph file\"; print \" --output output_file\"; print \" Name of plot output file\"; print \" --start start\"; print \" Cutoff value to start at\"; print \" --end end\"; print \" Cutoff value to end at\"; print \" --step step\";"}
{"pdf_id": "0712.3298", "content": "# Print out usage message sub usage print \"usage: $0 --input input_file [--output output_directory] [--start start] [--end end] [--step step]\"; print \" --input input_file\"; print \" Name of the input graph file\"; print \" --output output_directory\"; print \" Name of output directory. The default is graphs/input_file_prefix\"; print \" --start start\"; print \" Cutoff value to start at\"; print \" --end end\"; print \" Cutoff value to end at\"; print \" --step step\"; print \" Size of step between cutoff points\"; print \"\";"}
{"pdf_id": "0712.3298", "content": "my $delim = \"[ \\t]+\"; my $output_delim = \" \"; my $cos_file = \"\"; my $graphml = 0; my $threshold; my $start = 0.0; my $end = 1.0; my $inc = 0.01; my $sample_size = 0; my $sample_type = \"randomnode\"; my $out_file = \"\"; my $graphs = 0; my $all = 0; my $stats = 1; my $single = 0; my $verbose = 0;"}
{"pdf_id": "0712.3298", "content": "# Print out usage message sub usage print \"usage: $0 -c corpus_name -i url_file [-b base_dir]\"; print \" -i url_file\"; print \" Name of the file containing a list of URLs from which to build the network\"; print \" -c corpus_name\"; print \" Name of the corpus\"; print \" -b base_dir\"; print \" Base directory filename. The corpus is generated here\";"}
{"pdf_id": "0712.3298", "content": "# Print out usage message sub usage print \"usage: $0 --basedir base_dir --corpus corpus_name [--output output_file] [--query word] [--all] [--stemmed]\"; print \" --basedir base_dir\"; print \" Base directory filename. The corpus is generated here.\"; print \" --corpus corpus_name\"; print \" Name of the corpus.\"; print \" --output output_file\"; print \" Name of output file. If not given, dumps to stdout.\"; print \" --query word\"; print \" Term to query.\"; print \" --all\"; print \" Print out all words and IDF's. Default.\"; print \" --stemmed\"; print \" Set whether the input is already stemmed.\"; print \"\"; print \"example: $0 --basedir /data0/corpora/sfi/abs/produced --corpus ABS --output ./abs.idf --query hahn --stemmed\"; exit;"}
{"pdf_id": "0712.3298", "content": "# if there is one of the four conditions, then run the iteration: 1. the next word has a different frequency from the current one 2. the current word is the first one with frequency equal to min_freq 3. the current word is the first word in the ranked list and its frequency is greater than min_freq (evaluated in the above statement). 4. the current word is the k*50-th in the ranked list."}
{"pdf_id": "0712.3298", "content": "my $out_filename = \"$hist_prefix\".\"-cosine-hist\"; print OUT \"loglog(x(:,1), x(:,2));\"; print OUT \"title(['Degree Distribution of $hist_prefix']);\"; print OUT \"xlabel('Degree');\"; print OUT \"ylabel('Number of Nodes');\"; #print OUT \"v = axis;\"; #print OUT \"v(1) = 0; v(2) = 1;\"; #print OUT \"axis(v)\"; print OUT \"print ('-deps', '$out_filename.eps')\"; print OUT \"saveas(gcf, '$out_filename\" . \".jpg', 'jpg'); \"; close OUT;"}
{"pdf_id": "0712.3298", "content": "$out_filename = $hist_prefix . \"-cosine-cumulative\"; print OUT2 \"];\"; print OUT2 \"loglog(x(:,1), x(:,2));\"; print OUT2 \"title(['Degree Distribution of $hist_prefix']);\"; print OUT2 \"xlabel('Degree');\"; print OUT2 \"ylabel('Number of Nodes');\"; print OUT2 \"v = axis;\"; print OUT2 \"v(1) = 0; v(2) = 1\"; print OUT2 \"axis(v)\"; print OUT2 \"print ('-deps', '$hist_prefix-cosine-cumulative.eps')\"; print OUT2 \"saveas(gcf, '$out_filename\" . \".jpg', 'jpg'); \"; close OUT2;"}
{"pdf_id": "0712.3298", "content": "my $delim = \"[ \\t]+\"; my $sample_size = 0; my $sample_type = \"randomedge\"; my $fname = \"\"; my $out_file = \"\"; my $pajek_file = \"\"; my $graphml_file = \"\"; my $extract = 0; my $stem = 1; my $undirected = 0; my $wcc = 0; my $scc = 0; my $components = 0; my $paths = 0; my $triangles = 0; my $assortativity = 0; my $local_cc = 0; my $all = 0; my $output_delim = \" \"; my $stats = 1; my $degree_centrality = 0; my $closeness_centrality = 0; my $betweenness_centrality = 0; my $lexrank_centrality = 0; my $force = 0; my $graph_class = \"\"; my $filebased = 0;"}
{"pdf_id": "0712.3298", "content": "print \" --input in_file\"; print \" Input file to parse into sentences\"; print \" --directory in_dir\"; print \" Input directory to parse into sentences\"; print \" --type document_type\"; print \" Document type, one of: text, html, stem\"; print \" --singlefile\"; print \" If true, write output into a single file, one line per sentence\"; print \" --output output\"; print \" Output filename or directory\"; print \"\";"}
{"pdf_id": "0712.3329", "content": "Human intelligence is an enormously rich topic with a complex intellectual, social and political history. For an overview the interested reader might want to consult \"Handbook of Intelligence\" [Ste00] edited by R. J. Sternberg. Our objective in this section is simply to sketch a range of tests, theories and definitions of human and animal intelligence. We are particularly interested in common themes and general perspectives on intelligence that could be applicable to many kinds of systems, as these will form the foundation of our definition of machine intelligence in the next section."}
{"pdf_id": "0712.3329", "content": "We take this to be our informal working definition of intelligence. In the next section we will use this definition as the starting point from which we will construct a formal definition of machine intelligence. However before we proceed further, the reader way wish to revise the 10 definitions above to ensure that the definition we have adopted is indeed reasonable."}
{"pdf_id": "0712.3329", "content": "This definition has many similarities to ours. Firstly, it emphasises the agent's ability to choose its actions so as to achieve an objective, or in our terminology, a goal. It then goes on to stress the agent's ability to deal with situations which have not been encountered before. In our terminology, this is the ability to deal with a wide range of environments. Finally, this definition highlights the agent's ability to perform tests or tasks, something which is entirely consistent with our performance orientated perspective of intelligence."}
{"pdf_id": "0712.3329", "content": "This is not really much of a definition as it simply shifts the problem of defining intelligence to the problem of defining abstract thinking. The same is true of many other definitions that refer to things such as imagination, creativity or consciousness. The following definition has a similar problem:"}
{"pdf_id": "0712.3329", "content": "It is easy to see that for unbiased coins the most likely outcome is 1 head and thus the optimal strategy for the agent is to always guess 1. However if the coins are significantly biased it might be optimal to guess either 0 or 2 heads depending on the bias. If this were the case, then after a number of iterations of the game an intelligent agent would realise that the coins were probably biased and change its strategy accordingly."}
{"pdf_id": "0712.3329", "content": "rewards more heavily, conversely by reducing it we weight them less so. In other words, this parameter controls how short term greedy, or long term farsighted, the agent should be. To work out the expected future value for a given agent and environment interacting, we take the sum of these discounted rewards into the infinite future and work out its expected value,"}
{"pdf_id": "0712.3329", "content": "is going to predict which hypotheses are the most likely to be correct, it must resort to something other than just the observational information that it has. This is a frequently occurring problem in inductive inference for which the most common approach is to invoke the principle of Occam's razor:"}
{"pdf_id": "0712.3329", "content": "round is the most intelligent choice, given what you know, it is not the most successful one. An exceptionally dim individual may have failed to notice the obvious relationship between answers and getting the money, and thus might answer \"No\" in the 13th round, thereby saving his life due to what could truly be called \"dumb luck\"."}
{"pdf_id": "0712.3329", "content": "3.5 Example. Imagine a very complex environment with a rich set of relationships between the agent's actions and observations. The measure that describes this will have a high complexity. However, also imagine that the reward signal is always maximal no matter what the agent does. Thus, although this is a very complex environment in which the agent is unlikely to be able predict what it will observe next, it is also an easy environment in the sense that all policies are optimal, even very simple ones that do nothing at all. The environment contains a lot of structure that is irrelevant to the goal that the agent is trying to achieve."}
{"pdf_id": "0712.3329", "content": "Valid. The most important property of any proposed formal definition of intelligence is that it does indeed describe something that can reasonably be called \"intelligence\". Essentially, this is the core argument of this report so far: We have taken a mainstreaminformal definition and step by step formalised it. Thus, so long as our informal defini tion is reasonable, and our formalisation argument holds, the result can reasonably be described as a formal definition of intelligence."}
{"pdf_id": "0712.3329", "content": "The position taken by Albus is especially similar to ours. Although the quote abovedoes not explicitly mention the need to be able to perform well in a wide range of envi ronments, at a later point in the same paper he mentions the need to be able to succeed in a \"large variety of circumstances\"."}
{"pdf_id": "0712.3329", "content": "Here we see two distinct notions of intelligence, a performance based one and an information content one. This is similar to the distinction between nuid intelligence and crystallized intelligence made by the psychologist Cattell (see Subsection 2.5). The performance notion of intelligence is similar to our definition with the expectation that performance is measured in a complex environment rather than across a wide range of environments. This perspective appears in some other definitions also,"}
{"pdf_id": "0712.3329", "content": "argument yet another way: Succeeding in the real world requires you to be more than an insightful spectator! The final criticism is that while the definition is somewhat formally defined, still it leaves open the important question of what exactly the tests should be. Smith suggests that researchers should dream up tests and then contribute them to some common pool of tests. As such, this is not a fully specified definition."}
{"pdf_id": "0712.3329", "content": "In order to compare the machine intelligence tests and definitions in the previous section, we return again to the desirable properties of a test of intelligence.Each property is brieny defined followed by a summary comparison in Table 1. Al though we have attempted to be as fair as possible, some of the scores we give on this table will be debatable. Nevertheless, we hope that it provides a rough overview of the relative strengths and weaknesses of the proposals."}
{"pdf_id": "0712.3329", "content": "What we have attempted to do is very ambitious and so, not surprisingly, the reactions we get can be interesting. Having presented the essence of this work as posters at several conferences, and also as a 30 minute talk, we now have some idea of what the typical responses are. Most people start out skeptical but end up generally enthusiastic, even if they still have a few reservations. This positive feedback has helped motivate us to continue this direction of research. In this subsection, however, we will attempted to cover some of the more common criticisms."}
{"pdf_id": "0712.3825", "content": "Although the definition and measurement of intelligence is clearly of fundamental importance to the field of artificial intelligence, no general survey of definitions and tests of machine intelligence exists. Indeed few researchers are even aware of alternatives to the Turing test and its many derivatives. In this paper we fill this gap by providing a short survey of the many tests of machine intelligence that have been proposed."}
{"pdf_id": "0712.3825", "content": "An approach called Psychometric AI tries to address the problem of what to test for in a pragmatic way. In the view of Bringsjord and Schimanski, \"Some agent is intelligent if and only if it excels at all established, validated tests of [human] intelligence.\"[4] They later broaden this to also include \"tests of artistic and literary creativity, mechanical ability, and so on.\" With this as their goal, their research is focused on building robots that can perform well on standard psychometric tests"}
{"pdf_id": "0712.3825", "content": "Another complexity based test is the universal intelligence test [19]. Unlike the C-Test and Smith's test, universal intelligence tests the performance of an agent in a fully interactive environment. This is done by using the reinforcement learning framework in which the agent sends its actions to the environment and receives observations and rewards back. The agent tries to maximise the amount of reward"}
{"pdf_id": "0712.4126", "content": "Figure 3.3: Parameter estimates at various stages of our algorithm on the threecomponent Gaussian mixture model (a) Poor random initial guess (b) Local max imum obtained after applying EM algorithm with the poor initial guess (c) Exit point obtained by our algorithm (d) The final solution obtained by applying the EM algorithm using the exit point as the initial guess."}
{"pdf_id": "0801.0232", "content": "A concrete approach is presented in the well-defined setting of cellular automata. Here we define the models of \"observer\", \"entity\", \"environment\", \"intelligence\" and \"contradiction\". These definitions, which roughly correspond to the common meaning of these words, allow us to deduce a simple but strong result about these concepts in an unbiased, mathematical manner."}
{"pdf_id": "0801.0232", "content": "(1) Introduction (2) Background: contradiction in science, mathematics, philosophy (3) Some notes about our epistemological approach (4) A way of formalizing the problem • 4.1. A cellular automaton as a \"world\" in which we can study entities • 4.2. An observer judges the presence of entities • 4.3. A definition of the intelligence of an entity • 4.4. A definition of the contradictory nature of an entity (5) The key result in our model (6) Computational experiments (7) Some controversial points: our answers"}
{"pdf_id": "0801.0232", "content": "In this paper we are going to examine the relationship between intelligenceand contradiction, hopefully clarifying the presence and importance of incon sistency in thought and in the processes trying to emulate it. To arrive at ourobjective, we shall need to put the concepts of \"observer\", \"entity\" and \"envi ronment\" on a mathematical footing, so that formal definitions of intelligence and contradiction can be proposed."}
{"pdf_id": "0801.0232", "content": "survey of the concept of contradiction. From an epistemological point of view, an interesting debate about this and other problems concerning mathematics has recently been raised by the mathematician and philosopher G. C. Rota(cf., e.g., [49]). Another key reference is the work done by G. Priest, concern ing the relationship between contradiction and mathematical logic (cf., e.g., [46])."}
{"pdf_id": "0801.0232", "content": "Psychology and economics are also involved in research on contradiction. The concepts of inconsistency between attitudes or behaviors (cognitive dissonance) (cf. [17]) and time-inconsistent agent (cf.,e.g., [7,55]) are generally studied in these fields. However, it should be noted that the term \"inconsistent\" is often used in a precise or technical sense, depending on the particular scientific context."}
{"pdf_id": "0801.0232", "content": "In any cases the concept of contradiction is much more than just an inevitable practical problem, and even in software engineering many researchers have begun to accept inconsistencies not only as problems to solve but also as a reality to live with (cf., e.g., [3]), and some have developed a body of research that seeks to \"make inconsistency respectable\" (cf. [19]). It is also interesting to point out the presence of contradictions in the behavior of Search Engines for the World Wide Web (cf. [4])."}
{"pdf_id": "0801.0232", "content": "Furthermore, the constant presence of inconsistencies in our thoughts leads us to the following natural question: is contradiction accidental or is it the necessary companion of intelligence? As we pointed out previously, this question is no longer only important from a philosophical point of view, since any attempt to construct artificial entities capable of intelligent behavior demands an answer to this question"}
{"pdf_id": "0801.0232", "content": "The sole aim of this paper is to place this question in a mathematical frame work and to propose a formal line of attack. In order to do this we have chosento use the concept of cellular automaton (a structure invented by J. von Neu mann ([42]) to study the phenomenon of self-replication), since it combines simplicity of definition with the capability of simulating complex systems."}
{"pdf_id": "0801.0232", "content": "Note 1 In Section 4 we shall give formal definitions of the concepts we have mentioned in this section. We shall proceed by setting out some hypotheses in our model, in order to emulate some properties of the real world: for the sake of clarity we shall first informally describe each property we wish to emulate, and then we shall give its counterpart in the formal mathematical language of cellular automata. In Section 5 we shall obtain the above mentioned result concerning the connection between contradiction and intelligence. In Section 6 we shall present the results of three computational experiments supporting the line of thought expressed in this paper. In Section 7 some controversial points and our corresponding answers will be presented."}
{"pdf_id": "0801.0232", "content": "The first thing we need is a mathematical structure through which we can try to give an acceptable formalization of such concepts as entity, environment,intelligence and contradiction. Obviously, we are not interested in all the phe nomena involving such complex concepts, but only in constructing a simple model to preserve some key facts of a real case. Cellular automata are good"}
{"pdf_id": "0801.0232", "content": "Some people may think that such a simple structure cannot emulate or re produce intelligence. In particular, some may simply maintain that a Turing machine cannot have intelligence, for various reasons (cf. [52]). We do not want to enter into this debate, but we stress that most of the tools available for developing artificial intelligence (including discrete neural networks) can be emulated by a Turing machine, so that everything we use at the momentto study intelligence from a discrete-mathematical point of view can be re duced in principle to the functioning of a cellular automaton. Therefore, it is reasonable to choose a cellular automaton as a model for our proposals."}
{"pdf_id": "0801.0232", "content": "In any case we shall justify our choice of these definitions by showing their appropriateness to the real world. In order to do so, we shall use a more complex (but still simple) example that is not explicitly implemented in a cellular automaton, since it would be too large. However, this implementation is possible in principle, because of the properties previously mentioned. We proceed analogously when we informally speak about an algorithmic procedure without explicitly and formally giving a complete definition of the Turing"}
{"pdf_id": "0801.0232", "content": "We recall that cellular automata can be regarded as discrete dynamical systems and that they are theoretically capable of simulating every Turing machine. Moreover they seem to be a suitable structure in which to study self reproducing entities (cf., e.g., [42,33,2]). Considerable literature about cellular automata exists and we shall point to it for more details about the theory (cf., e.g., [9,10,23,56,44])."}
{"pdf_id": "0801.0232", "content": "The hypothesis that Pent and PENV are finite sets is important. It means that our observers are assumed to have limited capabilities, and it willplay a key role in our proof of the proposition stated in Section 5. We empha size that this hypothesis corresponds to the fact that in reality the observers can have neither infinite memory nor unbounded computational capabilities.We consider this as self-evident, but for skeptics, many references are avail able in the literature. As an example, Wooldridge and Jennings ([63]) take for granted that all real agents are resource-bounded. They also confront the famous Logical Omniscience Problem, which arises from the assumption of unbounded inference capabilities. Therefore, our hypothesis seems to be quite natural."}
{"pdf_id": "0801.0232", "content": "Obviously, human observers are much more complex than the ones we havedefined. Proximity in position during time, for instance, is important for recog nizing the presence of an entity in our world, in most cases. However, this and other properties are not necessary in order to derive the proposition about intelligence and contradiction that we wish to obtain in Section 5. For this reason we did not require these hypotheses in our definitions."}
{"pdf_id": "0801.0232", "content": "It may be opportune to observe that the structure of a classical intelligence test can easily fit into this framework. The role of observer is taken by the psychologist administrating the test, which usually consists of some trials andproblems that must be overcome by the person examined. Overcoming a dif ficulty (such as solving a problem) can be seen as a form of survival inside aparticular game. Obviously, when we use the word \"survival\" we do not nec essarily mean survival in a biological sense. In our setting, surviving simply means remaining a player in the game."}
{"pdf_id": "0801.0232", "content": "length of life and intelligence. For example, we could observe that if we consider a human being (a man, say) and a sequoia in a forest, it is likely that the man will \"survive\" for a far shorter time than the sequoia, but this is not a good reason for thinking that the former is less intelligent than the latter."}
{"pdf_id": "0801.0232", "content": "This kind of test is similar to what we do when we think about the intellectual deficiency of a living being. We do not look for a real proof of incapacity to react to \"dangers\". We simply simulate in our brain what would happen if such dangers occurred to the considered living being, by referring to a model represented in our imagination. In a \"virtual world\" of this kind, the lack of intelligence of the sequoia could easily be expressed in terms of a short duration of life."}
{"pdf_id": "0801.0232", "content": "Note 3 It is important to point out that measuring intelligence is becoming a key problem in computer science. As an example, the use of collaborative agent systems requires the ability to measure the extent to which a set of collaborative agents is able to accomplish the goals it was built for (cf., e.g., [43]). In other words, we want to know if it is reliable or not, and to compare its \"intelligence\" to that of other collaborative agent systems pursuing the same aim (e.g., think"}
{"pdf_id": "0801.0232", "content": "(1) act or an instance of contradicting; (2) a: a proposition, statement, or phrase that asserts or implies both the truth and falsity of something; b: a statement or phrase whose parts contradict each other (\"a round square is a contradiction in terms\"); (3) a: logical incongruity; b: a situation in which inherent factors, actions, or propositions are inconsistent or contrary to one another."}
{"pdf_id": "0801.0232", "content": "Therefore, a common property can be found in our definitions: an entity can be said to be contradictory if faced with the same circumstances, it does not exhibit the same behavior. In other words, the ordinary use of the term contradictory refers to a change in behavior of the same entity."}
{"pdf_id": "0801.0232", "content": "Analogously, when we speak about \"equivalent conditions\" for an observer, we should not think of an incompetent judgment due to lack of information or the presence of errors, since, in doing so, we would simply superimpose our own personal judgment on the opinion of the chosen observer. This act would be equivalent to a change of observer."}
{"pdf_id": "0801.0232", "content": "According to the previous definition, if the environment is deterministic its future state depends on the present state of the entity and the environment (i.e., all that the observer knows about the examined \"world\"). In any case, this dependence is not required to be explicit and computable, and the observer may not be able to anticipate the future environmental state."}
{"pdf_id": "0801.0232", "content": "Some environments appear to be deterministic, while others do not. Even far away from quantum mechanics, it may happen that the environment evolves in an unpredictable way, according to the observer's judgment. For example, the weather evolution may be predictable or unpredictable, depending on the computational capabilities of the observer looking at it and on the information that is available to him, expressed by the states he can perceive."}
{"pdf_id": "0801.0232", "content": "From a formal point of view it may be interesting to observe that, following our definitions, an environment is deterministic if and only if it is non contradictory as an entity, with respect to the dual observer that exchanges the roles of psent and psENV (provided we add the required special symbol 0 to PENV )."}
{"pdf_id": "0801.0232", "content": "The previous result can be reformulated in the following way: if an entityis intelligent enough with respect to a given observer, then either the en tity appears to be contradictory (and hence its behavior is unpredictable) or the environment is not deterministic (and hence no prediction can be made). This statement requires that the entity has a finite lifetime and the observer has bounded capabilities, and suggests that in the real world the previouslydescribed limitation about determinacy should be expected in intelligent sys tems."}
{"pdf_id": "0801.0232", "content": "Remark 15 Some comments should be made about the stipulation that the lifetime of entity E is finite. From a technical point of view, this stipulation is made in order to exclude the possibility of an observer judging a structure that endlessly repeats the same configurations to be alive. In the real world and in realistic models this type of endless repetition cannot occur, since mechanisms break down and living beings die sooner or later (some remains are usually left but the observer does not recognize them as being alive, as in the case of biological death). In this fashion, our stipulation characterizes the structures that are most interesting for our proposals."}
{"pdf_id": "0801.0232", "content": "Remark 16 From the observer's viewpoint, the contradictory behavior of the studied entity implies that its actions are unpredictable. In fact, the observer cannot foresee the next state of a contradictory entity as a consequence of its present state and the state of the environment. Thus, the statement we have proved implies the following assertion, valid for a deterministic environment:"}
{"pdf_id": "0801.0232", "content": "Many examples stressing the importance of the link between intelligence and unpredictable behavior might be done, showing how unforeseeable actions can be useful for survival. As an example of this kind, we could refer to the techniques that many animals adopt for escaping predators (think of a rabbit avoiding a pursuing fox by making unpredictable zigzag bounds across a field)."}
{"pdf_id": "0801.0232", "content": "Our experiment consists of 50 tests. In each test we have two groups of stock holders. Group A contains 100 non-contradictory stockholders. On each day of the week the number of shares to be sold or bought is chosen randomly, but we require that if, in the presence of a price p, the stockholder sells or buys a number x of shares, he/she makes the same choice every day the price takes the same value p. Group B contains 100 stockholders who are allowed to be contradictory. Therefore, in this case the number of shares to be sold or bought is chosen randomly on each day of the week, without any constraint on behavior in the presence of the same market price."}
{"pdf_id": "0801.0232", "content": "In our experiment it is quite natural to interpret the share price as the per ceived environment, while the selling-buying action of the stockholder and his/her wait for a new price can be seen as the information available to theobserver about the entity. The dependence of the share price on the price as signed on the previous day corresponds to the stipulation that the environment is deterministic."}
{"pdf_id": "0801.0232", "content": "• Objection i: \"What is the point of this paper? What is the point of proving the link between intelligence and contradiction?\" Answer: The point of this paper is, in the first place, to construct amathematical framework where the concepts of intelligence and contradic tion can be represented and formally treated"}
{"pdf_id": "0801.0232", "content": "Our attempt to define a mathematical model in which we can study the re lations between contradiction and intelligence is obviously only a subjective proposal. However, a systematic approach to problems involving the active role of contradiction in intelligent beings seems at this point to be essential to the study of complex systems."}
{"pdf_id": "0801.0232", "content": "This work owes its existence to Massimo Ferri and Francesco Livi, and to their love of beauty within complexity. The author wishes to thank Claudio Barbini, Andrea Vaccaro and Joelle Crowle for their helpful suggestions, and Michele d'Amico for his precious help in performing the experiments. Thanks also to Guido Moretti and Al Seckel for providing some beautiful pictures, and to Charles Stewart and Reuben Hersh for their illuminating and constructivecriticism. The author is profoundly grateful to Douglas R. Hofstadter for re vising the paper and for his valuable suggestions, which have made this paper better and clearer. Finally, the author is solely responsible for any errors."}
{"pdf_id": "0801.0386", "content": "some form of (arithmetics upon) the total number of authored papers, the average number of authored papers per year, the total number of citations, the average number of citations per paper, the mean number of citations per year, the median citations per paper (per year) and so on. Due to the power-law distribution followed by these metrics, they present one or more of the following drawbacks (see also [4]):"}
{"pdf_id": "0801.0386", "content": "The f-index. Now, we can define the proposed f-index in a spirit completely analogous to that of h-index. To compute the f-index of an author, we calculate the quantities N Ai for each one of his/her authored articles Ai and rank them in a non-increasing order. The point where the rank becomes larger than the respective N Ai in the sorted sequence, defines the value of f-index for that author."}
{"pdf_id": "0801.1063", "content": "As discussed in the preceding section, our goal is to provide a method for extracting ratable aspects from reviews without any human supervision. Therefore, it is natural to use generative models of documents, which represent document as mixtures of latent topics, as a basis for our approach. In this section we will consider applicability of the most standard methods for unsupervised modeling of documents, Probabilistic Latent Semantic Analysis, PLSA [17] and Latent Dirichlet Allocation, LDA [3] to the considered problem. This analysis will allow us to recognize limitations of these models in the context of the considered problem and to propose a new model, Multi-grain LDA, which is aimed to overcome these limitations."}
{"pdf_id": "0801.1063", "content": "We propose a model called Multi-grain LDA (MG-LDA), which models two distinct types of topics: global topics and local topics. As in PLSA and LDA, the distribution of global topics is fixed for a document. However, the distribution of local topics is allowed to vary across the document. A word in the document is sampled either from the mixture of global topics or from the mixture of local topics specific for the local context of the word. The hypothesis is that ratable aspects will be captured by local topics and global topics will capture properties of reviewed items. For example consider an extract"}
{"pdf_id": "0801.1063", "content": "here D is the number of documents, nd gl is the number of times a word in document d was assigned to one of the global topics and nd gl,z is the number of times a word in this document was assigned to global topic z. Similarly, counts nd,v loc and nd,v loc,z are defined for local topics in window v in document d. Now the conditional distribution P(vd,i = v, rd,i = r, zd,i = z|v', r', z', w) can be obtained by cancellation of terms in expressions (1-4). For global topics we get"}
{"pdf_id": "0801.1063", "content": "In both of these expressions counts are computed without taking into account assignments of the considered word wd,i. Sampling with such model is fast and in practice convergence with MG-LDA and can be achieved in time similar to that needed for standard LDA implementations. A sample obtained from such chain can be used to approximate the distribution of words in topics:"}
{"pdf_id": "0801.1063", "content": "In this section we present qualitative and quantitative experiments. For the qualitative analysis we show that local topics inferred by MG-LDA do correspond to ratable aspects. We compare the quality of topics obtained by MG-LDA with topics discovered by the standard LDA approach. For the quantitativeanalysis we show that the topics generated from the multi-grain models can significantly improve multi aspect ranking."}
{"pdf_id": "0801.1063", "content": "To perform qualitative experiments we used a subset of reviews for Mp3 players from Google Product Search4 and subsets of reviews of hotels and restaurants from Google Local Search.5 These reviews are either entered by users directly through Google, or are taken from review feeds provided by CNet.com,Yelp.com, CitySearch.com, amongst others. All the datasets were tokenized and sentence split. Prop erties of these 3 datasets are presented in table 1. Before applying the topic models we removed punctuation and also removed stop words using the standard list of stop words.6"}
{"pdf_id": "0801.1063", "content": "We manually assigned labels to coherent topics to renect our interpretation of their meaning. Note that the MG-LDA local topics in Table 2 and Table 3 represent the entire set of local topics used in MG-LDA models. In the meantime, for the LDA topics we selected only the coherent topics which captured ratable aspects and additionally a number of example topics to show typical LDA topics. Global topics of MG-LDA are not supposed to capture ratable aspects and they are not of primary"}
{"pdf_id": "0801.1063", "content": "To bucket the probabilities produced by LDA and MG-LDA we choose 5 buckets using thresholds to distribute the values as evenly as possible. We also tried many alternative methods for using the real value topic probabilities and found that bucketing with raw probabilities worked best. Alternatives attempted include: using the probabilities directly as feature values; normalizing values to (0,1) with and without bucketing; using log-probabilities with and without bucketing; using z-score with and without bucketing."}
{"pdf_id": "0801.1336", "content": "The brain is composed of several modules each of which is essentially an autonomous neural  network. Thus the visual network responds to visual stimulation and also during visual imagery,  which is when one sees with the mind's eye. Likewise, the motor network produces movement  and it is active during imagined movements. However, although the brain is modular, a part of it,  located for most people in the left hemisphere, monitors the modules and interprets their  individual actions in order to create a unified idea of the self. In other words, there is a higher  integrative or interpretive module that synthesizes the actions of the lower modules [1]."}
{"pdf_id": "0801.1336", "content": "As a caveat it must be said that this, in itself, will not endow the system with biological type of  intelligence since another hallmark of biological intelligence that we are not in a position to  simulate effectively in our implementations is that of reorganization with respect to changing  environment [2-4]"}
{"pdf_id": "0801.1336", "content": "Classical computers are based on ideas that developed in the 1930s and 1940s to give shape to the  intuition of how the rational mind performs computation. The general-purpose computing  machine was visualized to consist of four main parts. These are the parts relating to the arithmetic  logic unit, memory, control, and interface with the human operator."}
{"pdf_id": "0801.1336", "content": "In the classical computer's memory there is no fundamental distinction between data and  instruction, which is considered a shortcoming by some. Other claimed shortcoming are: the  memory is monolithic and it must be sequentially addressed; it is single dimensional whereas in  nature patterns of memory are multidimensional; and the attributes of data are not stored together  with it, which is in contrast to what obtains in a higher level language where we expect a generic  operation to take on a meaning determined by the meaning of its operands."}
{"pdf_id": "0801.1336", "content": "However, whereas some computations carried out by humans (especially those dealing with  numerical computations) do fall within the category that is well captured by serial computation,  there are a vast number of other computations that do not. In particular, tasks associated with  \"intelligence,\" which typically involves processing enormous amounts of data do not involve  deliberate computation. In such tasks, autonomous centers appear to carry out computations  independently, reducing the dimensions of the data and mapping it into an abstract space where  further computations are done."}
{"pdf_id": "0801.1336", "content": "Although much of the computations are done in parallel, this is not the parceling out of  computational tasks to different processors by taking advantage of the parallel components of the  algorithm, which is what happens in what is technically called \"parallel computing\" [5]. Rather,  here the entire data is seemingly pushed into a variety of autonomous processors, quite as a  stream of water is pushed into various channels with different function, justifying the term stream  computing. The higher-order processor cannot be generic and it must use specific application  knowledge to design it."}
{"pdf_id": "0801.1336", "content": "There is a wealth of experimental evidence from neuroscience that suggests that the conscious  mind \"creates\" its reality in order to have a narrative that is \"consistent\" with the information  reaching it from various specialized modules. This is seen most clearly in subjects who have  suffered brain injury where the effect becomes most pronounced."}
{"pdf_id": "0801.1336", "content": "In the 60s and the 70s, Kornhuber and Deecke performed a series of experiments to measure the  correlation between electrical activity in the brain (EEG) and a voluntary act. They found that the  EEG from the area corresponding to the finger in the motor cortex for a subject who is about to  move a finger starts to build up several hundred milliseconds before the conscious decision to  make the act is made [6]. The conscious mind appears to label such an act its own free decision  although one might dispute this."}
{"pdf_id": "0801.1336", "content": "Libet et al, in a variation of this experiment, showed that the EEG potential appeared to increase  about 0.3 seconds before the subject made his \"conscious choice\" to flex his finger. These results  are in agreement with the idea of the cortex constructs a model that is consistent with the  mediating experience [7]."}
{"pdf_id": "0801.1336", "content": "The left-hemisphere interpreter is not only a master of belief creation, but it will stick to  its belief system no matter what. Patients with \"reduplicative paramnesia,\" because of  damage to the brain, believe that there are copies of people or places. In short, they will  remember another time and mix it with the present. As a result, they will create  seemingly ridiculous, but masterful, stories to uphold what they know to be true due to  the erroneous messages their damaged brain is sending their intact interpreter."}
{"pdf_id": "0801.2069", "content": "MDPs are attractive because solution time is polynomial in the number of states. Consider, however, a sequential decision problem with m variables. In general, we need an exponentially large state space to model it as an MDP. So, the number of states is exponential in the size of the description of the task. Factored Markovdecision processes may avoid this trap because of their more compact task repre sentation."}
{"pdf_id": "0801.2069", "content": "The quality of the approximation depends on two factors: the choice of the basis functions and the approximation algorithm. Basis functions are usually selected by the experiment designer, and there are no general guidelines how to automate this process. For given basis functions, we can apply a number of algorithms to determine the weights wk. We give a short overview of these methods in Section 4. Here, we concentrate on value iteration."}
{"pdf_id": "0801.2069", "content": "The exact solution of factored MDPs is infeasible. The idea of representing a large MDP using a factored model was first proposed by Koller & Parr [17] but similar ideas appear already in the works of Boutilier, Dearden, & Goldszmidt [5, 6]. More recently, the framework (and some of the algorithms) was extended tofMDPs with hybrid continuous-discrete variables [18] and factored partially observ able MDPs [23]. Furthermore, the framework has also been applied to structured MDPs with alternative representations, e.g., relational MDPs [15] and first-order MDPs [24]."}
{"pdf_id": "0801.2069", "content": "Both the objective function and the constraints can be written in compact forms, exploiting the local-scope property of the appearing functions. Markov decision processes were first formulated as LP tasks by Schweitzer and Seidmann [25]. The approximate LP form is due to de Farias and van Roy [7].Guestrin et al. [13] show that the maximum of local-scope functions can be computed by rephrasing the task as a non-serial dynamic programming task and elim inating variables one by one. Therefore, (15) can be transformed to an equivalent,"}
{"pdf_id": "0801.2069", "content": "4.1.1. Applications. Applications of fMDP algorithms are mostly restricted to ar tificial test problems like the problem set of Boutilier et al. [6], various versions of the SysAdmin task [13, 10, 21] or the New York driving task [23]. Guestrin, Koller, Gearhart and Kanodia [15] show that their LP-based solutionalgorithm is also capable of solving more practical tasks: they consider the real time strategy game FreeCraft. Several scenarios are modelled as fMDPs, and solved successfully. Furthermore, they find that the solution generalizes to larger tasks with similar structure."}
{"pdf_id": "0801.2069", "content": "4.2. Sampling. Sampling techniques are widely used when the state space is im mensely large. Lagoudakis and Parr [19] use sampling without a theoretical analysis of performance, but the validity of the approach is verified empirically. De Farias and van Roy [8] give a thorough overview on constraint sampling techniques used"}
{"pdf_id": "0801.2069", "content": "If both A and B are structured, we can sharpen the lemma to give a much better (potentially exponentially better) bound. For this, we need the following definition: For any index set Z, a matrix A is called Z-local-scope matrix, if each column of A represents a local-scope function with scope Z."}
{"pdf_id": "0801.2345", "content": "eigenspectrum of matrices (Newman, 2006), (b) walktrap, a technique based on randomwalks (Pons & Latapy, 2006), (c) edge betweenness, the earliest community detection tech nique, based on vertex betweenness centrality (Girvan & Newman, 2002) (d) spinglass, a technique based on a spin-glass model and simulated annealing (Reichardt & Bornholdt, 2006)"}
{"pdf_id": "0801.2345", "content": "the vertices being within the largest component (280 out of a total of 291 vertices). This means that besides the four small separate components, the interdisciplinary research group studied here is perceived, as a whole, as a single coauthoring community. Figure 2 presents the number of scholars identified in the 27 structural communities identified by the leading eigenvector community detection algorithm."}
{"pdf_id": "0801.3654", "content": "of H we obtain a new graph isomorphic to H which we denote by P(H). The adjacency matrix of the permuted graph, AP (H), is simply obtained from AH by the equality AP (H) = PAHP T . In order to assess whether a permutation P defines a good matching between the vertices of G and those of H, a quality criterion must be defined. Although other choices are possible, we focus in this paper on measuring the discrepancy between the graphs after matching, by the number of edges (in the case of weighted graphs, it will be the total weight of edges) which are present in one graph and not in the other. In terms of adjacency matrices, this number can be computed as:"}
{"pdf_id": "0801.3654", "content": "The projection (6) can be performed with the Hungarian algorithm, with a complexity cubic in the dimension of the problem. The main disadvantage of this method is that the dimensionality (i.e., number of variables and number of constraints) of the linear program (6) is O(N 2), and therefore it is quite hard to process graphs of size more than one hundred nodes. Other convex relaxations of (1) can be found in [18] and [17]. In the next section we describe our new algorithm which is based on the technique of convex-concave relaxations of the initial problems (1) and (3)."}
{"pdf_id": "0801.3654", "content": "The QCV problem is a convex quadratic program that can be solved in polynomial time, e.g., by the Frank-Wolfe algorithm [29] (see Section 3.5 for more details). However, the optimal value is usually not an extreme points of D, and therefore not a permutation matrix. If we want to use only QCV for the graph matching problem, we therefore have to project its solution on the set of permutation matrices, and to make, e.g., the following approximation:"}
{"pdf_id": "0801.3654", "content": "The first series of experiments are experiments on small size graphs (N=8), here we are interested in comparison ofthe PATH algorithm (see Figure 2), the QCV approach (8), Umeyama spectral algorithm (4), the linear program ming approach (5) and exhaustive search which is feasible for the small size graphs. The algorithms were tested on the three types of random graphs (binomial, exponential and power). The results are presented in Figure 4. The"}
{"pdf_id": "0801.3654", "content": "Figure 4: Matching error (mean value over sample of size 100) as a function of noise. Graph size N=8. — Umeyama's algorithm, LP — linear programming algorithm, QCV — convex optimization, PATH — path minimization algorithm,OPT — an exhaustive search (the global minimum). The range of error bars is the standard deviation of matching errors"}
{"pdf_id": "0801.3654", "content": "Therefore it is interesting to compare our method with other approximate methods proposed for QAP. [18] proposed the QPB algorithm for that purpose and tested it on matrices from the QAP benchmark library [38], QPB results were compared to the results of graduated assignment algorithm GRAD [17] and Umeyama's algorithm. Results of PATH application to the same matrices are presented in Table 1, scores for QPB and graduated assignment algorithm are taken directly from the publication [18]. We observe that on 14 out of 16 benchmark, PATH is the best optimization method among the methods tested."}
{"pdf_id": "0801.3654", "content": "In this section, we present two applications in image processing. The first one (Section 6.1) illustrates how taking into account information on graph structure may increase image alignment quality. The second one (Section 6.2) shows that the structure of contour graphs may be very important in classification tasks. In both examples we compare the performance of our method with the shape context approach [19], a state-of-the-art method for image matching."}
{"pdf_id": "0801.3654", "content": "We have presented the PATH algorithm, a new technique for graph matching based on convex-concave relaxations of the initial integer programming problem. PATH allows to integrate the alignment of graph structural elements with the matching of vertices with similar labels. Its results are competitive with state-of-the-art methods in several graph matching and QAP benchmark experiments. Moreover, PATH has a theoretical and empirical complexity competitive with the fastest available graph matching algorithms.Two points can be mentioned as interesting directions for further research. First, the quality of the convex concave approximation is defined by the choice of convex and concave relaxation functions. Better performances"}
{"pdf_id": "0801.3654", "content": "may be achieved by more appropriate choices of these functions. Second, another interesting point concerns the construction of a good concave relaxation for the problem of directed graph matching, i.e., for asymmetric adjacency matrix. Such generalizations would be interesting also as possible polynomial-time approximate solutions for the general QAP problem."}
{"pdf_id": "0801.3654", "content": "The PATH algorithm does not generally find the global optimum of the NP-complete optimization problem. In this appendix we illustrate with two examples how the set of local optima tracked by PATH may or may not lead to the global optimum. More precisely, we consider two simple graphs with the following adjacency matrices:"}
{"pdf_id": "0801.3908", "content": "Summary. This paper shows how authority files can be encoded for the Semantic Web with the Simple Knowledge Organisation System (SKOS). In particular the application of SKOS for encoding the structure, management, and utilization of country codes as defined in ISO 3166 is demonstrated. The proposed encoding gives a use case for SKOS that includes features that have only been discussed little so far, such as multiple notations, nested concept schemes, changes by versioning."}
{"pdf_id": "0801.3908", "content": "Country codes are short codes that represent countries and dependent areas. The most common code for general applications is ISO 3166, but there are many othercountry codes for special uses. Country codes are managed by an agency that de fines a set of countries, with code, name and partly additional information. Examples"}
{"pdf_id": "0801.3908", "content": "of relevant systems of country codes beside ISO 3166 include codes that are used by the US government as defined by the Federal Information Processing Standard (FIPS), codes of the International Olympic Committee (IOC), codes of the World Meteorological Organization (WMO), and numerical country calling codes assigned by the International Telecommunications Union (ITU)"}
{"pdf_id": "0801.3908", "content": "SKOS was first developed in the SWAD-Europe project (2002-2004). It is a RDF based standard for representing and sharing thesauri, classifications, taxonomies, subject-heading systems, glossaries, and other controlled vocabularies that are used for subject indexing in traditional Information Retrieval. Examples of such systems are the AGROVOC Thesaurus, the Dewey Decimal Classification, and the dynamiccategory system of Wikipedia [8]. Encoding controlled vocabularies with SKOS al lows them to be passed between computer applications in an interoperable way"}
{"pdf_id": "0801.3908", "content": "and to be used in the Semantic Web. Because SKOS does not carry the strict and complex semantics of the Web Ontology Language (OWL), it is also refered to as \"Semantic Web light\". At the same time SKOS is compatible with OWL and can be extended with computational semantics for more complex applications.[9] SKOS is currently being revised in the Semantic Web Deployment Working Group of W3C to become a W3C Recommendation in 2008."}
{"pdf_id": "0801.3908", "content": "The basic elements of SKOS are concepts (skos:Concept). A concept in SKOS is a resource (identified by an URI) that can be used for subject indexing. Tostate that a resource is indexed with a specific concept, SKOS provides the property skos:subject. The concepts of ISO 3166 are countries and their subdivi sions. Hierarchical relations between concepts are encoded with skos:broader and skos:narrower. These relationships allow applications to retrieve resources that are index with a more specific concept when searching for a general term [18]. For representation and usage by humans, concepts are refered to by labels (names)."}
{"pdf_id": "0801.3908", "content": "ISO 3166 is does not only consist of country codes but it also has an internal struc ture. First the three parts ISO 3166-1, ISO 3166-2, and ISO 3166-3 are concept schemes of their own but their concepts refer to each other. Second the country subdivisions as defined in ISO 3166-2 can be grouped and build upon another. Forinstance France is divided in 100 departments which are grouped into 22 metropoli tan and four overseas regions, and Canada is disjointedly composed of 10 provinces and 3 territories. Figure 1 shows the structure of ISO 3166 with an extract of the definitions for France."}
{"pdf_id": "0801.3908", "content": "Newsletter I-1 (2000-06-21) Addition of 1 new territory: The new territory Nunavut split up from Northwest Territories.Newsletter I-2 (2002-05-21) Correction of name form of CA-NF: The name 'New foundland' changed to 'Newfoundland and Labrador'. Newsletter I-4 (2002-12-10) Change of code element of Newfoundland and Labrador: The country code CA-NF changed to CA-NL."}
{"pdf_id": "0801.3908", "content": "ensured by best practise rules in the final SKOS standards. Figure 4 contains an encoding of the changes of Canada in ISO 3166 as shown in figure 3. The changeof Newfoundland to Newfoundland and Labrador in newsletter I-2 and I-4 is en coded by an exact mapping between sequent versions (skos:exactMatch) while the split of Northwest Territories in newsletter I-1 is encoded by an skos:narrowMatch. Unchanged country codes are connected with owl:sameAs."}
{"pdf_id": "0801.4807", "content": "itself. Finding backgrounds is a lot simpler than finding text directly. It can be accomplished robustly by extracting some well chosen texture features. Once a potential background area has been selected, we then use a combination of shape and color features to detect whether text is present inside the area. Having pre-identified the background provides us witha sample of the background color and texture, and thus sim plifies the problem of determining whether there is text on thebackground. The search for the text area is performed hierar chically in a top-down fashion: if no text is found at a given scale, then we look for text at a smaller scale. This allows us to find the text without making prior assumptions regarding the font and area sizes."}
{"pdf_id": "0801.4807", "content": "directly, but rather find the text by first finding likely textcontexts and studying the features of each potential text con text to decide whether or not it contains text. False positives in the early stages thus do not constitute a problem, and so we can conservatively estimate the thresholds of the early decision parameters. The details of our approach are given in the next section. In Section 3, we present our experimental methodology and results before concluding in Section 4."}
{"pdf_id": "0801.4807", "content": "In other words, the value of the projection of a row of thematrix onto any one of these basis vectors quantifies the dif ference between the amount of color on two regions of equalsize within the block. Some elements of such a basis are il lustrated in Figure 2. The basis elements can be viewed as"}
{"pdf_id": "0801.4807", "content": "Once the uniform blocks have been selected, we group them together in order to form uniform regions. We begin by grouping sets of connected blocks based on color similarity. More precisely, we group together connected uniform blocks if the distance between their mean color vector is less than 45. Again, this threshold value was chosen empirically. A better value could be obtained from a training set. Once we have obtained connected uniform regions, we merge these regions based on color similarity and based on the variation of color in the space between them. More precisely, we merge regions such that"}
{"pdf_id": "0801.4807", "content": "Since the image areas containing the text itself are not uniform, then any uniform region corresponding to the background of a sign must have \"holes\". In other words, we as sume that the text is at least partially surrounded by a uniformarea. Any selected uniform area which is connected and con vex is thus eliminated. This simple step rules out most of the uniform regions identified with the previous steps. The few remaining regions (if any) go through the next and final step of our method. Note that one often needs to reach a small scale before a uniform region with an appropriate shape is identified."}
{"pdf_id": "0801.4807", "content": "Fig. 3. A Few Samples of our Experimental Results. (a) Text of varying sizes and color (including graphics). (b) Street sign in front of a smooth background (sky). (c) Non-rectangular text area. (d) Text written in English and Urdu both are successfully segmented. (e) Street sign in front of a textured background (cement). (f) Text printed on an irregular surface. (g) Shop display."}
{"pdf_id": "0801.4807", "content": "We tested our method on a database of 65 (three megapixel) images of outdoor signs and shop displays. Ten of these images contained outdoor signs written in both English and Urdu. The rest (55 images) contained English signs only, but some included simple graphics as well. All the text areawas correctly segmented in 63 (i.e., 97%) of these 65 im ages. In four of these 63 images, some other areas were also segmented as well. However, these areas all contain highly contrasting high level structures on a uniform background which in many ways resemble text (for example, a capital \"i\" letter) but could be ruled out from a semantic point of view."}
{"pdf_id": "0801.4807", "content": "We have presented a top-down hierarchical methods for find ing text areas in natural images. The key point of this method is that it begins by looking for text background areas before testing for the presence of text inside the selected areas. The method correctly segmented all the text in 97% of the images in a small database of outdoor signs and shop displays. In future work, we will test the method on a larger database of natural images. To improve the results, we will use trainingto choose the optimal parameters for all the decisions we per form. We will also investigate the use of more sophisticated text presence test (e.g., edge based or connected component"}
{"pdf_id": "0802.0745", "content": "Wikis provide a new way of collaboration and knowledge sharing. Wikis are soft ware that allows users to work collectively on a web-based knowledge base. Wikis are characterised by a sense of anarchism, collaboration, connectivity, organic development and self-healing, and they rely on trust. We list several concerns about applying wikis in professional organisation. After these concerns are met, wikis can provide a progessive, new knowledge sharing and collabora- tion tool."}
{"pdf_id": "0802.0745", "content": "Wikis are anarchistic in the sense that there is no power structure. In general, no user has more rights then any other user. On many wikis, anonymous users have the same rights as registered users. Sometimes some power structure is established. For instance, on Wikipedia there are sysops (system operators) that have additional functionality for the revertion of vandalism. Because of the anarchistic nature, a power structure can lead to connicts between users, e.g., when assigning new sysops. Because of the equality of rights, there is also no division of labour. There is no director that tells subordinates what to do. Each individual can select the role that best fits his or her preferences."}
{"pdf_id": "0802.0745", "content": "23] puts it: The frontiers of a book are never clear-cut: beyond the title, the first lines, and the last full stop, beyond its internal configuration and its autonomous form, it is caught up in a system of references to other books, other texts, other sentences: it is a node within a network"}
{"pdf_id": "0802.0745", "content": "Wikipedia uses the MediaWiki software. There are several Wikipedia-related projects that also use this wiki engine, such as Wiktionary (dictionary), WikiBooks (textbooksand manuals), WikiQuote, WikiSource (previously published documents) and Wiki News. Other well-known wikis include are the MeatBallWiki (about on-line culture and communities), the LinuxWiki, WikiTravel (a travel guide), and the SwitchWiki, which aims to be a list of all available wikis around the globe."}
{"pdf_id": "0802.0745", "content": "All successful examples of wiki implementations mentioned in section 2.3 are freely available on the Internet, and its user community consists completely of volunteers.Wikis are now gaining attention in professional organisation, and companies like Socialtext and JotSpot now provide wiki services to companies (see section 4). The appli cation of wikis in business might pro- vide a new way of knowledge sharing and mightconnect people with similar interest that are organisationally dispersed. However, be fore implementing the software straight away in a busi- ness environment, we see afew points of attention. We will discuss them in four groups: (1) motivational consid erations, (2) authoritan considerations, (3) strategic considerations, and (4) effectivity considerations"}
{"pdf_id": "0802.0745", "content": "Organisations are generally build around a certain authoritan model, where certain people (usually managers) have responsibility for subparts of the organisation, or theorganisation as a whole in the case of top management, and delegate tasks to sub ordinates. During the years the models of organisations have changed, going from hierarchical pyramids via networked organisation with high employee autonomy back to a sort of hierarchical diamond. However, the concepts of resposibility and delegating tasks have always been in place. As discussed in subsection 2.1, wikis are anarchistic by nature. In a pure wiki, there are no users with a higher authority as others, and each individual picks its own tasks.11"}
{"pdf_id": "0802.0745", "content": "One concern of large organisation is division in departments and units. This division is needed to keep the organisation managable, but at the same time it creates barriers between people that might work in related areas, and the organisation would benefit from knowledge sharing between those people. The trend of organisations adopting"}
{"pdf_id": "0802.0745", "content": "offers multimedia whiteboards for real-time collaboration. Users can collaborate usingmany types of multimedia, but the knowledge isnt stored in a manner that allows re trieval at a later point. All three commercial products have some navour of wikis, but are not exactly it. On the open-source side of wiki developments, a wiki engine called TWiki15 is geared more towards a professional application then other wiki engines. For instance, it allows the creation of forms so that users can easily enter data that will be grouped on wiki pages. Also, the best known wiki engine, MediaWiki, is used by several companies, like Gartner and Novell.16"}
{"pdf_id": "0802.1296", "content": "Until recently, Computer Science was mainly concerned with data storage and processing in purpose-built data basesand computers. With the advent of the Web and social com putation, the task of finding and understanding information arising from local interactions in spontaneously evolvingcomputational networks and data repositories has taken cen ter stage. As computers evolved from calculators, the key paradigm of Computer Science was computation-as-calculation, with the Turing Machine construed as a generic calculator, and with data processing performed by a small set of local operations. As computers got connected into networks, and captured a range of social functions, the paradigmof computation-as-communication emerged, with data processing performed not only locally, but also through distribution, merging, and association of data sets through vari"}
{"pdf_id": "0802.1296", "content": "If we zoom in even further, we will find that the state of user's preferences is usually not completely determined even in a completely static model: right after watching a movie, one usually needs to toss a \"mental coin\" to decide whether to assign 2 or 3 stars, say, to the performance of an actor; or to decide whether to pay more attention, while watching the movie, to this or that aspect, music, colors"}
{"pdf_id": "0802.1296", "content": "While the indeterminacy of information in a network can be reduced to an effect of noise, like in the standard model,and averaged out, it is interesting to ponder whether view ing this indeterminacy as an essential feature of network computation, rather than a bug, may lead to more realistic models of information systems"}
{"pdf_id": "0802.1296", "content": "Is the \"mental coin\", which resolves the superposition of the many components of my preferences when I need to measure them, akin to a real coin, which we all agree is governed by completely deterministiclaws of classical physics, and its randomness is just the ap pearance of its complex behavior; or is this \"mental coin\" governed by a more fundamental form of randomness, likethe one that occurs in quantum mechanics, causing the su perposition of many states to collapse under measurement?"}
{"pdf_id": "0802.1296", "content": "The unassigned ratings are again padded by zeros. In a user-balanced matrix, users' different rating habits,that some of them are more generous than others, are fac tored out. Only the satisfaction profile of each user is recorded, over the set of all items that she has rated. The average and unassigned ratings are identified, both with 0."}
{"pdf_id": "0802.1296", "content": "Comment. The purpose of balancing and normalization of raw semantic matrices is to factor out the aspects of ratingthat are irrelevant for the intended analysis. Whether a particular adjustment is appropriate or not depends on the in tent, and on the available data. E.g., padding the available ratings by assigning the average rating to all unrated items may be useful in some cases, but it skews the data when the sample is small.2 In the rest of the paper, we assume that all such adjustments have been applied to data as appropriate, and we focus on the methods for extracting information from them."}
{"pdf_id": "0802.1296", "content": "While LSI is a standard, well-studied data min ing method, FCA has been less familiar in the data analysis communities, although an early proposal of a concept-latticeapproach can be traced back to the earliest days of the infor mation retrieval research (Salton 1968), predating both FCA and even the standard vector space model"}
{"pdf_id": "0802.1296", "content": "The succinct presentation of LSI and FCA as special cases of the same pat tern, in our abstract model above, points to the fact that the Singular Value Decomposition, on which LSI is based, andthe Galois Connections, that lead to FCA, both subsume un der the abstract structure of isometric decomposition, just instantiated to the rig of reals for LSI, and to the booleanrig for FCA"}
{"pdf_id": "0802.1296", "content": "which need not be distributive lattices, but only orthomodu lar (Meyer 1986; Meyer 1993; Redei & Summers 2006).A crucial, frequently made observation, eventually lead ing into quantum statistics, is that the lattices of concepts,and of topics, induced by the various forms of latent seman tics, are not distributive. Indeed, since the lattice structure is induced by"}
{"pdf_id": "0802.1296", "content": "Similarity and rankingAt the core of the vector space model of information re trieval, data mining and other forms of data analysis lies the idea that the basic similarity measure, applicable to pairs ofobjects, or of attributes, or to the mixtures thereof, is ex pressible in terms of the inner product of their normalized (often also balanced) vectors:"}
{"pdf_id": "0802.1296", "content": "Corollary. The probability of users' future agreementP(X = Y ) cannot be derived by rescaling the past simi larities of their tastes s(x, y), where the similarity measure s is defined by the inner product. The reason is that formula (1), which would have to be satisfied, does not always hold."}
{"pdf_id": "0802.1296", "content": "Interpretation. Why is it not justified to predict future agreements from past similarities, both defined in intuitivelyobvious ways? One line of explanation is that the independence assumptions are violated. As usually, the dependencies can be explained in terms of hidden variables (e.g., offline interactions of the users), or in terms of non-local interactions. Another line of explanation is that the depen dencies are introduced in the model itself. Intuitively, this means that the users, whose agreements are predicted, have not been sampled in the same measure space, and that their preferences should not be statistically mixed."}
{"pdf_id": "0802.1296", "content": "This fact is not only intuitively natural, in the sense that, say, the data on the Web move not only in packets, along the Internet links, but they also get teleported from site to site, by people talking to each other, and thentyping on their keyboards; but it is also information theoretically robust, in the sense that there are always covert chan nels"}
{"pdf_id": "0802.1306", "content": "Outline of the paper. In section 2 we introduce the basic network model, and describe a first attempt to extract information about the nows through a network from the available static data about it. In sections 3 and 4, we describe the structure which allows us to lift the notion of rank, described in section 5, to path networks in section 6. Ranking paths allows us to extract a random variable, called attraction bias, which allows measuring the mutual information of the distributions of the inputs and the outputs of the network computation, which can be viewed as an indicator of non-local information processing that takes place in the given network. In the final section, we describe how the obtained data can be used to detect semantical"}
{"pdf_id": "0802.1306", "content": "The next example can be interpreted in two ways, either to show how forward and backward dynamics can be refined to take into account various navigation capabilities, or how to abstract away irrelevant cycles. Suppose that a surfer searches for the hubs on the network: he prefers to follow the hyperlinks that lead to the nodes with a higher out-degree. This preference may be realized by annotating the hyperlinks according to the out-rank of their target nodes. Alternatively, the surfer may explore the hyperlinks ahead, and select those with the highest out-degree; but we want to ignore the exploration part, and simply assume that he proceeds according to the out-rank of the nodes ahead. The probability that this surfer will move from i to j is thus"}
{"pdf_id": "0802.1738", "content": "The problem of representing text documents within an Infor mation Retrieval system is formulated as an analogy to theproblem of representing the quantum states of a physical sys tem. Lexical measurements of text are proposed as a way ofrepresenting documents which are akin to physical measure ments on quantum states. Consequently, the representation of the text is only known after measurements have been made, and because the process of measuring may destroy parts of the text, the document is characterised through erasure. The mathematical foundations of such a quantum representation of text are provided in this position paper as a starting pointfor indexing and retrieval within a \"quantum like\" Informa tion Retrieval system."}
{"pdf_id": "0802.1738", "content": "Lexical measurements on Textual Documents In a physical system, the state of the system is defined by the probabilities of the possible outcomes of measurements performed on that system. However, the state of a quantum system can only have some of the measurement outcomesdetermined, not all of them. For example, there is an im possibility of determining both position and velocity of an electron (Heisenberg indeterminacy principle): only one of the two properties can be determined with certainty, while the other becomes uncertain when the first is determined.For some pairs of measurements, the value of the corre sponding observables will not depend on the order in which"}
{"pdf_id": "0802.1738", "content": "Here the lighter gray areas represent one eraser, and the dark areas another. These two erasers are said to be compatible because the result is the same in any order: they commute. They also show an order relation: one of them includes the other because it preserves the same parts of the document, plus others."}
{"pdf_id": "0802.1738", "content": "3. They do not always commute. When some terms in a doc ument are erased by both projectors E1 and E2, and some occurrences of the central term ti of one is amongst them, it is easy to see that applying the erasers in a different order produces a different result (see figure 3)."}
{"pdf_id": "0802.1738", "content": "This is similar to the situation we find with measurementsin QT: there are particle-like properties, such as posi tion, that are incompatible with wave-like properties, such as wavelength (closely related to velocity). Measuringa particle-like property will always erase part of the in formation about wave-like properties, and the other way around, so the result is different when making the two measurements in two different orders."}
{"pdf_id": "0802.1738", "content": "contingent on the choice of documents. They will hold for some documents, but not for others.The simplest Selective Erasers are those which erase everything but the occurrence of a term. According to the def inition, they would be referred to as E(t,0). They will be represented by 1-dimensional projectors. If such Selective Erasers are applied to each term in the vocabulary then each projector will be orthogonal to one another, because if we apply one to the document, the result of applying another will erase the remainder:"}
{"pdf_id": "0802.1738", "content": "Probabilities Erasers can be seen as a proposition about a certain word (for example: term t1 is in the neighbourhood of term t2) that can be fulfilled or not by any token in a document (like being in the neighbourhood of an occurrence of a certain term). As such, they can be given a truth value for every token in a"}
{"pdf_id": "0802.1738", "content": "Mathematical representations for erasers and document can be derived from measured fractions F(ED) choosing them as to exactly, or approximately, reproduce these numbers with the traces of their products. A scheme similar to this has been proposed by Mana (2003) for probabilistic data analysis, but in a more general context."}
{"pdf_id": "0802.1738", "content": "To this aim, we will explore two main directions: (1) using order relations of Selective Erasers as a way to define clusters of documents, and (2) formulating an indexing scheme based on a density operator representation of documents, that allows the use of the rich mathematical structure of Hilbert Spaces to encode semantic information about documents"}
{"pdf_id": "0802.2127", "content": "The expressiveness of FOL and its relative mechanisability make automated theorem proving in FOL a useful instrument for suchapplications as verification [5,4,1,6] and synthesis [19] of hardware and software, knowledge representa tion [18], Semantic Web [16], assisting human mathematicians [21,3], background reasoning in interactive theorem provers [23], and others"}
{"pdf_id": "0802.2127", "content": "There are three possible outcomes of the saturation process on clauses: (1) an empty clause is derived, which means that the input set of clauses is unsatisfiable; (2) saturation terminates without producing an empty clause, in which case the input set of clauses is satisfiable (provided that a complete inference system is used); (3) the prover runs out of resources"}
{"pdf_id": "0802.2127", "content": "In the last decade there has been a sharp increase in performance of such systems3, which I attribute to the use of advanced calculi and inference systems (primarily, complete variants of resolution [2] andparamodulation [26] with ordering restrictions, and a number of compatible redundancy detection and simplification techniques), and intensified research on efficient implementation techniques, such as term index ing (see [12] and more recent survey [35]), heuristic methods for guiding proof search (see, e"}
{"pdf_id": "0802.2127", "content": "In sum, the coarseness of the clause selection principle deprives us of control over the proof search pro cess to a great extent, which translates into poor productivity of heuristics, restricts the choice of heuristics that can be implemented, and leads to littering the search state with too many \"undesirable\" clauses."}
{"pdf_id": "0802.2127", "content": "of inference selection will enhance the diversity of available strategies10. These advantages come at an affordable cost. The only involved overhead, caused by the need to store large numbers of selection units, is compensated by lower numbers of heuristically bad clauses which have to be created and stored only to maintain completeness.I would like to add one final consideration here. The calculi used in the state-of-the-art saturation based provers are designed with the aim of reducing search space. Partially, they do this by restricting the applicability of resolution and paramodulation rules. Often this is done by prohibiting inferences with"}
{"pdf_id": "0802.2127", "content": "To address the issues raised above, I propose a method for intelligent prioritising of search directions. The idea is as follows. We will estimate the potential of a clause to participate in solutions of the whole problem at hand by interacting with other currently available clauses. Precise estimation is impossible since it would require finding all, or at least some, solutions of the problem, so we are looking for a good approximation."}
{"pdf_id": "0802.2127", "content": "Static relevancy prediction. My original idea was to use some sort of clause abstractions for dynamic suppressing of potentially irrelevant search directions in the framework of saturation-based reasoning. Thisidea was inspired by [7] where the authors propose to use various clause abstractions for statically identi fying input clauses which are practically irrelevant, i.e. can not be useful in a proof attempt of acceptable complexity. Roughly, this is done by applying abstractions to an input clause set, exploring the space of all proofs of restricted complexity with the abstracted clause set, and throwing away the input clauses whose abstractions do not participate in any of the obtained proofs with the abstracted set."}
{"pdf_id": "0802.2127", "content": "Octopus approach. The Octopus system [25] runs a large number of sessions of the prover Theo [24] distributed over a cluster of computers. Each Theo session first runs on a weakening of the original problem, obtained by replacing one of the clauses with one of its generalisations. If one of the sessions succeeds in solving the weakened problem, the solution is used to direct the search for a solution of the original problem in two ways:"}
{"pdf_id": "0802.2127", "content": "The applicability of the semantic guidance approach seems limited because it relies on the costly op eration of establishing satisfiability of large clause sets. This overhead may be acceptable in solving very hard problems when the user can afford to run a prover for hours or even days. Many applications, however,require solving large numbers of simpler problems and much quicker response. I hope that generalisation based guidance can be more useful for this kind of applications because the associated overhead seems more manageable due to the nexibility of generalisation function choice. Anyway, a meaningful comparison of the two approaches can only be done experimentally, when at least one variant of the generalisation-based method is implemented."}
{"pdf_id": "0802.2127", "content": "Certain theoretical effort is required to formulate the method in full detail. It makes sense to consider a number of variants of the method and try to predict their strengths and weaknesses. It is also essential to have a clear picture of how the proposed use of generalisations will interact with the popular inference systems based on resolution, paramodulation and standard simplification techniques. In particular, it is necessary to consider the search completeness issues."}
{"pdf_id": "0802.2127", "content": "In contrast with the fine inference selection scheme which essentially requires creating a new imple mentation, the generalisation-based search guidance can be relatively easily integrated into some existingprovers, especially if it is implemented with naming and folding as outlined earlier. My experience with im plementing splitting-without-backtracking [31] (see also Chapter 5 in [30]) in the Vampire kernel suggeststhat only a moderate effort is required to implement naming and folding on the base of a reasonably man ageable implementation of forward subsumption, which is a standard feature in advanced saturation-based provers."}
{"pdf_id": "0802.2127", "content": "The most difficult task is likely to be the design and implementation of a nexible, yet manageable,mechanism for specifying generalisation functions, and to provide a higher-level interface for this mech anism which would enable productive use of heuristics. The reliance on heuristics also implies that very extensive experimentation will be required to assess the general effectiveness of the method and to compare its variants."}
{"pdf_id": "0802.2127", "content": "This paper is almost entirely based on my work on Vampire in the Computer Science Department at the University of Manchester. The work was supported by a grant from EPSRC. The first draft of this paper was also written in Manchester. I would like to thank Andrei Voronkov for useful discussions of the ideas presented here. Many thanks to Geoff Sutcliffe for his scribblings on the first draft of this paper."}
{"pdf_id": "0802.2429", "content": "6. TEST PROBLEM We experiment a cGA using anisotropic selection on a Quadratic Assignment Problem (QAP): Nug30. Our aim here is not to obtain better results with respect to other optimization methods, but rather to observe the behavior of a cGA with AS. In particular, we seek an optimal value for the anisotropy degree."}
{"pdf_id": "0802.3137", "content": "For instance, the Fastfood problem, described in Section 3, is represented naturally and compactly in our language, while its encoding inthe language of other DLP and ASP systems seems to be more involved causing compu tation to be dramatically less efficient, due to their more severe safety restrictions (domain predicates), and also to the lack of the \"min\" aggregate function (see Section 7"}
{"pdf_id": "0802.3137", "content": "(General) Atoms, Literals and Rules. An atom is either a standard atom or an aggregate atom. A literal L is an atom A (positive literal) or an atom A preceded by the default negation symbol not (negative literal). If A is an aggregate atom, L is an aggregate literal. A (DLPA) rule r is a construct"}
{"pdf_id": "0802.3137", "content": "DLPA Programs. A (DLPA) program P (program, for short) is a set of DLPA rules (pos sibly including integrity constraints) and weak constraints. For a program P, let Rules(P) denote the set of rules (including integrity constraints), and let WC(P) denote the set of weak constraints in P. A program is positive if it does not contain any negative literal."}
{"pdf_id": "0802.3137", "content": "However, the above rule is unsafe because of the variable T. Our language thus fails to naturally express a simple query which can be easily stated in SQL11. To overcome thisproblem, we introduce the notion of assignment aggregate and make appropriate adjust ments to the notion of safety and semantics."}
{"pdf_id": "0802.3137", "content": "Assignment Aggregate. We denote by def r(p) the set of defining rules of a predicate p, that is, those rules r in which p occurs in the head. Moreover, the defining program of a predicate p, denoted by def P(p), consists of def r(p) and the defining programs of all predicates which occur in the bodies of rules in def r(p). An aggregate atom is an assignment aggregate if it is of the form X = f(S), f(S) = X,or X = f(S) = X, where X is a variable and for each predicate p in S, def P(p) is negation stratified and non-disjunctive. The intuition of the restriction on the definition of the nested predicates is to ensure that these predicates are deterministically computable."}
{"pdf_id": "0802.3137", "content": "In this section, we show how aggregate functions can be used to encode several relevant problems: Team Building, Seating, and a logistics problem, called Fastfood. Moreover, we show how some properties of the input relations (e.g., the cardinality) can be simply computed by using aggregates, and we describe the encoding of a variant of the Fastfood problem."}
{"pdf_id": "0802.3137", "content": "(p1) The team consists of a certain number of employees. (p2) At least a given number of different skills must be present in the team. (p3) The sum of the salaries of the employees working in the team must not exceed the given budget. (p4) The salary of each individual employee is within a specified limit. (p5) The number of women working in the team has to reach at least a given number."}
{"pdf_id": "0802.3137", "content": "Information on our employees is provided by a number of facts of the form emp(EmpId, Sex, Skill, Salary). The size of the team, the minimum number of different skills in the team, the budget, the maximum salary, and the minimum number of women are specified by the facts nEmp(N), nSkill(N), budget(B), maxSal(M), and women(W). We then encode each property pi above by an aggregate atom Ai, and enforce it by an integrity constraint containing not Ai."}
{"pdf_id": "0802.3137", "content": "Seating. We have to generate a seating arrangement for k guests, with m tables and n chairs per table. Guests who like each other should sit at the same table; guests who dislike each other should sit at different tables. Suppose that the number of chairs per table is specified by nChairs(X) and that person(P)and table(T) represent the guests and the available tables, respectively. Then, we can gen erate a seating arrangement by the following program:"}
{"pdf_id": "0802.3137", "content": "However, since the maximum cardinality of p is not known in advance, the size of domain would have to be countably infinite, which is not feasible. In a similar way, again by assignment aggregates, one may compute the sum of the values of an attribute of an input relation (e.g., compute the sum of the salaries of the employees)."}
{"pdf_id": "0802.3137", "content": "It should be noted that this encoding relies heavily on assignment aggregates. The firstconstraint determines the cardinality of the input predicate depot using an assignment ag gregate and makes sure that any alternative assignment has the same cardinality. The final constraint also employs an assignment aggregate, in this case not directly involving an input predicate, but a predicate which has a deterministic definition (serves) and which involves yet another aggregate. In fact, it is unclear if and how this constraint could be encoded without an assignment aggregate, as the range for Cost is not known or bounded a priori."}
{"pdf_id": "0802.3137", "content": "The following theorems report on the complexity of the above reasoning tasks for propo sitional (i.e., variable-free) DLPA programs that respect the safety restrictions imposed in Section 2. Importantly, it turns out that reasoning in DLPA does not bring an increase in computational complexity, which remains exactly the same as for standard DLP. We begin with programs without weak constraints, and then discuss the complexity of full DLPA"}
{"pdf_id": "0802.3137", "content": "Implementing aggregates in the DLV system, has had a strong impact on DLV requiringmany changes to the modules of the DLV core, and, especially, to the \"Intelligent Ground ing\" (IG) and to the \"Model Generator\" (MG) modules. We next describe the main changes carried out in the modules of DLV core to implement aggregates."}
{"pdf_id": "0802.3137", "content": "In our implementation, an aggregate atom will be assigned a truth-value just like a stan dard atom. However, different from a standard atom, its truth-value also depends on the valuation of the aggregate function and thus on the truth-value of the nested predicates. Therefore, an aggregate atom adds an implicit constraint on models and answer sets: The"}
{"pdf_id": "0802.3137", "content": "The Model Checker (MC) receives a model M in input, and checks whether M is an answer set of the instantiated program P (see Subsection 5.1). To this end, it first computes the reduct PM, by (i) deleting the rules having a false aggregate literal or a false negative literals (w.r.t. M) in their bodies, and (ii) removing the aggregates literals and the negativeliterals from the bodies of the remaining rules. Since the resulting program is aggregate free, the standard DLV techniques can then be applied to check whether PM is an answer set. Thus, no further change is needed in MC, after the modification of the procedure computing the reduct."}
{"pdf_id": "0802.3137", "content": "DLVA Encode each problem in DLPA and solve it using our extension of DLV with aggregates. DLV Encode the problem in standard DLP and solve it using standard DLV.To generate DLP encodings from DLPA encodings, suitable logic defi nitions of the aggregate functions are employed (which are recursive for #count, #sum, and #times)."}
{"pdf_id": "0802.3137", "content": "The discussion on the \"right\" semantics for aggregate-unstratified programs is still going on in the DLP and Answer Set Programming (ASP) communities. Several proposals have been made in the literature, which can roughly be grouped as follows: In (Eiter, Gottlob, and Veith 1997; Gelfond 2002; Dell'Armi et al. 2003), aggregate atoms are basically treated like negative"}
{"pdf_id": "0802.3137", "content": "Our policy, in the development of DLV, is to keep the system language as much agreedupon as possible, and to try to guarantee a clear and intuitive semantics for the newly intro duced constructs. Thus, we disregard programs which are not aggregate-stratified, leaving their introduction in DLV to future work.14"}
{"pdf_id": "0802.3137", "content": "The intended meaning of this rule is that tooexpensive should be derived when the sum of the costs of all ordered items exceeds a threshold of 100. Note that here we specified two terms to be aggregated over, where the sum will be computed over the first one. This is important, as different items may incur the same cost. For instance if order(valve, 60) and order(pipe, 60) hold, then tooexpensive should be derived. One may try to write the following variant in the syntax of SMODELSA:"}
{"pdf_id": "0802.3137", "content": "Future work will concern the introduction of further aggregate operators like #any (\"Is there any matching element in the set?\") and #avg, investigations of a general framework that will allow adding further aggregates much more easily, extending semantics to classes of programs which are not aggregate-stratified, as well as the design of further optimization techniques and heuristics to improve the efficiency of the computation"}
{"pdf_id": "0802.3137", "content": "This work has greatly benefited from interesting discussions with and comments by Paolo Ferraris, Michael Gelfond, Vladimir Lifschitz, Nikolay Pelov, and from the comments and suggestions by the anonymous referees. It was partially supported by M.U.R. under the PRIN project \"Potenziamento e Applicazioni della Programmazione Logica Disgiuntiva\",and by M.I.U.R. under internationalization project \"Sistemi basati sulla logica per la rap presentazione di conoscenza: estensioni e tecniche di ottimizzazione\". Wolfgang Faber's work was funded by an APART grant of the Austrian Academy of Sciences."}
{"pdf_id": "0802.3285", "content": "Block schematic of DVB receiver  DVB-S  DVB-S([1],[2],[4]) is a satellite-based delivery system  designed to operate within a range of transponder bandwidths  (26 to 72 MHz) accommodated by European satellites such as  the Astra series, Eutelsat series, Hispasat, Telecom series,  Tele-X, Thor, TDF-1 and 2, and DFS [3]"}
{"pdf_id": "0802.3285", "content": "contains a Program ID (PID), which allows for the  identification of all packets belonging to the same data stream,  or alternatively it provides a mean for multiplexing data  streams within transport streams. It may be viewed as the  equivalent of the port number field in UDP packets. Finally,  the Continuity Counter field (CC) may be viewed as the  equivalent of the RTP sequence number. It is incremented by  one for each packet belonging to the same PID therefore  allowing for the detection of missing packets."}
{"pdf_id": "0802.3285", "content": "Notice that for this particular transport stream we have  received 14 different packets:  •  one video packet  •  3 audio packets  •  7 signaling packets  •  3 additional packets   Fields specifications  •  PID value: is assigned to each packet and it's different  from one transport stream to another"}
{"pdf_id": "0802.3285", "content": "Short comparison between TSA and Mosalina  •  They both perform analysis of one transport stream,  indicating the transport packets type, that are received in  Online or Offline mode;  •  TSA has a much more common interface, is very simple  and has less options than Mosalina"}
{"pdf_id": "0802.3285", "content": "• Extending the results in DVB-S and DVB-C with minor  modifications  REFERENCES  [1] ETS300421, Digital broadcasting systems for television,  sound and data services; Framing structure, channel coding  and modulation for 11/12 GHz satellite services- European  Telecommunications Standards Institute- Valbone, France,  1994  [2]ETR154,  Digital  Video  Broadcasting  (DVB);  Implementation guidelines for the use of MPEG-2 systems,  video and audio in satellite, cable and terrestrial  broadcasting applications- European Telecommunications  Standards Institute- Valbone, France, 1996"}
{"pdf_id": "0802.3288", "content": "or a wireless connection  • Standard IP video compression techniques could be used  • IP surveillance cameras may be added individually or in  groups according to your needs  The Embedded IP surveillance system that benefits from the  test procedure described in this paper has roughly the  following architecture (Fig.1 [1])."}
{"pdf_id": "0802.3288", "content": "In this drawing the test targeted VideoFPGA board is dashed.  The video acquisition board has a nonstandard architecture,  adding along the video acquisition and MPEG encoding  features, a FPGA core performing some video processing  specific tasks. This makes possible to implement intensive  video processing applications into FPGA and let the CPU to  perform concurrently additional tasks.  The simplified architecture of the board is presented in the  following image (Fig.2)."}
{"pdf_id": "0802.3288", "content": "The verification procedure of board identification has the  following points:  •  startup of PC in Windows mode  •  observing during boot process the PCI devices listing  where the correctly identified board appears ([2])  •  In Device Manager (Sound, Video and Game  Controllers) the board (Philips SAA7134) should appear  like in the following picture (without ! mark)"}
{"pdf_id": "0802.3288", "content": "Filling in the content with the appropriate values for the board  (equipped either with XC2V250 or XC2V1000 FPGA's)  allows recognition and use of the board in system.  The following image explains the memory map for the two  different configurations.  The content for XC2V250 board version is presented in fig.6."}
{"pdf_id": "0802.3288", "content": "(192.168.0.200) should be replaced with the default address  10.1.1.1 allocated at startup by Linux init procedure.  Preliminary operations necessary to apply this procedure:  •  Installation of Mozilla Firefox browser in Client PC  •  Connection of the client and the server directly or via"}
{"pdf_id": "0802.3288", "content": "http://192.168.0.200/videofpga.html  This should open the main test server page as in Fig.11.  From this window it is possible to launch individual tests, for  different functional blocks.  Image grabbing test  \"Grab image\" will create in the left window after few seconds  an image with the captured frame (Fig.12)."}
{"pdf_id": "0802.3288", "content": "Opening http://192.168.0.200/, main page of video server will  create the following menu (Fig.16).  Streamer Output link will create a screen where All live cams  link creates \"near\" live video (moving images) on your screen.  IV. CONCLUSIONS  This \"simple\" and affordable procedure allows the full"}
{"pdf_id": "0802.3293", "content": "We will be using the co-occurrence network of Reuters news [16] as a test network for our algorithms. We will be analyzing the \"importance\" of the persons in this network. It is constructed using the Reuters-21578 corpus which contains 21578 Reuters newswire articles which appeared in 1987, mostly on economics. This is a network with 5249 nodes and 7528 edges, where nodes represent individual people and there is an edge between two persons if they appear in an article together. We chose to use edges as unweighted."}
{"pdf_id": "0802.3293", "content": "These people are often well-known or powerful people of their time in politics or business. It was shown in [16] this network exhibits small-world properties, presented along with a study of different well-known ranking algorithms. We use a converted version of this undirected network to a directed network by using two arcs in both directions in place of an edge. The diameter of the undirected network is 13."}
{"pdf_id": "0802.3293", "content": "We can make an exact calculation using only local informa tion for a node if the supports of the citer nodes are disjoint. If we assume them to be disjoint when they are not, then we would overestimate the degree of support. Let us detail this with an example. Consider Fig.1(a), the neighbors of node 1 are nodes 2 and 3. We know from Eq.4 the support for v1 is:"}
{"pdf_id": "0802.3293", "content": "This is equivalent to doing a partial transformation on the immediate neighbors of a node, and accounting for the previous \"entanglement\" using an extra \"damping\" node, see Fig.2 for a demonstration of the idea. Recall that for small-world networks [17] it is shown that if vertex i is connected to vertex j and vertex k, then it is highly probable that vertices j and k are also connected. Damping function is therefore used to counter the effect of the clustering."}
{"pdf_id": "0802.3293", "content": "ERank-N can be found in [?] and [15]. Also, in [15] we offer a formal treatment of the theoretical framework presented here, introducing the Entity Transitive Relation Implication (ETRI) model for the mapping of a network into a PAS instance. In this previous work we present ERank as a special case tailored for the network ranking application of a general case algorithm named ETRI Support Propagation (ESP). However we chose to use ERank throughout this article for the sake of simplicity also omitting other details that are not crucial. For example in Fig.3 nodes 1 and 2 have an immediate cycle between them. Fig.4 shows how ERank-0 and ERank-1 perform when run on the network of Fig.3. It plots the average distance for a given iteration:"}
{"pdf_id": "0802.3293", "content": "In this figure, we plot the results when ERank-0 is run for 3 iterations, and when it is run for 12 iterations. For comparison we also plot the results from ERank-1 at 3 iterations. We observe ERank-0 algorithms with different iterations do comparably well, while ERank-1 outperforms others when d0 is chosen correctly. In our experimentation with the Reuters network we havenot seen any significant improvements in estimation per formances or ranking performances (as we introduce later) using these \"higher\" algorithms. This is probably because the Reuters network is undirected although we have not confirmed this. So we will not deal with the other ERank algorithms any further in this article due to space considerations."}
{"pdf_id": "0802.3293", "content": "As we have argued earlier, the exact dsp value of a node may be prohibitively hard to compute. On the Reuters network we have been able to compute the exact dsp values of nodes up to different maximum orders ranging from one (just the immediate neighbors) to 11. We use as many as possible of these as sample sets to plot the average distance using Eq.6. For example when comparing against ERank-0 run with 6 iterations, we use all of the sample set for which we could calculate the dsp values using the corresponding maximum order of 5. We do not include nodes without any links in these calculations."}
{"pdf_id": "0802.3293", "content": "In Fig.5 we consider the average distance on the Reuters network where comparisons are made against dsp calculations with a maximum order of 3. It contains the plots of ERank-0 for pl0 = 0.2 and p(ai) = 1/n using 3 and 4 iterations for the damping constant range [0, 1] along with corresponding dsp computations using maximum orders of 1 and 2. The results are offset in reference to dsp with maximum order 3 which isrepresented by the line y = 0. We observe that when ERank 0 has a good damping constant it can outperform exact dsp calculations of maximum order 2."}
{"pdf_id": "0802.3293", "content": "Similarly, in Fig. 6 we use the same probability values as in Fig.5 to compare how different ERank's perform on the Reuters network. Using Eq.6 we plot ERank results comparingthem to dsp computations with a maximum order of 5. ERank 0 appears here to perform as good as the higher order ERank algorithms. As we have argued above we believe this is because the conversion from undirected to directed network places cycles for all the nodes although we have not validated this yet."}
{"pdf_id": "0802.3293", "content": "for a given person i, 0 otherwise. Of the 5,249 persons in the network we find that 1,440 have a Wikipedia page. In the rest of this section we will use this function as apriori information on the importance of nodes and perform a comparative study of the algorithms. Table II shows the top 20 people when ranked according to article count values. Having a glance at this table can serve as a basic reality check for the utility of our defined functions. For example we see that most of the people we could expect to have high importance have H(i) = 1; President of USA, Prime Minister of Japan, Secretary of State of USA."}
{"pdf_id": "0802.3293", "content": "The function H(i) can be thought as placing each node in one of the two classes 0 and 1, i.e. those with and without English Wikipedia pages. Hence this becomes a clustering problem with an external criteria. We would ideally like an algorithm to rank all the persons labeled as H(i) = 1 higher than the ones labeled with 0, thus giving us a perfect separation of the collection into two clusters. There is a well-known statistic named \"Hubert's gamma\" which is used for assessing cluster validity in this class of problems [25]. Mathematically stated Hubert's gamma is:"}
{"pdf_id": "0802.3293", "content": "We have introduced a family of novel rapid approximation algorithms for applying a PAS based modeling and ranking to large complex networks (particularly small-world model networks). As far as we are aware, it is the first of its kind that is both practically applicable to large networks and formally founded in a quantitative reasoning framework. A problem known to be NP-complete is approximated using linear and near linear time algorithms for this specialized application domain. Thus ERank enables the use a new paradigm in"}
{"pdf_id": "0802.3528", "content": "Table 2: Image sequence number chosen: these are the images shown (in succes sion, from upper left) in Figure 8. For each image, 5 wavelet resolution scales are studied. 2D Lorentzian and Gaussian fits are shown: MSE (mean square error) used. An asterisk indicates whether Lorentzian or Gaussian fit is better."}
{"pdf_id": "0802.3528", "content": "31.9 43.3 1397.2 9.1 2982.0 10404.7 77135.4 122607.0 192195.0 276682.0 60 37.6 28.7 18.7 134.8 22180.5 26668.1 37069.2 44615.1 859.6 875.7 120 3.3 5.6 2.7 8.1 23.8 214.8 2.0 0.0 86422.3 1.4 180 49.1 6.6 0.6 5.4 9817.3 74.0 7739.2 5.5 51196.0 75436.2 240 0.5 0.8 0.3 23.4 88.0 5.8 591.3 46947.3 3315.3 85459.2 300 3.8 12.2 2506.9 10.3 39793.6 48.3 13137.1 108.6 211860.0 243913.0"}
{"pdf_id": "0803.0146", "content": "We list here four types of ratio problems. This include, in addition to the normalized cut problem and the ratio regions problem, also the densest subgraph problem and the \"ratio cut\" problem. We solve here only the first two. The third problem has been known to be polynomial time solvable, and the last problem is NP-hard."}
{"pdf_id": "0803.0146", "content": "Shi and Malik noted in their work on segmentation that cut procedures tend to create segments that may be very small in size. To address this issue they proposed several versions of objective functions that provide larger segments in an optimal solution. Among the proposed objective they formulated the normalized cut as the optimization problem"}
{"pdf_id": "0803.0146", "content": "This problem is equivalent to finding the expander ratio of the graph discussed in the next subsection. This objective function drives the segment S and its complement to be approximately of equal size. Indeed, like the balanced cut problem the problem was shown to be NP-hard, [19], by reduction from set partitioning. A variant of the problem also defined by Shi and Malik is"}
{"pdf_id": "0803.0146", "content": "it is the same as finding the expander ratio of a graph and again it drives to a roughly equal or balanced partition of the graph. The dominant techniques in vision grouping are spectral in nature. That is, they compute the eigenvalues and the eigenvectors and then some type of rounding process, see e.g. [21, 20]. Instead of the sum problem, there are other related optimization problems used for image segmentation. Sharon et al. [20] define the normalized cut as"}
{"pdf_id": "0803.0146", "content": "A salient segment in the image is one for which the similarity across its border is small, whereas the similarity within the segment is large (for a mathematical description, see Methods). We can thus seek a segment that minimizes the ratio of these two expressions. Despite its conceptual usefulness, minimizing this normalized cut measure is computationally prohibitive, with cost that increases exponentially with image size."}
{"pdf_id": "0803.0146", "content": "where L is the Laplacian matrix of the graph and W is a matrix appropriately defined. The use of spectral techniques involves real number computations with the associated numerical issues. Even an exact solution to the nonlinear problem is a vector of real numbers whereas the original problem is discrete and binary. However, this normalized cut problem (without the \"balanced\" requirement) is polynomial time solvable. We show an algorithm solving the problem in the same complexity as a single minimum s, t-cut on a related graph on O(n + m) nodes and O(n + m) edges."}
{"pdf_id": "0803.0146", "content": "This problem is shown here to be polynomially solvable by a parametric cut procedure, in the complexity of a single minimum cut. The problem is in fact equivalent to a binary and linear version of the Markov Random Fields problem, called the maximum s-excess problem in [14]. It is interesting to note that the pseudonow algorithm in [14] is set to solve the maximum s-excess problem directly. Our algorithm for the ratio regions problem applies for node weights that can be either positive or negative. This generalizes the application context of Cox et al. the node weighs were all positive."}
{"pdf_id": "0803.0146", "content": "The key is to formulate the problem as an integer linear programming problem, a 0-1 integer programming here, with monotone inequalities constraints. It was shown in [16] that any integer programming formulation on monotone constraints has a corresponding graph where the minimum cut solution corresponds to the optimal solution to the integer programming problem. Thus the formulation is solvable in polynomial time. To convert the ratio objective to a linear objective we utilize the reduction of the ratio problem to a linearized optimization problem."}
{"pdf_id": "0803.0194", "content": "An other method of evaluation is based on histogram , measuring the surface of  the peak around the medium level of grey.  2.A/D converter cuantisation parameters -A frame-grabber generally uses a flash  ADC , with a sampling frequency exceeding 10-15 Msps. Although a large offer of  such high performance converters exists, many producers don't offer any guarantees of  monotonicity , or missing codes. Evaluation of ADC used in inspection system, even  not complete [ 3 ] is important ."}
{"pdf_id": "0803.0194", "content": "Fig.3.Waveform used for Synchronisation accuracy test  We call the coordinates of the line image memory corresponding to the fall and  rise fronts transition points .In the ideal case , transition points for every line of  information have the same value . Assuming that the Q transition points for the k line  of information are:  ( ) ( ),..., ), k m k (9)  we consider as a measure of synchronisation accuracy the following formula:"}
{"pdf_id": "0803.0822", "content": "From a user's perspective, hypertext links on the web page form a directed graph between  distinct information sources. A website is a collection of web pages forming a hierarchically  nested graph (see Figure. 1). A web site generally has a \"root page\" from which there should  be author-designed paths to all local content. However different users have different needs.  The same user may need different information at different times. A web site may be designed  in a particular way, but be used in many different ways. Therefore, it is hard to organize a  web site such that pages are located where users expect to find them."}
{"pdf_id": "0803.0822", "content": "In this paper, an algorithm is proposed to identify all the destination pages in a web site  whose location is different from the location where users expect to find them. The key insight  is that users will backtrack if they do not find the page where they expect it. The point from  where they backtrack is the Intermediate Reference Location (IRL) for the page. IRL's with  maximum hits will then be made to include navigation links to the destination page. It is also  worth mentioning that users may try multiple IRL for a destination page."}
{"pdf_id": "0803.0822", "content": "User navigational patterns can be studied from the web access-logs generated by the system.  Web access-logs record the access history of users that visit a web server. Web servers  register a web log entry for every single access they get, in which important pieces of  information about accessing are recorded, including the URL requested, the IP address from  which the request originated, and a timestamp. A sequential access-pattern is generated out of  these logs. A sequential access-pattern represents an ordered group of pages visited by users. Mining of these access-patterns will lead to the identification of user' behaviour and thus the  solution."}
{"pdf_id": "0803.0822", "content": "As mentioned earlier, web pages are linked together and users travel through them back and  forth in accordance with the links and icons provided. Therefore, some node might be visited  only because of its location, not content. Consequently, such backwards traversals should be  taken into consideration in the research to study user's behaviour."}
{"pdf_id": "0803.0822", "content": "Single Destination Page: Here the user is looking for a single destination page. It starts from  the root node. The user chooses the link that appears most likely to lead to Destination. If any  of the page is not the Destination, the user will backtrack and go to some other page that has  maximum probability."}
{"pdf_id": "0803.0822", "content": "Each web log entry represents each user's access to a web page and contains the user's IP  address, the Timestamp, the URL address of the requested object, and some additional  information. Access requests issued by a user within a single session with a web server  constitute a user's access sequence. These data sets commonly used for web traversal mining  are collected at the server-level, proxy-level or client-level. Each data source differs in terms  of format, accuracy, scope and method of implementation."}
{"pdf_id": "0803.0822", "content": "Client-level logs hold the most accurate account of user behaviour over www. If a client  connection is through an Internet Service Provider (ISP) or is located behind a firewall, its  activities may be logged at this level. The primary function of proxy servers or firewalls is to  serve both as a measure of security to block unwanted users or as a cache resource to reduce  network traffic by reusing their most recently fetched files. Their log files may include many  clients accessing many Web Servers. In the log files, their client request records are  interleaved in their received order. The process of logging is automatic and requires less  intervention."}
{"pdf_id": "0803.0822", "content": "The web access log can be specialized to different sets of patterns based upon the IP address  and Time stamp as shown in Figure 2. The last two blocks consists of entry for a single client sorted by the timestamp. These extracted patterns can then be indexed to a database or to  some temporary buffer for mining. Note that only the html pages are considered for the  research work. So, all the other objects (jpg, gif, etc.) accessed by the users are ignored from  the pattern."}
{"pdf_id": "0803.0822", "content": "to the next page. In that case the user might have used a navigation link or hit the back button  to go to the next page. In either of the case the page is either an IRL or a DL. Next, the  algorithm compares the time currently spent at the page with the threshold time. If the current  time spent is greater than the threshold, then the page is a Destination Location else an  Intermediate Reference Location."}
{"pdf_id": "0803.0822", "content": "The algorithm identifies the IRL that has maximum probability of attempt for any user. This  IRL can then be made to include navigation links to the destination page. The recommended  IRL now becomes one of the Actual Location for the Destination page. Other way is to  restructure the web site using a similarity matrix on these extracted pages."}
{"pdf_id": "0803.0822", "content": "Figure 5 shows the earlier website structure before optimization. The research was focus  around this level deep of pages and the pattern was gathered till this level. Users who process  their orders at the service pages are considered for this research. The service pages at Level 4  were considered as the leaf pages and thus the Destination Location. All other pages other  than the root page can be a Destination Location as per the analysis."}
{"pdf_id": "0803.0822", "content": "The user expects to find the \"Internet Services\" page in the \"Residential\" page or \"Small  Business\" page instead of \"Services\" page. Similarly, in other observations it is noticed that  users enters the \"residential\" or \"small business\" page and expects to find all the services  offered under that group. According to the experimental results, around 20% of the  destination pages have Intermediate Reference Locations different from their Actual  Locations. On an average each service page has thousands of visitors among which potential  users are in hundreds."}
{"pdf_id": "0803.0822", "content": "In this study, an algorithm is proposed for mining user navigational patterns through web  access-logs to the advantage of web site owner. The Intermediate Reference Locations and  the destinations are identified taking into account user identification, page viewing time, web  site viewing time, etc. The performance of the proposed algorithm is examined  experimentally with real and synthetic data."}
{"pdf_id": "0803.0822", "content": "As a future work, it will be interesting to explore if there are better approaches to identify IRL  and DL accurately. One suggested approach would be to analyse the content of web pages to  find out similarities. Finally, predictive analytics model can be used to better forecast specific  user action/behaviour from access-patterns."}
{"pdf_id": "0803.1087", "content": "by modern science is a gloomy one. In about 6 billion years, it will be the end of our solar  system, with our Sun turning into a red giant star, making the surface of Earth much too hot  for the continuation of life as we know it. The solution then appears to be easy: move.  However, even if life would colonize other solar systems, there will be a progressive end of  all stars in galaxies. Once stars have converted the available supply of hydrogen into heavier  elements, new star formation will come to an end. In fact, the problem is worse. It is  estimated that even very massive objects such as black holes will evaporate in about 1098"}
{"pdf_id": "0803.1087", "content": "irreversibly decay towards a state of maximum entropy [b, d]1. If this model is correct [c],  then it clearly means that the indefinite continuation of life is impossible in this universe [f].  What is the point of living in a universe doomed to annihilation? Ultimately, why should we  try to solve mundane challenges of our daily lives and societies, if we can not even imagine a  promising future for intelligent life in the universe? If we recognize this heat death [1.12],  then we should certainly do something to avoid it [1.13], and thus try to change the future of  the universe [1.14]."}
{"pdf_id": "0803.1087", "content": "insufficient because none of them presently allows the indefinite continuation of intelligent  life. We will instead argue that intelligent civilization will in the far future produce a new  universe [4.0]. Although it sounds like a surprising proposition, resembling science fiction  scenarios, we will consider it seriously and carefully."}
{"pdf_id": "0803.1087", "content": "universe is at odds with traditional science. Indeed, the modern scientific worldview has  often suggested that the emergence of intelligence was an accident in a universe that is  completely indifferent to human concerns, goals, and values (e.g. Weinberg 1993; Stenger  2007). I thus challenge this proposition, and another one that is commonly associated with it,  which says that: [a] intelligent civilization can not have a significant influence on cosmic  evolution."}
{"pdf_id": "0803.1087", "content": "activity could be in the far future, if intelligent civilization is to have influence on cosmic  evolution. It is increasingly clear that simulations and computing resources are becoming  main tools of scientific activity [1.15]. More concretely, at a smaller scale than the universe,  we have already begun to produce and \"play\" with artificial worlds, with the practice of  computer simulations. In particular, efforts in the Artificial Life (ALife) research field have  shown that it is possible to create digital worlds with their own rules, depicting agents  evolving in a complex manner. We will see that such simulation promise to become more and  more complex and elaborated in the future."}
{"pdf_id": "0803.1087", "content": "In the first part, we argue that the path towards a simulation of an entire universe is an  expected outcome of our scientific simulation endeavours. We then examine how such a  simulation could be realized (instantiated, made physical) and solve the irreversible heat death  of the universe, expected to happen at some future time."}
{"pdf_id": "0803.1087", "content": "also to link it to physical evolution (a level below) and to cultural evolution (a level above)  will be a long-term outcome of our scientific simulation endeavours. Such a simulation would  allow us to probe what would happen if we would \"replay the tape of the universe\". We then  discuss in more depth the status and potential usefulness of a simulation of an entire universe,  making a distinction between real-world and artificial-world modelling. We outline and  criticize the \"simulation hypothesis\", according to which our universe has been proposed to  be just a simulation. Let us first summarize the historical trend of exponential increase of  computing resources."}
{"pdf_id": "0803.1087", "content": "g-1). Let us illustrate it with some examples (Chaisson 2003, 96). A star has a value ~1, planets  ~102, plants ~103, humans ~104 and their brain ~105, current microprocessors ~1010.  According to this metric, complexity has risen at a rate faster than exponential in recent times  [1.20]. We might add along this complexity increase, the hypothesis that there is a tendency to  do ever more, requiring ever less energy, time and space; a phenomenon also called  ephemeralization (Fuller 1969; Heylighen 2007), or \"Space-Time Energy Matter\" (STEM)  compression (Smart 2008). This means that complex systems are increasingly localized in  space, accelerated in time, and dense in energy and matter flows."}
{"pdf_id": "0803.1087", "content": "which is analogous to energy in the organic world. The analogue of memory is the spatial  resource. The agents thus compete for fundamental properties of computers (CPU time,  memory) analogous to fundamental physical properties of our universe. This design is  certainly one of the key reasons for the impressive growth of complexity observed in this  simulation."}
{"pdf_id": "0803.1087", "content": "considered as different in nature. This important insight is just a first step towards bridging  physical, biological and cultural evolution [1.32]. The information-theoretic endeavours are  certainly going in this direction (e.g. (Von Baeyer 2004; Prokopenko, Boschetti, and Ryan  2007; Gershenson 2007; Floridi 2003) as well as \"Big History\" thinkers (e.g. Christian 2004;  Spier 2005)."}
{"pdf_id": "0803.1087", "content": "and cultural integration between the different disciplines involved. In such an endeavour,  human-made social and academic boundaries between disciplines of knowledge must be  overcome [1.31]. I proposed to construct integrative scientific worldviews (or philosophies)  with systems theory, problem solving and evolutionary theory   as three generic"}
{"pdf_id": "0803.1087", "content": "interdisciplinary approaches (Vidal 2008). There should be a seamless link between  simulations in physics, biology and social sciences (culture). If this would happen, we would  have the basic tools to work towards a model and a simulation of the entire universe [1.33;  2.0]. In fact the search for such bridges is obviously necessary if we want to tackle such  difficult problems as the origin of life, where we aim to explain the emergence of life out of  physico-chemical processes."}
{"pdf_id": "0803.1087", "content": "remain the same if the tape of life were replayed?\". Paraphrasing and extending it to the  universe, the question becomes: \"what would remain the same if the tape of the universe were  replayed?\". We should first notice that the tape metaphor has its limits. Indeed, if the tape and  its player were perfect, we should get exactly the same results when re-running the tape. Yet if  our universe self-constructs, one question is whether small fluctuations could lead to slightly  different outcomes, or very different ones if for example the system is chaotic."}
{"pdf_id": "0803.1087", "content": "universes. He considered four fundamental constants, and then analysed \"100 universes in  which the values of the four parameters were generated randomly from a range five orders of  magnitude above to five orders of magnitude below their values in our universe, that is, over a  total range of ten orders of magnitude\" (Stenger 2000). Anthony Aguirre did a similar work  by exploring classes of cosmologies with different parameters (Aguirre 2001). These  simulations are only an early attempt in simulating other possible universes, and the enterprise  is certainly worth pursuing, with more complex models, more parameters to vary, etc."}
{"pdf_id": "0803.1087", "content": "chosen to be modelled and the rest ignored. When in turn such a simplified model is run on  hardware that is significantly more computationally efficient than the physical system being  modelled, this makes it possible to run the model faster than the phenomena modelled, and  thus to make predictions of our world. The paradigm of Artificial Life (ALife) strongly differs from traditional modelling, by studying not only \"life-as-we-know-it\", but also \"life-as-it could-be\" (Langton 1992, sec. 1). We propose to extend this modelling technique to any process and not just to life, leading to the more general distinction of processes-as-we know them and processes-as-they-could-be (Red'ko 1999) . We call the two kinds of modelling  respectively real-world modelling and artificial-world modelling."}
{"pdf_id": "0803.1087", "content": "For what would an  artificial-world simulation of an entire universe be useful? We would be able not only to  \"replay the tape of our universe\", but also to play and replay the tape of other possible  universes (thus tackling limitations A1 and A2 explicated by Ellis) [2"}
{"pdf_id": "0803.1087", "content": "hardware running it, whatever the realistic nature of the simulation. From this point of view,  we can argue that it remains a simulation, and not a realization (Harnad 1994). Is there  another possibility for realizing the simulation of an entire universe? That is what we will  explore now."}
{"pdf_id": "0803.1087", "content": "intelligent life to survive forever. However, they assume the additional hypothesis that life  should take another \"information-like\" form. Krauss and Strakman (2000) showed that there  are serious difficulties to the scenario proposed by Dyson. The reversible computation  scenario is also not sustainable in the long run, since, as Krauss and Strakman argue, no finite  system can perform an infinite number of computations with a finite amount of energy.  Furthermore, these scenarios give no clear link to the increasing abilities of intelligent life to  model the universe, nor do they relate to the fine-tuning problem."}
{"pdf_id": "0803.1087", "content": "we can add the hypothesis that we are not alone in the universe...), we can see the HD  problem as the longest-term problem for intelligent life in the universe. How should we react  to it? Charles Darwin's thought on the HD problem remains perfectly relevant: \"Believing as I  do that man in the distant future will be a far more perfect creature than he now is, it is an  intolerable thought that he and all other sentient beings are doomed to complete annihilation  after such long-continued slow progress\" (Darwin 1887, 70)"}
{"pdf_id": "0803.1087", "content": "(CNS) in order to tackle the fine-tuning problem (Smolin 1992; 1997). According to this  natural selection of universes theory, black holes give birth to new universes by producing the  equivalent of a Big Bang, which produces a baby universe with slightly different physical properties (constants, laws). This introduces variation, while the differential success in self reproduction of universes via their black holes provides the equivalent of natural selection.  This leads to a Darwinian evolution of universes whose properties are fine tuned for black  hole generation, a prediction that can in principle be falsified."}
{"pdf_id": "0803.1087", "content": "extended ensemble called a multiverse. Although the idea of a multiverse is a speculative one,  it is increasingly popular among many cosmologists. New universes are generally theorized to  appear from the inside of black holes, or from the Big Bang itself [3.0; 3.1]. Kuhn (2007)  distinguished many kinds of multiverse models: by disconnected regions (spatial); by cycles  (temporal); by sequential selection (temporal); by string theory (with minuscule extra  dimensions); by large extra dimensions; by quantum branching or selection; by mathematics  and even by all possibilities, whatever this may mean. Among these multiverse theories,  Smolin's CNS is arguably the most scientifically testable (Smolin 2007)."}
{"pdf_id": "0803.1087", "content": "mentioned authors. Inspired by Smolin's terminology we could speak of a \"Cosmological  Artificial Selection\" (CAS), artificial selection on simulated universes enhancing natural  selection of real universes (Barrow 2001, 151). The biological analogy is interesting here.  Humans who practice artificial selection on animals do not \"design\" or \"create\" new  organisms, nor do they replace natural selection. They just try to foster some traits over  others. In CNS, many generations of universes are needed to randomly generate a fine tuned"}
{"pdf_id": "0803.1087", "content": "consider a general physics. As in ALife, this \"Artificial Cosmogenesis\" discipline would have  two parts. One focusing on \"software\" universe simulations using computer models  (analogous to soft ALife); the other focusing on implementing the software in reality  (analogous to strong/wet ALife). It it clear however that the analogue of soft ALife (universe  simulation) is only in its infancy, and the analogue of strong/wet ALife (universe realization)  lies in the far future."}
{"pdf_id": "0803.1087", "content": "universe: to continue to explore and understand the functioning of our universe so as to  possibly reproduce it in the far future [2.3; 4.0]. This would make the indefinite continuation  of life possible, yet in another universe [4.2]. This scenario aslo fits with the ultimate goal of  evolution as a whole: survival. It is likely to be a difficult and stimulating enough challenge to  encourage and occupy intelligent civilization for the foreseeable future."}
{"pdf_id": "0803.1087", "content": "discovered. For example, how much might the physical properties of our existing universe  (physics of black holes, etc.) constrain the realization of a new universe? Furthermore, the  issue of the ethical responsibility of humanity in this proposition is outside the scope of this  paper and remains to be explored (see however (Gardner 2003, Part 6) and (Smart 2008) for  two different viewpoints)."}
{"pdf_id": "0803.1087", "content": "science. We have outlined the fast-moving changes occurring in our universe, and argued that  the limit of scientific simulations is the simulation of an entire universe. Furthermore, we  have formulated an hypothesis that the heat death of complexity in our universe could be  avoided through an artificial cosmogenesis, a discipline analogous to artificial life."}
{"pdf_id": "0803.1087", "content": "This annex presents the logical structure of the main arguments presented in this paper  represented by two maps. The problem is mapped in Fig. 2. and the proposed solution in Fig.  3. For an easier back-and-forth between the paper and the maps, the blocks are numbered in  the map (letters for Fig. 2, and numbers for Fig. 3) and those numbers appear in bold in the  text."}
{"pdf_id": "0803.1087", "content": "Allowing the possibility of a constructive discussion of assumptions and deductions.  For example, a critique can say \"the core problem is not P but Q\"; or \"I disagree that  hypothesis [X.XX] leads to [Y.YY], you need implicit hypothesis Z, ...\" or \"hypothesis  [Z.ZZ] is wrong because\"; or \"there is another solution to your problem, which is...\"  etc."}
{"pdf_id": "0803.1087", "content": "To draw those maps we used some of the insights of Eliyahu Goldratt's Theory of Constraints  (TOC) and its \"Thinking Process\" (see Goldratt and Cox 1984; Goldratt Institute 2001;  Scheinkopf 1999). The TOC is a well proven management technique widely used in finance,  distribution, project management, people management, strategy, sales and marketing . We see  it and use it as part of a generic problem solving toolbox, where causes and effects are  mapped in a transparent way. In our paper, the core problem is: \"how to make the indefinite  continuation of life possible?\"; and the proposed solution is that \"intelligent civilization can  reproduce the universe\". In this TOC framework, three fundamental questions are employed to tackle a problem:"}
{"pdf_id": "0803.1087", "content": "(1) Has the right problem been identified?  (2) Is this solution leading us in the right direction? (3) Will the solution really solve the problems? (4) What could go wrong with the solution? Are there any negative side-effects? (5) Is this solution implementable? (6) Are we all really up to this?"}
{"pdf_id": "0803.1457", "content": "This is why computer scientists, used to  think in terms of data structures, have early defended the use of diagrammatic  representations, for instance in problem solving, on the basis of the fact that these  representations were better adapted to specific domains (see [1] for an historical survey  and critiques of logicist AI)"}
{"pdf_id": "0803.1457", "content": "diagrammatic representations have long suffered from their reputation as mere tools in  the search for solutions. At the beginning of the 90's, Barwise and Etchemendy (B&E)  have strongly denounced this general prejudice against diagrams ([2], [3], [4]). To cope  with complex situations, they defended a general theory of valid inferences that is  independent of the mode of representation, and these works lead on the first  demonstration that diagrammatic systems can be sound and complete [5]."}
{"pdf_id": "0803.1457", "content": "linguistic form of representation, and, to quote B&E, \"human languages are infinitely  richer and more subtle than the formal languages for which we have anything like a  complete account of inference. [...]. As the computer gives us ever richer tools for  representing information, we must begin to study the logical aspects of reasoning that  uses nonlinguistic forms of representation\" [2]."}
{"pdf_id": "0803.1457", "content": "diagrammatic inferential systems, and add some comments about an example of human  hybrid reasoning in a mastermind game. In the next section, we will give some  arguments for the systematic study (and use) of HRS in AGI and cognition modeling,  and some hints for their usefulness in program specification and semantics."}
{"pdf_id": "0803.1457", "content": "In [2], B&E emphasized that the main properties of diagrammatic systems derive from  the existence of a syntactical homomorphism between icons and represented objects. In  many cases, this homomorphism yields to a very strong property called closure under  constraints. In closed under constraints systems, the consequences of initials facts are  included de facto in the representation and do not require extra computation. This  makes these systems very efficient. As we have underlined in [6] and [7], this also  shows a deep duality between two modes of reasoning."}
{"pdf_id": "0803.1457", "content": "initial properties of objects; (2) an explicit representation of abstract properties (or  relations among objects); and (3) a computational mechanism linking the two sources  of information (to establish the validity of a non-explicit consequence). Thus, by  construction, such systems require calculations. For instance, if you know that Ann is  on the left of Gaston on a bench, and that Gaston is on the left of Isabel, you need to  add that the relation \"be on the left of\" is transitive to prove that Ann is on the left of  Isabel."}
{"pdf_id": "0803.1457", "content": "representation of such abstract properties, because these properties are taken  automatically into account by syntactic constraints on the representation itself. In our  example, an iconic representation of the first fact will look like the (left) juxtaposition  of two symbols (say, A for Ann and G for Gaston, as in: A G); and the second fact will  yield to the juxtaposition of a third symbol (say, I for Isabel), as in: A G I."}
{"pdf_id": "0803.1457", "content": "without any computation. Since many consequences automatically appear on  representations, diagrammatic systems provide an easy treatment of conjunctions and  are computationally very efficient. Unfortunately, they have difficulties with  disjunctive casesi. Alternatives may require the use of several diagrams, which must  then be traversed one after the other, as in the linguistic case1. Note also that in many  diagrammatic systems, each representation corresponds to a genuine situation, and that  contradiction is impossible to represent (which can be good or bad depending on what  you need to represent)."}
{"pdf_id": "0803.1457", "content": "now, and IMM is still puzzling. We think that it could be sometimes linked to the  syntactic homomorphism, because our personal conclusion is that the main distinction  between linguistic (or symbolic) representation systems and analogical representation  systems (as diagrammatic systems) must be characterized in terms of the power of the  meta-language required to provide the semantics of the system. In the analogical case,  the metalanguage needs to reference syntactical properties of the object language, while  in the symbolic case, this is not obligatoryiv."}
{"pdf_id": "0803.1457", "content": "shortly comment a game of one player (grid on Figure 1). The grid ensures the  memorizing of preceding results, but, as we will see, it is also a geometrical support for  organizing proof and backtracking. Our player separates her game in two phases: first  determining the colors, and then determining the places. In both phases, she uses"}
{"pdf_id": "0803.1457", "content": "configuration of pawns. The second player can then dispose on a grid a tentative configuration of pawns, and  the leader replies by posting pins (on the right) indicating if and how pawns correspond to the solution one's.  A white pin means a good position and color for one pawn, and a black one a misplaced color. The rows  remain visible during the game, and the player has to find out the solution with a limited number of rows."}
{"pdf_id": "0803.1457", "content": "representations that can be qualified as mental models because they are very similar to  those of Johnson-Laird [15]. The interesting fact here is that these models (which also  correspond to LARS of S&O) are ordered both by increasing order of specificity, and  by decreasing order of probability. This makes backtracking easier, since the model  considered next is determined, and guarantees a quick convergence to the solution,  since these models are in decreasing order of probability."}
{"pdf_id": "0803.1457", "content": "possible replies revealed being statistically more informative than those of other colors  distributions (such as 3/2, 4/1, 5, 1/1/3 or 1/1/1/2, etc.). Given the pins on the right side,  she considers first the interpretation displayed on Figure 2, i.e. that one blue is placed  correctly, one yellow misplaced, and that there is no red. (She might take in his hand a  blue and a yellow pawn to help memorizing, and note mentally that the three colors are  exhausted)."}
{"pdf_id": "0803.1457", "content": "the notion of exhaustion introduced by Johnson-Laird. (Note however that the model  behind the schema of Figure 2 is more specific, since it includes some information on  places, but in this first phase of the game, the player does not pay much attention to  them). Then, she plays the second row, trying new places for blue (anticipation on  future reasoning about blue places), and introducing a new color: orange. By luck, both  orange and blue are missing colors, and the interpretation of the second row is obvious.  Blue being excluded, she switches to a new model based on a new interpretation of the  first row: [1 Y] 1R."}
{"pdf_id": "0803.1457", "content": "directions (both grounded on the grid): (1) a left-to-right orientation of the possible  models within a row, and (2) the natural vertical ordering of the rows. This systematic  ordering helps remembering which model has to be consider next in case of backtrack.  This global strategy applies as well in the second phase of the game. Here for instance,  the ordering on the first row is:"}
{"pdf_id": "0803.1457", "content": "prevent here from incoherence, instead of introducing errors (as many people claimed  they merely do). Here this is due to the use of limited abstraction diagrams in which  contradiction is impossible to represent. Furthermore, partially because of the  specificity property mentioned in the first section, LARS appear to be good candidates  for ordering models by inclusion. Models may also be orderly among other dimensions,  by using probabilities or other specific attributes."}
{"pdf_id": "0803.1457", "content": "necessarily to be handle. In the domain of reasoning, the objection that situations in  which a unique homomorphism applies are rare is as well not too serious, because you  can use several homomorphisms. The situation is just that the subsystems denote  different properties of models or objects, and what expresses in one subsystem do not  express necessarily in the other. Nevertheless, some information can be transfer from  one system to another (on the basis of safe correspondences), endowing the global  system with superior inferential and computational capacities. And there is no special  need of an intermediate language."}
{"pdf_id": "0803.1457", "content": "We also believe that the addition of iconic features in theoretical languages  or tools could bring major advances in other fields of Computer Science, less  concerned by world representations, as for instance, in the domain of semantics of  programming languages, or in software design in general"}
{"pdf_id": "0803.1457", "content": "more specifically the nature of the relation between language and thought, the goal is to  develop a model of language understanding and use that attains observational  adequacy, i. e. that is able to pass the Turing test. To achieve this goal, we must aim  higher, by trying to reach explanatory adequacy, that is, to develop a model of how the  system can reasonably acquire the \"knowledge\" (i. e., systems of knowledge/belief,  etc.) that enables it to attain observational adequacy."}
{"pdf_id": "0803.1457", "content": "This is because of the way the world is (it is  rich and varied, and the basic conceptual apparatus needed to represent time and  temporal relations, for instance, must use different resources obeying different  constraints than that needed to represent spatial relations, or interpersonal relations and  other minds, or causal interactions, etc)"}
{"pdf_id": "0803.1457", "content": "with a set of procedures for developing and enhancing the innate basis. While some of  these are no doubt domain-specific, others must be domain-independent. We  hypothesize that the human mind starts life with an innate basis for domain-specific  knowledge that is more analogical or diagrammatic in nature, and that one of the  important ways it develops is in the enrichment of the innate representational capacities  with more symbolic representational capacities5."}
{"pdf_id": "0803.1457", "content": "needs to solve, choosing from a repertoire of representational capacities that include  more analogical and more symbolic notations is more flexible, hence more \"intelligent\"  (more apt to solve its problems, hence to survive). We postulate that humans have this  kind of mind. To handle this ability to choose between several representational  capacities, and to keep its repertoire relatively unchanged (after a certain level of  development), a mind needs also to have generic and global cognitive procedures to  construct representations on the fly."}
{"pdf_id": "0803.1457", "content": "of transfer from a source (or base) to a target. The capacity of organisms to carry out  such projections lies at the heart of cognition in its many forms. The analyses given by  Fauconnier are numerous and based on a rich array of linguistic data (counterfactuals;  time, tense, and mood; opacity; metaphor; fictive motion; grammatical constructions;  and quantification over cognitive domains). Further developments of the theory study  another very interesting operation, conceptual blending [21], which also depends  centrally on structure projection and dynamic simulation. Like standard analogical  mapping, blending aligns two partial structures, but in addition, blending projects"}
{"pdf_id": "0803.1457", "content": "obviously be use to handle some notion of focus. Focus theories have not yet been  successfully design, but it is a lack in our theoretical tools. There are many fields where  some notion of focus would be of great help (in perception theory, in discourse theory,  etc.). One reason of this failure might be precisely that the theories of focus require  references to the underlying computational mechanism (as reflective properties of the  programming language)v."}
{"pdf_id": "0803.1457", "content": "required to provide the semantics of a system has to reflect (in some way) the  possibilities of configurations of terms in the representational language, then we have  to investigate the following questions: what syntax do we need to easily provide the  semantics of HRS? Would it be enough to add simple reflective and local graphical  feature (as those of some of our programming languages) to a traditional functional and  symbolic language, or should this syntax be trickier?"}
{"pdf_id": "0803.1457", "content": "Works done so far on diagrammatic reasoning provide fragments of evidence about  how people use iconic representations, and identify some of the problems raised by the  project of AGI. Yet, there is still much to do to understand the variety of forms in  which information can stored and manipulated in intelligent control systems. We  believe that we could make important progress in studying in details the relation  between iconic and symbolic features in hybrid representation systems, as well as in  paying attention to them in the theoretical tools and symbolic languages that we use."}
{"pdf_id": "0803.1457", "content": "diagrams, and we will see some exemplars in the next section (see Figure 5). It is also possible to have iconic  symbols of second order in purely diagrammatic systems. C.S. Peirce first suggested to represent disjunctions  in the form of a line connecting two iconic symbols. But in a formal system, the introduction of such symbols  requires the definition of transformation rules on diagrams."}
{"pdf_id": "0803.1457", "content": "level (in the graphic server itself), in order to link the keyboard (and/or events on the pointer of the mouse) to  a particular window. The development of graphical interfaces (and networks) has introduced considerable  changes in the previous programming framework. (1) There are other sources of input than letters (at least,  mouse inputs), and other sorts of output (graphics, sound). (2) The input/output data are of distinct nature, but  they may be link together in the system (as the mouse and the screen). (3) The sharing of input/output devices  by several programs adds some additional complexity to the emerging framework."}
{"pdf_id": "0803.1500", "content": "This paper describes  NCore, presents and analyzes its architecture, tools and services;  and reports on the experience of NSDL in building and operating  a major digital library on it over the past year and the experience  of the Digital Library for Earth Systems Education in porting  their existing digital library and tools to the NCore platform"}
{"pdf_id": "0803.1500", "content": "1. INTRODUCTION  The National Science Digital Library (NSDL) project [33] was  created by the National Science Foundation \"to provide organized  access to high quality resources and tools that support innovations  in teaching and learning at all levels of science, technology,  engineering, and mathematics education.\" The NSDL Core  Integration team at Cornell University designs and implements  the technical infrastructure and tools for the library. Its mission is  both to create the best possible library for NSDL and to push the  frontiers and capabilities of digital library technology."}
{"pdf_id": "0803.1500", "content": "As part of that mission, the Cornell team has created a new, open source, digital library platform called NCore (for NSDL Core).  This platform consists of a central repository, based on  Fedora[19], a data model and API that define the structure of the  library in the repository, and a growing suite of library tools and  services that mediate among users, information providers, and the  central repository. Since January 2007, NCore has supported the  production library activities of NSDL."}
{"pdf_id": "0803.1500", "content": "While the initial application of NCore is the implementation of  NSDL, the platform itself is not specific to NSDL or to STEM  education. Instead, it is an architecture and software ecosystem  that can support digital library and digital repository needs  ranging from cultural heritage materials in the arts and  humanities, to scholarly communication and collaboration, to  education at every level and in every discipline. Work has already  begun on using the open-source release of NCore to catalog and  manage the teacher training resources at a major urban public  school system and to serve as the central repository and digital  library platform for an alliance of eleven major research libraries."}
{"pdf_id": "0803.1500", "content": "2. RELATED WORK  This paper builds on extensive work over the past seven years in  creating NSDL. Work on the first version of the NSDL  architecture, a metadata-based union-catalog paradigm, was  described in [15], and a discussion of the design and motivation  for the second major version of the NSDL architecture, NSDL  2.0, from which NCore derives, is presented in [16, 18]. Earlier  related work on annotation systems, resource linking, and the  importance of context for learning is extensively discussed and  cited in [16] and will not be repeated here. Earlier work on the  role of collections and aggregations in digital libraries is cited  extensively in the section below on organizing the repository."}
{"pdf_id": "0803.1500", "content": "There is a large body of previous work on digital library platforms  and the closely related area of institutional repository platforms.  Significant open-source digital library platforms in wide  production use include Fedora[19], Greenstone[31], DSpace[30],  and EPrints[23]. Compared to the latter three, by building on top"}
{"pdf_id": "0803.1500", "content": "of Fedora, NCore inherits many of Fedora's key advantages: an  open architecture and data model; a highly flexible architecture of  relationships among digital objects in the model; and the easy  ability to extend the repository, metadata, relationships, and  content types. Compared to the base Fedora system, a middleware  package that requires extensive development to create an end-user  accessible tool, NCore provides a specific data model, organizing  relationships, and a wide suite of extensible tools and services.  Like Fez [13], NCore is built on Fedora, but it is much more of an  extensible and integrated platform of digital library tools than  Fez, which is designed as a digital repository management and  workflow system."}
{"pdf_id": "0803.1500", "content": "3. NCORE: THE CENTRAL CORE  At the heart of the NCore platform lies the Fedora-based  repository, the data model and digital objects that define the  content of the library, and the relationships that organize the  materials and provide both structure and context. Real life, real  resources and real information are never neat and hierarchical.  Instead they form a complex web of relationships and bits of  information with varying degrees of certainty. NCore is designed  both to capture and represent this chaotic reality, and to make it  accessible to users and other services in ways that enable  discovery, usability, and understanding."}
{"pdf_id": "0803.1500", "content": "3.1 The Repository and Data Model  A full description of the initial repository architecture of NCore  can be found in [16, 17], but we will briefly review the key  concepts here. The rest of section 3 will discuss changes to the  architecture and implementation as a result of two years of  development and production experience since the initial report."}
{"pdf_id": "0803.1500", "content": "author, title, audience); an aggregation object that  collects resources and other aggregations together in a set; a  metadata provider object, a special type of aggregation object that  aggregates and provides provenance information for metadata  objects, and an agent object that specifies the source for metadata  statements and the selector for aggregations"}
{"pdf_id": "0803.1500", "content": "Faced with the prospect of managing this multi-sourced and  potentially user-contributed context, the topics of access and  control become particularly relevant. How can a library curator  retain editorial control over which user-contributed content is  considered to be \"in\" the library's public face? How can this"}
{"pdf_id": "0803.1500", "content": "content be incorporated into library services in a way that  provides additional value rather than additional noise? In fact,  many challenges of next generation digital libraries can be framed  in terms of management and interpretation of aggregations. Thus,  there is a strong case for designing digital library infrastructures  with aggregations as first-class objects. The NSDL has adopted  this approach in its design of NCore, where aggregations occupy a  central role in representing and mediating context within the  repository."}
{"pdf_id": "0803.1500", "content": "3.3 Defining and Characterizing Aggregations  The word \"collection\" as it applies to digital libraries can seem  familiar, ambiguous, and loaded at the same time. There is much  literature in which the term's meaning is assumed to be  understood, yet in those instances where a \"collection\" is defined,  it is not always defined consistently, nor do these definitions  always share the same characteristics [10, 20, 25]."}
{"pdf_id": "0803.1500", "content": "Static virtual collections are  taken to imply a long-lasting assembly of resources for a  particular purpose oriented towards some community, whereas  dynamic are taken to be user-created aggregations that support a  particular task or reflect an individual's view of current library  contents for some duration of time"}
{"pdf_id": "0803.1500", "content": "At this point, it makes sense to consider the distinction between  an aggregation and a collection. As previously noted, the term  \"collection\" in a digital library sense implies a certain degree of  semantic meaning or intent. \"Aggregation\", on the other hand,  tends to imply merely an assembly of items and nothing more.  For the purposes of this paper, an aggregation is defined as a  named set of digital library objects, where digital library objects  may  be  primary  digital  content  (resources),  metadata,  aggregations, or agents. In this light, a collection is an instance of  an aggregation that carries with it some specific semantics."}
{"pdf_id": "0803.1500", "content": "Through the experience of developing the NCore architecture, we  have come to appreciate aggregations as one of the fundamental  building blocks for various structures found in the library,  collections being only one example. As such, we have identified  some relevant characteristics to successfully engineering working  structures out of aggregations:"}
{"pdf_id": "0803.1500", "content": "3.4.2 Multiple categorization  Although nested aggregations may be used to create hierarchical  structure, nested aggregations do not imply a hierarchical  structure. Indeed, in an environment such as NSDL, where many  independent agents have the ability to create new aggregations,  the resulting structure is far from hierarchical. A hierarchy  implies that each member has exactly one parent. In order to  support  multiple  agents  creating  their  own  orthogonal  organizational structures across a shared set of resources, some  resources and aggregations must be members of more than one  aggregation."}
{"pdf_id": "0803.1500", "content": "There is also strong case that allowing objects to be a member of  multiple aggregations is a powerful tool to hand to users. Karger  and Quan [11] argue that multiple-categorization is more valuable  to users organizing data than are hierarchies, and find that users are generally \"less inhibited\" in doing so. Indeed, with multiple categorization, assigning a resource to a particular aggregation  does not come at the cost of removing it from another."}
{"pdf_id": "0803.1500", "content": "3.4.3 Complex objects  Complex objects are single entities that are composed of multiple  parts, each of which is an entity in and of itself. In order to  support complex objects in a digital library, it is necessary to  demarcate the \"boundary\" around a set of resources and  manipulate that composite as a first-class object. Buchanan et al.  [4] describe these as composite aggregations, and note that they  represent a particularly difficult class of aggregation that is  problematic in the few digital library systems that support them."}
{"pdf_id": "0803.1500", "content": "The importance of aggregations in defining complex objects is  recognized not only in the digital library context as in [4], but also  plays an important role in efforts such as OAI-ORE  (http://openarchives.org/ore/) that focus on exchange and  interoperability. With that in mind, complex objects may  currently be represented in the NCore model as an aggregation  containing the constituent members on an ad-hoc basis. At"}
{"pdf_id": "0803.1500", "content": "present, the NSDL is awaiting the formal OAI-ORE specification  and related discussion to inform further development of complex  object support. While it is certain that complex objects will be  based on aggregations, to truly support them in an interoperable  fashion is likely to require representing additional semantics on  top of the base NCore model, perhaps in the form of specific  object properties, relationships, or metadata."}
{"pdf_id": "0803.1500", "content": "3.5 Semantics of Aggregations  There is overwhelming consensus on the importance of metadata  to describe the semantics of collections [2, 8, 10]. Since  aggregations themselves are devoid in semantics (but rich in  context), it is apparent that the ability to describe aggregations  with metadata is necessary. Meghini and Spyratos[4] characterize  aggregations in terms of extension (the set of objects within it)  and intension (the meaning of the aggregation, as differentiates it  from others and specifies a homogeneity criterion for the  resources within it). In that sense, in the NCore model,  aggregations themselves exclusively represent extension, while  aggregation  (collection)  metadata  statements  exclusively  represent intension."}
{"pdf_id": "0803.1500", "content": "While it is important to have the ability to describe the intension  of an aggregation, we have found that it is equally important not  to require it, nor to require a particular standard of quality or  completeness. In a sense, this echoes the sentiment of [8], in that  for certain tasks, such as organization of resources as encountered  in personal information management, ease of use is the dominant  requirement. Indeed, any description of an aggregation a user  provides is likely to be very different from metadata describing a  curated collection. Folksonomic tagging[9] is perhaps an  appropriate example of a form of lightweight metadata that  describes an aggregation in a meaningful way to a user."}
{"pdf_id": "0803.1500", "content": "3.5.1 Property/membership duality  There is more than one way to classify a resource. There exists an  uncomfortable duality between aggregations and metadata  properties when either membership in an aggregation or a  metadata property are able to achieve the same goal of  classification[8, 25]. For example, is it better create an  aggregation of resources that conform to a particular educational  standard, or is it better to create metadata for each resource saying  so directly?"}
{"pdf_id": "0803.1500", "content": "consensus[8]. For aggregation membership, however, there is no  ambiguity. Children of nested aggregations are defined to be  related to their ancestors by transitive membership. NCore  services such as search make use of this definition, and allow  selection of all resources that are \"under\" (i.e. related via direct or  transitive membership) a given aggregation. While all the  implications of this are out of the scope of this paper, the concept  of membership inheritance is important for using aggregations to  demarcate \"areas\" in the repository in a scalable fashion by  building them from nested aggregations rather than individually."}
{"pdf_id": "0803.1500", "content": "Figure 1 illustrates a forest of nested aggregations in an NCore  repository. For example, Region I represents part of the content  and structure of the NSDL \"Whiteboard Report\" publication.  Individual articles R1 and R2 are aggregated into Issue 42, which  in turn is a member of the overall \"Whiteboard Report\"  aggregation. Considering membership as a transitive relation,  each of R1, R2, and Issue 42 are members of the \"Whiteboard  Report\" aggregation, and also members of the top-level \"NSDL  Collection\"."}
{"pdf_id": "0803.1500", "content": "metrics between aggregations or between items and aggregations.  Renda et al.[26], for example, provide an algorithm for  calculating the \"centroid\" of the terms found in the documents  within an aggregation, and are able to compare this with the terms  found in any given document. The degree of match is used to  determine if a particular resource is similar to the resources within  the aggregation, and thus a candidate for recommendation."}
{"pdf_id": "0803.1500", "content": "NSDL has not yet implemented any such recommender services,  but has identified this as an area for future research and potential  implementation. In encouraging the creation and use of aggregations in NCore and its related tools, and by soliciting user provided content, NSDL has ensured that the platform fully  supports these potential extensions."}
{"pdf_id": "0803.1500", "content": "3.7 Motivating Users to Create Aggregations  As mentioned in the previous section, user-created aggregations  can add significant value to the library, leveraging the collective  intelligence of the users to enhance services for browsing and recommendation, among others. But how do these user contributed aggregations make it into the repository? Why would  a user want to organize library resources into aggregations in the  first place? What's in it for the user?"}
{"pdf_id": "0803.1500", "content": "These tools aggregate  user contributions by source, so, for example all a user's blog  posts may fall into an aggregation, or the resources mentioned in  a blog post may be aggregated together, as well as by the  structure imposed by the particular tool, so that all posts to a  specific blog or category may form an aggregation"}
{"pdf_id": "0803.1500", "content": "Personal information management is another means by which  user-contributed data may find its way into the library. Borgman  et al.[3] found that personal digital libraries were not only useful  for geography faculty to collect and organize resources for their  teaching or research, but also in providing resources and context  to the library."}
{"pdf_id": "0803.1500", "content": "NSDL is currently investigating how best to incorporate personal  bookmarking/tagging systems, such as Connotea, del.icio.us, and  Technorati, into NCore. In such a system it would be easy to  create an aggregation composed of all the resources bookmarked  by a single user, or all those tagged with a particular folksonomic  tag."}
{"pdf_id": "0803.1500", "content": "Application developers and contributors to the library can also  benefit from creating aggregations in the library. Doing so can  expose the aggregation in search and browse interfaces.  Aggregations can also be used to \"brand\" resources as part of a  particular collection. Several NCore tools (see section 5) can be  used to create and manage such collections in the repository."}
{"pdf_id": "0803.1500", "content": "The content and organization contributed by these users and  applications via aggregations may be incorporated by the library  at will to support or enhance library services such as multi-faceted  browsing, presenting the context around a resource, or the  creation of personalization or recommendation services"}
{"pdf_id": "0803.1500", "content": "As first-class objects, membership in an aggregation is separate  from the metadata properties that may describe a resource in the  library. Access to an aggregation's members or parents can be  achieved in a uniform fashion, and may be subject to universal  rules regarding consistency and permissions. The NCore model  and API implements all these characteristics in the context of a  Fedora repository. It provides a read/write API to the underlying  objects, specifically treats aggregations as first-class objects with  requisite functions to manipulate them, and provides a security  and referential integrity model for aggregation membership."}
{"pdf_id": "0803.1500", "content": "In conjunction with a consistency and permissions model, such as  that provided by NCore, aggregations may be used to mediate the  contributions of individual agents in a repository and enable  building a cohesive library from these disparate pieces. As  mentioned earlier in section 3.5.2, aggregations may be used to  define the boundaries around \"areas\" in a repository. For this  purpose, recall that aggregation membership is considered to be a  transitive property. Aggregations, then, may be used to define the  boundaries of a digital library itself within a repository."}
{"pdf_id": "0803.1500", "content": "For example, one may consider a library to be defined as  composed of the objects specifically in the library and those  specifically considered not in the library, where membership in  both sets implies not in. Two aggregations, combined with  transitive membership, can realistically be expected to completely  represent the boundaries of a digital library in terms of the  resources within it. In a more general sense, aggregations may be  used for defining arbitrary \"views\" of content within a repository."}
{"pdf_id": "0803.1500", "content": "NSDL, for example, may be defined as an aggregation  representing the extent of the library. Within this aggregation are  the aggregations of all the collections that are considered to be  part of NSDL. Implicitly, these collection's members are also  considered to be part of NSDL by transitive membership. This  implicit membership is important, since it eliminates the need for  every item to be explicitly added to the NSDL library  aggregation. Without it, such definition would not be scalable or  maintainable."}
{"pdf_id": "0803.1500", "content": "Referring again to Figure 1, the entirety of the NSDL library is  represented as the area underneath the \"NSDL Collection\"  aggregation, denoted as region II. As is evident, there are only  two direct members of the NSDL aggregation—all items  underneath these two are members of the \"NSDL Collection\"  aggregation due to the transitive nature of membership."}
{"pdf_id": "0803.1500", "content": "This is a form of delegated authority that arises  when one mixes aggregations of different ownerships, and is a  motivation for creating an explicit \"not in NSDL\" aggregation  where the curation policy for NSDL may not match the curation  policies of those collections operating under delegated authority"}
{"pdf_id": "0803.1500", "content": "While NCore allows such aggregations of metadata, fully  supporting these to create independent views of the library is  dependent on indexing services (see section 4.3). We are currently  investigating appropriate index strategies that would fully support  filtering search queries by both resource and/or metadata  aggregation at the same time."}
{"pdf_id": "0803.1500", "content": "4. NCORE: BACK-END SERVICES  A major challenge for NCore was the need to support a highly  robust and scalable digital library platform. To support the needs  of NSDL, NCore must provide 7x24 operation; high availability  and quick recovery; security, authentication and authorization;  support for one of the largest Fedora repositories currently in  production; and an automated workflow capable of handling over  150K resource updates per month with minimal staff intervention."}
{"pdf_id": "0803.1500", "content": "4.1 The Production NSDL Data Repository  NSDL on the NCore platform is currently in production and  accessible to the end user through http://nsdl.org. As of January  21, 2008, the library contained 3.02 million resource objects, 2.3  million metadata objects, 990 aggregation objects, and 816  agents. To support the high availability requirements of NSDL,  the production system makes use of a Fedora-level transaction  journaling system developed by the Cornell NSDL team.  Transactions on the repository are replicated in real time to two  \"follower\" systems, ensuring minimal downtime for all updates  and failures."}
{"pdf_id": "0803.1500", "content": "The metadata harvesting and ingest process creates an  aggregation of all the resources associated with a particular  metadata provider, and a separate aggregation of all the metadata  objects. These aggregations can overlap with other existing  library aggregations, for example when two metadata providers  both describe the same web resource. Since an OAI-PMH  metadata provider is defined by the organization, the OAI server,  and the particular set, arbitrarily granular collections can be  created for a single organization or OAI server."}
{"pdf_id": "0803.1500", "content": "The search service can filter resource search results based on  aggregation membership, allowing a single search service to  support multiple \"views\" of the library at the resource level. It is  also possible to use the search service to obtain metadata-level  \"views\" of the library by including or excluding specific metadata  providers and their associated aggregations of metadata (see  section 3.9). However, each such view currently requires building  a separate search index."}
{"pdf_id": "0803.1500", "content": "The search index is currently updated nightly using incremental  harvest from the repository's OAI provider feed. While  satisfactory for OAI harvested collections, the delay is  undesirable for resources and relationships created by the new  NCore interactive front-end tools. Work is underway to support  very fast incremental updates to the search index."}
{"pdf_id": "0803.1500", "content": "5. NCORE: FRONT-END TOOLS  The quality and flexibility of user-facing tools is critical to  achieving the goal of creating a collaborative digital library.  Fortunately, the Web 2.0 phenomenon has unleashed a flood of  open-source tools that specifically support user contribution and  collaboration, with the goal of building value by harnessing the  collective intelligence of the users of the Web."}
{"pdf_id": "0803.1500", "content": "The NCore development team has sought to leverage existing  general open-source Web 2.0 tools (e.g. blogs, wikis) as well as  specialized tools (e.g. course management systems, learning  module creation tools) by writing simple plug-in extensions that  integrate these tools into the NCore platform. By minimizing the  development effort required to integrate a tool into NCore, the  team has maximized the quality, range and impact of the tools  that are being made available."}
{"pdf_id": "0803.1500", "content": "To support user authentication for the front-end tools, NCore  makes use of a highly scalable sign-on system using the Internet2  Shibboleth technology (http://shibboleth.internet2.edu/). In its  implementation for NSDL, the primary identity provider for  community sign-on is operated by Columbia University as part of  NSDL Core Integration. However, the tools and authentication  will operate with any appropriate Shibboleth identity provider."}
{"pdf_id": "0803.1500", "content": "5.1 The NSDL.org Web  The primary public channel for access to NSDL and the contents  of the NSDL repository is through the web portal at nsdl.org. The  site supports several different access mechanisms to NSDL  resources and metadata. The search and search results interface  provides a number of specialized audience views of all the  materials in the repository that have been chosen to be \"in\" the  library. \"More info\" and \"resource page\" views of resources  provide a complete picture of all the information that is known  about a resource: collection membership, metadata statements and  relationships to other resources. The \"resource page\" views are  also indexed by Google and other search services."}
{"pdf_id": "0803.1500", "content": "Other user interface views of the library include browsing by  subject, collection, and Science Literacy Maps4, which allow  teachers and students to graphically explore the space of  interrelated STEM concepts, associated educational standards and  benchmarks, and the library resources related to those concepts  and standards."}
{"pdf_id": "0803.1500", "content": "5.2 Expert Voices: Blogging in NSDL  Expert Voices was developed as a collaborative tool to increase  community contributions to the library, relate library resources to  real-world science events, and provide context for science  resources in the library. Expert Voices provides the infrastructure  for engaging teachers, scientists, librarians, and students in  conversations about STEM topics. As an integrated component of  NCore, Expert Voices makes it easy for users to find content from  the library, and it allows them to exchange ideas and point each  other to useful online materials."}
{"pdf_id": "0803.1500", "content": "There are a number of models for making use of Expert Voices  blogs within NSDL. These include the discovery team model, in  which teams of teachers, scientists, and media specialists blog  about science discoveries and real-world science applications; the  classroom model, where teachers use blogs to create lesson plans  for their students, and students then use them for writing and  collaboration [27]; the community model, where members of a  particular science and education community present news, discuss  topics of interest, and promote educational outreach; and the  research dissemination model, where a particular research team  uses the blog to present ongoing research activities, research  results, and links to publications and related work."}
{"pdf_id": "0803.1500", "content": "Blogging provides a low barrier opportunity for time-constrained  teachers to connect to busy scientists. Scientists, in turn, can share  their knowledge and zeal through a blog, using it to debate the  results of studies or events in real time, organize information, and  relate their work to background materials, relevant areas of  science, and the real world[28]."}
{"pdf_id": "0803.1500", "content": "Expert Voices has many individual blogs on a variety of topics,  designed for various audiences. To help visitors find posts of  interest, the home page of the Blogosphere has a section  displaying blog titles by audience, another for posts by topic or  category, and a section displaying the more recent posts in Expert  Voices. Because the system is built on popular blogging  software, the basic functionality is familiar to the average blog  user. Experienced visitors use their favorite news reader to point  to specific blog RSS newsfeeds. There is also a plug-in for email  subscription for those not as comfortable with RSS newsfeeds."}
{"pdf_id": "0803.1500", "content": "Expert Voices is built using a standard, open-source blogging  system (WordPress MultiUser5) and supports blogging standards,  themes, templates, and plug-in functionality. In addition to being  able to add and edit blog content, authorized contributors can also  add new resources to NSDL, embed links in their blog entries to  new and existing NSDL resources, and add metadata to resources,  all via custom WordPress plug-ins. These plug-ins utilize publicly  available NSDL REST-based web services: the NSDL search  service and the NDR API"}
{"pdf_id": "0803.1500", "content": "Expert Voices forms a collection or aggregation, and each blog is  an aggregation whose members are individual blog entries. When  a blog post is published to the NDR, the blog author can either  reference existing NSDL resources within the post, optionally  adding new metadata, or they can create new resource entries in  the library by adding a reference to the resource together with  basic resource metadata (see figure 2). Within the NDR, the blog  entry serves as an annotation of the resources it references. It also  imposes a human-created inferred relationship among all the"}
{"pdf_id": "0803.1500", "content": "5.3 The NSDL Wiki  The NSDL Wiki is the second major collaborative tool to be  integrated  into  NSDL.  The  core  MediaWiki  software  (http://mediawiki.org) is used by millions of Wikipedia users and  contributors every day. It provides a familiar functionality of  collaborative authoring using a simplified markup language,  hyperlinks, and user categories to create and modify wiki pages.  In addition to the default wiki functionality, the NSDL Wiki  provides the ability to add newly created wiki pages to the NSDL  Data Repository as resources with simple structured metadata (see  figure 3)."}
{"pdf_id": "0803.1500", "content": "Users or groups can also use the wiki pages to collect and  organize NSDL resources for information dissemination or for  teaching. A wiki editor can directly reference NSDL resources as  well as pages from other wikis or the web. These organizational  pages can, in turn, be added back to the library as new  aggregations of the resources they reference. The aggregations are  then available as part of the library, accessible through nsdl.org,  the search service and NDR API, for other users to discover and  repurpose."}
{"pdf_id": "0803.1500", "content": "6. IMPLEMENTING DLESE IN NCORE  The Digital Library for Earth Systems Education (DLESE) is a  long-standing and successful effort to create a community digital  library of geoscience materials [21]. Over the past eight years, in  addition to the resources and metadata in the library itself, the  project has created a significant and valuable infrastructure of  tools, processes, and standards for metadata and collections to  support the library."}
{"pdf_id": "0803.1500", "content": "In 2007, DLESE was challenged to come up with a sustainability  model that would free the project from needing to run on  dedicated hardware and software systems. To achieve this, the  DLESE project and its partners at Digital Learning Sciences  decided to implement DLESE on the NCore platform, and to  potentially migrate the entire existing library, its processes,  services, resources, and metadata, into the NSDL Data  Repository. This would allow DLESE to implement their  community library model through a standard hosted web site  linked to the data, services and tools hosted on the NCore  platform by NSDL Core Integration, dispensing with DLESE's  dedicated  hardware,  software,  and  associated  system"}
{"pdf_id": "0803.1500", "content": "The primary metadata format used to describe resources in NSDL  is a specific implementation of qualified Dublin Core called  nsdl_dc. DLESE metadata is stored in two separate formats:  ADN6 and dlese_anno. DLESE provides a crosswalk from ADN  to nsdl_dc, but significant information, particularly the support  for DLESE's community review process provided in the  dlese_anno format, is lost in the crosswalk."}
{"pdf_id": "0803.1500", "content": "Since metadata objects in NCore can support multiple  independent metadata datastreams, the DLESE team simply  added datastreams to support ADN and dlese_anno to the  metadata object. This allows DLESE-specific processes to access  the ADN and dlese_anno streams while maintaining full  compatibility with all existing NCore tools and services."}
{"pdf_id": "0803.1500", "content": "6.2 Implementing DLESE Tools and Services  The most critical end-user functionality of DLESE is the search  service. This service takes full advantage of the detailed  categorization of DLESE resources represented in the ADN  metadata, as well as the teaching tips, reviews, editor's summaries  and other information represented in dlese_anno, to allow detailed  searching and filtering. The crosswalk to nsdl_dc does not provide  enough information to support this service, and DLESE's ability  to use the NCore API to store and access this metadata was  critical."}
{"pdf_id": "0803.1500", "content": "In fact, no change to the DLESE search service code was needed.  Since the DLESE search service runs directly from index files  built from the DLESE system, it was only necessary to write a  process that built the index from the NDR using the API. After an  initial upload of the DLESE information to the NDR and creation  of the index, the search service was fully functional."}
{"pdf_id": "0803.1500", "content": "The other key DLESE tool is the Digital Collection System  (DCS)7. This is a flexible, XML-driven cataloging tool to create  and manage metadata for educational resources, as well as  providing collection workflow processes. Most of the work in  embedding DLESE in NSDL was in rewriting the DCS system to  use the NDR API to access the DLESE ADN and dlese_anno  metadata and to create and manipulate the digital objects needed  to support the DLESE data model in NCore."}
{"pdf_id": "0803.1500", "content": "Since the DCS is an XML-driven system, once the changes were  made to access and manipulate NCore digital objects through the  NDR API, it was relatively easy to replace the existing DLESE  metadata XML schema with an XML schema for nsdl_dc. At that  point, the DCS became the NCS (NSDL Collection System), and  the tool could be used to manipulate arbitrary collection and item  metadata in the NSDL Data Repository. The NSDL project is  currently in the process of replacing its former collection  management system with NCS. And, as part of NCore, NCS will  be available as a metadata management and cataloging tool to  support any project using the NCore platform."}
{"pdf_id": "0803.1500", "content": "6.3 Viewing DLESE in NSDL  As it happens, the scope of the DLESE materials falls fully within  the scope of NSDL. However, the aggregation and view model of  NCore allows complete flexibility in the membership of resources  in NSDL and in DLESE. The \"DLESE view\" can include only the  materials uploaded and managed by DLESE, or it can also include  other NSDL aggregations. The \"NSDL view\" can include all or  only some of the DLESE collections, since aggregations can be  explicitly included or excluded from the NSDL view of the  library. It would even be possible to run DLESE as a completely  independent digital library from NSDL within the same NCore  instance of the repository."}
{"pdf_id": "0803.1500", "content": "Proposed new near-term development work on the NCore  platform includes: an NCore toolkit providing Java, PHP, and  Javascript tools to support the easy integration of 3rd party  software with NCore; the ability to harvest RSS feeds, together  with a system to allow individual users or organizations to  register feeds for ingest into the library; and extensions to  integrate NSDL with existing open-source course management  systems, either Moodle, Sakai, or both."}
{"pdf_id": "0803.1500", "content": "8. CONCLUSION  NCore implements a flexible, extensible platform for creating a  new kind of digital library that integrates the best features of  traditional libraries with the collaborative tools of Web 2.0 to  empower the collective creation of library materials and context  by any community in any discipline. NCore has already demonstrated the ability to integrate different off-the-shelf open source tools and to support different digital libraries. The flexible  architecture and implementation of aggregations has been one key  to the power and versatility of the NCore platform."}
{"pdf_id": "0803.1500", "content": "NCore provides a compelling suite of data models, services, and  end-user tools combined with the proven ability to support a  large, production digital library. It serves as both a model for digital library architectures and implementations and as an open source platform on which digital library creators can build their  own production systems. Finally, NCore embodies a vision of a  new generation of collaborative, community-driven digital  libraries that fully integrate with all the tools, infrastructure, and  social and informational networks of the World Wide Web."}
{"pdf_id": "0803.1500", "content": "9. ACKNOWLEDGMENTS  This material is based upon work supported by the National  Science Foundation under Grants No. DUE-0733600, 0424671,  0227648, and 0227888. The authors wish to gratefully  acknowledge the efforts and support of the DLESE/DLS projects  and development team, with particular thanks to Tamara Sumner,  Michael Wright, Kathryn Ginger, Jonathan Ostwald, and John  Weatherley. Thanks are also due to the entire NSDL Core  Integration team at Cornell, UCAR, and Columbia. Finally,  particular thanks go to James Blake, Tim Cornwell and Carl  Lagoze for their contributions to this paper and the research  described herein."}
{"pdf_id": "0803.1500", "content": "[3] Borgman, C.L., Smart, L.J., Millwood, K.A., Finley, J.R.,  Champeny, L., Gilliland, A.J. and Leazer, G.H. Comparing  faculty information seeking in teaching and research:  Implications for the design of digital libraries: Research  Articles. Journal of the American Society for Information  Science and Technology, 56 (6), 2005. 636-657. Available at  http://dx.doi.org/10.1002/asi.v56:6"}
{"pdf_id": "0803.1586", "content": "Abstract—We present the SAMMI lightweight object detection method which has a high level of accuracy and robustness, and which is able to operate in an environment with a large number of cameras. Background modeling is based on DCT coefficients provided by cameras. Foreground detection uses similarity in temporal characteristics of adjacent blocks of pixels, which is a computationally inexpensive way to make use of object coherence. Scene model updating uses the approximated median method for improved performance. Evaluation at pixel level and application level shows that SAMMI object detection performs better and faster than the conventional Mixture of Gaussians method."}
{"pdf_id": "0803.1586", "content": "Transient objects are considered foreground. A foreground object may be stationary for part of the recording, while the background may contain movement, e.g. a swaying tree. The paper is organized as follows. In section II, previous work in the field is described. In section III, a general overview of the system and context in which the spatio-activity based object detection operates is given. In section IV, we present the details of our SAMMI (Spatio-Activity Multi-Mode with Iterations) method. Finally, in sections V and VI, we evaluate the method and draw conclusions."}
{"pdf_id": "0803.1586", "content": "sufficient without defining a further relationship between the pixels. The most obvious relationship between pixels is based on the visual characteristics of the pixels, such as color. Such relationships are complex, e.g. because of texture, and also computationally expensive. This approach depends very much on the progress in still image segmentation."}
{"pdf_id": "0803.1586", "content": "The underlying assumption is that for a given DCT block at a given point in time in an image sequence, a mode is more likely to be a match if the adjacent DCT blocks match modes that were created at a similar time to when that mode was created"}
{"pdf_id": "0803.1586", "content": "Mode persistence is used to improve classification. While some object detection applications focus on tracking moving objects, other applications have a greater need for stable and consistent detection of stationary objects. By including a probability measure that is added to the match probability for modes seen within the last few frames, this trade-off can be adjusted by users of the system. Low (or zero) contributions from mode persistence result in better detection of moving objects. Increasing the mode persistence probability results in more stable and consistent stationary object detection, which also reduces the impact of noise eroding a stationary object."}
{"pdf_id": "0803.1586", "content": "in systems where the system is allocated a fixed maximum amount of memory, e.g. in the context a bigger system where a large number of cameras is supported. In addition, more modes means more processing power is needed. A maximum number of modes may be introduced to make the system performance feasible and predictable. The second reason is regardless of the availability of system resources. Modes must be removed from the system in order to reduce the probability of new objects being matched to unrelated mode models. Determining when to remove a mode is a trade-off decision. If modes are removed too soon, objects that are occluded"}
{"pdf_id": "0803.1586", "content": "Like other object detection algorithms, the SAMMI algo rithm has general applicability. Whether the produced output is good in a relative or absolute sense depends on the context in which it is used. The requirements for object detection in an intruder alert system are very different from those in a people counting application. Similarly, a system that alerts a security guard will give a higher penalty to false alarms than a system that does event-based recording. We evaluate the system output at two levels:"}
{"pdf_id": "0803.1586", "content": "than pixel, viz. 8x8 blocks. Hence, it is not possible for our method to score the maximum on this level, while pixel-based algorithms could theoretically reach a score of 100%. Also, the problem of inconsistency in ground truths mentioned before may not even allow a perfect segmentation algorithm to score 100%."}
{"pdf_id": "0803.1586", "content": "Computationally inexpensive background modeling can be done without a significant penalty in accuracy. The use of DCT information without transforming image information to the pixel domain still allows for good accuracy while making significant savings in resource usage. The use of a fast approximated median method makes the modeling robust to noise in bright and dark regions of a scene, while it isfaster than the conventional exponential moving average ap proach. Fragmentation noise is reduced by several iterations of neighbor adapted classification based on temporal coherency of objects.Another advantage of the SAMMI system is its config urability. Users can configure the trade-off between detecting new moving objects and existing stationary objects using the"}
{"pdf_id": "0803.1586", "content": "active mode bonus. Similarly, users can make trade-offs for removing modes by specifying the minimum percentage of time a part of the scene must remain visible to retain its temporal information. The spatial processing outlined in this paper allows for a greater variability in the size of objects, particularly small objects, that can be successfully detected. The filtering of local noise in the image sequence that would otherwise cause spurious blobs to be detected is embedded within the scene modeling process. Through low resource usage while preserving acceptable accuracy, the lightweight object detection method presented in this paper increases the feasibility of deploying video analysis systems in the real world."}
{"pdf_id": "0803.2220", "content": "and poorer performance in certain tasks. To clarify this aspect, we compare our engine with other well-known inverted file-based IR systems (like Terrier) and discuss the results of this comparison. The rest of this paper is organized as follows: Section 2 describes the overall architecture of the engine. Section 3 describes brieny each component. Section 4 reports experimental results, and finally, Section 5 concludes the paper and identifies issues for further work and research."}
{"pdf_id": "0803.2220", "content": "The crawler roams the web, identifies all the hyperlinks in each page and adds them to a list of URLs to visit. URLs are then recursively visited accordingto a set of policies. Currently, three traversal policies are supported: Breadth first (BFS), Depth-first (DFS) and Depth-within-site (DWS). Crawler can be configured to download only files of a specific type (e.g. html, pdf, rdf) as well as to ignore others based on extension (e.g. *.tmp). The identification of files is based on extension and on content for dynamic web pages. Furthermore it is compatible with the Robots Exclusion Protocol1 to ignore specified files or"}
{"pdf_id": "0803.2220", "content": "The Lexical Analyzer plays a major part in the pre-processing of the documents. It is responsible for converting a string of characters into a stream of tokens. Most IR systems use single words as terms. The Lexical Analyzer is called by the indexer for each document, with its file type and encoding as parameters. After processing the document it returns a hash map that contains all document's words, along with their frequency and position. The process of document analysis can be divided in the following steps:"}
{"pdf_id": "0803.2220", "content": "reduction caused by stemming) and 3435040 occurences (28.8% reduction caused by stopwords). That function also approximates (ACC = 0.996) a power law but with slightly decreased exponent, i.e. 1.18. Although the log-log distributions of both functions follow a power law, we observe a top concavity deviation, frequently met on many datasets[6]."}
{"pdf_id": "0803.2220", "content": "The Indexer iterates through all the records of the Document Index and uses the Lexical Analyzer component to create a hash table that contains the words and their exact positions for each document in the Repository. The index is built on top of a DBMS (in particular over PostgreSQL 8.3). The database schema can be seen in Table 5. The use of a relational DBMS is motivated by the following facts:"}
{"pdf_id": "0803.2220", "content": "The Ranker provides a number of link analysis techniques. At first it constructs a directed graph where each node represents a fetched document and the edges of each node represent the corresponding hyperlinks of that document. The graph is constructed using the IDs and the out-links of the fetched documents that are stored in the Document Index (derived by the Cralwer). It implements the PageRank [5] ranking algorithm and the resulting ranks are stored in the rank"}
{"pdf_id": "0803.2220", "content": "The final step of the retrieval process is the presentation of the results. Contrary to popular web search engines, Mitos computes all the results at once. For each page in the results, a small surrogate is presented, including the title of the page and a short excerpt that we call best text. This excerpt should ideally contain all words of the query. To find such query-dependent excerpts Mitos keeps a copy of the full text of the pages (in addition to the index) at a cost of extra storage"}
{"pdf_id": "0803.2220", "content": "The engine was developed as a student project in the IR course (CS463) at the Computer Science Department of the University of Crete in two semesters (spring2006 and spring 2007). Many thanks to all students that have contributed: Evan gelos Boutsakis, Nikos Dimaresis, Stefanos Dubulakis, Dimitra Emmanouilidou, Manos Frantzolakis, Giorgos Georgopoulos, Katerina Gkirtzou, Nikos Grispos,Nikos Kampitakis, Kostas Kapakiotis, Stelios Kapetanakis, Giorgos Konstan tinidis, Manos Kritsotakis, Michael Markogiannakis, Antonis Melakis, Yiannis Papadakis, Kostas Perakakis, Kyriakos Sidhropoulos, Apostolos Stamou, Manos Tavlas and Axilleas Tziatzios."}
{"pdf_id": "0803.2363", "content": "Image segmentation is the basic approach in image pro cessing and computer vision [22]. It is used to locate specialregions and then extract information from them. Image segmentation is used to partition an image into different com ponents or objects and is an essential procedure for image preprocessing, object detection and extraction, and objecttracking. Image segmentation is also related to edge detec tion.Even though there is no unified theory for image seg mentation , some practical methods have been studied overthe years such as thresholding, edge based segmentation, re gion growing, clustering (unsupervised classification), and"}
{"pdf_id": "0803.2363", "content": "The maximum entropy method was first proposed by Ka pur, Sahoo, and Wong [15]. It is based on the maximization of inner entropy in both the foreground and background. The purpose of finding the best threshold is to make both objects in the foreground and background, respectively, as smooth as possible. [15] [22] [1] If F and B are in the foreground and background classes,respectively, the maximum entropy can be calculated as fol lows;"}
{"pdf_id": "0803.2363", "content": "Even though we calculated the entropy or variance ineach connected component that is different from the standard maximum entropy and the Otsu's method in image seg mentation, the philosophy remains the same as in these two popular methods. The results are very promising. Thesetwo new methods can be easily applied in other region growing segmentations. A large amount of further research should be done to support and the new methods. We will implement the method proposed in subsection E in section III, and compare it with the results obtained in [11]."}
{"pdf_id": "0803.2812", "content": "Linear high dynamic range images can beconstructed using Spatially Varying pixel Ex posures (SVE) technique, proposed in [11], [12].This technique allows to construct high dy namic range images using information fromthe neighbour pixels. When a pixel is satu rated in the acquired image, it is likely to have a neighbour pixel that is not. Analysing the neighbour pixel's values, it is possible to construct a high dynamic range image. Such image is non-linear, hence linearization of theconstructed SVE image is necessary. Lineariza tion of a constructed SVE image is performed using correction coefficients that are obtained at the preliminary stage of calibration."}
{"pdf_id": "0803.2812", "content": "the correction coefficients must be calculated in order to compensate non-linearity of the SVEimaging system. The linear part of the radio metric function is fitted to a line aT +b, where T is an exposure time (see Fig. 2). The accuracy offitting a line to the experimental data is signif icant: slight deviation of a line produces greaterrors on the reconstructed images. The Trust Region [13], [14] fitting algorithm was used"}
{"pdf_id": "0803.2812", "content": "nomial is fitted to the data obtained at the calibration stage. Thus an unknown correction coefficient can be calculated for almost any non-linear data value of the SVE constructed image. It is significant to estimate the accuracy of the reconstructed images due to complexity of the reconstruction process. The quantitative results of the reconstruction and linearization of the SVE images are provided below."}
{"pdf_id": "0803.2812", "content": "The high dynamic range scene was created for the optical experiments. The photo of the test scene is presented in Fig 4 (image is scaled down to 8-bit and contrasted for publishing). Scene's background is a light-absorption fabric, and the test image is illuminated by LED lamp. The properties of the lightsources used in this work as well as transmittance coefficients are described in Table 1. It should be noted that transmittance coefficients for Bayer mosaic are obtained for used in this work commercial digital camera Canon EOS 400D."}
{"pdf_id": "0803.2812", "content": "The test image consists of binary graphics,periodical elements, textual elements of dif ferent size, and gradient bars. Gradient bars are used for the estimation of the halftone stability of the reconstructed images. The test image was captured by the digital camera with an exposure time varied from 1/4000 to 2 seconds. All captured images were processed by DCRAW [17] converter in the \"document"}
{"pdf_id": "0803.2812", "content": "Reconstructed images using only first ex tra pixels are characterised by linear dynamic range of 71-84 dB and the NRMS error between the original image and reconstructed images of 5-10% (see Fig. 6). Such NRMS error isconsidered as acceptable for practical applica tions in optical-digital imaging systems. For the reconstruction process there were used around 87% of first extra pixels. Using first and second extra pixels it is possible to reconstruct images with dynamic range of 87-95 dB. The NRMS error between the original image and reconstructed images is around 11-15%. There were used 96-98% of"}
{"pdf_id": "0803.2812", "content": "The halftone stability of the reconstructed images was evaluated as well. From Fig. 7 it can be noted that images with dynamic range more than 84 dB are characterised by less stable halftone relations. Instability of the halftonerelations in the range of 85 to 90 dB can be ex plained by transition to the second extra pixels usage. It also should be noted that halftones on the red-illuminated images are more dense, i.e., recovered image became darker than the"}
{"pdf_id": "0803.2812", "content": "But when second extra pixels are used there are observed significant NRMS error and halftones destabilisation (see Fig. 9 and Fig. 10).Although the dynamic range of such recon structed images is more than 85 dB, the NRMS error is 20-35%. Thus for the green light is needed more sophisticated algorithm in orderto provide better images stability. As it men tioned above in this subsection, it is difficult to"}
{"pdf_id": "0803.2812", "content": "Obtained experimental results for green light, which are summarized in Table 3, allowto argue that using SVE technique it is possible to reconstruct oversaturated images to lin ear high dynamic range images with dynamic range up to 80 dB and NRMS error less than 7%. However further increasing of dynamic range is required more sophisticated algorithm for image's reconstruction."}
{"pdf_id": "0803.2812", "content": "Images were reconstructed using only first extra pixels (green in this case). Reconstructed images are characterised by linear dynamic range of 70-88 dB and NRMS error between the original image and reconstructed images of 9-15%. Such NRMS error is large enough and may lead to degradation of the reconstructed image. In Fig 11 is presented recovered image with bright spots (probably due to parasitic renection from the laser printer's toner of the printed test image). Less than 58% of first extra pixels were used for the reconstruction."}
{"pdf_id": "0803.2812", "content": "range of 90-95 dB. The NRMS error between the original image and reconstructed images is around 11-18% (see Fig. 12). There were used 94% of the first extra pixels and 88% of thesecond extra pixels to reconstruct such over saturated images. From Fig. 13 it can be noted that images with dynamic range more than 88 dB are characterised by less stable halftone relations."}
{"pdf_id": "0803.2812", "content": "It can be noted that using first extra pixels one can reconstruct oversaturated images tolinear high dynamic range images with dy namic range up to 88 dB and NRMS error less than 15% (see Table 4). Increasing dynamic range using first and second extra pixels can produce images with less stable halftone."}
{"pdf_id": "0803.3192", "content": "IEC RELATED WORK  IEC is an optimization technique based on evolutionary  computation (genetic algorithm, genetic programming, evolution  strategy, or evolutionary programming) and used when it is hard  or impossible to formalize efficiently the fitness function (the  method that gives the performance of a solution to a given  problem) and where the fitness function is therefore replaced by a  human user"}
{"pdf_id": "0803.3192", "content": "Subsequently, much work was done in the area of computer  graphics: for instance using IEC for optimizing lighting  conditions for a given impression [1], applied to fashion design  [9], or transforming drawing sketches into 3D models represented  by superquadric functions and implicit surfaces, and evolving  them by using divergence operators (bending, twisting, shearing,  tapering) to modify the input drawing in order to converge to  more satisfactory 3D pieces [12]"}
{"pdf_id": "0803.3192", "content": "the obligation to evaluate manually all the individuals of each  generation [14, 16]. For instance, most often the user is asked to  give a mark to each individual or to select the most promising  individuals according: it still requires active time consuming  participation during the interaction. The number of individuals of  a classical IEC is about 20 (the maximum that can be represented  on the screen), and about the same for the number of generations."}
{"pdf_id": "0803.3192", "content": "However, some tricks are used to overcome those limits, e.g.,  trying to accelerate the convergence of IEC by showing the fitness  landscape mapped in 2D or 3D, and by asking the user to  determine where the IEC should search for a better optimum [6].  Other work tries to predict fitness values of new individuals based  on previous subjective evaluation. This can be done either by  constructing and approaching the subjective fitness function of the  user by using genetic programming [4] or neural networks, or also  with Support Vector Machine [10, 11]. In the latter case,  inconsistent responses can also be detected thanks to graph based  modeling."}
{"pdf_id": "0803.3192", "content": "Nonetheless, previous work is mostly algorithmic-oriented and  not really user-oriented, which seems to be the future domain for  IEC [13, 16]. In the next section, we will present material that can  be combined with Interactive Evolutionary Computation in order  to significantly reduce the active participation of the user during  the evaluation process and to consequently reduce considerably  the fatigue of the user and the slowness of IEC approaches."}
{"pdf_id": "0803.3192", "content": "3.2 How to use an eye-tracker in IEC?  If we consider that either phenotype or genotype of individuals  are graphically displayable on a screen, we can easily envisage  using an eye-tracker during the evaluation process of IEC. Our  proposal consists in using this hypothesis: the more an individual  is examined, the better the fitness of this particular individual will  be. So, a new evolutionary algorithm called Eye-Tracking  Evolutionary Algorithm (E-TEA) is proposed:"}
{"pdf_id": "0803.3192", "content": "4. APPLICATION TO THE INTERACTIVE  ONE-MAX OPTIMIZATION PROBLEM  Our optimization problem will be borrowed from [3] where the  One-Max problem is considered as an interactive optimization  problem in order to compare Interactive Genetic Algorithm (IGA)  and Human-Based Genetic Algorithm (HBGA), and also in order  to demonstrate the advantages of using HBGA. Recall that the"}
{"pdf_id": "0803.3192", "content": "classical One-Max optimization problem consists in maximizing  the number of 1s in a string of bits (0 or 1). It is the simplest  optimization problem and it is used here in order to parameterize  our system. In the next paragraph, we will verify whether one-max  optimization could be adapted to RGB colors. Then we present  our interactive one-max problem."}
{"pdf_id": "0803.3192", "content": "4.1 One-max optimization vs. color  optimization  In this section, we try to show that one-max optimization is rather  equivalent to white color optimization in the RGB model even if it  is not the best choice. Three distances for an objective fitness  have been proposed [3]:"}
{"pdf_id": "0803.3192", "content": "When the user estimates he has finished watching solutions of a  generation, we give him the possibility to click on his preferred  color among the 8 presented. In that case, the estimated fitness is  empirically cubed. The user also has the possibility to choose  none of them. Thus, in Figure 2, we can see that during only the  first 9 iterations colors are converging towards brighter colors."}
{"pdf_id": "0803.3192", "content": "4.3 Results  For the moment, it is difficult to give significantly quantitative  results in so far as the application developed is only restricted to  the use of a mouse and movements the user would give to it in  order to simulate an eye-tracker. It is tedious work, but, we can  say that it is easier to only move the mouse than to choose and  click on the most promising individuals, or to evaluate them. In  the future, it should be faster because interactions would be only  with the eyes of the user. We estimate doubling, at a minimum the  number of iterations in the Interactive Evolutionary Computation  exploring a larger search space."}
{"pdf_id": "0803.3192", "content": "instance, when the number of transitions between individuals  is seriously decreasing or when the total time used to watch a  generation is also decreasing, there is a chance that the user  is bored. A pause can be made and the interactive  evolutionary algorithm can be resumed later. However, the  time used to watch individuals could be interpreted  differently: the user is quickly converging toward a very  good solution. More research has to be done to detect this  fatigue.  Of course, each new system has its drawbacks, but they are few  compared to the advantages:"}
{"pdf_id": "0803.3192", "content": "In this article, we have presented a new algorithm that should  considerably improve the speed of Interactive Evolutionary  Computation. To do so, we have presented the Eye-Tracking  Evolutionary Algorithm (E-TEA) that uses an eye-tracker in order  to minimize user interaction for evaluating individuals. We have  tested the approach by simulating an eye-tracker with a mouse  during an interactive one-max optimization problem. The user had  to move the mouse exactly to where he is interested by an  individual. The only difference with a real eye-tracker is the loss  of crucial information about cognitive intensity represented by the  pupil diameter. Nonetheless, we are convinced that time taken  during the evaluation process can be significantly reduced."}
{"pdf_id": "0803.3363", "content": "The results of the performance evaluation using the test dataset in IV-B derived from the network models in IV-A are demonstrated. Let's start with the first class of the network models (real organization) and learn the implication of the method. Fig.3 shows the precision (p), recall (r), and F measure (F) in the trial where the experimental condition is that the node nCS10 in the model (A) is the target covert node to discover"}
{"pdf_id": "0803.3501", "content": "The role of the decision support system (DSS) is to provide a decision-making support to the actors in order to assist them during a crisis case. The DSS allows also managers to anticipate the occur of potential incidents thanks to a dynamic and a continuous evaluation of the current situation. Evaluation is realised by comparing the current situation with past situations stored in a scenarios base. The latter can be viewed as one part of the knowledge we have on the specific domain. The DSS is composed of a core and three parts which are connected to it (figure 1):"}
{"pdf_id": "0803.3501", "content": "• A set of user-computer interfaces and an intelligent interface allow the core to communicate with the environment. The intelligent interface controls and manages the access to the core of the authenticated users, filters entries information and provides actors with results emitted by the system; • An inside query MAS ensures the interaction between the core and world information. These information represent the knowledge the core need. The knowledge includes the scenarios, that are stored in a scenarios base, the ontologies of the domain and the proximity measures; • An outside query MAS has as role to provide the core with information, that are stored in network distributed information systems."}
{"pdf_id": "0803.3501", "content": "Information are coming from the environment in the form of semantic fea tures without a priori knowledge of their importance. The role of the first layer(the lowest one) is to deal with these data thanks to factual agents and let emer gence detect some subsets of all the information [7]. More precisely, the set of these agents will enable the appearance of a global behaviour thanks to their interactions and their individual operations. The system will extract thereafter from this behaviour the pertinent information that represent the salient facts of the situation."}
{"pdf_id": "0803.3501", "content": "The role of the synthesis agents is to deal with the agents emerged from the first layer. Synthesis agents aim to create dynamically factual agents clusters according to their evolutions. Each cluster represents an observed scenario. The set of these scenarios will be compared to past ones in order to deduce their potential consequences."}
{"pdf_id": "0803.3501", "content": "Finally, the upper layer, will build a continuous and incremental process of recollection for dynamic situations. This layer is composed of prediction agentsand has as goal to evaluate the degree of resemblance between the current sit uation and its associate scenario continuously. Each prediction agent will be associated to a scenario that will bring it closer, from semantic point of view, to other scenarios for which we know already the consequences. The result of this comparison constitutes a support information that can help a manager to make a good decision."}
{"pdf_id": "0803.3501", "content": "To formalise a situation means to create a formal system, in an attempt to capture the essential features of the real-world. To realise this, we model the world as a collection of objects, where each one holds some properties. The aim is to define the environment objects following the object paradigm. Therefore, we build a structural and hierarchical form in order to give a meaning to the various relations that may exist between them. The dynamic change of these objects states and more still the interactions that could be entrenched between them will provide us a snapshot description of the environment. In our context, information are decomposed in atomic data where each one is associated to a given object."}
{"pdf_id": "0803.3501", "content": "An internal automaton describes the behaviour and defines the actions of the agent. Some indicators and an acquaintances network allow the automaton operation, that means they help the agent to progress inside its automaton and to execute actions in order to reach its goal. These characteristics express the proactiveness of the agent."}
{"pdf_id": "0803.3501", "content": "• Initialisation state: the agent is created and enters in activities; • Deliberation state: the agent searches in its acquaintances allies in order to achieve its goals; • Decision state: the agent try to control its enemies to be reinforced; • Action state: it is the state-goal of the factual agent, in which the latter demonstrates its strength by acting and liquidating its enemies."}
{"pdf_id": "0803.3501", "content": "ATN transitions are stamped by a set of conditions and a sequence of actions. Conditions are defined as thresholds using internal indicators. The agent must validate thus one of its outgoing current state transitions in order to pass to the next state. The actions of the agents may be an enemy aggression or a friend help. The choice of the actions to perform depend both on the type of the agent and its position in the ATN."}
{"pdf_id": "0803.3501", "content": "Factual Agent Indicators The dynamic measurement of an agent behaviour and its state progression at a given time are given thanks to indicators. These characters are significant parameters that describe the activities variations of each agent and its structural evolution. In other words, the agent state is specified by the set of these significant characters that allow both the description of its current situation and the prediction of its future behaviour [4] (quoted above). Factual agent has five indicators, which are pseudoPosition (PP), pseudoSpeed(PS), pseudoAcceleration (PA), satisfactory indicator (SI) and constancy indi cator (CI) [8]. The \"pseudo\" prefix means that these indicators are not a real"}
{"pdf_id": "0803.3501", "content": "PP, PS and PA represent thresholds that define the conditions of the ATN transitions. The definition of this conditions are specified to a given application. As shown in the previous formulae, only PP is specific. However, PS and PA are generic and are deduced from PP. SI and CI are also independent of the studied domain and are computed according to the agent movement in its ATN."}
{"pdf_id": "0803.3501", "content": "The paper has presented a decision support system which aims to help decision makers to analyse and evaluate a current situation. The core of the system rests on an agent-oriented multilayer architecture. We have described here the first layer which aims to provide a dynamic information representation of the current"}
{"pdf_id": "0803.4074", "content": "The operator arg in Equation (6) means that the medoid is the node nj belonging to ck, which maximizes M(ck, nj). The quantity M(ck, nj) in Equation (6) represents the total degree of resemblance of one artwork nj to the other artworks in the cluster ck. It is defined by Equation (7)."}
{"pdf_id": "0803.4074", "content": "The operator arg in Equation (9) means the following. The maximal value of W(nPIDi, nj) is searched for among all the artworks nj belonging to the cluster ck. The primary cluster cPRM(nPIDi) is the cluster that gives the maximal value of max W(nPIDi, nj) among the clusters ck. W(nPIDi, nk) in Equation (9) represents the strength of the preference of the subject nPIDi to the artwork nk. It is defined by Equation (10)."}
{"pdf_id": "0803.4074", "content": "The operator arg means that nGTW|PRM(PIDi) is the artwork that gives the maximal value of W(nPIDi, nk) among nk belonging to the primary cluster cPRM(nPIDi). There may be multiple gateway artworks. Links are drawn between the subject and the gateway artworks in the primary cluster. The secondary cluster cSCN(nPIDi) is calculated by Equation (13)."}
{"pdf_id": "0803.4074", "content": "Finally, links are drawn between the disjoint clusters so that the switch object nSWTi can connect the subject nPIDi and the gateway artwork in the secondary cluster nGTW|SND(nPIDi), as in Figure 1 [b]. The preference diagram uses the spring model [Fruchterman 1991] as a graph-drawing method. The spring model converts the strength of the relationship across the link between two nodes into Hooke's constant of the spring, which is placed between the nodes imaginarily, and calculates the equilibrium position of the nodes."}
{"pdf_id": "0803.4074", "content": "The experiment was carried out according to the renection process described in 2.3. Fifty artworks (classical portraits, landscapes, abstract paintings, modern pop art) are used in Q1 in Figure 2. Thirty-two subjects participated in the prior stage. The coordinator generated preference diagrams as presented in 2.2. The main stage was carried out three separate times, with four, two, and five subjects. It took sixty to ninety minutes to finish the main stage. The four diagrams that include the cluster structures were presented in the part 1 group discussion. Finer granularity diagrams (the number of clusters |c|=3, 5) and courser granularity diagrams (|c|=7, 8) were presented at the same time. The subjects could recognize the primary clusters, compare the details of the diagrams, and"}
{"pdf_id": "0803.4253", "content": "In this section we present a very simple implementation of the alternated propagation search phases to solve Su-Doku puzzles as CSP. This is for illustrative purpose and by no means the only way to implement propagation and search, or to strike a balance between propagation and search in CSP solutions. Some of the ideas here are inspired by [15], and, for lack of a better name, we simply call this algorithm the PS-1-2 algorithm."}
{"pdf_id": "0803.4253", "content": "Propagation. With each cell in the grid, the algorithm maintains an array of the valid values which can be used for this cell, its so-called domain that the propagation phase seeks to reduce as much as possible provided the constraints. Initially for a n order Su-Doku puzzle, all domains Di,j are the same set Mn2 of the first n2 integers. Propagation resolves into iterating four separate steps:"}
{"pdf_id": "0803.4253", "content": "The iteration is stopped when no further reduction happens in step 4 of the above propagation process. Reductions are done in any order as it does not impact the final result after the system reaches a quiescent state. The \"1\" in the algorithm name comes from the choice of reducing domains on a single constraint type (and its dual): the unicity of values for CSP variables."}
{"pdf_id": "0803.4253", "content": "Data representation. In order to lower the computation costs, the domains for each of the n2 variables representing the puzzle cells are implemented aspacked arrays in C. Reduction then becomes a logical operation on a bit ar rays. Step 3 of the previous propagation process requires the domains to be transposed: for each line, file and block, n2 new bit arrays are computed, the i-th of which is made of bits i of the n2 domain bit arrays."}
{"pdf_id": "0803.4253", "content": "The previous code fragment details the solveStep function which propagatesassignments of values to cells by calling the (not-represented) propagate func tion, which in turn operates on the domain bit array representations, deleting the assigned values from other cells' domains in each relevant line, file andblock. This is in fact step 2 of the PS-1-2 algorithm as described in the pre vious section. Then the dual step in domain reduction is taken by calling the (not-represented) reduceLine, reduceColumn and reduceBlock functions which handle the transposition and reduction in step 3 of the PS-1-2 algorithm. This function exits when no domain can be further reduced to a singleton through the iteration of the basic propagate and reduce operations. In addition the function maintains various counters, namely step and main"}
{"pdf_id": "0803.4253", "content": "When it succeeds, however, the function backs up the current search state, here an array of domain bit arrays representing the remaining possible values for each cell in the puzzle, assigns first the highest value of the pair domain to the cell and propagates this assignment by calling the previously mentioned solveStep"}
{"pdf_id": "0803.4253", "content": "The process called the search procedure 11 times, when the propagation/reduction operations reach quiescence as indicated by a 0 in the Red(uctions) column. The Srch column indicates whether the h(igh) or l(ow) value of the pair searched is used for the next propagation phase. In the particular instance, backtrack occurred only once at the sixth pair search: both high and low value were propagated to find the solution."}
{"pdf_id": "0803.4253", "content": "Conclusions.The canonical procedure to solve CSP-formulated problems al ternates a propagation phase, where data is used to reduce domains of thevariables as far as possible, also known as filtering, with a search phase, a back track procedure which explores incremental steps towards a solution. There is ample room for variability in this framework both in the balance between"}
{"pdf_id": "0803.4253", "content": "propagation and search, and within each phase in the criteria used in filtering and in search. In the case of Su-Doku puzzles, we have presented a naive algorithm, PS-1-2, which only filters on unicity of the variable value and of this value per group (line, file or block) in the propagation phase, and only uses binary search in the alternating search phase. Although there should be pathological cases where the binary search phase might fail, the PS-1-2 algorithm was successful at solving quickly all the puzzles we submitted, including so-called minimal puzzles."}
{"pdf_id": "0803.4253", "content": "naive_puzzle( A00, A01, A10, A11, B00, B01, B10, B11,C00, C01, C10, C11, D00, D01, D10, D11 ) : system_time(T0), cpu_time(T10), real_time(T20), assign( A00 ), assign( A01 ) assign( A10 ) assign( A11 ) assign( B00 ) assign( B01 ) assign( B10 ) assign( B11 ) assign( C00 ) assign( C01 ) assign( C10 )"}
{"pdf_id": "0803.4253", "content": "naive_all_different(A00, A10, C00, C10 ) naive_all_different(A01, A11, C01, C11 ) naive_all_different(B00, B10, D00, D10 ) naive_all_different(B01, B11, D01, D11 ) system_time(T), cpu_time(T1), real_time(T2) write( 'time T0: '), write(T0), write(', time T: ' ),write(T), nl, write( 'time T0: '), write(T10), write(', time T1: ' ),write(T1), nl, write( 'time T0: '), write(T20), write(', time T2: ' ),write(T2), nl."}
{"pdf_id": "0803.4253", "content": "C00, C01, C10, C11, D00, D01, D10, D11) : fd_domain( A00, 1, 4 ) fd_domain( A01, 1, 4 ) fd_domain( A10, 1, 4 ) fd_domain( A11, 1, 4 ) fd_domain( B00, 1, 4 ) fd_domain( B01, 1, 4 ) fd_domain( B10, 1, 4 ) fd_domain( B11, 1, 4 ) fd_domain( C00, 1, 4 ) fd_domain( C01, 1, 4 ) fd_domain( C10, 1, 4 ) fd_domain( C11, 1, 4 ) fd_domain( D00, 1, 4 ) fd_domain( D01, 1, 4 ) fd_domain( D10, 1, 4 ) fd_domain( D11, 1, 4 )"}
{"pdf_id": "0803.4253", "content": "fd_all_different([A00, A10, C00, C10 ]) fd_all_different([A01, A11, C01, C11 ]) fd_all_different([B00, B10, D00, D10 ]) fd_all_different([B01, B11, D01, D11 ]) system_time(T0), cpu_time(T10), real_time(T20) fd_labeling([A00, A01, A10, A11 B00, B01, B10, B11 C00, C01, C10, C11 D00, D01, D10, D11],[variable_method(most_constrained)]), system_time(T), cpu_time(T1), real_time(T2) write( 'time T0: '), write(T0), write(', time T: ' ),write(T), nl, write( 'time T0: '), write(T10), write(', time T1: ' ),write(T1), nl write( 'time T0: '), write(T20), write(', time T2: ' ),write(T2), nl."}
{"pdf_id": "0803.4253", "content": "A00 = 1 A01 = 2 A10 = 3 A11 = 4 B00 = 3 B01 = 4 B10 = 1 B11 = 2 C00 = 2 C01 = 1 C10 = 4 C11 = 3 D00 = 4 D01 = 3 D10 = 2 D11 = 1 ? ; time T0: 296, time T: 312 time T0: 1609, time T1: 1625 time T0: 155875, time T2: 158472"}
{"pdf_id": "0803.4253", "content": "A00 = 1 A01 = 2 A10 = 3 A11 = 4 B00 = 3 B01 = 4 B10 = 1 B11 = 2 C00 = 4 C01 = 1 C10 = 2 C11 = 3 D00 = 2 D01 = 3 D10 = 4 D11 = 1 ? ; time T0: 296, time T: 343 time T0: 1609, time T1: 1656 time T0: 155875, time T2: 228535"}
{"pdf_id": "0803.4253", "content": "Definition 3 Bipartite Graph. A graph G consists of a finite, non-empty set of elements V called nodes, or vertices, and a set of unordered pair of nodes E called edges. If V can be partitioned into two disjoint, non-empty sets X and Y such that all edges in E join a node in X to a node in Y, G is called bipartite with partition (X,Y); we also write G = (X,Y,E)."}
{"pdf_id": "0803.4253", "content": "Definition 5 Maximum Matching. A subset of edges in a graph G is a match ing if no two edges have a vertex in common.A matching of maximum cardi nality is called a maximum matching. A matching covers a set of vertices X isf every node in X is an endpoint of an edge in the matching."}
{"pdf_id": "0803.4253", "content": "The count of exact hitting sets is the number of solutions to the constraints used in Su-Doku formulations. Generally speaking, the number of exact hitting sets for permutation constraints, i.e. in which the number of values is the same as variables, is given by the permanent of the representation matrix [12]."}
{"pdf_id": "0803.4253", "content": "Note that the representation matrix of an exact hitting set (or exact cover problem) is amenable to a doubly stochastic matrix, in the case of permutation, by replacing each entry equal to 1 with 1/n. Van der Waerden made a conjecture on the lower bound for the permanent of doubly stochastic matrices in 1926 [2] which was later proved (in 1981) by Egoritchev and by Falikman as exposed by Knuth in [8]."}
{"pdf_id": "0803.4253", "content": "search( k ): If S_Header.r == S_Header, print the current solution and return. Otherwise choose a column structure . Cover column . For each row in while , - set S_Covering[k]=; - for each in while , cover column ; - search( k+1 ); - set =S_Covering[k], and ; - for each in while , uncover column . Uncover column and return."}
{"pdf_id": "0803.4253", "content": "The disconnected then reconnected links perform what Knuth called a \"dance\" which gave its name to this implementation known as the \"Dancing Links\". The running time of the algorithm is essentially proportional to the number of times it applies the remove operation, counted here with the updates variable. It is possible to get good estimates of the running time on average by running the above procedure a few times and applying techniques described elsewhere by Knuth [?] and Hammersley and Morton [?] (so called \"Poor Man's Monte Carlo\")."}
{"pdf_id": "0803.4253", "content": "x1 x2 x3 x4 C1 C2 C3 C4 x = 1 x = 1 x = 1 x = 1 x = 2 x = 2 x = 2 x = 2 x = 3 x = 3 x = 3 x = 3 x = 4 x = 4 x = 4 x = 4"}
{"pdf_id": "0803.4253", "content": "n4, cells in n2 lines by n2 files, and n2 blocks. The full size A matrix for the Dancing Links algorithm has n4 + n4 + n4 + n4 = 4n4 columns, one for each of the cells, and n2 for each of the line, file and block in the grid. It also has n6"}
{"pdf_id": "0803.4253", "content": "Enumerating size-2 Su-Doku grids. Running the Dancing Links algorithm on the 64 by 64 size-2 Su-Doku A matrix, produces the first of the 288 solutions almost immediately: Read 64 columns from sud2.mat Read 64 rows from file sud2.mat [16] New covering 1/1 in 0 secs, 0 usecs: Depth Covers Backtracks Degrees 37 25 22 16"}
{"pdf_id": "0803.4253", "content": "28 16 19 10 10 16 10 10 11 12 16 13 10 14 15 Total 256 16 Estimation of solution path: 7620 The sud2.mat file is the A matrix for the size-2 Su-Doku grid. The trace table shows the depth, i.e. the value of k which indicates the depth in the backtrack tree; the cover count, which is the number of elementary remove operations in the circular lists; the number of backtracking steps at each depth level; and the degree, the number of children nodes explored at each level. Finally the estimation of the average number of operations to reach a solution is printed according to the \"Poor Man's Monte Carlo\" method."}
{"pdf_id": "0803.4253", "content": "Counting Su-Doku grids. The algorithm can be used to count the number of Su-Doku grids, here for the size-2 grid: Read 64 columns from sud2.mat Read 64 rows from file sud2.mat 16 7620 7620 16 7620 15240 16 5316 20556 16 5316 25872 16 7620 33492 16 7620 41112 16 7620 48732 16 7620 56352 16 5316 61668 10 16 5316 66984 11 16 7620 74604 12 16 7620 82224 13 16 7620 89844 14 16 7620 97464 15 16 5316 102780 16 16 5316 108096 17 16 7620 115716 18 16 7620 123336"}
{"pdf_id": "0803.4355", "content": "A semantic network is also known as a multi-relationalnetwork or directed labeled network. In a semantic net work, there exists a heterogeneous set of vertex types anda heterogeneous set of edge types such that any two ver tices in the network can be connected by zero or more edges. In order to make a distinction between two edgesconnecting the same vertices, a label denotes the mean ing, or semantic, of the relationship. A semantic network"}
{"pdf_id": "0803.4355", "content": "triples [1]. For this reason, and due to the fact that RDF is becoming a common data model for various disciplines including digital libraries [4], bioinformatics [41], and computer science [39], all of the constructs ofthe grammar-based random walker model will be presented according RDF and its ontology modeling lan guage RDFS.RDF identifies vertices in a semantic network by Uni form Resource Identifiers (URI) [5], literals, or blank nodes (also called anonymous nodes) and edge labels are represented by URIs. An example RDF triple where all components are URIs is"}
{"pdf_id": "0803.4355", "content": "Due the heterogeneous nature of the vertices and edges in a semantic network, an ontology is usually defined asway of specifying the range of possible interactions be tween the vertices in the network. Ontologies articulatethe relation between abstract concepts and make no ex plicit reference to the instances of those classes [45]. For example, the ontology for the web citation network can"}
{"pdf_id": "0803.4355", "content": "be defined by a single class representing the abstract con cept of a web page and the single semantic relationshiprepresenting a web link or citation (i.e. href). This simple ontology states that the network representing the se mantic model of the web is constrained to only instances of one class (a web page) and one relationship (a web link). Given the previous single triple represented in Figure 1, the semantic network ontology could be represented as diagramed in Figure 2, where the lanl:hasFriend property must have a domain of lanl:Human and a range of lanl:Human, where lanl:marko and lanl:johan are both lanl:Humans."}
{"pdf_id": "0803.4355", "content": "as the Web Ontology Language (OWL) [24, 29]. OWL allows a modeler to represent restrictions on properties(e.g. cardinality) and provides a broader range of property types (e.g. inverse relationships, functional relation ships). Even though RDFS is limited in its expressiveness it will be used as the modeling language for describing the grammar-based random walker ontology. Note that it is trivial to map the presented concepts over to other modeling languages such as OWL. For a more in-depth review of ontology modeling languages, their history, and their application, please refer to [24] and [20].The next section brings together the concepts of ran dom walkers, semantic networks, and ontologies in orderto formalize this article's proposed grammar-based ran dom walker model."}
{"pdf_id": "0803.4355", "content": "rwr:Context, p will execute the rwr:Context's collection of rwr:Rules, while at the same time respect ing rwr:Context rwr:Attributes. The collection of rwr:Rules is an ordered rdf:Seq [11]. This meansthat p must execute the rules in their specified se quence. This is represented as the set of properties rdf: 1, rdf: 2, rdf: 3, etc. (i.e. rdfs:subPropertyOf rdfs:ContainerMembershipProperty). Any grammar-based random walker p has three local variables:"}
{"pdf_id": "0803.4355", "content": "that is traversed is strongly connected and aperiodic. If the traversed subset of Gn is not strongly connected or is periodic, then the rwr:Reresolve rule can be usedto simulate grammar-based random walker \"teleporta tion\". With the inclusion of the rwr:Reresolve rule, a grammar-based PageRank can be executed on Gn."}
{"pdf_id": "0803.4355", "content": "This section will demonstrate the application ofgrammar-based random walkers to a scholarly seman tic network denoted Gn. Figure 11 diagrams the ontology of Gn where the tail of the edge is the rdfs:domain and the head of the edge is the rdfs:range.The dashed lines represent the rdfs:subClassOf re lationship.This ontology represents the relation ships between lanl:Institutions, lanl:Researchers, lanl:Articles, and their respective children classes. The first example calculates the stationarydis tribution of the subset of Gn that issemanti cally equivalent to the coauthorship networkre sulting from lanl:ConferenceArticles written by lanl:Researchers that are lanl:locatedAt lanl:University only. The second example presents a grammar for calculating the stationary distribution over all vertices in a semantic network irrespective of the edge labels (i.e. an unconstrained grammar). The second"}
{"pdf_id": "0803.4355", "content": "[47] Wasserman, S., and K. Faust, 1994, Social Network Anal ysis: Methods and Applications (Cambridge University Press, Cambridge, UK). [48] Zhuge, H., and L. Zheng, 2003, in Proceedings of the Twelfth International World Wide Web Conference (WWW03) (Budapest, Hungary). [49] The superscript 1 on G1 denotes that the network is asingle-relational network as opposed to a semantic net"}
{"pdf_id": "0804.0528", "content": "Proposed algorithms:  In the whole of our algorithms, we use four basic axioms upon the balancing of the  successive granules:  Step (1): dividing the monitored data into groups of training and testing data  Step (2): first granulation (crisp) by SOM or other crisp granulation methods   Step (2-1): selecting the level of granularity randomly or depend on the obtained error  from the NFIS or RST (regular neuron growth)   Step (2-2): construction of the granules (crisp)"}
{"pdf_id": "0804.0528", "content": "Balancing assumption is satisfied by the close-open iterations: this process is a guideline to  balancing of crisp and sub fuzzy/rough granules by some random/regular selection of initial  granules or other optimal structures and increment of supporting rules (fuzzy partitions or  increasing of lower /upper approximations ), gradually"}
{"pdf_id": "0804.0528", "content": "The main benefit of this algorithm is to looking for  best structure and rules for two known intelligent system, while in independent situations each  of them has some appropriate problems such: finding of spurious patterns for the large data  sets, extra-time training of NFIS or SOM"}
{"pdf_id": "0804.0528", "content": "It must be noticed that for unrecognizable objects in test data (elicited by rules) a fix value  such 4 is ascribed. So for measure part when any object is not identified, 1 is attributed. This  is main reason of such swing of EM in reduced data set 6 (figure 5-b). Clearly, in data set 5  SORST gains a lowest error (15 neurons in SOM)."}
{"pdf_id": "0804.0558", "content": "tion and information that describe them are formatted according to a model of \"semantic features\", inspired by the memento design pattern rules [Gamma and al. 1995]. Moreover, the system apprehends these information via software agents (called factual agents) and according to an ontology of the studied domain. The collaboration of these agents and their comparisons with each other, form dynamic agents clusters. The latter are compared by past known scenarios. The final object of the study is to permit to prevent the occur of a crisis situation and to provide an emergency management planning."}
{"pdf_id": "0804.0558", "content": "The role of the Decision Support System is quite wide.In general, the purpose is \"to improve the decision making ability of managers (and operating per sonnel) by allowing more or better decisions within the constraints of cognitive, time, and economic limits\"[Holspace C.W. and al. 1996]. More specifically, the pur poses of a DSS are:"}
{"pdf_id": "0804.0558", "content": "In our context, the DSS is used as an emergency man agement system, able to assist actors in urban disasters mitigation and to prevent them about potential future critical consequences. The system includes a body ofknowledge which describes some aspects of the decision maker's world and that comprises the ontology of the domain and past known scenarios."}
{"pdf_id": "0804.0558", "content": "Representation layer : This layer is composed by factual agents and has as essential aim to represent dynamically and in real time the information of the current situ ation. Each new entering information is dealt by a factual agent that intends to renect a partial part of an observedsituation. Agents interactions and more precisely, aggres sions and mutual aids reinforce some agents and weaken some other."}
{"pdf_id": "0804.0558", "content": "Characterisation layer : This layer has as aim to gather factual agents, emerged from the precedent layer, using clustering algorithms. We consider a cluster of agents, a group of which agents are close from dynamic and evolution manner point of view. The goal here, is to form dynamic structures, where each one is managed by a characterisation agent."}
{"pdf_id": "0804.0558", "content": "Our perception of the environment focuses on two as pects: on the one hand, we observe the concrete objectsof the world, the changes of their states and their interac tion. On the other hand, we observe the events and the actions that may be created naturally or artificially. We have defined therefore, three categories of objects (Figure 2): Concrete object, Action object and Message object."}
{"pdf_id": "0804.0558", "content": "Action object : This type is divided into activities and phenomena objects. Both are created at a given time and are limited temporally without a priory knowledge of the bounds. Phenomena are unpredictable events that start at a given time. Their observation is the most complex because of their uncertainties and their rapid evolutions. Activities are the actions sequences performed by actors. Generally, they are ordered and emitted for a particular purpose."}
{"pdf_id": "0804.0558", "content": "The picture Figure 3 shows the hierarchy classes of theRCR disaster space. Each object in the world has prop erties such as its position, its shape ans its state. We distinguish two main objects categories: moving objects and motionless objects. First ones represent actors of the disaster world and they are modelled by Person object in our taxonomy. The second category consists of both buildings and networks roads and they are modelled by Passive object in the taxonomy."}
{"pdf_id": "0804.0558", "content": "the classes hierarchy. Each object of the environment has a type and is localised in time and space. We have assigned therefore to Object class a type, a time and a localisation attributes. In the second level, three classesinherit the Object class. Two abstract classes: ActionOb ject and ConcreteObject, and a concrete class Message."}
{"pdf_id": "0804.0558", "content": "ActionObject class is the superclass of Phenomenon and Activity classes. The first one is the superclass of Fire, Break, Injury and Blockade classes and has an additional attribute intensity. The latter represents the intensityand the progression degree of the phenomenon. For ex ample, a fire may have the following intensities: starting, strongly and extremely"}
{"pdf_id": "0804.0558", "content": "ConcreteObject class is the superclass of the concrete classes: Person, PassiveObject and Mean classes. Person class has three additional attributes: buriedness, damage and hitPoint. The first one shows how much a person is buried in the collapse buildings. The second one shows the necessity of medical treatment. The last one shows the health level, a person in good health has a hitPoint = 10000, and 0 when his is dead. PassiveObject and Mean classes has only the inherited attributes."}
{"pdf_id": "0804.0558", "content": "Semantic features are related with each other, that means they have a semantic dependencies. We defined therefore proximity measures in order to compare between them.The proximity value is comprised between [-1,1]. Two semantic features are opposite in their subjects if the prox imity measure is negative, they are closed if it is positive and independent if it equals zero. More the proximity is near to 1 (-1), more the two semantic features are closed (opposite). We distinguish three types of proximities: asemantic proximity which is determined thanks to the on"}
{"pdf_id": "0804.0558", "content": "tology, a spatial and a time proximities that are related to specific scales. As example, a break and a block are closed semantically, because if a building is broken, the nearest road will be certainly blocked. Moreover, to givemore precision to this confrontation, we compare the lo calisations and the times of observation of the two events.If they are distant, we consider the two events are inde pendent, and inversely."}
{"pdf_id": "0804.0558", "content": "The graphic tool is composed by a grid that shows in real time points now representing factual agents. Agents are projected on three axis: PP, PS and PA. Factual agents progress extremely quickly, so it is too hard to follow theirevolution. We have created therefore, an interactive in terface (agent interface). This interface has two essential functionalities. The first one permits to select a givenfactual agent and to show all its information: its seman tic feature, its current state and its current indicators values. The second one permits to freeze all the factual agents at a given time and to reanimate them thereafter. This allows us to obtain an instantaneous view of all the agents during their evolution and to study consequently, information about any agent."}
{"pdf_id": "0804.0558", "content": "Picture Figure 6 shows an instantaneous image of the cur rent situation of the RCRSS disaster space in the eighth cycle of the simulation. Information shown in the table,in the right, are related to the blue building, that is burn ing. A new factual agent, carrying the semantic feature (Phenomenon#67068017, type, fire, intensity, starting, localisation 22989100|3755100, time, 8), is created and updated according to information sent by the fire brigade agent, situated just near to the building. This factual agent is represented by the green ellipse in the grid and has as coordinates (PP=207,PS=3,PA=1). In the agent interface, we can see all information about this agent,"}
{"pdf_id": "0804.0558", "content": "notably, its indicators and its state which is the decision state. We note, that all indicators are strictly positive and the agent is in advanced state in its ATN. This means the agent has acquired importance and the event that it represents is more and more significant. This evolution is the result of information sent by the fire brigade agentand the interaction of the factual agent with other fac tual agents. The latter carry other related information,that can be messages announcing the fire, or actions per formed to extinguish it."}
{"pdf_id": "0804.0599", "content": "This section describes how to apply symmetry breaking in MaxSAT. First, the construc tion process for the graph representing a CNF formula is brieny reviewed [6, 1], as it will be modified later in this section. Afterwards, plain MaxSAT is considered. The next step is to address partial, weighted and weighted partial MaxSAT."}
{"pdf_id": "0804.0599", "content": "Symmetry breaking for MaxSAT and variants requires a few modifications to the ap proach used for SAT [6, 1]. This section summarizes the basic approach, which is then extended in the following sections. Given a graph, the graph automorphism problem consists in finding isomorphic groups of edges and vertices with a one-to-one correspondence. In case of graphs with colored vertices, the correspondence is made between vertices with the same color. Itis well-known that symmetries in SAT can be identified by reduction to a graph au tomorphism problem [6, 1]. The propositional formula is represented as an undirected"}
{"pdf_id": "0804.0599", "content": "Table 1 summarizes the problem transformations described in this section, where MS represents plain MaxSAT, PMS represents partial MaxSAT, WMS represents weighted MaxSAT, and WPMS represents weighted partial MaxSAT. The use of SBPs introduces a number of hard clauses, and so the resulting problems are either partial MaxSAT or weighted partial MaxSAT."}
{"pdf_id": "0804.0599", "content": "Overall, the inclusion of SBPs should be considered when a hard problem instance is known to exhibit symmetries. This does not necessarily imply that after breaking symmetries the instance becomes trivial to solve, and there can be cases where the new clauses may degrade performance. However, in a significant number of cases, highly symmetric problems become much easier to solve after adding SBPs. In many of these cases the problem instances become trivial to solve."}
{"pdf_id": "0804.0599", "content": "Symmetries are a well-known research topic, that serve to tackle complexity in many combinatorial problems. The first ideas on symmetry breaking were developed in the 90s [16,6], by relating symmetries with the graph automorphism problem, and by proposing the first approach for generating symmetry breaking predicates. This work was later extended and optimized for propositional satisfiability [1].Symmetries are an active research topic in CP [8]. Approaches for breaking symme tries include not only adding constraints before search [16] but also reformulation [17]"}
{"pdf_id": "0804.0852", "content": "The Anisotropic selection is a selection method in which the neighbors of a cell may have different probabilities to be selected [12]. The Von Neumann neighborhood of a cell C is defined as the sphere of radius 1 centered at C in manhattan distance. The Anisotropic selection assigns different probabilities to be selected to the cells of the Von Neumann neighborhood according to their position. The probability pc to choose the center cell C remains fixed at"}
{"pdf_id": "0804.0852", "content": "A common analytical approach to measure the selective pressure is the computation of the takeover time [9] [14]. It is the time needed for the best solution to colonize the whole population when the only active evolutionary operator is selection [5]. When the takeover time is short, it means that the best solution's propagation speed in the population is high. So, worse solutions' life time in the population is short and thus the selective pressure is strong. On the other hand, when the takeover time is high, it means that the best solution colonizes slowly the population, giving a longer lifetime to worse solutions. In that case, the selective pressure is low. So the selective pressure in the population is inversely proportionnal to the takeover time."}
{"pdf_id": "0804.0852", "content": "where p(i) gives the location offa cility in the current permutation p. Nugent, Vollman and Ruml proposed a set of problem instances of different sizes noted for their difficulty [2]. The instances they proposed are known to have multiple local optima, so they are difficult for a genetic algorithm."}
{"pdf_id": "0804.0852", "content": "In this section, we present statistic measures on the evolu tion of the genotypic diversity in the population. Three kinds of measures are performed : The global average diversity, the vertical/horizontal diversity and the local diversity. The global average diversity measure is made on a set of 50 runs of one instance of QAP for each kind of algorithm. It consists in computing the genotypic diversity between each solutions generation after generation."}
{"pdf_id": "0804.0852", "content": "where d(x1, x2) is the distance between solutions x1 and x2. The distance used is inspired from the Hamming distance: It is the number of locations that differ between two solutions divided by their length n. The results for each generation are averaged on 50 runs. We obtain a curve representing the evolution of the global"}
{"pdf_id": "0804.0852", "content": "diversity in the population through 2000 generations.The vertical/horizontal diversity measures the average di versity in the columns and in the rows of the grid. The vertical (resp. horizontal) diversity is the sum of the average distance between all solutions in the same column (resp. row) divided by the number of columns (resp. rows):"}
{"pdf_id": "0804.0852", "content": "CONCLUSION AND PERSPECTIVES This paper presents a comparative study of two selectionoperators, the anisotropic selection and the stochastic tour nament selection, that allow a cellular Genetic Algorithm to control the selective pressure on the population. A study on the innuence of the selection operators on the selective pressure is made by measuring the takeover time and the genotypic diversity. We analyse the average performance obtained on three instances of the well-known Quadratic Assignment Problem. A threshold value for the parametersof both of the selection operators that gives optimal per formance has been put in evidence. These threshold values give the adequate selective pressure on the population for the QAP. However, the selective pressure is different for"}
{"pdf_id": "0804.1046", "content": "AbstractIn this paper, a new discrete scheme for Gaussian curvature is pre sented. We show that this new scheme converges at the regular vertex with valence not less than 5. By constructing a counterexample, wealso show that it is impossible for building a discrete scheme for Gaus sian curvature which converges over the regular vertex with valence 4. Moreover, the convergence property of a modified discrete scheme for the Gaussian curvature on certain meshes is presented. Finally, asymptotic errors of several discrete schemes for Gaussian curvature are compared."}
{"pdf_id": "0804.1046", "content": "This shows that G(2) and G(3) are equivalent, which means these two schemes obtain the same value for the same triangular mesh. In [18], the author proves that the discrete scheme G(1) has quadratic convergence rate under the parallelogram criterion. In the following theorem, we shall show that the discrete scheme G(3) has also quadratic convergence rate under the same criterion."}
{"pdf_id": "0804.1046", "content": "Since Fk dj can be written as the linear combinatorics of ti, tij, tijk and tijkl, all the inner products in (11) and (12) can be expressed as linear combinations of gij, gijk, gijkl, eijkl, eijklm and fijklm. Substituting (11) and (12) into (8), (9) and (10), and then substituting (8), (9) and (10) into the expression"}
{"pdf_id": "0804.1046", "content": "The aim of this section is to exhibit the numerical behaviors of the discrete schemes mentioned above. For a real vector a = (a20, a11, a02), we define a bivariate function fa(x, y) := a20x2 + a11xy + a02y2, and regard the graph of the function fa(x, y) as a parametric surface"}
{"pdf_id": "0804.1046", "content": "Acknowledgments. Part of work is finished when the first author visits Technical University of Berlin in 2007-08. Zhiqiang Xu is Supported by the NSFC grant 10401021 and a Sofia Kovalevskaya prize awarded to Olga Holtz. Guoliang Xu is supported by NSFC grant 60773165 and National Key Basic Research Project of China (2004CB318000)."}
{"pdf_id": "0804.1448", "content": "The recent improvements of graphics processing units (GPU) offer to the computer vision community a powerful processing platform. Indeed, a lot of highly parallelizable computer vision problems can be significantly accelerated using GPU architecture. Among these algorithms, the k nearest neighbor search (KNN) is awell-known problem linked with many applications such as classification, estimation of statistical properties, etc. The main drawback of this task lies in its compu tation burden, as it grows polynomially with the data size. In this paper, we show that the use of the NVIDIA CUDA API accelerates the search for the KNN up to a factor of 120."}
{"pdf_id": "0804.1448", "content": "Entropy estimation In information theory, the Shannon entropy [CT91, Sha48] or information entropy is a measure of the uncertainty associated with a random variable. It quantifies theinformation contained in a message, usually in bits or bits/symbol. It is the mini mum message length necessary to communicate information. This also represents an absolute limit on the best possible lossless compression of any communication: treating a message as a series of symbols, the shortest possible representation totransmit the message is the Shannon entropy in bits/symbol multiplied by the num ber of symbols in the original message. The entropy estimation has several applications like tomography [Gzy02], motion estimation [BWD+06], or object tracking [GBDB07]. The Shannon entropy of a random variable X is"}
{"pdf_id": "0804.1448", "content": "Content-based image retrievalContent-based image retrieval (CBIR) [LSDJ06, Low03] is the application of com puter vision to the image retrieval problem, that is, the problem of searching fordigital images in large databases. \"Content-based\" means that the search will an alyze the actual contents of the image. The term \"content\" in this context might refer colors, shapes, textures, or any other information that can be derived from the image itself. The techniques, tools, and algorithms that are used originate fromfields such as statistics, pattern recognition, signal processing, and computer vi sion. Given an image database and a query image, Schmid and Mohr propose in [SM96] a simple KNN-based CBIR algorithm:"}
{"pdf_id": "0804.1448", "content": "The initial goal of our work is to speed up the KNN search process in a Mat lab program. In order to speed up computations, Matlab allows to use external Cfunctions (Mex functions). Likewise, a recent Matlab plug-in allows to use ex ternal CUDA functions. In this section, we show, through a computation time comparison, that CUDA greatly accelerates the KNN search process. We compare three different implementations of the BF method and one method based on kd-tree (KDT) [AMN+98]:"}
{"pdf_id": "0804.1448", "content": "In the table 1, N corresponds to the number of reference and query points, and Dcorresponds to the space dimension. The computation time given in seconds, cor responds respectively to the methods BF-Matlab, BF-C, KDT-C, and BF-CUDA. The chosen values for N and D are typical values that can be found in papers using the KNN search."}
{"pdf_id": "0804.1448", "content": "The main result of this paper is that, in most of cases, CUDA allows to greatly reduce the time needed to resolve the KNN search problem. BF-CUDA is up to 120 times faster than BF-Matlab, 100 times faster than BF-C, and 40 times fasterthan KDT-C. For instance, with 38400 reference and query points in a 96 dimen sional space, the computation time is approximately one hour for BF-Matlab and BF-C, 20 minutes for the KDT-C, and only 43 seconds for the BF-CUDA. The considerable speed up we obtain comes from the highly-parallelizable property of the BF method."}
{"pdf_id": "0804.1448", "content": "In this paper, we propose a fast k nearest neighbors search (KNN) implementation using a graphics processing units (GPU). We show that the use of the NVIDIACUDA API accelerates the resolution of KNN up to a factor of 120. In particu lar, this improvement allows to reduce the size restriction generally necessary tosearch KNN in a reasonable time in KNN-based content-based image retrieval ap plications."}
{"pdf_id": "0804.1982", "content": "Cubical space with direct adjacency, or (6,26)connectivity space, has the simplest topology in 3D dig ital spaces. It is also believed to be sufficient for the topological property extraction of digital objects in 3D. Two points are said to be adjacent in (6,26)-connectivity space if the Euclidean distance of these two points is 1, i.e., direct adjacency. Let M be a closed (orientable) digital surface in the 3D grid space in direct adjacency. We know that there are exactly 6-types of digital surface points [3][2]."}
{"pdf_id": "0804.1982", "content": "Proof. Scan through all points (vertices) in M and count the neighbors of each point. We can see that a point in M has 4 neighbors indicating that it is in M4 as are M5 and M6. Put points to each category of Mi. Then use formula (5) to calculate the genus g."}
{"pdf_id": "0804.1982", "content": "The above idea can be extended to simplicial cells(triangulation) or even general CW k-cells. This is be cause, for a closed discrete surface, we can calculate Gaussian curvature at each vertex point using formula(4). (The key is to calculate all angles separated by 1 cells at a vertex) Then use (3) to obtain the genus g. Since each line-cell (1-cell) is involved in exactly two 2-cells, it is only associated with four angles. Therefore"}
{"pdf_id": "0804.1982", "content": "Homology groups are other invariants in topological classification. For a k-manifold , Homology group Hi,i = 0, ..., k indicates the number of holes in each i skeleton of the manifold. Once we obtain the genus of a closed surface, we can then calculate the homology groups corresponding to its 3-dimensional manifold. Consider a compact 3-dimensional manifold in R3"}
{"pdf_id": "0804.1982", "content": "whose boundary is represented by a surface. We show its homology groups can be expressed in terms of its boundary surface (Theorem 3.4). This result follows from standard results in algebraic topology. Since it does not seem to be explicitly stated or proved in any standard reference, we include a self-contained proofhere [7]. This result follows from standard results in al gebraic topology. It also appears in [6] in a somewhatdifferent form. For the convenience of readers, we in clude a short self-contained proof here. First, we recall some standard concepts and results in topology. Given a topological space M, its homology"}
{"pdf_id": "0804.1982", "content": "Proof. Step 1 uses linear time. We can first track all points in the object using breadth-first-search. We assume that the points in the object are marked as \"1\" and the others are marked as \"0.\" Then, we test if a point in the object is adjacent to both \"0\" and \"1\" by using 26-adjacency for linking to \"0.\" Such a point is called a boundary point. It takes linear time because the total number of adjacent points is only 26. Another"}
{"pdf_id": "0804.1982", "content": "algorithm is to test if each line cell on the boundary has exactly two parallel moves on the boundary [3]. This procedure only takes linear time for the total number of boundary points in most cases. Step 2 is also in linear time by Lemma 2.2. Step 3 is just a simple math calculation. For H0, H2, and H3, they can be computed in constant time. For H1, the counting process is at most linear."}
{"pdf_id": "0804.1982", "content": "To some extent, researchers are also interested in space complexity that is regarded to running space needed beyond the input data. Our algorithms do notneed to store the past information, the algorithms pre sented in this paper are always O(log n). Here, log n is the bits needed to represent a number n. Acknowledgement. The authors would like to thankProfessor Allen Hatcher for getting the authors connected which led to the result of this paper. The second author is partially supported by NSF grant DMS 051391."}
{"pdf_id": "0804.2057", "content": "Haack J. 33 Haidt D. 11 Hamon O. 28 Handschuh D. 11 Hanlon E.M. 18 Hapke M. 11 Harjes J. 11 Haydar R. 26 Haynes W.J. Hedberg V. 21 Heinzelmann G. 13 Henderson R.C.W. 18 Henschel H. 33 Herynek I. 29 Hildesheim W. 11 Hill P. 11 Hilton C.D. 22 Hoeger K.C. 22 Huet Ph. Hufnagel H. 14 Huot N. 28"}
{"pdf_id": "0804.2273", "content": "ABSTRACT  The OAI Object Reuse and Exchange (OAI-ORE) framework  recasts the repository-centric notion of digital object to a bounded  aggregation of Web resources. In this manner, digital library  content is more integrated with the Web architecture, and thereby  more accessible to Web applications and clients. This generalized  notion of an aggregation that is independent of repository  containment conforms more closely with notions in eScience and  eScholarship, where content is distributed across multiple services  and databases. We provide a motivation for the OAI-ORE  project, review previous interoperability efforts, describe draft  ORE specifications and report on promising results from early  experimentation that illustrate improved interoperability and reuse  of digital objects."}
{"pdf_id": "0804.2273", "content": "(OAI-PMH) [2], reflects this mission and its grounding in  mainstream digital library concepts: harvesting metadata  (primarily bibliographic) from repositories. OAI-PMH has been  widely deployed, and despite a number of issues related to  metadata quality and complexity [23], is considered a successful  interoperability mechanism. Its deployment does not compare to  related Web-based syndication standards such as RSS and  ATOM, due in part to its architectural focus on digital libraries  rather than more general Web notions."}
{"pdf_id": "0804.2273", "content": "September 2007, when the following goal was stated: \"ORE will  develop specifications that allow distributed repositories to  exchange information about their constituent digital objects\".  While this original mission reflects an evolution beyond the  metadata-centric nature of OAI-PMH to a focus on content, the  mission remains based on core digital library notions, in this case  digital objects stored in repositories [20]."}
{"pdf_id": "0804.2273", "content": "OAI-ORE work, a set of specifications and user guides [26] that  state: \"Open Archives Initiative Object Reuse and Exchange  (OAI-ORE) defines standards for the description and exchange of  aggregations of Web resources.\" This represents yet another  evolution of the OAI mission: from a repository-centric focus and  a conceptualization of content as stored in repositories, which has  characterized most digital library work, to a resource-centric  focus in which machines (e.g. Web servers) act as service points  to content independent of location. The salient aspects of the  conceptual differences between OAI-PMH to OAI-ORE are  illustrated in Table 1."}
{"pdf_id": "0804.2273", "content": "most digital libraries must exist within the capabilities and  constraints of that Web Architecture. Because of the virtual  hegemony of Web browsers as an information access tool and  Google as a discovery tool, failure to heed to Web Architecture  principles, and therefore requiring somewhat special treatment by  these \"monopoly applications\" (which is rarely if ever granted),  effectively means falling into an information black hole."}
{"pdf_id": "0804.2273", "content": "to URI schemes (e.g., http, ftp, gopher) and each scheme  defines the mechanism for assigning URIs within that scheme.  Within the common http scheme, the URI is an identifier key  in an HTTP (hypertext transfer protocol) request message,  which may result in the return of information about the respective Resource. However, the ability to automatically de reference an http URI is not true for all URIs (nor even for all  http URIs)."}
{"pdf_id": "0804.2273", "content": "common usage, a link is expressed via link or anchor tags (a  hyperlink) in an HTML Representation of the originating  Resource to the URI of another Resource. An extension of  this, where links are typed relationships, is one of the goals of  the Semantic Web."}
{"pdf_id": "0804.2273", "content": "the digital library notion of a repository, is not included in the  Web Architecture. This does not mean that the digital library  notion of a repository is irrelevant, and in fact we argue that issues  essential to digital libraries such as preservation, authority, and  integrity largely rely on the repository as a management entity.  However, a repository-centric approach to interoperability may produce results that do not coordinate well with the resource centric architecture of the Web, leading to the \"black hole\"  scenario mentioned above."}
{"pdf_id": "0804.2273", "content": "that is a compound aggregation, is another concept without strict  equivalence in the Web Architecture. The repository technologies  that originally motivated the ORE work, such as DSpace, Fedora,  aDORe, ePrints and arXiv, all store content that is more than a  simple file, albeit, they differ in how they implement this and in  the richness of their functionality. A look at the arXiv for  example shows that most content is available in multiple formats  (e.g., PDF, LaTeX), is versioned, is represented by some metadata  format, and has citations to other papers. Collectively this  aggregation of elements is the \"document\" in arXiv."}
{"pdf_id": "0804.2273", "content": "Architecture, it is prevalent across general Web space. For  example, a \"photo\" in Flickr is an aggregation of multiple  renditions in different sizes, and that photo is aggregated along  with other \"photos\" into a \"collection\". Similarly, the blog entry  that we think of as a singleton is in fact an aggregation composed  of the original entry combined with multiple comments (and  comments on comments). That blog entry is itself aggregated in a  subject partition of a blog."}
{"pdf_id": "0804.2273", "content": "examples of aggregations, with components that are distributed across multiple services and databases. For example the multi part \"virtual data\" objects envisioned by the National Virtual  Observatory Project [43], the \"datuments\" described in the  chemistry community [30] and the learning objects implemented  by NSDL [24] all share the property that their components are  distributed over multiple databases, web servers, databases, and  the like. In this context, the notion of a repository as a container  is not especially relevant. Rather content is distributed and made  available via distributed service points."}
{"pdf_id": "0804.2273", "content": "DOIs that identify the whole object. This identity is important  as the means of expressing citation, lineage, and rights. We  argue that it is also relevant in the Web context, especially in  the Semantic Web where identities are the subjects and objects  of RDF assertion, and an assertion about a splash page needs to  be distinct from an assertion about an aggregation as a unit."}
{"pdf_id": "0804.2273", "content": "possible to deterministically enumerate its constituents. This is  vital for services such as preservation (what to preserve) and  rights management (who is responsible for what). While not  defined in the Web Architecture, the importance of boundary  has also been acknowledged in Web applications. It is  therefore part of the requirement set of the Protocol for Web  Description Resources (POWDER) [4] work, which aims to  provide mechanisms to publish properties shared by a set of  Web resources."}
{"pdf_id": "0804.2273", "content": "eScience/eScholarship applications. At the time of writing this  paper, the ORE specifications are still in alpha status and, while  they have been the subject of a number of experiments (described  later in this paper), real applications that exploit them have yet to  be built. However, we propose the following applications for the machine-readable descriptions of aggregations defined by OAI ORE:"}
{"pdf_id": "0804.2273", "content": "what is informally known as the Kahn-Wilensky Framework  (KWF) [20]. Originally published as a web page in 1995, the  KWF was the architecture for the Computer Science Technical  Report (CS-TR) project [5]. The CS-TR project later merged with  the WATERS project [28] to form the basis for the Dienst  protocol [22] and the NCSTRL project [16]. Lessons learned with  Dienst and NCSTRL later significantly influenced the design of  OAI-PMH."}
{"pdf_id": "0804.2273", "content": "(DC) community, resulting in the Warwick Framework [21],  which was later extended with \"distributed active relationships\"  [15], which later evolved into Fedora [25]. The KWF also formed  the basis for a prototype implementation for the Library of  Congress National Digital Library Program [6]. The  representation of metadata in digital objects in the NDLP  influenced the Making of America II project [17], which gave rise  to the Metadata Encoding and Transmission Standard (METS)  [29]."}
{"pdf_id": "0804.2273", "content": "been extensive and its contributions can be grouped into the areas  of 1) repository protocols, 2) digital objects and 3) identifiers. In  the subsections below we explore each of these topics further,  starting with their origins and continuing to their present status  and influence on ORE."}
{"pdf_id": "0804.2273", "content": "protocols approached interoperability via support of distributed  (or \"federated\") searching. The aforementioned Dienst protocol  provided many things, including: mediated access to holdings in a  repository conformant to a structured data model, bibliographic  metadata exchange and support for distributed searching. While  Dienst  provided  interoperability  with  other  Dienst"}
{"pdf_id": "0804.2273", "content": "implementations, other projects such as the Stanford Simple  Digital Library Interoperability Protocol [18], attempted to  provide interoperability between heterogeneous systems (e.g.  Dienst, Z39.50, etc.) by providing a generic, \"wrapper\" protocol  that abstracted the shared semantics between various systems. A  similar project, Stanford Protocol Proposal for Internet Retrieval  and Search (STARTS) [18], defined a method for repositories to  expose just enough information about their holdings and  capabilities to facilitate distributed searching."}
{"pdf_id": "0804.2273", "content": "has moved from the protocols to the formats of the digital objects.  The concept of digital objects, including typed, recursive and  composite digital objects, is fundamental to the KWF. Drawing  from Arm's observation that \"users want intellectual works, not  digital objects\" [5], repositories have co-developed with object  description formats to describe and manage these \"intellectual  works\" (or \"works\" and \"expressions\" in FRBR terminology [1])."}
{"pdf_id": "0804.2273", "content": "and is (or was) the default object description format for many  repository projects, such as DSpace [36] and Fedora. Other  communities have created or adopted their own object formats:  IMS-LOM [33], from the Learning Objects community, and  MPEG-21 DIDL, originally from the consumer electronics  community and adapted to the DL environment by Los Alamos  National Laboratory [7]. Although the syntax and application  domain for these formats differ, they all have goal of combining  descriptive, structural and administrative metadata to conjure  digital manifestations of \"intellectual works\"."}
{"pdf_id": "0804.2273", "content": "descriptive metadata, OAI-PMH has been combined with object  formats such as METS and DIDL to create \"resource harvesting\"  [42]. This has been studied in the context of transferring digital  objects between repositories in the APS-LANL project,  effectively combining OAI-PMH and Open Archival Information  System (OAIS) reference model [9]."}
{"pdf_id": "0804.2273", "content": "interoperability becomes more difficult. For example, in the  Archive Ingest and Handling Test [35] the four participants  ingested the same resources in their respective, differing  repositories. When they encoded their contents for export (3 in  METS, 1 in MPEG-21 DIDL), none of the parties could ingest the  export of the others without significant pre-processing; format  expressiveness had come at the cost of at least initial  interoperability. Secondly, there is no clear mapping of these  compound objects into the Web Architecture. To borrow from  FRBR terminology again, object description formats, and the  identifiers they use, are primarily about \"works\" or \"expressions\"  and the Web Architecture is primarily about manifestations  (resources) and items (representations)."}
{"pdf_id": "0804.2273", "content": "community. But their ubiquity underlies their ambiguity: in the  context of the Web, what do they actually identify? This is really  the larger question of resolvable and non-resolvable identifiers.  From the DL perspective, there is significant value in the ability  of  a  non-resolvable  identifier  such  as"}
{"pdf_id": "0804.2273", "content": "browsing (humans can often distinguish when the URI is  identifying the intellectual work and when it is identifying an  HTML page), it does hinder the development of automated  services that do not always understand the subtle convention that  http://arxiv.org/abs/cs/0610031v1 is in fact just one of  many members of the intellectual work properly identified by  info:arxiv:cs/0610031v1 and not the intellectual work itself.  The present ambiguity of allowing, depending on context, the  former URI to represent both a set and a member of a set is one of  the remaining fundamental problems of interoperability."}
{"pdf_id": "0804.2273", "content": "issues raised in the related work described in the previous section.  The ORE alpha specifications were made public on 10 December  2007 [26] for a period of review and consultation. Discussion  groups, meetings and experimentation will guide evolution  through beta to final specifications, the release of which are  expected in 3rd quarter 2008. The suite of documents contains  both specifications and user guide documents. We focus here on  three key aspects: the data model, serialization, and discovery."}
{"pdf_id": "0804.2273", "content": "readable information to the Web that augments the human readable Web. Various discovery mechanisms provide hooks  whereby browsers and agents surfing the human-readable Web  can find out about ORE information which may then be used to  direct or augment the functions available (e.g. \"print whole  chapter\" from a web page displaying a page image). The central  notion of an aggregation adds boundary information to a set of  web resources that may be arbitrarily distributed over many servers (e.g. a large dataset, model code, an article, and open review commentaries)."}
{"pdf_id": "0804.2273", "content": "that encapsulates a set of RDF statements1. The notion of  associating a URI with a set of RDF statements is based on the  concept of a named graph developed in the Semantic Web  community [12]. The creation of a Resource Map instantiates an  aggregation as a resource with a URI distinct from the Resource  Map, enumerates the constituents of the aggregation, and defines  the relationships among those constituents."}
{"pdf_id": "0804.2273", "content": "Resource Map is independent of other notions of aggregations or  compound digital objects in repositories or other servers. An ORE  Aggregation exists only in tandem with, and in fact, due to the  existence of a single Resource Map. As described below, this  binding is enforced by the URI syntax of Resource Maps and  Aggregations. Also, the sections below describe the means of  establishing linkages between an Aggregation and digital objects  in other architectural contexts."}
{"pdf_id": "0804.2273", "content": "these concepts should not be conflated and that they should have  separate URIs. This separation is the only manner in which  assertions about them can remain distinct. However, it is likely  and appropriate that many repository systems will include splash  pages as an Aggregated Resource in an Aggregation, but they  should not consider a splash page as one representation of the  Aggregation."}
{"pdf_id": "0804.2273", "content": "on ReM-1 must yield a serialization of the Resource Map. Note  also that ReM-1 appears as a node in the graph and is the subject  of several triples. First, there must be triples stating that resource  ReM-1 is a Resource Map, that resource A-1 is an Aggregation,  and linking the Resource Map to the Aggregation that it describes:"}
{"pdf_id": "0804.2273", "content": "AR-2, unrelated and not described except for their status as  constituents of the Aggregation, A-1. There are significant  applications where this is already useful: for example the notion  of grouping in intellectual objects used by Google Scholar -- links  to the splash page, PDF and HTML version of an article should be  considered links to the same intellectual object. However, in  many cases additional description will be useful."}
{"pdf_id": "0804.2273", "content": "other identifiers, then these are expressed using either the  owl:sameAs or ore:analogousTo predicate. It is important to  understand that owl:sameAs makes a strong statement of  equivalence between two URIs: they identify the same resource  and thus one URI may be substituted for the other. We introduce  the  weaker  relation,  ore:analogousTo,  which  implies"}
{"pdf_id": "0804.2273", "content": "more than one Aggregation, each described by a Resource Map  (say ReM-1 and ReM-2). To support discovery, the predicate  ore:alsoInResourceMap allows specifying that an Aggregated  Resource from one Resource Map is also an Aggregated Resource  in another Resource Map. For example, ReM-1 might contain the  following triple expressing that AR-1 is known to also be  aggregated in ReM-2 (not shown in figure):"}
{"pdf_id": "0804.2273", "content": "ore:fromResourceMap is that it should only be interpreted in  the context of the asserting Resource Map. Standard RDF models  (triples) don't support this notion but systems that retain context  information (quad stores etc.) can. Systems than cannot  understand context should interpret ore:fromResourceMap in  the same way as ore:alsoInResourceMap which is less  expressive but correct."}
{"pdf_id": "0804.2273", "content": "Atom for ORE, a Resource Map is mapped to an Atom feed, and  each Aggregated Resource to an Atom entry. The four metadata  elements about the Resource Map are provided using feed-level  Atom metadata elements. The rules for mapping all entities of the  ORE Model to and from Atom are described in detail in the  specification. Here we illustrate the key points with the example  shown in Figure 2 which is a Resource Map for an arXiv e-print  with just two components shown: a PDF version and a HTML  splash page."}
{"pdf_id": "0804.2273", "content": "\"describes\" is an ORE addition3 to indicate the Aggregation  described by the feed. The mandatory modification time and  creator metadata elements map to the Atom /feed/updated and  /feed/author elements, respectively. The /feed/author  element admits name, uri and email sub-elements. Only the  name or uri sub-elements have meaning in the ORE model and  are mapped to the dc:creator triple with either a literal (name)  or a resource (uri) as the object of the triple."}
{"pdf_id": "0804.2273", "content": "URIs (/feed/id and /feed/entry/id) and some additional  metadata (e.g. /feed/title and /feed/entry/title); these  have no correspondence in the ORE Model. Feed creating  applications must mint these URIs to produce valid Atom feeds  and should be careful that they are globally unique and persistent,  but must not reuse the Aggregation and Aggregated Resource  URIs. For the feed and entry titles it is recommended to use the  Resource Map and Aggregated Resource URIs, prefixed with  \"Resource Map\" and \"Aggregated Resource\" to provide a  human readable description of the content."}
{"pdf_id": "0804.2273", "content": "feature in serializing core elements of the ORE Data model as  described above. Arbitrary elements from other namespaces,  including RDF, are permitted within Atom feed documents so it is  possible to create an Atom serialization that expresses  relationships among aggregated resources. However, because  these are extensions without standard ATOM semantics,  conventional Atom applications will effectively ignore them."}
{"pdf_id": "0804.2273", "content": "intended to preclude the use of other serializations. However,  different serializations may be able to represent aggregations  conforming to the ORE data model with differing degrees of  fidelity. Clearly, any format capable of serializing an arbitrary  RDF graph can be used to serialize a Resource Map with  complete fidelity, and examples include N3, RDF/XML, Trix, and  Trig. As mentioned above, Atom serialization for Resource Maps  is less expressive, and can, for example, not express a relationship  where an Aggregated Resource is the object (instead of subject) of  a relationship triple."}
{"pdf_id": "0804.2273", "content": "bi-directional mapping to the ORE Model. A test of this mapping  is that one must be able to make the round trip between the model  and representation without data loss or corruption. However,  because of the possibility of both limited expressiveness and/or of  additional features in a particular serialization we must be careful  to define the round trip. The mapping must preserve intact all  information on the second and subsequent round trips. For  example, to check the mapping to format X one must find the  common  expressiveness  by  doing  the  first  round  trip"}
{"pdf_id": "0804.2273", "content": "There is no single, best method for discovering Resource Maps,  and we expect best practices for discovery to evolve over time.  The Resource Map Discovery Document [27] covers a variety of  suggested Resource Map discovery mechanisms, grouped into the  categories of Batch Discovery, Resource Embedding and  Response Embedding."}
{"pdf_id": "0804.2273", "content": "en masse. Note that Resource Maps are not limited to describing  Aggregations on the server where the Resource Maps reside. This  means that a machine in domain A can make Resource Maps  available that describe aggregations of resources from domains B,  C and D. Assuming the Aggregated Resources are not remotely  editable, batch discovery techniques are the most direct method of  publishing third party aggregations."}
{"pdf_id": "0804.2273", "content": "HTTP response header) can be used to direct agents from the  Aggregated Resource to a corresponding Resource Map that  describes the Aggregation of which the resource is part. While  this is a common case, there are actually four different scenarios  regarding members of an Aggregation and knowledge about their  corresponding Resource Maps:"}
{"pdf_id": "0804.2273", "content": "Resource Map. It is possible for Aggregated Resources to  simultaneously have full knowledge about one Resource Map  (typically authored by the same creators of the resources) and  have zero knowledge about third party Resource Maps that  describe aggregations of the same resources. Full, indirect or  limited knowledge can be interpreted as the Resource Map being  \"endorsed\" by the resource creator. However, there is no concept  of a \"negative endorsement\" — zero knowledge could mean the  creators either do not endorse the Resource Map or are simply  unaware of the Resource Map."}
{"pdf_id": "0804.2273", "content": "Library Research & Prototyping Team of the Los Alamos  National Laboratory (LANL) conducted an experiment in which  the Zotero citation manager browser plug-in [13] was modified to  detect the existence of a compound information object from the  HTML splash page for a scholarly article. When detected, the  enhanced Zotero offered the user the ability to download any  number of constituent resources of the compound object,  including, obviously, its bibliographic description. In this  experiment, compound information objects were represented as  special-purpose ATOM feeds. Leveraging ATOM as a strategy to  integrate compound scholarly objects into the mainstream Web  has remained a theme throughout the ORE effort."}
{"pdf_id": "0804.2273", "content": "version of the ORE specifications was set, the coordinators of the  ORE effort engaged with the Andrew W. Mellon Foundation in  the U.S.A. and with the Joint Information Systems Committee  (JISC) in the U.K. to secure funding for a limited number of  small-scale experiments that have the implementation of the ORE  specifications at their core, and that should result in demonstrable  showcases that illustrate the enabling nature of the specifications  in the realm of scholarly communication, research, and education.  The Mellon Foundation funded two such projects."}
{"pdf_id": "0804.2273", "content": "University, explores how the ORE framework can be leveraged  to provide new digital preservation functionality outside of the  typical repository environment. More particularly, it  investigates how Resource Maps for arbitrary Aggregations  can be combined with JavaScript, Wikis and email to provide a  preservation function that puts client applications, such as  browsers, instead of servers in the driver seat."}
{"pdf_id": "0804.2273", "content": "University of Illinois at Urbana Champaign. It addresses the  challenge of text-on-text annotation of digitized books. Current  schemes for identifying and describing annotation targets tend  to be representation-specific and are expressed in idiosyncratic  ways. The project investigates whether Resource Maps can be  used to reveal richer targets for annotation in an interoperable  and transparent way."}
{"pdf_id": "0804.2273", "content": "experiments is still open, but the outlines of one proposed project  are known. The project led by Robert Sanderson and Richard  Jones at the University of Liverpool and the Bristol HP Labs,  respectively, will work with JSTOR to automatically produce  Resource Maps for all of JSTOR's holdings. Resource Maps will  go down to the page level of articles, and will express detailed  resource properties wherever possible. In a next project phase, HP  Labs will explore the synergy between the ORE and SWORD [3]  specifications and leverage both to ingest the JSTOR Resource  Maps into a DSpace repository, taking into account the rights  statements for the articles expressed in those Resource Maps."}
{"pdf_id": "0804.2273", "content": "students from several departments at the California Institute of  Technology is developing an application that will allow  researchers to discuss Web-based publications in online journal  clubs, and to attach additional resources to those publications  such as comments, keyword tags, figures, video, etc. The  project is investigating the use of Resource Maps to aggregate  these resources and the publication to which they pertain into a  logical whole."}
{"pdf_id": "0804.2273", "content": "EnVision currently lacks a solution to record and maintain a  consistent trail of the variety of information entities involved in  creating a specific visualization, including the source data set,  the parameters used for the visualization, the resulting images,  and further metadata and annotations for the images"}
{"pdf_id": "0804.2273", "content": "and repository interoperability efforts so that they are more  closely integrated with the Web Architecture and best practices of  the Web community at large. Although the specifications have  just been released, they are informed by the technologies from and  experiences with both digital libraries and Semantic Web. In the  same way that SiteMaps assist services by clearly enumerating the  resources available at a web site, Resource Maps unambiguously  enumerate distributed Aggregated Resources, and can express  their types and relationships."}
{"pdf_id": "0804.2273", "content": "the Coalition for Networked Information, Microsoft, and the  National Science Foundation (IIS-0430906). The authors  acknowledge the contributions to the OAI-ORE effort from the  ORE Technical Committee, Liaison Group and Advisory  Committee. Many thanks to Lyudmila Balakireva, Ryan Chute,  Stephan Dresher, and Zhiwu Xie of the Digital Library Research  & Prototyping Team of the Los Alamos Laboratory for their  experimental work."}
{"pdf_id": "0804.2354", "content": "The goal of an information filtering system is to alleviate the work of user, to make  more effective the persistent search of relevant information. A software module for  text filtering is the important part of recommender systems and information filtering  systems. Recommender systems could be classified as content-based systems  (presented in this work) and collaborative filtering systems.7  The recommender system could be based on thesaurus (e.g., WordNet 11) or an  ontology.12 The experimental comparison 2, 8, 19 of algorithms searching for related  terms based on WordNet 1, 5, 10, 15-16, 20, GermaNet 14 and English Wikipedia 19 shows  an advantage of Wikipedia."}
{"pdf_id": "0804.2354", "content": "The development of the text filtering approach based on the wiki indexing requires:  (i) to develop the text filtering approach, (ii) to design the architecture of the wiki  indexing system, (iii) to implement the indexing system and run the experiments.  The paper structure corresponds to the formulated tasks."}
{"pdf_id": "0804.2354", "content": "a As of 27 January 2008, see http://en.wikipedia.org/wiki/Wikipedia:Size_comparisons.  b As of 30 October 2006, see http://stats.wikimedia.org/EN/TablesWikipediaEN.htm.  c See http://simple.wikipedia.org.  d The  average  number  of  words  per  article  is  400,  as  of  October  2005,  see"}
{"pdf_id": "0804.2354", "content": "1. Banerjee S., Pedersen T. An Adapted Lesk algorithm for word sense  disambiguation using WordNet. Third International Conference on Intelligent  Text Processing and Computational Linguistics (CICLING-02). Mexico City,  February 2002. http://www.d.umn.edu/~tpederse/Pubs/cicling2002-b.pdf  2. Calderan M. Semantic Similarity Library. Technical Report #DIT-06-036,  University  of  Trento,  2006."}
{"pdf_id": "0804.2354", "content": "http://multiwordnet.itc.it/paper/WordnetWumNAACL.pdf  12. Middleton S. E., Alani H., Shadbolt N. R., Roure D. C. D. Exploiting synergy  between ontologies and recommender systems. Semantic Web Workshop 2002.  Hawaii, USA, 2002. http://eprints.ecs.soton.ac.uk/6487/1/www-paper.pdf  13. Milne D., Medelyan O., Witten I. H. Mining domain-specific thesauri from  Wikipedia: a case study. International Conference on Web Intelligence  (IEEE/WIC/ACM  WI'2006).  Hong  Kong,  2006."}
{"pdf_id": "0804.2401", "content": "Definition 3.2 (atom, literal, clause). An atom in IL is defined by: if Ti (i = 1, 2, 3) are terms in IL, then I(T1, T2, T3) is an atom. A literal is defined to be an atom (called positive literal) or its negation (called negative literal). A clause is the disjunction of a finite set of literals."}
{"pdf_id": "0804.2401", "content": "Definition 3.6 (valid valuation). Let A be a formula, and let M be an independency model defined on U. A valuation v in M is valid for A if for each atom I(T1, T2, T3) appeared in A, v(T1), v(T2), and v(T3) are pairwise disjoint, where v(T) is the valuation of T in M."}
{"pdf_id": "0804.2701", "content": "• SPIRES & arXiv. Because of their similar histories and mostly non-overlapping func tions, SPIRES and arXiv could be considered as a single system. arXiv functions as the back-end data storage, as well as managing all of the complexities of submission. SPIRES provides a front-end interface, as well as giving further context to the arXiv submissions by matching them with published literature and adding citation, keywords and other data3. Examples of their symbiosis include the fact that all of the arXiv content of HEP relevance is indexed in SPIRES and arXiv relies on SPIRES for tasks like citation analysis."}
{"pdf_id": "0804.2701", "content": "Like virtually everyone else with internet access, HEP scholars also use Google [13] and Google Scholar [14] as information resources. One of the targets of this study is indeed to assess the penetration of these resources in the HEP scholarly-communication landscape. It is important to remark that arXiv and SPIRES have let their content be harvested by Google and then partly organized in Google Scholar."}
{"pdf_id": "0804.2701", "content": "The number of respondents can be compared with the number of HEP physicists active in 2006, which is about 20,000 [15], or the number of authors who have published an article listed in SPIRES in the last decade, which is between 30,000 and 40,000, depending on how one handles similar names"}
{"pdf_id": "0804.2701", "content": "The survey explicitly inquired about the level of change that HEP scholars would expect and require from their information resources: 75% expected \"some\" to \"a lot of\" change in the next five years, while only 12% expected no change4.' To structure this perception of change, respondents were asked to imagine their ideal information system in five years and tag the importance of 11 possible features on a five-step scale from \"not important\" to \"very important\". These features are:"}
{"pdf_id": "0804.2701", "content": "We are grateful to our colleagues who shared their insight in the field of information management,which were crucial in the preparation of the survey: Catherine Cart, Jocelyne Jerdelet, Jean Yves Le Meur, Tibor Simko, Tim Smith, and Jens Vigen at CERN; Zaven Akopov and Kirsten Sachs at DESY; and Pat Kreitz and Ann Redfield at SLAC"}
{"pdf_id": "0804.2701", "content": "This study would not have reached such a large audience without the collaboration of Paul Ginsparg and Simeon Warner at arXiv, Enrico Balli at SISSA/Medialab, Bob Kelly and Erick Weinberg at APS and Christian Caron at Springer, who kindly disseminated information about the survey, and to whom we are indebted"}
{"pdf_id": "0804.2701", "content": "In addition to the results presented above, the survey collected thousands of free-text answers, inquiring about features of current systems and their most-desired evolution. A detailed studyof these comments is underway and outside the scope of this Article. However, it is particu larly interesting to distill some of these answers here, in order to complete the assessment of the engagement of the HEP community with the systems which serve its information needs and its expectations for future developments. Some of the most inspiring free-text answers were along the following lines:"}
{"pdf_id": "0804.2701", "content": "Table 8: Perceived importance of additional features of a HEP information system. The first five features concentrate on the access to information, the second four are part of a wider service to the community while the last three are services tailored to authors. The last column summarizes the fraction of respondents who answered these questions."}
{"pdf_id": "0804.3234", "content": "regions delimited by crossings (due to the 3D to 2D projection). Consequently, only the outer contour of the cell is represented, thus missing the innermost structures. This fact is illustrated in Fig. 1(b), where the light gray shaded innermost regionsrepresent areas inaccessible to traditional contour following algorithms, thus yield ing just the red curve as the respective contour."}
{"pdf_id": "0804.3234", "content": "Also, the results reported in our work can also be useful for the unsolved 3D cases by confocal microscopy. In addition, there are more important aspects regarding the importance and applicability of our contribution, and these are as follows. First, there are dozens of other microscopic techniques which cannot yield 3D, but only 2D images, necessarily implying tangling of neuronal branches which can be treated by our method. Such microscopy techniques are often required instead of confocal microscopy because they can reveal specific properties of the analyzed tissues and structures which cannot be imaged by confocal methodology."}
{"pdf_id": "0804.3234", "content": "In short, the BTA is an algorithm aimed at the segmentation of each distinct branch within a 2D neuron image other than the soma and intercepting regions. The BSCEAis an algorithm intended to the extraction of the parametric contour from a 2D neu ron image, based on the BTA."}
{"pdf_id": "0804.3234", "content": "For clarity's sake, this paper is presented in increasing levels of detail, hence devel oping as follows. Section 2 contains an overview of the proposed framework, which is further detailed in Section 3. Experimental results considering real neuronal cells are presented in Section 4. The paper concludes in Section 5, by identifying the main contributions, as well as possibilities for future works. Low level descriptions has been left to the Appendices A.2 and A.1."}
{"pdf_id": "0804.3234", "content": "Usually, an optical acquisition device yields an image as output, corresponding to a summary and incomplete representation of the information originally present in the original object [4]. As a result, images are normally devoid of some information,such as related to depth, a problem arising from the supression of the third dimen sion in the 3D original object as implied by its object projection onto the 2D plane.In the context of complex shape images, like neurons, depth information is of ex treme importance to properly discern the structures in the image. The current work approaches this problem, more especifically the extraction of contours of neuronal cells imaged onto 2D frames. In particular, the 2D neuron images used herein have been obtained through a camera lucida device."}
{"pdf_id": "0804.3234", "content": "Initially, our approach considered the existence of only two types of structuresamong branches, namely bifurcations and crossings. However the number of ad jacent segments at each critical region proved not to be enough to properly classifythem, leading to misclassifications. Only through the incorporation of additional information, namely the identification of several geometrical features along the neuronal shape, it has been possible to achieve correct classification of the critical re"}
{"pdf_id": "0804.3234", "content": "The category Points comprises three classes of extremity points: primary seeds, secondary seeds and terminations. Each extremity point is classified regarding its location, i.e. a primary seed corresponds to a junction point between a dendritic tree and the soma, while a secondary seed refers to a junction point between a critical region and a dendritic subtree. Basically, the difference between a primary seed and a secondary seed is that a primary seed is necessarily adjacent to the soma, while a secondary seed is not. Terminations are end points of branches. The reason for distinguishing between points is that the tracking starts from the primary seeds and finishes at terminations, occasionally repeating itself in a recursive-like fashion from secondary seeds."}
{"pdf_id": "0804.3234", "content": "The category Lines encompasses two cases: segments and branches. Each line is classified with respect to its extremity points, i.e. a segment may grow out from either a primary or a secondary seed, but does not necessarily end at a termination. Segments are lines of pixels delimited by a pair of minor structures, for instance aseed and a critical region, or two critical regions, or a critical region and a termi nation. Conversely, a branch may stem from either a primary or a secondary seed, ending necessarily at a termination. It follows from such a definition that a branch"}
{"pdf_id": "0804.3234", "content": "Though all critical regions share the property of being formed by pixels with neigh borhood greater than two, their shape structure are quite different. The reason for distinguishing between critical regions is to assure that both the tracking and the contour extraction algorithms behave as expected whenever such structures arefound. The algorithms undergo different processings for each kind of critical re gion."}
{"pdf_id": "0804.3234", "content": "At this point, it is worth emphasizing the difference between a crossing and a su perposition: although both share the property of having an inward segment splittinginto three outward segments, their shapes are slightly different. Notice that a cross ing appears as just a cluster of pixels, while a superposition is apparently made up of two clusters of pixels (bifurcations) attached by a short line. In spite of the fact that both structures have been originated from overlapping processes, the angle of inclination between these processes plays a central role, in that the steeper the slope between them, the greater the chance of obtaining a crossing, while the smoother the slope between them, the greater the chance of obtaining a superposition, as"}
{"pdf_id": "0804.3234", "content": "The category Collections simply represents groups of the aforedefined objects. A Dendritic Arbour is a collection of branches having roots in the soma. Hencerforth the collection of Dendritic Arbours, that is, the neuron without the soma, is simply referred as the Periphery. These concepts are summarized in the Table. 1."}
{"pdf_id": "0804.3234", "content": "• Branch Tracking Algorithm. The BTA has two main goals: to label each branch and to classify each critical region. It is applied for every primary seed present in the queue. The labelling procedure starts at the segment adjacent to the primary seed. After reaching a critical region, the current segment will have been entirely labeled, so a decision concerning the next segment to continue with the tracking"}
{"pdf_id": "0804.3234", "content": "must be taken. In addition to finding the optimal segment to move ahead, thealgorithm also identifies the current critical region as either a bifurcation, a su perposition or a crossing. If the current critical region is a bifurcation, the BTA stores the related secondary seed in an auxiliary queue, otherwise the BTA stores the addresses of the current segment end point and the next segment starting point. By doing so, the BTA labels all the segments comprising each dendritic branch in a recursive-like fashion, until reaching a termination."}
{"pdf_id": "0804.3234", "content": "• Branching Structure Contour Extraction Algorithm. The BSCEA main role is to extract the parametric contour c(t) = (x(t), y(t)) along the segments comprising a 2D neuron image by using the labeled branches and classified critical regionsobtained in the previous step. Basically, the BSCEA follows the segments defin ing branching structures (resulting from the union between the labeled skeleton and the soma) by entering all the shape innermost regions. During the contouring process, whenever a branching region is found, the BSCEA contours the shape"}
{"pdf_id": "0804.3234", "content": "outwards, as the traditional algorithm would. On the other hand, whenever a crossing or a superposition is found, the BSCEA contours the shape inwards, by traversing the current critical region through the addresses stored in pointers by the BTA. Finally the BTA gives as a result the contour parametric functions x(t) and y(t) as well as a contour image (Fig.16(b))."}
{"pdf_id": "0804.3234", "content": "Some important shape parts are detected by taking into account specific features, such as the number of each pixel's neighbors and the size of the shape. For example, pixels of branches are expected to have only 2 neighbors each, while critical regions and the soma have more. Moreover, the soma area is greater than the areas of the critical regions."}
{"pdf_id": "0804.3234", "content": "Initially, a preprocessing pipeline involving mathematical morphology transforma tions 2 is carried out on the input image, so as to obtain the separate components of the neuron image, that is the skeleton comprised of 8-connected one-pixel-wide branches, the critical regions, the terminations, the soma and the queue of primaryseeds. The referred separate components on different images are obtained as de scribed in the nowchart diagram depicted in the Fig. 6."}
{"pdf_id": "0804.3234", "content": "a clear pattern, making their segmentation critical. Herein, the soma segmentationis attained through erosion, noise filtering by area opening, followed by a dila tion. Casual noisy pixels surrounding the soma image are wiped out through the skeleton area opening. Then, additional processing is applied in order to obtain an 8-connected skeleton with one-pixel wide branches [16](??)."}
{"pdf_id": "0804.3234", "content": "The most critical and perhaps difficult template to define would be that portrayed in Fig. 5 for the Hit-or-Miss operation. The Hit-or-Miss is a mathematical morphology operation [10], being a sort of loose template matching, because the template itself is an interval, instead of a specific shape. Whenever certain small structure present on the image fits inside this interval, it is marked. Herein, the Hit-or-Miss operation is applied using the template depicted in Fig. 5(a) to detect redundant skeleton pixels which should be ruled out, as shown in Fig. 5(b)."}
{"pdf_id": "0804.3234", "content": "One of the main goals at this stage is to label each dendritic branch as a wholeobject on its own. This is achieved by pixel-by-pixel labeling of each branch. Con sidering the sequential nature of such a processing, this problem may be describedas estimating the spatial coordinates (x, y) of each subsequent branch pixel. Be"}
{"pdf_id": "0804.3234", "content": "Fig. 7. Preprocessing results: (a) The darkest pixels were removed by the Hit-or-Miss filter ing yielding the 8-connected skeleton with one-pixel wide branches shown in lighter cyan; (b) Pruned 8-connected skeleton (cyan) with one-pixel wide branches superimposed to the skeleton (black); (c) Soma (red), seeds (blue), critical regions (green) and skeleton(black); (d) Critical Regions (green and red) and skeleton (black)."}
{"pdf_id": "0804.3234", "content": "Tracking is usually divided into Prediction, Measure and Update stages [1]. Dur ing the Prediction stage, the algorithm estimates the next state of the system. On the Measure stage, the algorithm probes the system by looking for plausible statesnearby, in this case valid pixels, through some measures, herein the spatial coordi nates (x, y) of pixels. During the Update stage, the algorithm merges both pieces of information gathered on the previous two stages, through a linear combination, giving as a result the optimal estimation for the next state. So, in terms of Tracking, the BTA Prediction and Measure stages are carried out in a single step, through the 8-neighborhood scanning by using the chain-code [8]."}
{"pdf_id": "0804.3234", "content": "The BTA Update stage is related to the pixel labeling. This stage labels each den dritic subtree growing out of the soma in the same way, i.e. by starting from therelated primary seed and labeling the entire branch adjacent to it, up to its termina tion. Meanwhile, its branches are marked to be labeled afterwards. Thereafter, every"}
{"pdf_id": "0804.3234", "content": "The BTA is mainly composed of two nested loops. The outermost loop is on primary seeds, being related to the labeling of each dendrite having root in the soma. The innermost loop is on secondary seeds, being related to the labeling of each branch within a given dendrite. This algorithm is depicted in the nowchart of Fig. 8. It is worth mentioning that, for our purposes, valid pixels are defined as simultaneously non-labeled and non-critical foreground pixels. Then, for each primary seed, the BTA starts by subsequently stacking every valid pixel from a segment to be labeled afterwards, until either a termination or a critical region is reached."}
{"pdf_id": "0804.3234", "content": "On arriving at a critical region, the BTA may perform one or two of the followingtasks, Continuity of the Tangent Orientation Assessment and Critical Regions Clas sification. The former (detailed in the Section 3.2.1) is always carried out, while the latter (described in the Section 3.2.2) is performed only if the current critical region has not been classified yet. Notice that though the critical regions are now available from the previous preprocessing step, they are not classified yet, i.e. we do not know which is a bifurcation, a crossing or a superposition. This classification is important for the contour extraction step."}
{"pdf_id": "0804.3234", "content": "Analogously to the tracking process during branches labeling as described in 3.2,this step also comprises Prediction, Measure and Update, however in a slightly dif ferent fashion. Coming to a critical region in this step is similar to approaching theocclusion case in tracking problems [11], where different objects follow trajecto ries which apparently overlap."}
{"pdf_id": "0804.3234", "content": "Every time a critical region is encountered, the Breadth-First Search is triggered and all the forward neighboring pixels are iteratively enqueued into an auxiliary queue, while passing across the just detected critical region. At each Breadth-First Search iteration, the auxiliary queue is run through in search of critical pixels. Thestop condition for the Breadth-First Search is set beforehand as a number C of con secutive executions through the auxiliary queue without finding any critical pixel. This procedure is detailed in an example in Appendix A.1."}
{"pdf_id": "0804.3234", "content": "The starting pixel of the optimum segment to proceed is lastly stacked and labeled. Also, the alternative path origin is considered as a secondary seed, that is a side branch seed to be enqueued in case a bifurcation is detected. Conversely, in case either a superposition or a crossing is detected, the next segment starting point Vn+1 and the current segment last point Vn (Fig. 13(b)) addresses are stored into the Pointers Map."}
{"pdf_id": "0804.3234", "content": "The system became more and more robust, as we moved further bytaking into account new pieces of information, such as orientation between incom ing and outgoing direction vectors, proximity relation between neighbor crossing regions, besides the basic and first criterion of number of adjacent segments to each crossing region"}
{"pdf_id": "0804.3234", "content": "iii the input is followed in a counter-clockwise sense. iv all the N points of the parametric contour are stored in a suitable data structure E(1..N). Each element E(n) keeps the nth contour point coordinates, i.e. E(n).x and E(n).y, which are the computational representation for x(t = n) and y(t = n) respectively. When the contour is closed, x(t = 1) = x(t = N) and y(t = 1) = y(t = N)."}
{"pdf_id": "0804.3234", "content": "The BSCEA starts by a raster scanning, i.e., from left to the right, from top to the bottom, in search of the first contour pixel E(1), which should be the first background pixel found that is also a neighbor of a foreground pixel. In the sequel, the BSCEA will contour the shape all the way, until coming back to the first pixel, closing the cycle and having E(1) = E(N)."}
{"pdf_id": "0804.3234", "content": "Since the input for the BSCEA is a union of the labeled skeleton and the soma im ages, it is necessary to adopt a policy to properly find the next pixel in each case. Hence, the BSCEA considers contouring branches as the default case, taking thefirst background pixel which is also neighbor of a foreground pixel in the neighbor hood defined by the chain-code. Conversely, the BSCEA considers contouring the soma as a particular case, taking the last pixel, instead of the first one, to be included as contour. By so doing, the BSCEA is able to contour branches, while preservingthe ability of more traditional approaches to circumvent the problem of contour ing occasional one-pixel wide entrances into the soma, consequently allowing the contour to be closed [8]."}
{"pdf_id": "0804.3234", "content": "It is also necessary to devise a strategy for critical regions processing, according to their classes, as described in section 3.2.2. Regions classified as Bifurcation shouldbe contoured outwards, while those ones classified as either Superposition or Cross ing should be contoured inwards, through pointer addresses written to the Pointers Map data structure during the tracking stage. The integration between soma and labeled skeleton is critical for the successful contour extraction, since it guarantees the contour closing."}
{"pdf_id": "0804.3234", "content": "The BSCEA can deal with both cases by taking into account the labels of previousand current pixels, which convey valuable information concerning particular situa tions, i.e. if the critical region is a bifurcation, \"contour it outwards\" (see Fig. 10 and Fig.12), as well as the traditional contour extraction algorithm would [8]. In case it is a superposition or a crossing, \"contour it inwards\", (see Fig. 10 and Fig. 13), which means to trace a line between the current segment end point and the next segment starting point. Both points are known from the pointers marked by the BTA. The line is traced by using the Bresenham algorithm [2] for tracing a digital straight line segment."}
{"pdf_id": "0804.3234", "content": "Notice that the BSCEA cannot tell which pixels of a superposition or crossing are related one another or to a branch, since the projection from the 3D neuron onto the 2D plane suppresses this information. Such a problem is circumvented by replacing the shared pixels in the critical region by two short intercepting segments given by the Bresenham's algorithm, as illustrated in Fig.13."}
{"pdf_id": "0804.3234", "content": "Fig. 12. Contouring a bifurcation. Branches appear labeled in blue and green, while the critical region previously classified as a bifurcation appears in magenta. The contour is shown in brown. (a) By detecting labels transition, the BSCEA identifies that it has arrived at a bifurcation, thus deciding to contour the shape outwards. (b) Having left the critical region behind, it proceeds until reaching another critical region."}
{"pdf_id": "0804.3234", "content": "Results for the Branching Structures Contour Extraction Algorithm are presented inFigure 16, where one can see the parametric contour trace for the shape and a com parison between the results obtained by using both the traditional and the BSCEAapproaches. Observe from Figures 16(a), 16(c) and 16(e) how the traditional al gorithm did not afford access to the innermost neuron contour portions, while theBSCEA conversely ensured full access to all neuronal processes, as shown in Fig ures 16(b), 16(d) and 16(f)."}
{"pdf_id": "0804.3234", "content": "The proper shape characterization of branching structures is a particularly impor tant problem, as it plays a central role in several areas of medicine and biology, especially in neuroscience. Indeed, the current understanding of the physiological dynamics in biological neuronal networks can be reinforced through the proper characterization of neuronal cells shapes, since both the amount of synapes and the way in which neurons organize in networks are strongly related to the cells shapes."}
{"pdf_id": "0804.3234", "content": "Because the proposed system begins with a series of transformations (preprocess ing) on the 2D projection of a 3D branching structure image, so as to obtain asuitable skeleton, obviously any skeletonization scheme other than the morpholog ical thinning might be adopted, such as exact dilations [8], medial axis transform, and so on, provided that an 8-connected skeleton with one-pixel wide branches is obtained as a result. Besides, the skeletonization scheme will affect the choice of all the preprocessing parameters, which in this work have been picked out by trial and error. One should bear in mind that the method gist is supplying the tracking algorithms with an adequate skeleton as input."}
{"pdf_id": "0804.3234", "content": "As for the BTA, there may be particular cases for further consideration yet, for ex ample images with high density values of critical regions and/or the presence of structures whose topologies might favour the appearance of superpositions. Thefirst case, i.e. high critical regions densities may be due to particular shape topolo gies in the image or due to the image resolution itself, causing the BTA to cluster critical regions ocurring very close to one another. Notice that, in an effort to fulfil the previously set stop condition for the Breadth-First Search, the BTA has bunched both bifurcations of type 1 (Fig. A.3-(a)) into a cluster of bifurcations appearing as a bifurcation of type 4 (Fig. A.3-(b)). A possible solution is to use breadth-first"}
{"pdf_id": "0804.3234", "content": "The most expensive operation in the BTA would be to check every pixel at some 8-neighborhood to decide whether or not it should be labeled. However this is done at most a constant number of times. So, tracking would be eventually of O(n) with respect to the number of object pixels (far less than the size of the image). Similarly, in BSCEA, every pixel in the neighborhood of a labeled pixel is visited to check whether it has a blank neighbor which will ultimately become a contour pixel, so it would also be of O(n)."}
{"pdf_id": "0804.3234", "content": "The main original contributions of the present work 5 encompass both the tracking and the parametric contour extraction from branching structures, like neuron cells. Future developments include the extension of the methodology to separate cells in images containing multiple cells. Several applications of the methodology proposed in this work can be made regarding neural networks images as well as other types of biological structures such as retinal vessel trees."}
{"pdf_id": "0804.3234", "content": "[21] K. Rothaus, P. Rhiem, X. Jiang, Separation of the retinal vascular graph in arteries and veins, in: F. Escolano, M. Vento (eds.), GbRPR 2007, Graph-Based Representations in Pattern Recognition, 6th IAPR-TC-15 International Workshop, Alicante, Spain, Proceedings, vol. 4538 of Lecture Notes in Computer Science, Springer Verlag, 2007, http://www.springerlink.com/content/d573048432h4k13x/."}
{"pdf_id": "0804.3234", "content": "Fig. A.3. (a) Two distinct bifurcations of type 1 will be seen as (b) one bifurcation of type 4, an immediate consequence from the agglutinating effect caused by the Breadth First Search algorithm, when encountering two close bifurcations, as though the current local analysis had given place to a more global analysis by switching into a larger analyzing scale"}
{"pdf_id": "0804.3361", "content": "Our classifier uses 38 features of 4 classes to characterize interictal EEG signal. The power spectral features describeenergy distribution in the frequency domain. Fractal dimen sions outline the fractal property. Hjorth parameters describe the chaotic behavior. Mean and standard deviation represent the amplitude statistics. Since normalization is very important to distance-based classifier, features are normalized before fed into PNN."}
{"pdf_id": "0804.3361", "content": "where Wi is the i-th row of W and bi is the i-th element of bias vector b. 1) Radial Basis Layer Weights: Each row of W is the feature vector of one trainging sample. The number of rows equals to the number of training samples. 2) Radial Basis Layer Biases: All biases in radial basis layer are set to"}
{"pdf_id": "0804.3361", "content": "1) normal EEG (sets A and B) and interictal EEG (sets C and D) 2) normal EEG (sets A and B) and ictal EEG (set E) 3) interictal EEG (sets C and D) and ictal EEG (set E) 4) interictal EEG sampled from epileptogenic zone (set C) and interictal EEG sampled from opposite hemisphere (set D)"}
{"pdf_id": "0804.3361", "content": "The first two experiments evaluate the performance of our algorithm using interictal EEG and ictal EEG respectively. The last two experiments evaluate the feasibility of ouralgorithm on seizure monitoring and focus localization, re spectively.The classifier is validated using leave-one-out cross validation (LOO-CV) on 400, 300, 300 and 200 samples respectively in experiments 1, 2, 3 and 4. Our algorithm is implemented using the MATLAB Neural Network Toolbox. Table I lists the overall accuracy and classification time of four experiments. The spread constant of PNN, is seleted according to overall accuracy. As illustrated in Fig. 6, all experiments achieve the highest accuracy, when spread constant is 0.1. In our experiments, therefore, spread constant is set to 0.1."}
{"pdf_id": "0804.3361", "content": "[1] H. Gastaut, Dictionary of Epilepsy. Part I: Definitions. World Health Organization, 1973. [2] K. Lehnertz, F. Mormann, T. Kreuz, R. Andrzejak, C. Rieke, P. David, and C. Elger, \"Seizure prediction by nonlinear eeg analysis,\" IEEE Engineering in Medicine and Biology Magazine, 2003. [3] Atlas: Epilepsy Care in the World. World Health Organization, 2005."}
{"pdf_id": "0804.3599", "content": "1. INTRODUCTION To improve the precision of retrieval output, especially within the very few (e.g, 5 or 10) highest-ranked documents that are returned, a number of researchers [36, 13, 16, 7, 22,34, 25, 1, 18, 9] have considered a structural re-ranking strat egy. The idea is to re-rank the top N documents that someinitial search engine produces, where the re-ordering uti lizes information about inter-document relationships within that set. Promising results have been previously obtained by using document centrality within the initially retrieved list to perform structural re-ranking, on the premise that if the quality of this list is reasonable to begin with, thenthe documents that are most related to most of the docu"}
{"pdf_id": "0804.3599", "content": "would be a natural measure of how \"good\" v is, since a node that is \"strongly\" pointed to by high-quality hubs (which, by definition, tend to point to \"good\" nodes) receives a high score. But where do we get the hub score for a given node u? A natural choice is to use the extent to which u \"strongly\" points to highly authoritative nodes:"}
{"pdf_id": "0804.3599", "content": "The well-known cluster hypoth esis [35] encapsulates the intuition that clusters can revealgroups of relevant documents; in practice, the potential util ity of clustering for this purpose has been demonstrated a number of times, whether the clusters were created in aquery-independent fashion [14, 4], or from the initially most highly-ranked documents for some query [13, 22, 34] (i"}
{"pdf_id": "0804.3599", "content": "2.3 Alternative scores: PageRank and innux We will compare the results of using the HITS algorithmagainst those derived using PageRank instead. This is a nat ural comparison because PageRank is the most well-known centrality-induction algorithm utilized for ranking documents, and because in earlier work [18], PageRank performed quitewell as a tool for structural re-ranking of non-Web doc uments, at least when applied to document-to-document graphs. One can think of PageRank as a version of HITS in which the hub/authority distinction has been collapsed. Thus, writing \"PR\" for both auth and hub, we conceptually have the (single) equation"}
{"pdf_id": "0804.3599", "content": "(Proof omitted due to space constraints.) Interestingly, this result shows that while one might have thought that clusters and documents would \"compete\" for PageRank score when placed within the same graph, in our document-as-authority and document-as-hub graphs this is not the case. Earlier work [18] also considered scoring a node v by its innux, P"}
{"pdf_id": "0804.3599", "content": "2.4 Algorithms based on centrality scoresClearly, we can rank documents by their scores as com puted by any of the functions introduced above. But when we operate on document-as-authority or document-as-hub graphs, centrality scores for the clusters are also produced. These can be used to derive alternative means for ranking documents. We follow Liu and Croft's approach [25]: first, rank the documents within (or most strongly associated to) each cluster according to the initial retrieval engine's scores; then, derive the final list by concatenating the within-cluster lists in order of decreasing cluster score, discarding repeats. Such an approach would be successful if cluster centrality is strongly correlated with the property of containing a large percentage of relevant documents."}
{"pdf_id": "0804.3599", "content": "3. RELATED WORK The potential merits of query-dependent clustering, that is, clustering the documents retrieved in response to a query, have long been recognized [30, 36, 23, 34, 25], especially ininteractive retrieval settings [13, 22, 32]. However, automatically detecting clusters that contain many relevant documents remains a very hard task [36]. Section 5.2 presents results for detecting such clusters using centrality-based clus ter ranking."}
{"pdf_id": "0804.3599", "content": "5.2 Re-Ranking by Cluster Centrality We now consider the alternative, mentioned in Section 2.4, of using the centrality scores for clusters as an indirect means of ranking documents, in the sense of identifying clusters that contain a high percentage of relevant documents. Note that the problem of automatically identifying such clusters"}
{"pdf_id": "0804.3599", "content": "6. CONCLUSION We have shown that leveraging the mutually reinforcing relationship between clusters and documents to determinecentrality is very beneficial not only for directly finding rel evant documents in an initially retrieved list, but also for finding clusters of documents from this list that contain a high number of relevant documents.Specifically, we demonstrated the superiority of cluster document bipartite graphs to document-only graphs as the input to centrality-induction algorithms. Our method for finding \"authoritative\" documents (or clusters) using HITSover these bipartite graphs results in state-of-the-art perfor mance for document (and cluster) re-ranking."}
{"pdf_id": "0804.3791", "content": "This article introduces preliminary results from the MESURproject, all of which strongly confirm the potential of scholarly usage data as a tool to study the dynamics of scholar ship in real time, and to form the basis for the definition of novel metrics of scholarly impact. Section 2 describes the size, origin, and representation of the MESUR reference dataset. Section 3 discusses initial findings in the realm of sam ple bias, and Section 4 shows the first ever map of science created on the basis of a substantial scholarly usage data set. Section 5 introduces a variety of impact metrics derived from both usage and citation data, and describes findings regarding their interrelation. Conclusions are presented in Section 6."}
{"pdf_id": "0804.3791", "content": "1. The usage events span nearly 5 years (2002-2007) of activity, although not all data from the aforementioned contributors span the same time period. 2. The collected usage data spans more than 100,000 serials, including scholarly journals, newspapers, etc. 3. The collected journal citation data spans about 10,000 journals and nearly 10 years. 4. In addition to raw usage events, journal usage statisticshave been collected in the form of COUNTER reports [21] that cover nearly 2000 institutions world wide."}
{"pdf_id": "0804.3791", "content": "With the exception of COUNTER reports, the obtained usage data was required to contain at least the following data fields: an anonymous session and/or user identifier, anarticle identifier, a date and time at which a request pertain ing to the identified article took place, and an indication of the request type (e.g. article download, abstract view, etc.) As a result, it is possible to extract the various articles thatusers requested a service for in the course of a given ses sion, and to reconstruct the clickstream of these users in the information system that recorded the usage data."}
{"pdf_id": "0804.3791", "content": "1. Anonymization: Understandably, privacy concerns arecentral to discussions with potential suppliers of usage data. Most agreements thus contain explicit state ments with this regard. As a result, all usage data in the MESUR reference data set is anonymized bothregarding individual and institutional identity. In cer tain cases, the usage data is provided by the source inan anonymized form, in other cases MESUR is respon sible for the required processing."}
{"pdf_id": "0804.3791", "content": "It should be noted that both the filtering and de-duplication sub-tasks are inherently statistical procedures, and that the achieved success rates innuence the quality of the reference data set. Therefore, uncertainty quantification is important to MESUR as it will help to assess the reliability of results obtained from mining the reference data set. At the time ofwriting, a formal approach with this regard is being devel oped."}
{"pdf_id": "0804.3791", "content": "• 200 million article-level usage events: A subsetconsisting of the most thoroughly validated and de duplicated usage events. • Journal-level usage events: All article-level usage events were converted to journal-level usage events tofacilitate the interpretation and cross-validation of ini tial results.• All request types included: Instead of making arbi trary determinations regarding the relative importanceof various request types, all requests that are indica tive of a user's interest in a given article are included. Multiple consecutive requests pertaining to the same article are connated to one event. Future analysis will focus on determining which request types most validly represent user interest."}
{"pdf_id": "0804.3791", "content": "Clearly, the CSU community is significantly larger and more diverse than LANL. Interestingly enough, the usage-based ranking for CSU better approximates the IF, although the Journal of American Child Psychology, and the American Journal of Psychiatrics, ranked fourth and fifth respectively, clearly still reveal community bias, i.e. they have high usage within the CSU community but a comparatively low IF."}
{"pdf_id": "0804.3791", "content": "The contrast between the rankings derived from the afore mentioned institution-specific data sets and those computed for the current MESUR research data set is striking. As mentioned, by the end of 2007, this data set consisted of200 million usage events recorded by a variety of institutional linking servers, and online services operated by pub lishers and aggregators; this preliminary data set already spans a broad user community. Table 3 lists the resultingfive highest-ranked journals; it indicates a strong conver gence towards the IF, with the exception of the Lecture Notes on Computer Science (LNCS) which is nevertheless considered an important publication."}
{"pdf_id": "0804.3791", "content": "The rankings listed in Tables 1, 2, and 3 illustrate two im portant considerations regarding usage data sampling. First, the characteristics of the community for which usage is recorded strongly shape usage-based impact rankings. Second, as the sample grows in size and scope, the preferences or biases of a particular community are leveled out, and an increasingconvergence with the IF is observed. The observed conver gence suggests that it is feasible to create a reference data set from which rankings with global reach can be derived. The authors are anxious to compute further rankings as the"}
{"pdf_id": "0804.3791", "content": "cated by anonymized session identifiers: the degree of re lationship between any pair of journals is a function of thefrequency by which they are jointly accessed within user ses sions. Fig. 2 illustrates this process. Within a usage data set, usage events are grouped according to the session in which they occur. This allows determining how frequently a given pair of journals is accessed within the same session.This frequency determines the strength of the connection be tween this particular pair of journals. The connections thus extracted for each pair of journals can then be combined to form a journal usage network."}
{"pdf_id": "0804.3791", "content": "Both usage and citation networks can not be visualized intheir entirety due their large number of journals and connec tions. Therefore, Fig. 3 and Fig. 4 display a relevant subset of all journals and connections. This subset is selected as follows. First, all connections are ranked according to theirconnection strength (i.e. the number of citations or usage co occurrences), and then only the top 5,000 connections are selected. Next, for each remaining journal, a maximum of 12 connections is shown. In addition, the visualization only"}
{"pdf_id": "0804.3791", "content": "includes journals that are part of the network's Largest Con nected Component, which is the largest possible sub-network in which every journal is directly or indirectly connected to every other journal.This prevents the maps to be clut tered with small \"island\" networks. The remaining networkis then graphically layed-out according to the Fruchterman Reingold heuristic which uses \"force-directed\" placement to position connected journals in each other's proximity and minimize connection crossings [9]. The maps show only the titles of the most central journals within a given cluster to further reduce clutter. The radius of the circles in the mapsis given by the natural logarithm of the number of connec tions for the journal. Journals with few connections thus have smaller circles."}
{"pdf_id": "0804.3791", "content": "5. USAGE-BASED METRICS The journal usage and citation networks also enable the calculation of a variety of impact metrics. A total of 47 possible impact metrics were calculated, and the resulting rankings were analyzed to determine the degree to whichusage- and citation-based metrics express similar or dissim ilar aspect of scholarly impact."}
{"pdf_id": "0804.3791", "content": "5.1Defining and validating usage-based met ricsThe most common indicator of journal status is Thom son Scientific's journal Impact Factor (IF) that is published every year for a set of about 8,000 selected journals. TheIF is defined as the average citation rate for articles pub lished in a particular journal. A similar statistical approach to journal ranking has been proposed for journal usage data"}
{"pdf_id": "0804.3791", "content": "The correlation matrix C can be used to map the similari ties and dissimilarities between the various metrics using aPrincipal Component Analysis (PCA) [10]. A PCA deter mines the set of \"dominant\" eigenvectors, i.e. those with thehighest eigenvalues, for the correlation (or co-variance) ma trix between a set of variables. These original correlationsare then mapped into the space spanned by the k eigenvec tors with the highest eigenvalues, the latter referred to as the principal components. A PCA that uses only the first 2 principal components of matrix C will thus result in a 2D"}
{"pdf_id": "0804.3791", "content": "usage-based metrics, or the cluster that combines citation betweenness and citation PageRank.These PCA results constitute only a preliminary, proof-of concept analysis executed on the basis of a limited set of possible metrics. Nevertheless, they provide useful insights regarding the nature and interrelation of a set of common, plausible metrics of impact, both usage- and citation-based.As the MESUR reference data set expands and the set of investigated metrics grows, a more complete survey of usage and citation-based metrics should result."}
{"pdf_id": "0805.0120", "content": "Nonnegative matrix factorization (NMF) was popularized as a toolfor data mining by Lee and Seung in 1999. NMF attempts to approx imate a matrix with nonnegative entries by a product of two low-rank matrices, also with nonnegative entries. We propose an algorithm called rank-one downdate (R1D) for computing a NMF that is partly motivated by singular value decomposition. This algorithm computes the dominant singular values and vectors of adaptively determined submatrices of a matrix. On each iteration, R1D extracts a rank-one submatrix from the dataset according to an objective function. We establish a theoretical result that maximizing this objective functioncorresponds to correctly classifying articles in a nearly separable cor pus. We also provide computational experiments showing the success of this method in identifying features in realistic datasets."}
{"pdf_id": "0805.0120", "content": "finding good rank-one submatrices of A and subtracting them from A. The classical greedy rank-one downdating algorithm is Jordan's algorithm for the SVD, described in Section 3. Related work on greedy rank-one downdating for NMF is the topic of Section 4. The subroutine ApproxRankOneSubmatrix, presented later in this section, is a heuristic routine to maximize the following objective function:"}
{"pdf_id": "0805.0120", "content": "Perhaps unexpectedly, the dominant right singular vector of A is very close to being proportional to [1; 1; 1; 1], i.e., the two topics are entangled in one singular vector. The reason for this behavior is that the matrix B has two nearly equal singular values, so its singular vectors are highly sensitive tosmall perturbations (such as the matrix E). R1D avoids this pitfall by com puting the dominant singular vector of a submatrix of the original A instead of the whole matrix."}
{"pdf_id": "0805.0120", "content": "• Thus, the preceding lemmas imply that heavy acceptable entries from a single topic k must dominate the optimal solution. Therefore, we show in Lemma 8 that the left and right singular vectors of the optimal A(M, N) can be estimated from P(M, k) and the vector of lengths of documents indexed by N respectively."}
{"pdf_id": "0805.0120", "content": "Proof. The sum of squares of entries in A(M, N) from unacceptable docu ments is bounded above by the sum of squares of entries in A of unacceptable documents, for which we have the estimate given by (27). The sum of squares of entries of A(M, N) which are acceptable but not heavy is bounded above by the same quantity for all of A, which is given by (31). Adding these two upper bounds gives a quantity less than half of the lower bound in (28), which proves the result."}
{"pdf_id": "0805.0192", "content": "FORTRAN or C/C++), with several drawbacks: (i) lack of portability between big endian and little-endian platforms (and vice-versa), or between 32-bit and 64-bit platforms; (ii) difficulties to read the files written by F77/90 codes from C/C++ software (and vice versa); (iii) lack of extensibility, as one file produced for one version of the software might not  be readable by a past/forthcoming version"}
{"pdf_id": "0805.0192", "content": "It provides also functions to inquire  about the content of a file (names of variables, associated dimensions and attributes), to access  the information associated to a variable name (in full or by segments), to copy it, to rename  attributes or variables, or to delete some of its content"}
{"pdf_id": "0805.0192", "content": "The ability of NetCDF to retrieve  the information, irrespective of the actual physical layout of the file, is a key characteristic  allowing exchange of data between different software (and also different versions of the same  software), that contrasts with the rigidity of the usual binary representations"}
{"pdf_id": "0805.0192", "content": "In addition, we provide names for  variables that can be either mandatory or not (in the context of a file containing a  density/potential, or a wavefunction, or crystallographic data, or other large numerical data not  yet taken into account), but for which a NetCDF description has been agreed"}
{"pdf_id": "0805.0192", "content": "2. General specifications for NQ/ETSF NetCDF files  2.1. Global attributes of NQ/ETSF NetCDF files  Global attributes are used for a general description of the file, mainly the file format  convention. Important data is not contained in attributes, but rather in variables.  Table 1 gather specifications for required attributes in any NQ NetCDF files. Table 2 presents  optional attributes for NQ/ETSF NetCDF files.  Detailed description (tables 1 and 2)  file_format Name of the file format for NQ/ETSF wavefunctions.  file_format_version Real version number for file format (e.g. 2.2 ).  Conventions NetCDF recommended attribute specifying where the conventions for the file"}
{"pdf_id": "0805.0192", "content": "title Short description of the content (system) of the file.  2.2. Generic attributes of variables in NQ/ETSF NetCDF files  A few attributes might apply to a large number of variables. They are gathered in Table 3 .  Detailed description (table 3)  units It is one of the NetCDF recommended attributes, but it only applies to a few variables in"}
{"pdf_id": "0805.0192", "content": "our case, since most are dimensionless. For dimensional variables, it is required. The  use of atomic units (corresponding to the string \"atomic units\") is advised throughout  for portability. If other units are used, the definition of an appropriate scaling factor to  atomic units is mandatory. Actually, the definition of the name \"units\" in the  NQ/ETSF files is only informative : the \"scale_to_atomic_units\" information should  be the only one used to read the file by machines."}
{"pdf_id": "0805.0192", "content": "number_of_symmetry_operations The number of symmetry operations.  number_of_atoms The number of atoms in the unit cell.  number_of_atom_species The number of different atom species in the unit cell.  symbol_length Maximum number of characters for the chemical symbols  Detailed description (Table 5)  max_number_of_states The maximum number of states"}
{"pdf_id": "0805.0192", "content": "2.5. Optional variables  In order to avoid the divergence of the formats in the additional data, we propose names and  formats for some information that is likely to be written to the files. None of these data is  mandatory for the file formats to be described later. Some of the proposed variables contain  redundant information.  Tables 6 to 8 present these optional variables, grouped with respect to their physical  relevance: atomic information, electronic structure, and reciprocal space.  Detailed description (tables 7 to 10)  valence_charges Ionic charges for each atom species.  pseudopotential_types Type of pseudopotential scheme   = \"bachelet-hamann-schlueter\", \"troullier-martins\", \"hamann\","}
{"pdf_id": "0805.0192", "content": "2.6 Naming conventions  NetCDF files, that respect the NQ/ETSF specifications described in the present document,  should be easily recognized, thanks to the final substring \"-etsf.nc\" . The appendix \".nc\" is a  standard convention for naming NetCDF files [2].  3. Specification for files containing crystallographic data  A NQ/ETSF NetCDF file for crystallographic data should contain the following set of  mandatory information :  (1) The three attributes defined in Table 1  (2) The following dimensions from Table 4 (dimensions that do not lead to a splitting) :  - number_of_cartesian_directions"}
{"pdf_id": "0805.0192", "content": "4. Specification for files containing a density and/or a potential  A NQ/ETSF NetCDF file for a density should contain the following set of mandatory  information :  (1) The three attributes defined in Table 1  (2) The following dimensions from Table 4 :  - number_of_cartesian_directions"}
{"pdf_id": "0805.0192", "content": "reduced_symmetry_translations, reduced_symmetry_matrices)  (7) The information related to each kpoint, as defined in Table 12  (8) The information related to each state (including eigenenergies and occupation numbers), as  defined in Table 13  (9) In case of basis set representation, the information related to the basis set, and the variable  coefficients_of_wavefunctions , as defined in Table 14  (10) In case of real-space representation, the variable real_space_wavefunctions, see Table 15.  Detailed description (Table 12)  reduced_coordinates_of_kpoints k-point in relative/reduced coordinates  kpoint_weights k-point integration weights. The weights must sum to 1. See the description  of the density construction, section 5.2.  Detailed description (Table 13)  number_of_states Number of states for each kpoint, if varying (the attribute k_dependent"}
{"pdf_id": "0805.0192", "content": "used_time_reversal_at_gamma is set to yes (only allowed for the plane wave basis  set), then, for the Gamma k point - reduced_coordinates_of_kpoints being equal to (0  0 0) - the time reversal symmetry has been used to nearly halve the number of plane  waves, with the coefficients of the wavefunction for a particular reciprocal vector  being the complex conjugate of the coefficients of the wavefunction at minus this  reciprocal vector. So, apart the origin, the coefficient of only one out of each pair of  corresponding plane waves ought to be specified. Note also that the dimension  max_number_of_coefficients  actually  governs  the  size  of"}
{"pdf_id": "0805.0192", "content": "wavefunctions must be normalized to 1 per unit cell, i.e. the sum of the absolute  square of the coefficients of one wavefunction, for all points in the grid, divided by the  number of points must be 1. See section 5.2 . Note that this array has a number of  dimensions that exceeds the maximum allowed in FORTRAN (that is, seven). This  leads to practical problems only if the software to read/write this array attempts to  read/write it in one shot. Our suggestion is instead to read/write sequentially parts of  this array, e.g. to write the spin up part of it, and then, add the spin down. This might  be done using Fortran arrays with at most seven dimensions."}
{"pdf_id": "0805.0192", "content": "where wk is contained in the array \"kpoint_weights\" of Table 12, and  fn,k is contained in the array \"occupations\" of Table 13.  This relation generalizes to the collinear spin-polarized case, as well as the non-collinear case  by taking into account the \"number_of_components\" defined in Table 5 , and the direction of  the magnetization vector.  (2) On the Kleinman-Bylander form factors.  One can always write the non-local part of Kleinman-Bylander pseudopotential (reciprocal  space) in the following way :"}
{"pdf_id": "0805.0202", "content": "The paper is organized as follows. The first section introduces both the MQC problem and the MQI problem. The following section develops a Pseudo Boolean Optimiza tion (PBO) model for the MQC problem and Section 4 proposes three optimizations to the PBO model. Section 5 shows the experimental results obtained and Section 6 presents some conclusions and points some directions for future research."}
{"pdf_id": "0805.0202", "content": "Suppose that quartet number t is the quartet [i, j|l, m]. The model associates two new variables to each of the conditions (7) and (8). Let d1i,j,l,m be associated with condition (7) and d2i,j,l,m be associated with condition (8). The associated variable qt is encoded as a gate OR:"}
{"pdf_id": "0805.0202", "content": "Both the conditions (7), (8) consist of logical ANDs of two greater than conditions. Thus variable d1i,j,l,m and d2i,j,l,m are encoded as gates AND in a analogous way to variables c1i,j,l. The cost function of the PBO model is then to maximize the number of quartets that are consistent, that is:"}
{"pdf_id": "0805.0202", "content": "This section describes three optimizations to the basic PBO model. The first optimiza tion aims reusing auxiliary variables that serve for encoding of some of the circuits associated with the PBO model. The second optimization is related with the Boolean variables used for representing the value of each entry in the ultrametric matrix. The third optimization sets the values for some of M(i, j) variables when it is known that si and sj are siblings."}
{"pdf_id": "0805.0202", "content": "The objective of the first optimization is to reduce the number of variables used in the encoding. The reduction is achieved by exploiting the information provided by the auxiliary variables used for encoding cardinality constraints. In order to implement this optimization, sequential counters [8] are used. The uniqueness constraint (1) of the PBO model in Section 3 is split into two constraints. The first constraint deals with the need to have one at least one variable selected by adding the constraint:"}
{"pdf_id": "0805.0202", "content": "leads to lower CPU time spent by the PBO-solver. Nevertheless, model PBO+(scd+trd)reduces even further the model by considering the selection variables as bits of the binary representation of values in M. Again, it can be seen from Table 2, that the reduc tion on the number of variables and constraints used by the encoding resulted in lower CPU times spent by the PBO-solver, where the model PBO+(scd+trd) is on average approximately 4 times faster than the PBO+trd and 1.6 times faster than PBO+fst. Comparing the best of our PBO models (PBO+(scd+trd)) with the ASP model, the ASP model is more effective when the percentage of modified quartets is small, but the PBO+(scd+trd) model becomes more when the percentage of modified quartets increases."}
{"pdf_id": "0805.0459", "content": "In other view, in monitoring of most  complex systems, there are some generic challenges for example sparse essence,  conflicts in different levels, inaccuracy and limitation of measurements ,which in  beyond of inherent feature of such interacted systems are real obstacle in their  analysis and predicating of behaviors"}
{"pdf_id": "0805.0459", "content": "Based upon the above, hierarchical nature of complex systems [6], developed  (developing) several branches of natural computing (and related limbs) [7],  collaborations, conflicts, emotions and other features of real complex systems, we  propose a general framework of the known computing methods in the connected (or  complex hybrid) shape, so that the aim is to inferring of the substantial behaviors of  intricate and entangled large societies"}
{"pdf_id": "0805.0459", "content": "Complexity of this system, called MAny  Connected Intelligent Particles Systems (MACIPS), add to reactions of particles  against information flow, and can open new horizons in studying of this big query: is  there a unified theory for the ways in which elements of a system(or aggregation of  systems) organize themselves to produce a behavior?[8]"}
{"pdf_id": "0805.0459", "content": "Developing of such intelligent hierarchical networks, investigations of their  performances on the noisy information and exploration of possible relate between  phase transition steps of the MACIPS and flow of information in to such systems are  new interesting fields, as well in various fields of science and economy"}
{"pdf_id": "0805.0459", "content": "Developed algorithms use four basic axioms upon the balancing of the successive  granules assumption:  • Step (1): dividing the monitored data into groups of training and testing data  • Step (2): first granulation (crisp) by SOM or other crisp granulation methods  Step (2-1): selecting the level of granularity randomly or depend on the obtained  error from the NFIS or RST (regular neuron growth)  Step (2-2): construction of the granules (crisp)"}
{"pdf_id": "0805.0459", "content": "the test data and coefficients must be determined, depend on the used data set.  Obviously, one can employ like manipulation in the rule (second granulation)  generation part, i.e., number of rules (as a pliable regulator).  Determination of granulation level is controlled with three main parameters: range of  neuron growth, number of rules and error level. The main benefit of this algorithm is  to looking for best structure and rules for two known intelligent system, while in  independent situations each of them has some appropriate problems such: finding of  spurious patterns for the large data sets, extra-time training of NFIS or SOM."}
{"pdf_id": "0805.0459", "content": "The main benefit of this algorithm is to looking for  best structure and rules for two known intelligent system, while in independent  situations each of them has some appropriate problems such finding of spurious  patterns for the large data sets, extra-time training of NFIS for large data set"}
{"pdf_id": "0805.0459", "content": "Despite of the aforesaid background behind the proposed algorithms, we can assume  interactions of the two layer of algorithm as behaviors of complex systems such:  society and government, where reactions of a dynamic community to an \"absolute  (solid) or flexible\" government (regulator) is controlled by correlation factors of the  two simplified systems"}
{"pdf_id": "0805.0459", "content": "It must be noticed, we may choose other two general connected networks  or other natural inspired systems involve such hierarchical topology for instances:  stock market and stock holders, queen and bees, confliction and quarrel between two  countries, interaction among nations (so its outcome can be strategy identifying for  trade barriers[19]) and so on"}
{"pdf_id": "0805.0459", "content": "In this part of paper, we ensue our algorithms on the \"lugeon data set\" [15]. This  study only considers phase transition view of our proposed algorithms and direct  applications of the mentioned systems in other data sets can be found in [15], [16].To  evaluate the interactions due to the lugeon values we follow two situations where  phase transition measure is upon the crisp granules (here NG): 1) second layer gets a  few limited rules by using NFIS; 2) second layer gets all of extracted rules by RST  and under an approximated progressing."}
{"pdf_id": "0805.0459", "content": "4 10 ), may display another feature of society alteration: the proper chaos related  to the later fashion has larger values so that is not relatively agreed with N.G. In fact,  our government loses pervious relative order. In both two former and latter options,  the phase transition has been occurred gradationally likewise one can consider three  discrete steps to these conversions: society with \"silent dead (laminar)\", in transition  and in triggering of revolutionary community."}
{"pdf_id": "0805.0459", "content": "Developing of such intelligent hierarchical networks, investigations of their  performances on the noisy information and exploration of possible relate between  phase transition steps of the MACIPS and flow of information in to such systems are  new interesting fields, as well in various fields of science and economy"}
{"pdf_id": "0805.0642", "content": "Based upon the above, hierarchical nature of complex  systems [6], developed (developing) several branches  of natural computing (and related limbs) [7],  collaborations, conflicts, emotions and other features  of real complex systems, we propose a general  framework of the known computing methods in the  connected (or complex hybrid) shape, so that the aim is  to inferring of the substantial behaviors of intricate and  entangled large societies"}
{"pdf_id": "0805.0642", "content": "Complexity  of  this  system, called MAny Connected Intelligent Particles  Systems (MACIPS), add to reactions of particles  against information flow, can open new horizons in  studying of this big query: is there a unified theory for  the ways in which elements of a system(or aggregation  of systems) organize themselves to produce a  behavior?[8]"}
{"pdf_id": "0805.0642", "content": "then investigate several levels of responses in facing  with the real information. We show how relatively  such our simple methods that can produce (mimic)  complicated  behavior  of  government-nation  interactions .Mutual relations between proposed  algorithms layers identify order-disorder transferring  of such systems. Developing of such intelligent  hierarchical  networks,  investigations  of  their  performances on the noisy information and exploration  of possible relate between phase transition steps of the  MACIPS and flow of information in to such systems  are new interesting fields, as well in various fields of  science and economy."}
{"pdf_id": "0805.0642", "content": "obtained  error  (measured  error)  from  second  granulation on the test data and coefficients must be  determined, depend on the used data set. Granulation  level is controlled with four main parameters: range of  neuron  growth,  number  of  rules,  number  of  discretization of attributes in RST and/or error level.  The main benefit of SONFIS is to looking for best  structure and rules for two known intelligent system,  while in independent situations each of them has some  appropriate problems such: finding of spurious  patterns for the large data sets, extra-time training of  NFIS or SOM."}
{"pdf_id": "0805.0642", "content": "To evaluate the  interactions due to the lugeon values we follow two  situations where phase transition measure is upon the  crisp granules (here NG): 1) second layer takes a few  limited rules by using NFIS; 2) second layer keep all  of extracted rules by RST and under an approximated  progressing (with changing of scaling)"}
{"pdf_id": "0805.0642", "content": "neural computing techniques for comparing with words,  eds. Pal, S. K., Polkowski, L., Skowron, A. pp.219— 250(2004).  18. Bonabeau E., Dorigo M., Theraulaz G.: Swarm  Intelligence: From Natural to Artificial Systems. New  York, NY: Oxford University Press (1999)  19. Owladeghaffari,H., Pedrycz,W.: Many Connected Intelligent Particles Systems: A Path Towards Society Government Interactions. Preparing for Nature  20. Copeland.,B.R.: Strategic Interaction among Nations:  Negotiable and Non-negotiable Trade Barriers. Canadian  Journal of Economics.pp.2384-108, (1990)"}
{"pdf_id": "0805.0785", "content": "Abstract: If a computer node is infected by a virus, worm or a backdoor, then this is a security risk for the complete network structure where the node isassociated. Existing Network Intrusion Detection Systems (NIDS) provide a cer tain amount of support for the identification of such infected nodes but suffer from the need of plenty of communication and computational power. In this article, we present a novel approach called AGNOSCO to support the identification of infectednodes through the usage of artificial ant colonies. It is shown that AGNOSCO overcomes the communication and computational power problem while identifying in fected nodes properly. Keywords: Network Protection, Intrusion Detection, Bio-inspired Computing, Ant Colonies."}
{"pdf_id": "0805.0785", "content": "In the current working and life environment, connected nodes - computers, servers, etc. - are essential. These nodes are under constant assault form attacks like e.g. worms, trojans, and hackers. Nowadays, there exist several approaches to protect a computer node or a network against criminal attacks like virus- and malwareguards, symbolic NIDS-solutions like SNORT [9, 2, 10], and bio-inspired NIDS solutions (Artificial Immune Systems, [6, 7, 11]). These protection-systems check each packet, which traverses a network node, and evaluate if this packet intends to attack or not. However, many NIDS solutions suffer from identifying (new) attacks"}
{"pdf_id": "0805.0785", "content": "as well as from the need of plenty of computational power; furthermore, there exist applied techniques to camounage attacks in a way that NIDS are not able to identify the attack at all. Hence, there are situations when an attack infects a node and when a computer network risks to be infected by the node. This is much more critical as it seems since infections can cause a backdoor to other attacks, infections can send packets containing an attack to infect healthy nodes. The identification of such an infected node - sometimes also zombie-node called - is a well-know problem. In the current research community, only a few approaches of identifying infected nodes are known, for example"}
{"pdf_id": "0805.0785", "content": "• Inference from Network Traffic Analysis: If a network node is infected, the network node releases several packets containing an attack in order to infect also other nodes of the network. This behaviour can be recognized using intrusion detection and an intelligent inference system is used in order to derive to the infected node."}
{"pdf_id": "0805.0785", "content": "Unfortunately, all these approaches have significant disadvantages. First, they need information from the computer network that must be collected, fusioned, and further processed. Consequently, this results in high communication costs wherethe centric evaluation affords plenty of computational power. Second, the last ap proach shares several other disadvantages, e.g., defining an incorrect answer and deciding when a node should not send any packets. Following this, our motivationis that novel (bio-inspired) systems can significantly contribute to a higher identi fication rate of infected nodes."}
{"pdf_id": "0805.0785", "content": "where b is the number of infected (bad) packets over this connection and the parameter inc the increasing-factor of the system. The parameter dec is the decreasing-factor of the system and #good-packetsi the number of good packetswhich travelled over the connection after the i-th bad packet. In this test simula tion, we adjusted inc to the value of 20 and dec permanently to 0.95. Then, the worknow of the affinity-function is as follows:"}
{"pdf_id": "0805.0785", "content": "added in order to store the pheromone-value, at most 10kB per connection. TheNetwork Protocols must not be changed and AGNOSCO is compatible with ex isting protocols. Essentially, the NIDS-Behaviour concerning identified maliciouspackets must be changed; if the NIDS identifies a packet as malicious, addition ally it must send a confirmation-packet for this bad-packet in order to update the pheromone-values on the path from source to destination."}
{"pdf_id": "0805.0785", "content": "The affinity-function is biologically inspired. In human affinity-functions, an event increases the affinity heavily and, over time if no new event occurs, the value of the affinity-function decreases primarily heavily and afterwards slowly. This means, that the gradient of the function is primarily high and decreases afterwards. Thus, the human body reacts using the affinity-function to an event heavily; thereafter,with the high gradient, the human body tries to compensate an error; and after wards, with the low gradient, it tries to reach a stable value."}
{"pdf_id": "0805.0785", "content": "follows the behaviour of ant colonies. AGNOSCO is implemented, simulated and tested; AGNOSCO efficiently identifies the infected network nodes unless taking both additional computational power and additional communication bandwidth. We are sure that AGNOSCO can enhance commonly used NIDS as well as SANA. Future enhancements of SANA especially the communication and collaboration of the artificial Cells in SANA will be our next challenges."}
{"pdf_id": "0805.0785", "content": "SANA and AGNOSCO are part of the project INTRA (= INternet TRAffic management and analysis) that are financially supported by the University of Luxem bourg. We would like to thank the Ministre Luxembourgeois de l'education et de la recherche for additional financial support and Jacob Zimmermann (Queensland University of Technology) for worthful discussions."}
{"pdf_id": "0805.1096", "content": "To solve these problems, we propose adaptive AP, including: adaptive adjustment of the damping factor to  eliminate oscillations (called adaptive damping), adaptive escaping oscillations by decreasing p when  adaptive damping method fails (called adaptive escape), and adaptive searching the space of p to find out the  optimal clustering solution suitable to a data set (called adaptive preference scanning). The adaptive AP is  proposed in Section 2, and experimental results are in Section 3. Finally, Section 4 gives the conclusion."}
{"pdf_id": "0805.1096", "content": "2 Adaptive Affinity Propagation  In this section, the adaptive damping and escape methods are discussed first to eliminate oscillations, and  then the adaptive scanning of p is designed. Finally, a cluster validity method is adopted to find the optimal  clustering solution. It is noted that the same initial value is assigned to all the p(i) in the diagonal of matrix S."}
{"pdf_id": "0805.1096", "content": "If it fails to depress oscillations by increasing lam (e.g., lam is increased to 0.85 or higher), an adaptive  escape technique will be designed to avoid oscillations. That large lam brings little effect suggests that  oscillations are pertinacious under the given p, so the alternative is to decrease p away from the given p to  escape from oscillations. This escape method is workable due to that it works together with adaptive  scanning of p discussed below, different from AP that works under a fixed p."}
{"pdf_id": "0805.1096", "content": "The number of identified clusters depends on input p, but it is unknown which value of p will give best  clustering solution for a given data set. Generally, cluster validation techniques (usually based on validation  indices) [3] are used to evaluate which clustering solution is optimal for a data set. AP algorithm need give a  series of clustering solutions with different NCs, among which the optimal clustering solution is found by a  cluster validation index. There is no exact corresponding relation between the p and output NC, so we design  the method of scanning space of p to obtain different NCs."}
{"pdf_id": "0805.1096", "content": "The adaptive p-scanning technique is designed as follows: (1) specify a large p to start the algorithm; (2)  an iteration runs and gives K exemplars; (3) check whether K exemplars converge (the condition is that  every exemplar satisfies preset continuously unchanging times v); (4) go to step (5) if K exemplars converge,  otherwise go to step (2); (5) decrease the p by step ps if K exemplars converge too in additional dy iterations  (this is for more reliable convergence), otherwise go to step (2); (6) go to step (2)."}
{"pdf_id": "0805.1096", "content": "Thus, a series of clustering results with different NCs can be gained through scanning p, and the scanning  of p space is designed inside the iterative process to keep the advantage of speed. To avoid possible repeated  computation, in the p-scanning process we continue to calculate R(i,k) and A(i,k) based on (or using) the  current values of R(i,j) and A(i,j) after each reduction of p (then S(i,i)=p(i) is changed but other elements of S  are unchanged)."}
{"pdf_id": "0805.1096", "content": "In order to check whether the convergence condition is satisfied, another monitoring window B (similar to  that in adaptive damping method) is adopted to record the continuously unchanging times v of K exemplar,  and the window size is set to be v=40, which is consistent with default convergence times 50 in AP [1] (v=40  pluses delay times of 10)."}
{"pdf_id": "0805.1096", "content": "Now the adaptive AP gives clustering solutions with different NCs through the p-scanning process, and  then cluster validation technique is used to evaluate quality of these solutions. It is the validity indices that  are usually used to evaluate quality of clustering results and to evaluate which clustering solution is the  optimal for the data set. Among many validity indices, Silhouette index, which reflects the compactness and  separation of clusters, is widely-used and has good performance on NC estimation for obvious cluster  structures. It is applicable to both the estimation of the optimal NC and evaluation of clustering quality.  Hence, we adopt Silhouette index, as an illustration, to find the optimal clustering solution."}
{"pdf_id": "0805.1096", "content": "With Sil(t) for each sample, overall average silhouette Sil for n samples of the data set is obtained directly.  The largest overall average silhouette indicates the best clustering quality and the optimal NC [3]. Using  formula (1), a series of Sil values corresponding to clustering solutions under different NCs are calculated,  and the optimal clustering solution is found at the largest Sil."}
{"pdf_id": "0805.1096", "content": "3 Experimental Results  This section compares the clustering performance between adaptive AP method (adAP) and AP algorithm  (AP). The items of clustering performance include: whether adAP can eliminate oscillations (if oscillations  occur) automatically so as to give correct clustering results, whether adAP can give correct clustering results  based on the Silhouette index (or cluster validation technique). The adAP and AP use same initial lam=0.5  (but lam=0.8 in Travelroute experiment), and AP uses fixed p=pm and maxits=2000. For Document and  Travelroute experiments, both methods use fixed p from prior knowledge [1]."}
{"pdf_id": "0805.1096", "content": "Twelve data sets in Table 3 are used in the experiments, where the first eight data sets have known class  labels. Their features include: far and close well-separated clusters, slight overlapping clusters, tight clusters  and loose clusters. The first four data sets are simulated data, while other data sets are real data. The Yeast  and NCI60 are gene expression data, and a subset of dataset Exons is used, i.e., the first 3499 samples and  the last one (= 3500 samples) from 75067 samples are used."}
{"pdf_id": "0805.1096", "content": "In Table 4 one can see: for all the datasets except the last four datasets, adAP gives correct NC in all the  cases, while AP fails in all the cases; FM values of adAP are higher than that of AP, indicating that adAP  gives better clustering quality than AP; and the oscillations lead AP to poor solutions for 22k10far and  Ionosphere"}
{"pdf_id": "0805.1096", "content": "The clustering task is to find representative sentences (or cluster centers) for Document data,  and both adAP and AP find the same four representative sentences; and the task is to find the appropriate  airport (or cluster centers) as airport hub for Travelroute data, and both adAP and AP find the same seven  airports"}
{"pdf_id": "0805.1154", "content": "Examing the full count of scientific citations from Wikipedia a marked increasebecomes apparent with a rise in the number of citations from 2007 to the exam ined dump of March 2008, see Figure 1: From 74,776 citations in the October 2007 dump to 228,593 in the March 2008 dump.Whereas astronomy journals received comparably many citations from Wikipedia in the 2007 dumps, and journals such as The Journal of Biological Chemistry had relatively few citations when compared to the Journal Citation Re"}
{"pdf_id": "0805.1154", "content": "A few examples of items in a sample of clusters from an NMF run with twenty clusters are shown in Table 2. These kinds of results may be written to an HTML page and put on the web to serve as an online overview of how science is cited from Wikipedia."}
{"pdf_id": "0805.1288", "content": "One of the most important stages of the Neuro-fuzzy TSK network generation is the establish ment of the inference rules. Often employed method is used the so-called grid method, in which  the rules are defined as the combinations of the membership functions for each input variable.  If we split the input variable range into a limited number (say  in for i=1, 2... n) of membership"}
{"pdf_id": "0805.1288", "content": "In this part, we reproduce the proposed a hybrid intelligent algorithm in (Owladeghaffari et al,  2008):  Step (1): dividing the monitored data into groups of training and testing data  Step (2): first granulation (clustering) by SOM or other crisp granulation methods  Step (2-1): selecting the level of granularity randomly or depend on the obtained error from  the NFIS or RST (regular neuron growth)  Step (2-2): construction of the granules (no-fuzzy clusters)"}
{"pdf_id": "0805.1288", "content": "Step (4): extraction of knowledge rules Balancing assumption is satisfied by the close-open iterations: this process is a guideline to balancing of crisp and sub fuzzy/rough granules by some random/regular selection of initial granules or other optimal structures and increment of supporting rules (fuzzy partitions or increas ing of lower /upper approximations ), gradually"}
{"pdf_id": "0805.1288", "content": "With considering this point that the creation of discernible matrix-in RST- is depend on the  transferring of data in to the arbitrary-or best- ranges (bins)-symbolic values-, we employ one  dimensional topology grid SOM, in which attributes are transferred within 3 categories: low (1), medium (2) and high (3) (fig3)"}
{"pdf_id": "0805.1288", "content": "where m is the number of test data .  Figure 6(a&b) indicate the results of the aforesaid system (so, performance of selected  SONFIS-R on the test data). In this case, we set the range of first granules (crisp clusters)  between 5 and 20, as well as lower and upper floor. So, the number of leanings in second  layer of SONFIS is supposed as a constant value, i.e., 20, for all inserted crisp granules.  Add to this, we use Gaussian membership functions in fuzzy clustering. After 45 time steps"}
{"pdf_id": "0805.1288", "content": "The results of first granulation by 17*1 neurons in competitive layer of SOM has been portrayed  in figure 7, as matrix plot form. It must be notice here; we reduced all of objects in to the 17  patterns, which are in balance with the simplest rules of NFIS, while we had employed error measure criteria to balancing. SONFIS-R which has been employed in other comprehensive da ta set, show ability of this system in detection of the dominant structures on the attributes and  representation of the simplest rules, as well as one wishes to catch up (Owladeghaffari et  al,2008)."}
{"pdf_id": "0805.1473", "content": "(where k and l might be 0). It has been shown in [4] that ll-closed constraints are a largest tractable language in the sense that every TCL that strictly contains one of our two languages has an NP-complete constraint satisfaction problem. The presented algorithm for ll-closed constraints has a running time that is quadratic in the size of its input.Traditionally, one of the main algorithmic tools in constraint satisfaction, and in par ticular in temporal reasoning, are local consistency techniques [1,10,16,25,28], for instance algorithms based on establishing path-consistency. Consistency based algorithms can be"}
{"pdf_id": "0805.1473", "content": "formulated conveniently as Datalog programs [2,13,21]. Roughly speaking, Datalog is Pro log without function symbols, and comes from Database theory [12]. We show that, unlikeOrd-Horn [28], ll-closed and dual ll-closed constraints can not be solved by a Datalog pro gram. In our proof we apply a pebble-game argument that was originally introduced for finite domains [13,21], but has been shown to generalize to a wide range of infinite domain constraint languages, including TCLs [2]. This is interesting from a theoretical point ofview: for constraint satisfaction problems of languages over a finite domain, all known algo rithms are essentially based on algebraic algorithms or Datalog [13]. However, the algorithm we present for temporal reasoning is neither algebraic nor based on Datalog."}
{"pdf_id": "0805.1473", "content": "Finally, if there is no sink left, but not all variables have been projected out, then we can compute the strongly connected components of the resulting constraint (again, this can be done in linear time using depth-first search on our data structure), and since we know which variables are blocked, we can also find the sink components"}
{"pdf_id": "0805.1727", "content": "In this paper we present a novel algorithm inspired by an intriguing hypothesis by Franks and Sendova-Franks  concerning the biological mechanisms underlying annular sorting. In their article, the authors state that \"The  mechanism that the ants use to re-create these brood patterns when they move to a new nest is not fully known. Part  of the mechanism may involve conditional probabilities of picking up and putting down each item which depend on  each item's neighbours ... The mechanisms that set the distance to an item's neighbour are unknown. They may be  pheromones that the brood produce and which tend to diffuse over rather predictable distances ...\"(Franks and  Sendova Franks, 1992)"}
{"pdf_id": "0805.1727", "content": "In Section 2 we present the background to the problem, before describing our model in Section 3. In Section 4 we  describe in detail the metrics for assessing the quality of solutions generated, and in Section 5 we present and  discuss the results of experimental investigations (including extended parametric and convergence analyses). We  conclude in Section 6 with a discussion of the implications of our findings. This article is an extended version of  work first presented in (Amos and Don, 2007)."}
{"pdf_id": "0805.1727", "content": "Wilson et al. proposed the first model of \"ant-like annular sorting\"to simulate the behaviour of Temnothorax ants  using minimalist robot and computer simulations (Wilson et al., 2004). Three models for annular sorting were  presented: \"Object clustering using objects of different size\", \"Extended differential pullback\"and \"leaky  integrator\". The first was run exclusively as a computer simulation, since modifying robots to allow them to move  objects of different sizes proved to be too complex. Despite this, the computer simulation modelled physical robot  behaviour faithfully, preserving the limitations of movement inherent in simple robots, and even going so far as to  build in a 1% sensor error that matched the rate seen in the machines."}
{"pdf_id": "0805.1727", "content": "was used to select parameter values. Two subsequent models (Hartmann, 2005; Vik, 2005) both use a neural  network controller for individual ants, with network weights being evolved using a genetic algorithm. These models  have been successfully applied to the problems of clustering and annular sorting of objects (with spatial restrictions  imposed, see the later discussion.) Other related work has studied emergent sorting using cellular automata  (Scheidler et al., 2006)."}
{"pdf_id": "0805.1727", "content": "We now propose an alternative algorithm for annular sorting. In contrast to previous work, we focus our attention on  the items to be sorted rather than on the agents performing the sorting. Our algorithm is a distributed system in  which agents probabilistically pick up or drop items depending on an assessment of the item's \"score\"(calculated as  a function of its current position). Brood items of different sizes are represented by \"objects\". Agents and objects are  spatially distributed at random on a two-dimensional \"board\"of fixed size."}
{"pdf_id": "0805.1727", "content": "Each object has a placement score; agents move randomly across the board, and when they collide with an object  they calculate its placement score. This score is then used to probabilistically determine whether the agent should  pick up the object and become laden. Laden agents carry objects around the board, and at every time-step they  evaluate what placement score the carried object would have if it were to be deposited at the current point. This  score is then used to probabilistically determine whether the object should be deposited."}
{"pdf_id": "0805.1727", "content": "We initially solved this problem by introducing the  notion of \"energy\"; each agent starts with a fixed amount of energy, represented as an integer value, which is  decremented every time the agent picks up an object (the amount of energy lost is a function of the object's size)"}
{"pdf_id": "0805.1727", "content": "number of agents and numbers of objects of each size may be specified in advance. Agents may move over other  agents or over objects; this is in contrast to previous work modelling robotic agents, where inherent spatial  restrictions exist. We impose no such limitations, and discuss in a later section the implications for comparison of  results. Movement may occur continuously in any direction on the Cartesian plane; we do not impose a discrete,  cell-based \"neighbourhood\". The algorithm is depicted in flowchart form in Figure 4. The pseudo-code expression  of the algorithm is as follows:"}
{"pdf_id": "0805.1727", "content": "In order to assess the quality of sorted structures, we apply three performance metrics: separation, shape, and radial  displacement, as defined in previous work (Wilson et al., 2004). Separation and shape are expressed as a percentage,  with a value of 100% being interpreted as ideal. Separation measures the degree to which objects of similar size are  kept apart from objects of differing size (i.e., the degree of \"segregation\"). The distance to the structure centroid is  calculated for each object, and the upper and lower quartiles computed for each object type. We then perform three  individual counts:"}
{"pdf_id": "0805.1727", "content": "this by constructing a graph, with each vertex representing a small object, and an edge connecting two vertices if the  corresponding objects are within 2.5 spatial units of one another. We then divide the size of the largest connected  component of this graph by the total number of small objects. The second stage of the shape calculation involves  finding the deviation from some common radius for each object size, since each object would ideally lie on the same  radius as every other object of that size. For the medium and large objects, we first calculate the common radius by  taking the mean radial distance from the centroid ("}
{"pdf_id": "0805.1727", "content": "Radial displacement is used to measure the \"compactness\" of a structure, and yields a distribution of distances from  the centroid for each object type. Previous studies (Wilson et al., 2004; Hartmann, 2005) provide precise formulae  for the calculation of compactness for a given structure, but this is difficult for our model. Earlier work used objects  of uniform size, which makes the task of calculating an optimal \"packing\"relatively straightforward. Here, however,  we use objects of non-uniform size, and little work has been done on packing collections of such objects."}
{"pdf_id": "0805.1727", "content": "We should note that it is difficult to draw direct comparisons between our results and those of (Wilson et al., 2004),  as their model enforces strict spatial constraints on the movement of agents and objects. In addition, (Hartmann,  2005) presents results only in the context of genetic algorithm fitness evaluations, with no individual breakdowns for  each metric, so direct comparisons are again difficult (although this paper does use the same separation and shape  algorithms as the those used by (Wilson et al., 2004) and ourselves). Nonetheless, the metrics provide a useful  standardised framework for performance analysis."}
{"pdf_id": "0805.1727", "content": "The first set of experiments replicated the initial conditions described in (Wilson et al., 2004): 15 objects of each  type, randomly distributed across the surface, with 6 agents. The average separation and shape scores for 50 initial  configurations were 11.85% and 48.21% respectively. The results obtained are depicted in Table I, with the best  figures obtained highlighted in bold. The radial displacement distributions and a typical final pattern are depicted in  Figure 5."}
{"pdf_id": "0805.1727", "content": "The figures of 79.52% and 70.88% for separation and shape respectively compare well with the figures of 59% and  68.5% obtained by the leaky integrator of (Wilson et al., 2004) (noting that their simulation includes extra spatial  constraints and uses objects of uniform size, whilst ours has no such constraints but handles objects of different  sizes)."}
{"pdf_id": "0805.1727", "content": "The aim of the second set of experiments was the assess the algorithm against the type of configuration observed in  actual Temnothorax nests; that is, where there are many more small brood items than large (i.e., older) items  (Franks and Sendova Franks, 1992) (see Figure 1). In these experiments, we randomly distributed 40 small objects,  20 medium objects and 10 large objects. In order to retain the agent-to-object ratio used in the previous set of  experiments, we used 10 agents in this set. The average separation and shape scores for 50 initial configurations  were 17.16"}
{"pdf_id": "0805.1727", "content": "shape. The algorithm clearly performs best when applied to distributions of objects that roughly match those  observed in nature. The high separation score of 93.04% is in general partly due to the observed creation of a large,  densely-packed core of small objects at the centre of the structure. Once built, this core is rarely disturbed by the  agents, and sorting only occurs in the outer bands."}
{"pdf_id": "0805.1727", "content": "The aim of the second set of experiments was the assess the algorithm's ability to perform annular sorting of objects  that were pre-sorted into piles. We created three piles, each one consisting of 15 objects of a particular size  randomly clustered around a fixed point (Figure 7). As in the first experiment, 6 agents were used. The separation  and shape scores for initial configurations are clearly meaningless in this context, so we omit them here. The results  obtained are depicted in Table III, with the best figures obtained highlighted in bold. The radial displacement  distributions and a typical final pattern are depicted in Figure 8."}
{"pdf_id": "0805.1727", "content": "Our studies show that the algorithm is able to convert a pre-sorted configuration into one that is sorted in an annular  fashion. Given sufficient energy, there is little difference in performance in sorting either pre-sorted or randomly  distributed configurations. Observation of the algorithm shows that, in general, the agents form two clusters of  roughly equal size and composition (Figure 7). These are gradually merged into a single structure which is then  refined in terms of shape and separation. It is important to note that no modifications (either to the model code or to  the parameters) were necessary in order for these results to obtained. This suggests that the model is robust and  capable of dealing with a variety of initial configurations."}
{"pdf_id": "0805.1727", "content": "We first investigated the effect of changing the amount of energy allocated to each agent, the idea being to establish  the optimal amount, given that termination of the algorithm only occurs when every agent's energy is exhausted. In  these sets of experiments, we used one agent per object."}
{"pdf_id": "0805.1727", "content": "In the case of uniform object numbers (Figure 9), we began by giving each of the 45 agents 10 units of energy, and  then gradually increased this amount up to a maximum of 200. The previous experiments suggested that no  performance benefit could accrue beyond this point (9000/45=200), which was confirmed by this set of trials. Both  performance curves began to flatten at around 100, and no increase was seen after 200 units. Run time increased  linearly with increases in energy."}
{"pdf_id": "0805.1727", "content": "The uniform situation (Figure 9) required rather more energy to achieve stability than the the mixed situation (Figure  10); we believe that this is due again to the formation, in the mixed case, of a core of small objects which are then  rarely disturbed. Again, we observed a linear relationship between energy and run time."}
{"pdf_id": "0805.1727", "content": "We then examined the effect of the ratio of agents to objects, the idea being to establish the point at which collective  (as opposed to individual) computation becomes effective. For each set of such trials, we established, from the  previous experiments the optimal net energy in the system, and then distributed this over a varying number of  agents. For example, we already established that the optimal system energy in the uniform case was"}
{"pdf_id": "0805.1727", "content": "Clearly, from Figures 11 and 12, the ratio of agents to objects has little effect on the overall quality of the solutions  generated. Both sets of performance metrics are in line with those previously observed. However, the average  duration of a run varied dramatically, with small numbers of agents yielding large run times (remembering that runs  are terminated by the exhaustion of energy). In both cases (uniform (Figure 11) and mixed (Figure 12) distribution  of object numbers), average run time stabilises when the number of agents is roughly half that of the objects. After  this point, adding extra agents appears to have no significant effect on reducing run time."}
{"pdf_id": "0805.1727", "content": "This set of experiments concerned the biological realism of forcing each agent to expend an amount of energy  proportional to the size of the object carried. We performed a set of control trials, where energy is removed as  described in the original algorithm, and then ran a series of trials where the energy penalty for moving an object was  fixed, regardless of its size. The agent numbers and their initial energy values were determined using the results  obtained from the previous sets of experiments. In the uniform case (Table IV), we ran with"}
{"pdf_id": "0805.1727", "content": "The results (Tables IV and V) suggested that a size-dependent penalty is moderately beneficial. A large fixed  penalty led to premature convergence of the algorithm, as the system energy was expended before the agents have  had a chance to construct a good configuration. Conversely, a small fixed penalty did not offer any improvement  over the control (apart from a small reduction in run time in the mixed case)."}
{"pdf_id": "0805.1727", "content": "5.5. Convergence analysis In the final set of experiments, we performed some trials without the use of energy, choosing instead to terminate the  algorithm after a fixed number of \"steps\". The aim here was to investigate the convergence behaviour of the  algorithm for different initial configurations, and to establish (based on earlier discussions (Melhuish, 2005))  whether or not the use of energy provided a satisfactory termination method."}
{"pdf_id": "0805.1727", "content": "For each initial configuration type, we first varied the number of agents, and investigated the relationship between  population size and convergence of the task towards \"completion\" (in terms of separation and shape performance).  For each trial we define a step as the execution of one agent's instructions, assessed the quality of the configuration  every 25,000 steps, and terminated the run after 1,000,000 steps. As in previous experiments, results were averaged  over 50 trials."}
{"pdf_id": "0805.1727", "content": "The results obtained are depicted in Figures 13 and 14. Based on these results, we then investigated the impact of the  choice of termination mechanism (energy or steps) on the real elapsed run-time of the algorithm. In each case, we  ran 50 trials, one set using energy termination, and the other terminated after a fixed number of steps."}
{"pdf_id": "0805.1727", "content": "In both cases, the use of energy as the termination mechanisn led to high-quality final configurations, but the use of  steps facilitated comparable results in a shorter period of time (Tables VI and VII). Future work will consider the  scalability of the algorithm, and attempt to derive general guidelines concerning the choice of termination  conditions."}
{"pdf_id": "0805.1727", "content": "In theoretical terms, more work is required  on analysis of our algorithm's convergence properties; similar work in related fields such as particle swarm  optimization has generated good results, so we are hopeful that the algorithm will soon be solidly grounded in theory  to augment existing empirical work"}
{"pdf_id": "0805.1727", "content": "We have a particular interest in modelling biological systems  at levels both above and below that of individual organisms, and the notion of attraction-repulsion has clear  significance for both molecular and cellular self-assembly and related macro-scale biological phenomena, such as  the formation of biofilms or spatio-temporal patterns in response to stress"}
{"pdf_id": "0805.1854", "content": "Semi-automated, or interactive, image segmentationmethods have successfully been used in different appli cations, whenever human knowledge may be provided asinitial guiding clues for the segmentation process. Exam ples of such methods are the region-growing technique, marker-based watersheds [16], the IFT [7], graph-cutsand Markov-random fields [1, 14, 15], amongst oth ers. Another source of a priori information for segmentation are image models, which consist of representative instances of desired objects, conveying different types of features (e.g. color, shape, geometry, relations, etc.) that describe"}
{"pdf_id": "0805.1854", "content": "This paper proposed a novel algorithm for performing in teractive model-based image segmentation using attributed relational graphs to represent both model and input images. This approach allows the usage of information ranging from appearance features to structural constraints. Topological differences between graphs are dealt with by means of a deformation ARG, a structure which allowed the design of an optimization algorithm for graph matching that evaluatespossible solutions according to local impacts (or deforma tions) they determine on the model. The faster performance of the algorithm in comparison with the one proposed in [4],the reusability of the model graph when segmenting sev eral images, as well as the satisfying quality of the resultsdue to the adequate use of structural information, character ize the main contributions of the method."}
{"pdf_id": "0805.1854", "content": "Our ongoing work is devoted to reducing interaction when reusing the model to segment various images. For now, it is required that the user places the stamp over the area of interest of the image. In the future, we hope to beable to apply the model ARG without the need of this inter active positional information. This shall be accomplished through the investigation of MAP-MRF methods appliedwithin this framework in order to make more robust models and improve segmentation quality under different con ditions such as object translation and rotation. Furthermore,we intend to perform a quantitative study to compare the ac curacy of our results with those of other related methods."}
{"pdf_id": "0805.2045", "content": "The PageRank algorithm [1] renects the idea that a web page is im portant if there are many pages linking to it, and if those pages areimportant themselves. The same principle was employed for folk sonomies in [13]: a resource which is tagged with important tags byimportant users becomes important itself. The same holds, symmet rically, for tags and users. By modifying the weights for a given tag in the random surfer vector, FolkRank can compute a ranked list of relevant tags. Ref. [13] provides a detailed description."}
{"pdf_id": "0805.2045", "content": "A possible justification for these different behaviors is that the cosine measure is measuringthe frequency of co-occurrence with other words in the global con texts, whereas the co-occurrence measure and — to a lesser extent — FolkRank measure the frequency of co-occurrence with other words in the same posts"}
{"pdf_id": "0805.2045", "content": "The first natural aspect to investigate is whether the most closely related tags are shared across relatedness measures. We consider the 10, 000 most popular tags in del.icio.us, and for each of them wecompute the 10 most related tags according to each of the related ness measures. Table 4 reports the average number of shared tags forthe three relatedness measures. We observe that relatedness by co occurrence (freq) and by FolkRank share a large fraction of the 10 most closely related tags, while the cosine relatedness displays little overlap with both of them. To better investigate this point, we plot in"}
{"pdf_id": "0805.2045", "content": "Figure 1 the average rank (according to global frequency) of the 10 most closely related tags as a function of the rank of the original tag. The average rank of the tags obtained by co-occurrence relatedness (black) and by FolkRank (green) is low and increases slowly with the rank of the original tag: this points out that most of the related tags are among the high-frequency tags, independently of the original tag.On the contrary, the cosine relatedness (red curve) displays a differ ent behavior: the rank of related tags increases much faster with that of the original tag. That is, the tags obtained from cosine-similarity relatedness belong to a broader class of tags, not strongly correlated with rank (frequency).6"}
{"pdf_id": "0805.2045", "content": "hypernym edge (up) and one hyponym edge (down), i. e., these paths do lead to siblings. Notice how the path composition is very different for the other relatedness measures: in those cases roughly half of the paths consist of two hypernym edges in the WordNet hierarchy. We observe a similar behavior for n = 1, where the cosine relatedness has no statistically preferred direction, while the other measures of relatedness point preferentially to hypernyms."}
{"pdf_id": "0805.2308", "content": "ABSTRACT: This study, fundamentals of fuzzy block theory, and its application in assessment of stability in underground openings, has surveyed. Using fuzzy topics and inserting them in to key block theory, in two ways, fundamentals of fuzzy block theory has been presented. In indi rect combining, by coupling of adaptive Neuro Fuzzy Inference System (NFIS) and classic  block theory, we could extract possible damage parts around a tunnel. In direct solution, some  principles of block theory, by means of different fuzzy facets theory, were rewritten."}
{"pdf_id": "0805.2308", "content": "2 INDIRECT METHOD: PARRLILIZATON OF KEY BLOCK THEORY Figure (1) summaries two branches of uncertainty .Modern uncertainty theory has been ex tended by Lotfi..A.Zadeh (Zadeh.1965):\"fuzzy set theory\". Fuzzy logic (FL) is essentially coextension with fuzzy set theory and in narrow sense; fuzzy logic is logical system which is aimed at a formalization of modes of reasoning which are approx imate rather than exact.  FL in wide sense has four principal facets: The logical facet, FL/L; the set-theoretic facet (FL/S), the relational facet (FL/R) and the epis temic facet FL/E. (Dubois&Prade.2000)"}
{"pdf_id": "0805.2308", "content": "Figure3. A combined algorithm on KBT, TSK  Some results of the proposed algorithm can be highlighted as follows:  1-Detection of membership functions (MFs) for any input and output (figure 4)  2-The dominated rules in if-then format between inputs and output (safety factor for any  block)  3-Possible damage parts around tunnel. In similar conditions; a compression between DDA (discontinuous deformation analysis)-MacLaughlin&Sitar.1995- and results of mentioned al gorithm has been accomplished. See figure5."}
{"pdf_id": "0805.2308", "content": "Certain ideas in fuzzy geometry have been introduced and studied in a series of paper. See (Ro senfeld, 1998; Rosenfeld, 1990; Buckley &Eslami.1997a, b; Zhang 2002)  In a few of these papers, the authors considered the area, height, diameter and perimeter of  fuzzy subset of the plane. But in other view fuzzy planes and fuzzy polygons have a real fuzzy numbers (Buckley &Eslami.1997a, b). In new definitions of fuzzy geometry, aim is to link gen eral projective geometry to fuzzy set theory. (Kuijken. & VanMaldeghem.2003). From solid  modeling view, base on CAD, some methods to representation of fuzzy shapes with inserting  of\" linguistic variables \", in definition of solid shape, has been highlighted. (Zhang etl, 2002)"}
{"pdf_id": "0805.2308", "content": "With former description on PBR, analysis of imprecise variables can be emerged  PBR only is based on geometry and don't consider force effects. By fuzzy vectorial key block analysis or possibility (or fuzzy) programming on blocks, generalized possibility of block's removability can be highlighted. (GPBR).So, relationships between PBR and GPBR, may be ex pressed as theorems."}
{"pdf_id": "0805.2308", "content": "This study, briefly, employed some fuzzy facets with key block theory. The role of uncertainty  in geomechanic, and advancing of new uncertainty theories may give new ideas in assessment of  vagueness or\" granule\" of information. This idea was innate feature of this paper. New terms  such \"PBR or PBC\" in evolution of Shi's theorem was added to main version of KBT, in two"}
{"pdf_id": "0805.2440", "content": "= 1, 2,..., n and k = 1, 2, ..., M of the fuzzifier functions and the linear parameters  (weights Pkj) of TSK functions. In contrary to the Mamdani fuzzy inference system, the  TSK model generates a crisp output value instead of a fuzzy one. The defuzzifier is not  necessary.  The TSK fuzzy inference systems described by equation 3 can be easily implanted in  the form of a so called Neuro-fuzzy network structure."}
{"pdf_id": "0805.2440", "content": "determined, depend on the used data set. Obviously, one can employ like manipulation  in the rule (second granulation) generation part, i.e., number of rules.  Determination of granulation level is controlled with three main parameters: range of  neuron growth, number of rules and error level. The main benefit of this algorithm is to  looking for best structure and rules for two known intelligent system, while in  independent situations each of them has some appropriate problems such: finding of  spurious patterns for the large data sets, extra-time training of NFIS or SOM."}
{"pdf_id": "0805.2690", "content": "presented in Fig. 1a for DCRAW converter and in Fig. 1b for conventional Canon converter. Saturation level for the DCRAW converted data is equal 3726 DN, and for Canon converted data saturation level is equal to 65535 DN. One can see that DCRAW processed data for radiometric function is linear up to saturation level."}
{"pdf_id": "0805.2690", "content": "Temporal component of the dark noise was also estimated. For such purpose there were taken 64 dark frames. Then arrays of pixels were averaged and the RMS noise of each pixel was calculated. After such procedure two another arrays are created: the array of pixel's mean values Amean and the array of pixel's standard deviations Astd (and consequently the array of pixel's variations Avar). This procedure is analogous to the PixeLink's method [9]. To estimate the temporal dark noise quantitatively, the average variation of the Avar need to be calculated and square root is taken. Consequently, the temporal dark noise can be evaluated as follows:"}
{"pdf_id": "0805.2690", "content": "The light-depended noise was evaluated as well. There were taken and averaged images of the nat-field scene. The lighting used was matrix of red, green, and blue LEDs driven with DC current. ISO setting was ISO 100, the smallest available in the camera. Objective was removed in order to achieve nat-field homogeneity. A 1024 by 1024 pixel area from the centre of the image was used for the analysis."}
{"pdf_id": "0805.2690", "content": "Temporal light-depended noise is an uncertainness of light's measuring by each pixel, hence the calculation procedure is analogous to the procedure for temporal dark noise evaluation (see Subsection 2.3.2). The RMS values for each pixel of the nat-field scene's image was calculated, forming two another arrays: the array of pixel's mean values Amean and the array of pixel's variations Avar. Then obtained array was decomposed accordingly to the light components R, G, and B, same as for PRNU estimation. Hence temporal light-depended noise was evaluated for each colour channel separately:"}
{"pdf_id": "0805.2739", "content": "Having always been at the forefront of information management and open access,  High-Energy Physics (HEP) proves to be an ideal test-bed for innovations in scholarly  communication including new information and communication technologies. Three  selected topics of scholarly communication in High-Energy Physics are presented  here: A new open access business model, SCOAP3, a world-wide sponsoring  consortium for peer-reviewed HEP literature; the design, development and  deployment of an e-infrastructure for information management; and the emerging  debate on long-term preservation, re-use and (open) access to HEP data."}
{"pdf_id": "0805.2739", "content": "HEP experimental research takes place in international accelerator research centres in Europe,  such as the European Organization for Nuclear Research (CERN) in Geneva or the Deutsches  Elektronen-Synchrotron (DESY) in Hamburg; in the United States mainly at the Stanford  Linear Accelerator Center (SLAC) in California and the Fermi National Accelerator  Laboratory (Fermilab) in Illinois; and in Japan at the High Energy Accelerator Research  Organization (KEK) in Tsukuba"}
{"pdf_id": "0805.2739", "content": "With the start-up of CERN's Large Hadron Collider (LHC) in 2008 and preparations for the  International Linear Collider (ILC) in full swing, we expect revolutionary results explaining  the origin of matter, unravelling the nature of dark matter and providing glimpses of extra  spatial dimensions or grand unification of forces"}
{"pdf_id": "0805.2739", "content": "At the same time, these desires have to be balanced against budget efficiency and optimization  of resources for research. HEP has been proposing solutions to these needs since decades, as  described in Section 3, while HEP ante-litteram open access tradition, which dates back half a  century, is discussed in Section 4."}
{"pdf_id": "0805.2739", "content": "With the intention of informing, and possibly inspiring, the ongoing debates in the wider arena  of innovation in scholarly communication, and its intersection with academic publishing in  Europe and beyond, this contribution discusses the vision of HEP along three axes of  innovation: a new open access business model (Section 5); the design, development and  deployment of an e-infrastructure for information management, a next-generation repository  (Section 6); the emerging debate on long-term preservation, re-use and (open) access to HEP  data (Section 7).  3. Scholarly communication in HEP  To set the scene, it is useful to quote five numbers and a concept. The five numbers, which  parameterize scholarly communication in HEP, are:"}
{"pdf_id": "0805.2739", "content": "In this scene, three revolutions mark the advances in scholarly communication in HEP, with  repercussions in the contemporary innovations affecting other disciplines. 1974, information technology meets (HEP) libraries. The SPIRES database, the first grey literature electronic catalogue, saw the light at SLAC4. Shortly thereafter the SLAC and DESY  libraries joined forces to cover the complete HEP literature including preprints, reports, journal  articles, theses, conference talks and books. In 1985, the database contained already more than  140,000 records. It now contains metadata for about 760,000 HEP articles, including links to  full-text, standardized keywords, publication notes. It offers additional tools like citation  analysis and is interlinked with other databases containing information on conferences,  experiments, authors and institutions."}
{"pdf_id": "0805.2739", "content": "1991, the first repository. arXiv, the archetypal repository, was conceived in 1991 by Paul  Ginsparg5, then at the Los Alamos National Laboratory in New Mexico, and is now hosted at  Cornell University in New York. It evolved the four-decade old preprint culture into an  electronic system, offering all scholars a level playing-field from which to access and  disseminate information. Today arXiv has grown outside the field of HEP, becoming the  reference repository for many diverse disciplines beyond physics, from mathematics to some  areas of biology. It contains about 450'000 full-text preprints, receiving about 5'000 new  articles each month."}
{"pdf_id": "0805.2739", "content": "Five of those six journals carry a majority of HEP content. These are Physical Review D  (published by the American Physical Society), Physics Letters B and Nuclear Physics B  (Elsevier), Journal of High Energy Physics (SISSA/IOP) and the European Physical Journal C  (Springer). The aim of the SCOAP3 model is to assist publishers to convert these \"core\" HEP"}
{"pdf_id": "0805.2739", "content": "Figure 2. Contributions by country to the HEP scientific literature published in the largest  journals in the field. Co-authorship is taken into account on a pro-rata basis, assigning  fractions of each article to the countries in which the authors are affiliated. This study is based  on over 11'000 articles published in the years 2005 and 2006. Countries with individual  contributions less than 0.8% are aggregated in the \"Other countries\" category14."}
{"pdf_id": "0805.2739", "content": "reached a critical mass, and thus demonstrated its legitimacy and credibility, it will issue a call  for tender to publishers, aimed at assessing the exact cost of the operation, and then move  quickly forward with the formal establishment of the consortium and its governance, then  negotiating and placing contracts with publishers"}
{"pdf_id": "0805.2739", "content": "To date, most European countries have endorsed the project and major library consortia in the  United States are in the process of completing the American share: SCOAP3 has already  received pledges for about a third of its budget envelope16, with another third having the  potential to be pledged in the short-term future, as presented in Figure 3"}
{"pdf_id": "0805.2739", "content": "Such  an assessment serves two purposes: within the field, it informs on the need for HEP-specific  community-based resources and their real role in the present internet landscape, inspiring their  future evolution; globally, it provides an in-depth case study of the impact of discipline-based  information resources, as opposed to institution-based information resources or cross-cutting  (commercial) information platforms"}
{"pdf_id": "0805.2739", "content": "In addition to inquiring about the most heavily used systems for different tasks, the survey  aimed to assess the importance of various aspects of information resources. Respondents were  asked to tag the importance of 12 features of an information system on a five-step scale,  ranging from \"not important\" to \"very important\". The results are presented in Figure 5.  Access to full-text stood out clearly as the most valued feature, following close behind are  depth of coverage, quality of content and search accuracy."}
{"pdf_id": "0805.2739", "content": "The survey explicitly inquired about the level of change that HEP scholars would expect, and  require, from their information resources in the next five years: 75% expected \"some\" to \"a lot  of \" change and 90% of the users tagged three features as the most important areas of change:  the linked presentation of all instances of a result, centralization, and access to data in figures  and tables.  The survey also collected thousands of free-text answers, inquiring about features of current  systems and their most-desired evolution. Some of the most inspiring free-text answers were  along the following lines:"}
{"pdf_id": "0805.2739", "content": "The results of this survey and strategic discussions between four leading HEP laboratories  (CERN, DESY, Fermilab and SLAC), in synergy with other partners (notably arXiv) and in a  continuous dialogue with major publishers in the field, led to a roadmap towards a future HEP  information system, consisting of the following steps:"}
{"pdf_id": "0805.2739", "content": "It will integrate the content of present repositories and databases to host the entire  body of metadata and the full-text of all open access publications, past and future, including  conference material, and will embody the one-stop shop HEP researchers are waiting for,  encompassing all content of arXiv as well as decades of previous articles"}
{"pdf_id": "0805.2739", "content": "It is interesting to note that the last features are already available in many services \"overlaid\"  on arXiv, as a proto-form of alternative peer-review, but their acceptance is limited, due to the  reduced usage of these sites when compared with the main access points to the literature. An  inspiring experiment will be the deployment of these Web2.0 features in the production  systems that the vast majority of HEP users adopts for their daily access to the literature: will  this naturally lead to these additional means of communications entering the mainstream of the  research workflow?"}
{"pdf_id": "0805.2739", "content": "19The JADE and OPAL collaborations, Eur.Phys.J.C17 (2000) 19, hep-ex/0001055  20To continue the story they bought a juke-box to store CDs of OPAL data after the completion of this later experiment.  21S. Mele, \"Preservation, re-use and (open) access to HEP data\" contributed to Tools & Trends in Digital Preservation, The  Hagues, 31 October 2007;  J. Engelen, presentation at the Conference of the Alliance for Permanent Access, Brussels, 15 November 2007,  http://www.alliancepermanentaccess.eu/power/Engelen_Alliance_151107.ppt"}
{"pdf_id": "0805.2739", "content": "Due to the complexities of these issues, HEP may be considered as a worst-case  scenario in the topic of data preservation, re-use and (open) access, but a scenario that has the  potential to inspire other fields of science, as in the other endeavours of HEP in the field of  scholarly communication"}
{"pdf_id": "0805.2739", "content": "22S. Mele, \"Preservation, re-use and (open) access to HEP data\" contributed to Tools & Trends in Digital Preservation, The  Hagues, 31 October 2007;  J. Engelen, presentation at the Conference of the Alliance for Permanent Access, Brussels, 15 November 2007,  http://www.alliancepermanentaccess.eu/power/Engelen_Alliance_151107.ppt.  23The FP7 PARSE.Insight project (Insight in Permanent Access to the Records of SciencE) has among its objectives to  understand the implications, not only technical, for HEP to start a process of preserving its data. PARSE.Insight will deliver its  report in 2010. http://parse.digitalpreservation.eu/."}
{"pdf_id": "0805.2739", "content": "Conclusions  With 50 years of preprints and 17 years of repositories, not to mention the invention of the  web, HEP has spearheaded (open) access to scientific information and is now in a period of  change at two frontiers: the cross road of open access and peer-reviewed literature and the  inception of a next-generation repository which has to adapt the current technological advances  to the research workflow of HEP scientists"}
{"pdf_id": "0805.2739", "content": "In the spirit of their collaborative tradition, HEP scientists are now proposing to pool together  resources from libraries and HEP institutes worldwide to sponsor the transition to open access  of the entire literature of the field, through the SCOAP3 initiative (Sponsoring Consortium for  Open Access Publishing in Particle Physics)"}
{"pdf_id": "0805.2855", "content": "A technique for converting Library of Congress Subject Headings MARCXML to Simple  Knowledge Organization System (SKOS) RDF is described. Strengths of the SKOS vocabulary  are highlighted, as well as possible points for extension, and the integration of other semantic  web vocabularies such as Dublin Core. An application for making the vocabulary available as  linked-data on the Web is also described."}
{"pdf_id": "0805.2855", "content": "libraries around the United States, and the world, to reuse and enhance bibliographic metadata.  The cataloging of library materials typically involves two broad areas of activity: descriptive  cataloging and subject cataloging. Descriptive cataloging involves the maintenance of a catalog  of item descriptions. Subject cataloging on the other hand involves the maintenance of controlled  vocabularies like the Library of Congress Subject Headings and classification systems (Library of  Congress Classification) that are used in descriptive cataloging. As Harper (2007) has illustrated,  there is great potential value in making vocabularies like LCSH generally available and  reference-able on the Web using semantic web technologies."}
{"pdf_id": "0805.2855", "content": "for computer processing as MARC, and more recently as MARCXML. The conventions  described in the MARC21 Format for Authority Data are used to make 265,000 LCSH records  available via the MARC Distribution Service. The Simple Knowledge Organization System  (SKOS) is an RDF vocabulary for making thesauri, controlled vocabularies, subject headings and  folksonomies available on the Web (Miles et al., 2008). This paper describes the conversion of  LCSH/MARC to SKOS in detail, as well as an approach for making LCSH available with a web  application. It concludes with some ideas for future enhancements and improvements to guide  those who are interested in taking the approach further."}
{"pdf_id": "0805.2855", "content": "provided a concrete XSLT mapping for converting MARCXML authority data to SKOS. Both  SKOS and LCSH/MARC have a concept-oriented model. LCSH/MARC gathers different forms  of headings (authorized/non authorized) into records that correspond to more abstract conceptual  entities, and to which semantic relationships and notes are attached. Similarly SKOS vocabularies"}
{"pdf_id": "0805.2855", "content": "Harper (2006), where the text of the authorized heading text was used to construct a URL: e.g.  http://example.org/World+Wide+Web. The authors preferred using the LCCN in concept  identifiers, because headings are in constant flux, while the LCCN for a record remains relatively  constant. General web practice (Berners-Lee, 1998) and more specifically recent semantic web  practice (Sauermann et al., 2007) encourage the use of URIs that are persistent, or change little  over time. Persistence also allows metadata descriptions that incorporate LCSH/SKOS concepts  to remain unchanged, since they reference the concept via a persistent URL."}
{"pdf_id": "0805.2855", "content": "(4XX) headings. Similarly the SKOS vocabulary provides two properties, skos:prefLabel and  skos:altLabel, that that allow a concept to be associated with both preferred and alternate natural  language labels. In general, this allows authorized and non-authorized LCSH headings to be  mapped directly to skos:prefLabel and skos:altLabel properties in a straightforward fashion."}
{"pdf_id": "0805.2855", "content": "headings, a technique that is commonly referred to as pre-coordination. For example, a topical  heading Drama can be combined with the chronological heading 17th century, which results in an  LCSH/MARC record with the authorized heading Drama--17th century. In LCSH/MARC this  information is represented explicitly, with original headings and subdivision 'facets'. In the  LCSH/SKOS representation, headings with subdivisions are flattened into a literal, e.g.  \"Drama--17th century\". This is an area where an extension of SKOS could be useful."}
{"pdf_id": "0805.2855", "content": "The links in LCSH/MARC use the established heading as references, whereas in LCSH/SKOS  conceptual resources are linked together using their URIs. This requires that the conversion  process lookup URIs for a given heading when creating links. In addition LCSH/MARC lacks  narrower relationships, since they are inferred from the broader relationship. When creating  skos:broader links, the conversion process also creates explicit skos:narrower properties as well.  Once complete conceptual resources identified with URIs are explicitly linked together in a graph  structure similar to Figure 1, which represents concepts related to the concept \"World Wide  Web\"."}
{"pdf_id": "0805.2855", "content": "Number ranges, the date that the record was created, and the date that a record was last modified.  While the SKOS vocabulary itself lacks properties for capturing this information, the flexibility  of RDF allows other vocabularies such as Dublin Core to be imported and mixed into SKOS  descriptions: dcterms:lcc, dcterms:created, dcterms:modified. The flexibility to mix other  vocabularies in to resource descriptions at will, without being restricted to a predefined schema is  a powerfully attractive feature of RDF."}
{"pdf_id": "0805.2855", "content": "client, a web server can examine the Accept header sent by the client, to determine the preferable  representation of the resource to send (Berrueta et al, 2008). The LCSH/SKOS delivery  application currently returns the following representations: rdf/xml, text/n3,  application/xhtml+xml, application/json representations, using the URI patterns illustrated in  Figure 3."}
{"pdf_id": "0805.2855", "content": "naturally by \"following your nose\" (Summers, 2008) to related concepts, simply by clicking on  links in your browser (see Figure 4). It also allows semantic web and web2.0 clients to request  machine-readable representations using the very same LCSH concept URIs. In addition the use  of RDFa (Adida et al., 2008) allows browsers to auto-detect and extract semantic content from  the human readable XHTML."}
{"pdf_id": "0805.2855", "content": "somewhat from that taken by Harper (2006). Instead of using XSLT to transform records, the  pymarc library was used, which provides an object-oriented, streaming interface to MARCXML records. In addition a relational database was not used, and instead the rdflib BerkeleyDB triple store backend was used to store and query the 2,625,020 triples that make up the complete LCSH/ SKOS dataset. The conversion process itself runs in two passes: the first to create the concepts  and mint their URIs, and the second to link them together. To convert the entire dataset (377 MB)  it takes roughly 2 hours, on a Intel Pentium 4 CPU 3.00GHz machine."}
{"pdf_id": "0805.2855", "content": "classification schemes, subject heading lists, taxonomies, folksonomies) it lacks specialized  features to represent some of the details found in LCSH/MARC. As discussed above in 2.3,  LCSH/MARC distinguishes between several types of concepts: geographic, topical, genre/form,  and chronological. However LCSH/SKOS has only one type of entity skos:Concept to represent  all of these. As an RDF vocabulary, SKOS could easily be extended with new sub-classes of  skos:Concept: lcsh:TopicalConcept, lcsh:GeographicConcept, lcsh:GenreConcept,  and"}
{"pdf_id": "0805.2855", "content": "of Congress Classification, Name Authority File, and LCCN Permalink Service which could be  made available as RDF. The authors are also involved in the conversion of the RAMEAU, a  controlled vocabulary that is very similar to LCSH. Once converted these vocabularies would be  useful for interlinking with LCSH."}
{"pdf_id": "0805.2855", "content": "day from web-crawling robots (Yahoo, Microsoft. Google) and semantic web applications like  Zitgist and OpenLink. The server logging was adapted to also capture accept HTTP header  information, in addition to referrer, user agent, IP address, concept URI. After 6 months has  elapsed it will be useful to review how robots and humans are using the site: the representations  that are being received, how concepts are turning up search engines like Google, Yahoo, Swoogle  (http://swoogle.umbc.edu/) and Sindice (http://sindice.com)."}
{"pdf_id": "0805.2855", "content": "data with LCSH/SKOS concept URIs. However, given the volume of data, a SPARQL endpoint  (Prud'hommeaux et al., 2008) would enable users to programmatically discover concepts without  having to download and index the entire data set themselves. For example MARC bibliographic  data has no notion of the LCCN for subjects that are used in descriptions. This indirection makes  it impossible to determine which SKOS/LCSH concept URI to use without looking for the  concept that has a given skos:prefLabel. A SPARQL service would make this sort of lookup  trivial."}
{"pdf_id": "0805.2855", "content": "valuable on a variety of levels. The experiment highlighted the areas where SKOS and semantic  web technologies excel: the identification and interlinking of resources; the reuse and mix-ability  of vocabularies like SKOS and Dublin Core; the ability to extend existing vocabularies where  generalized vocabularies are lacking. Hopefully the Library of Congress' mission to provide data  services to the library community will provide fertile ground for testing out some of the key ideas  of semantic web technologies that have been growing and maturing in the past decade."}
{"pdf_id": "0805.3126", "content": "Unconscious procedures are a major aspect of learning, although, as with combinational  learning, we cannot yet synthesize neural circuits that enable such learning in practice.  Unconscious procedures are conjectured to be the result of interneurons that synapse  between words of long term memory, forming a neural state machine. Neural state  machines are efficient in that procedural steps avoid passing through the processing  associated with short term memory."}
{"pdf_id": "0805.3126", "content": "Cue Editor  The cue editor in this architecture is envisioned as in Figure 2. All cues are assumed  called into and taken from short term memory. But these cues are inconsistent when an  image cannot be remembered immediately. So cues are appropriately masked in a  pseudorandom way as shown, using a neural shift register counter, typically fed by neural  exclusive OR gates. Counters like this can produce a unique subset of cues. Resulting  associative recalls will have some correct features, but not necessarily all the right  features; recalls can be analyzed many tens per second."}
{"pdf_id": "0805.3126", "content": "Subliminal analyzer  The analyzer has the task of determining an index of importance for each subliminal set  of features. Digital signals from long term memory or the senses appear on interneurons,  and are re-encoded as suggested in Figure 3. Note that encoders are not necessarily  simple and have yet to be synthesized in a realistic way. Using identical neural circuitry,  the digital contents of short term memory are re-encoded into an index of importance; we  note that as short term memory fades, importance drops, so new thoughts are expected.  At any given time, these encoders assign a digital value to recall-related neural signals.  A subliminal image whose index approaches that of current short term memory will be"}
{"pdf_id": "0805.3126", "content": "Memorization enable  The availability of blank memory words to hold new information is assumed unlimited.  Memorization in this architecture is triggered by a memorization enable block which is  sensitive to recurring images in short term memory, that is, rehearsal.  In the example circuit in Figure 4, conditions for committing something to memory are  true if cues are presented but there are no matches, or recalls. Additionally, if a given"}
{"pdf_id": "0805.3126", "content": "image, as identified by the above importance encoder, appears in short term memory  twice, separated by a given delay, it will be committed to long term memory. The delay  can be implemented by short term neurons in a standard digital filter arrangement. A  simple neural multi write circuit ensures that only one word is programmed for a given  memorization enable."}
{"pdf_id": "0805.3126", "content": "References  [1] J. Anderson, The architecture of cognition, Harvard University Press, 1983.  [2] Daniel M. Wegner, The illusion of conscious will, MIT Press, 2002.  [3] Ray R. Hassin, James S. Uleman and John A. Bargh, Editors, The New Unconscious,  Oxford University Press, 2005: Ap Dijksterhuis, Henk Aarts, Pamela K. Smith, The  power of the subliminal: On subliminal persuasion and other potential applications.   [4] J. R. Burger, Explaining the logical nature of electrical solitons in neural circuits,  http://arxiv.org/abs/0804.4237, 2008."}
{"pdf_id": "0805.3217", "content": "In this paper, we focus on statistical region-based active contour models where image features (e.g. intensity) are random variables whose distribution belongs to some paramet ric family (e.g. exponential) rather than confining ourselves to the special Gaussian case. Using shape derivation tools, our effort focuses on constructing a general expression for the derivative of the energy (with respect to a domain) and derive the corresponding evolution speed. A general result is stated within the framework of multi-parameter exponential family. More particularly, when using Maximum Likelihood estimators, the evolution speed has a closed-form expression that depends simply on the probability density function, whilecomplicating additive terms appear when using other estimators, e.g. moments method. Experimental results on both syn thesized and real images demonstrate the applicability of our approach."}
{"pdf_id": "0805.3217", "content": "This section presents some experimental results on noisy im ages. The initial noise-free image is shown in Fig.1. For four different Battacharya distances (BD), we have systematically corrupted this image with two types of noise: Poisson and Rayleigh. The Battacharya distance is used as a measure of \"contrast\" between objects and background. It is defined as :"}
{"pdf_id": "0805.3217", "content": "For each combination of BD value and noise type, 50 noisy images were generated. Each noisy image was then segmented using four different energy functionals, namely Chan-Vese [9], and our method with -log-likelihood and ML estimator with three assumed noise models: Gaussian, Rayleigh and Poisson. For each segmented image with each method at each"}
{"pdf_id": "0805.3217", "content": "BD value, the average false positive fraction (FPF) and truepositive fraction (TPF), over the 50 simulations were com puted. The bottomline of these experiments is to show thatusing the appropriate noise model will yield the best performance in terms of compromise between specificity (oversegmentation as revealed by the FPF) and sensitivity (under segmentation as revealed by the TPF)."}
{"pdf_id": "0805.3217", "content": "Fig.2 depicts the average FPF (left) and TPF (right) as afunction of the BD for Poisson ((a)-(b)) and Rayleigh ((c)(d)) noises. As expected, the FPF exhibits a decreasing ten dency as the BD increases, while the TPF increases with BD, which is intuitively acceptable. More interestingly, the best performance in terms of compromise between FPF and TPF is reached when the contaminating noise and the noise model in the functional are the same. This behaviour is more salient at low BD values, i.e. high noise level. One can also point out that the Chan-Vese functional is very conservative at the price of less sensitivity. Clearly this method under-segments the objects."}
{"pdf_id": "0805.3218", "content": "In this paper, we propose to combine formally noise and shape priors in region-based active contours. On the one hand, we use the general framework of exponential family as a priormodel for noise. On the other hand, translation and scale in variant Legendre moments are considered to incorporate theshape prior (e.g. fidelity to a reference shape). The combi nation of the two prior terms in the active contour functional yields the final evolution equation whose evolution speed is rigorously derived using shape derivative tools. Experimentalresults on both synthetic images and real life cardiac echog raphy data clearly demonstrate the robustness to initialization and noise, nexibility and large potential applicability of our segmentation algorithm."}
{"pdf_id": "0805.3218", "content": "The shape prior is used as an additional fidelity term (e.g. to a reference shape), designed to make the behaviour of the segmentation algorithm more robust to occlusion and missing data and to alleviate initialization issues. Here, orthogonal Legendre moments with scale and translation invariance were used as shape descriptors [9]. Indeed, moments [13] give a region-based compact representation of shapes through the projection of their characteristic functions on an orthogonal basis such as Legendre polynomials. The shape prior is then defined as the Euclidean distance between the moments of the evolving region and ones of the reference shape,"}
{"pdf_id": "0805.3218", "content": "In general, the reference shape can have different orien tation and size compared to the shape to be segmented. Thiswill then necessitate an explicit registration step in order to realign the two shapes. In order to avoid this generally problematic registration step, we here use scale and translation invari ant Legendre moments as in [9]. In the geometric momentsdefinition, the scale invariance is embodied as a normaliza tion term:"}
{"pdf_id": "0805.3218", "content": "tial contour position. We compared the result of our method (fig.2), with (d) and without (c) the shape prior, to an expert manual segmentation (b), and a segmentation provided by the Active Appearance and Motion Model (AAMM) method (e) designed for echocardiography [14, 15]. Again, the saliencyof our method is obvious. Our method gives the closest segmentation to the expert manual delineation. This is quanti tavely by the Hamming distance plots (f), showing that our method outperformes AAMM."}
{"pdf_id": "0805.3218", "content": "This paper concerns the incorporation of both noise and shape priors in region-based active contours. The evolution of the active contour is derived from a global criterion that combinesstatistical image properties and geometrical information. Sta tistical image properties take benefit of a prespecified noisemodel defined using parametric pdfs belonging to the exponential family. The geometrical information consists in mini"}
{"pdf_id": "0805.3218", "content": "mizing the distance between Legendre moments of the shape and those of a reference. The Legendre moments are designedto be scale and translation invariant in order to avoid the reg istration step. The combination of these terms gives accurateresults on both synthetic noisy images and real echocardio graphic data. Our ongoing research is now directed towards the integration of a complete shape learning step."}
{"pdf_id": "0805.3267", "content": "Abstract. The paper introduces a new technique for compressing Binary Decision Diagrams in those cases where random access is not required. Using this technique, compression and decompression can be done in linear time in the size of the BDD and compression will in many cases reduce the size of the BDD to 1-2 bits per node.Empirical results for our compression technique are presented, including comparisons with previously introduced techniques, show ing that the new technique dominate on all tested instances."}
{"pdf_id": "0805.3267", "content": "We refer to idb(v) and idl(v) by \"the BFS id of v\" and \"the layer id of v\" respectively. Note that if all edges in a layered DAG has the same length then the ordering idl and idb will be the same. In our compression scheme we will make use of the following well-known fact:"}
{"pdf_id": "0805.3267", "content": "To achieve such an encoding each node v is encoded using two bits. The first bit and the second bit is true iff v contains a left and a right child respectively. In order to make decoding possible the order in which the children of already decoded nodes appear in the encoded data must be known. This can for example be ensured by encoding the nodes in a DFS or BFS order with either left-first or right-first traversal. As an example, the encoding of the nodes of the binary tree in Figure 1(c) in BFS order is (11, 11, 00, 11, 00, 00, 00)."}
{"pdf_id": "0805.3267", "content": "1. Build a spanning tree on the BDD (Section 3.1). 2. Encode edges in the spanning tree, using Lemma 7 3. Encode by one bit the order in which the two terminals appear in the spanning tree.4. Encode the length of the edges in the spanning tree where neces sary (Section 3.2). 5. Encode the edges that are not in the spanning tree (Section 3.3).6. Compress the resulting data using standard compression tech niques."}
{"pdf_id": "0805.3267", "content": "Definition 8 (Spanning Tree). A spanning tree GT (V T , ET ) on a BDD G(V, E) is a subgraph of G, for which V T = V , and any two vertices are connected by exactly one path of edges in ET . An edge is called a tree edge if it is contained in the spanning tree and a nontree edge otherwise."}
{"pdf_id": "0805.3267", "content": "Example 9. The spanning tree in Figure 1(b) contains three long edges, whereas the spanning tree in Figure 1(c) only contains one. The latter of these would be the one constructed by our encoder upon compressing the BDD in Figure 1(a). The single long edge in figure 1(c) has to be included in the tree as it is the only possible way for the spanning tree to include the node in layer 1."}
{"pdf_id": "0805.3267", "content": "The spanning tree is stored as a binary tree where all edges have the same length. Since some of the edges in the spanning tree may correspond to long edges in the BDD, the binary tree itself may not be sufficient to reconstruct the layer information of the nodes during decoding. In order to enable the decoder to deduce the correct layer we therefore encode the location and the length of each long edge"}
{"pdf_id": "0805.3267", "content": "When the spanning tree and the layer information is encoded, we only need to encode the nontree edges, that is, those edges in the BDD that are not contained in the spanning tree. We know that half of the edges in the BDD will be encoded as nontree edges as it follows from the following observation:"}
{"pdf_id": "0805.3267", "content": "Using the above encoding, we are left with a sequence of nontree children L, with very few repetitions. When encoding this sequence we will exploit the fact that the sequence of integers in idl(L) will in most instances tend to be increasing. Below we argue why this is the case."}
{"pdf_id": "0805.3267", "content": "What follows from Observation 11 is, roughly stated, that incom plete children with parents in the \"left part\" of a layer are boundto have one of the smaller layer ids in the layer, whereas the in complete children with parents in the \"right part\" of the of a layer can have any layer id occurring in the layer."}
{"pdf_id": "0805.3267", "content": "As a conclusion of three the reasons mentioned above about why we expect the sequence of incomplete children to tend towards being increasing, and as we have observed the increasing trend of id(L) in the instances we have tested on, we choose to exploit this fact by encoding the sequence idl(L) by delta coding:"}
{"pdf_id": "0805.3267", "content": "encode the length of the forward edges. If there are very few long edges it might not be worth the effort to write the labelling on the edges. Hence we set a threshold on the number of forward edges that it needed in order to make the encoding of these edges useful. If the threshold is not exceeded all long forward edges are instead encoded as described in Section 3.3.2."}
{"pdf_id": "0805.3267", "content": "In this section we provide empirical results from compressing a large set of BDDs from various sources using the new encoder describedin this paper and as well as the encoders from [8] and [5]. For fur ther comparison we also provide the results from a naive encoder. The naive encoder outputs the size of each layer followed by a list ofchildren. This representation is very similar to the in-memory repre sentation of a BDD except that the layer information is not stored for each node but rather implicitly using the layer sizes."}
{"pdf_id": "0805.3267", "content": "Many of the instances we show results for are taken from the con figuration library CLib [12]. As a BDD only allows binary variables,additional steps must be taken in order to encode solutions to prob lems containing variables with domains of size larger than 2. For each non-binary variable in a problem its customary to either use a numberof binary variables logarithmic in the size of the domain of the vari able and adjust the constraints accordingly or use one variable for each domain value. These methods are known as log-encoding[14] and direct-encoding respectively. In the instances we have tested withall those named with the suffix \"dir\" was compiled using direct encoding, while the remaining were build using log-encoding. The in stances fall into the following groups:"}
{"pdf_id": "0805.3267", "content": "From the empirical results shown in Figure 3 we can immediately see that it is worthwhile to make use of a dedicated BDD encoder,as the naive encoding, being only compressed by LZMA, is outper formed with a factor of up to 20 on some instances. Furthermore we can see that the encoder introduced in this paper is consistently able to perform as well or better than the other encoders on all tested instances. In particular the largest BDD in our test (\"complex-P3\") required about twice as much space when using either of the two other dedicated encoders."}
{"pdf_id": "0805.3267", "content": "multiplier instances all turn out to compress less efficiently. An ad ditional important trend is that nodes which cannot be reached by following a short edge from a parent are very rare, meaning that ourencoder in by far the most cases only need to provide layer informa tion for less than 1% of the nodes, which is a significant advantage over previous encoders."}
{"pdf_id": "0805.3518", "content": "In everyday life it happens that a person has to reason about what other people think and how they behave, in order to achieve his goals. In other words, an individual may be required to adapt his behaviour by reasoning about the others' mental state. In this paper we focus on a knowledge representation language derived from logic programming which both supports the representation of mental states of individual communities and provides each with the capability of reasoning about others' mental states and actingaccordingly. The proposed semantics is shown to be translatable into stable model se mantics of logic programs with aggregates. To appear in Theory and Practice of Logic Programming (TPLP)."}
{"pdf_id": "0805.3518", "content": "the individual reasoning, we remark that our focus is basically concerning to theknowledge-representation aspects, with no intention to investigate how this reason ing layer could be exploited in the intelligent-agent contexts. However, in Section 8, we relate our work with some conceptual aspects belonging to this research field. Consider now the first example."}
{"pdf_id": "0805.3518", "content": "Agent1 will go to the party only if at least the half of the total number of agents (not including himself) goes there. Agent2 possibly does not go to the party, but he tolerates such an option. In case he goes, then he possibly drives the car. Agent3 would like to join the party together with Agent2, but he does not trust on Agent2's driving skill. As a consequence, he decides to go to the party only if Agent2 both goes there and does not want to drive the car. Agent4 does not go to the party."}
{"pdf_id": "0805.3518", "content": "The standard approach to representing communities by means of logic-based agents (Satoh and Yamamoto 2002; Costantini and Tocchio 2002; De Vos et al. 2005; Alberti et al. 2004; Subrahmanian et al. 2000) is founded on suitable extensions of logic programming with negation as failure (not) where each agent is represented by a single program whose intended models (under a suitable semantics) are the agent's desires/requests. Although we take this as a starting point, it is still not suitable to model the above example because of two following issues:"}
{"pdf_id": "0805.3518", "content": "In order to solve the first issue (item 1.) we use an extension of standard logic pro gramming exploiting the special predicate okay(), previously introduced in (Buccafurri and Gottlob 2002). Therein a model-theoretic semantics aimed to represent a common agreement in a community of agents was given. However, representing the requests/acceptances of single agents in a community is not enough. Concerning item 2 above, a social language should also provide a machinery to model possible interference amongagents' reasoning (in fact it is just such an interference that distinguishes the so cial reasoning from the individual one). To this aim, we introduce a new construct providing one agent with the ability to reason about other agents' mental state and then to act accordingly. Program rules have the form:"}
{"pdf_id": "0805.3518", "content": "• Social conditions model reasoning conditioned by the behaviour of other agents in the community. In particular, it is possible to represent collective mental states, preserving the possibility of identifying the behaviour of each agent.• It is possible to nest social conditions, in order to apply recursively the social conditioned reasoning to agents' subsets of the community. • Each social model represents the mental state (i.e. desires, requirements, etc.) of every agent in case the social conditions imposed by the agents are enabled."}
{"pdf_id": "0805.3518", "content": "could be specified by means of SCs possibly nested in it. Anyway, a further prop erty is required to SCs with cardinal selection condition in order to be well-formed. In particular, given a non-simple SC s (with cardinal selection condition), all the SCs nested in s with cardinal condition must not exceed the cardinality constraints expressed by cond(s)."}
{"pdf_id": "0805.3518", "content": "Observe that ATP, when applied to an interpretation I , extends the classical immediate consequence operator TP, by collecting not only heads of non-tolerance rules whose body is true w.r.t. I , but also each atom a occurring as okay(a) in the head of some rule such that both a and the rule body are true w.r.t. I ."}
{"pdf_id": "0805.3518", "content": "Now we introduce the concept of social interpretation, devoted to representing the mental states of the collectivity described by a given SOLP collection and then we give the definition of truth for both literals and SCs w.r.t. a given social interpretation. To this aim, the classical notion of interpretation is extended by means of program identifiers introducing a link between atoms of the interpretation and programs of the SOLP collection."}
{"pdf_id": "0805.3518", "content": "to traditional logic programs6, and then we apply such a transformation to each SOLP program in a given SOLP collection. Finally, we combine the traditional logic programs so obtained into a single program. Before introducing the mapping, we need a preliminary processing of all tolerance rules in a SOLP program. This is done by means of the following transformation:"}
{"pdf_id": "0805.3518", "content": "In this section we introduce some relevant decision problems with respect to the So cial Semantics and discuss their complexity. The analysis is done in case of positive programs. The extension to the general case is straightforward. First, we consider the problem of social model existence for a collection of SOLP programs."}
{"pdf_id": "0805.3518", "content": "Now, we introduce several computationally interesting decision problems associ ated with the social semantics. Each of them corresponds to a computational task involving labeled atom search inside the social models of a SOLP collection.The traditional approach used for classical non-monotonic semantics of logic pro grams, typically addresses the two following problems:"}
{"pdf_id": "0805.3518", "content": "Consider a house having m rooms. We have to distribute some objects (i.e. furniture and appliances) over the rooms in such a way that we do not exceed the maximum number of objects, say c, allowed per room. Constraints about the color and/or the type of objects sharing the same room can be introduced. We assume that each object is represented by a single program encoding both the properties and the constraints we want to meet. Consider the following program:"}
{"pdf_id": "0805.3518", "content": "In addition, it is possible to encode, by means of social rules, the dependence of designer i's module properties from those of other designers. For instance, given an integer d, by means of the following rules designer i requires that module 4 is placed on the same row (rule r18) as designer j's module 1 and such that a distance of exactly d cells exists between them (rules r19, r20)."}
{"pdf_id": "0805.3518", "content": "Social rule r29 collects admissible solutions to the placement problem. The rules from r30 to r37 are used to represent the smallest rectangle enclosing all the placed modules. Then, the actual design area is computed by rule r38. In case an an upper bound b to be satisfied (resp. an exact value s to be matched) is given, then the following rule r39 (resp. r40) may be added:"}
{"pdf_id": "0805.3518", "content": "A king wishes to determine which of his three wise men is the wisest. He arranges them in a circle so that they can see and hear each other and tells them that he will put a white or a black spot on each of their foreheads but that at least one spot will be white. He then repeatedly asks them, \"Do you know the colour of your spot?\". What do they answer?"}
{"pdf_id": "0805.3518", "content": "men. It is possible to extend the reasoning encoded in the above programs, in order to write a general program for n wise men, by exploiting the nesting feature of the social conditions in such a way that reasoning on both the content and the temporal sequence of the wise men's statements is enabled."}
{"pdf_id": "0805.3518", "content": "Logic-based Multi-Agent Systems - A related approach, where the semantics of acollection of abductive logic agents is given in terms of the stability of their interac tion can be found in (Bracciali et al. 2004) where the authors define the semanticsof a multi-agent system via a definition of stability on the set of all actions per"}
{"pdf_id": "0805.3747", "content": "The subject of automatic taxonomy creation has attracted much attention fromthe academic community because of its close ties to important topics in philoso phy, cognitive and computer sciences, and information technology. A taxonomy is a classification system that helps people organize their knowledge of the world hierarchically through broader-narrower (superclass-subclass) relations between concepts. One of the best known taxonomies is the Linnean classification of living organisms. There are alternative classification systems for organizing knowledgethat do not rely exclusively on strict hierarchies. These include faceted classi fication schemes, which combine multiple taxonomies to represent objects, the"}
{"pdf_id": "0805.3747", "content": "In addition to \"nat\" keywords or tags, some social Web sites have recently began to provide a feature that enables users to hierarchically organize content with broader/narrower relations. We believe that in the future many more social Web sites will allow their users to specify complex semantic relations, not only tags. We brieny describe how this feature is implemented on Flickr and del.icio.us."}
{"pdf_id": "0805.3747", "content": "all the photos within it, while the collection name is usually broad enough to cover all the sets within it. On Del.icio.us,4 there is no explicit interface to group bookmarks into sets and collections as on Flickr. Instead, users can group their tags into tag bundles. This feature helps users to search and visualize tags as their number increases. Similar to sets and collections on Flickr, a user can assign an arbitrary name to a bundle. In general, the name of the bundle subsumes all associated tags."}
{"pdf_id": "0805.3747", "content": "From the problem definition above, we follow three main steps in aggregating relations: (1) term extraction and normalization; (2) relation connict resolution; (3) concept prunning and linking. The first step is necessary because of variationsin the names associated with the same concept, e.g., capitalization and punc tuation. Thus, exact names are too sparse to be useful. Fortunately, we found that most of \"similar\" collections and sets share common terms. We use these instead of the full names and apply relation delegation as previously mentioned. The second step is necessary because of variations in the direction of relations among users. The last step prunes \"uninformative\" concepts and then links the rest into deeper hierarchies."}
{"pdf_id": "0805.3747", "content": "Concept pruning and linking : After the connict resolution step, there are still some concepts which subsume too many other concepts, e.g., all set, allrest, occasion, and have few concepts subsume them. We feel that these \"un informative\" concepts seem to be too broad to be useful. From our informal analysis, we postulate that a number of parent and child concepts can be used to determine if a concept is uninformative. The formulation for this heuristic is provided as follows."}
{"pdf_id": "0805.3747", "content": "In particular, we found that Rxoi, can indicate if x is uninformative: the higher the ratio, the more uninformative the concept x is. In many concepts, they have no parent concepts and divided-by-zero can occur. To avoid such, we smooth both dinx and doutx with a very small number relative to a number of all concepts. After pruning uninformative concepts, concepts are then linked together through their subsumption relations."}
{"pdf_id": "0805.3747", "content": "the study were expressed through the shallow hierarchies of photo sets and col lections created by Flickr users to manage their photos. Our approach is general, and can be applied to other systems that allow users to specify relations: e.g., the social bookmarking site Del.icio.us allows users to group related tags into tag bundles."}
{"pdf_id": "0805.3799", "content": ", scenes) is innovative in a few ways, including respecting the sequence of film script units, and taking as input the \"direction\" of the film script content rather than having a more static framework for the input (which we found empirically to work less well in that it was far less discriminatory)"}
{"pdf_id": "0805.3799", "content": "In this section we address the issue of plausibility of appreciable analysis of content based on what are ultimately the statistical frequencies of co-occurrence of words. Words are a means or a medium for getting at the substance and energy of a story (p. 179, [15]). Ultimately sets of phrases express such underlying issues (the \"subtext\", as expressed by McKee, a term we avoid due to possible confusion with subsets of text) as connict or emotional connotation (p. 258). We have already noted that change and evolution is inherent to a plot. Human"}
{"pdf_id": "0805.3799", "content": "We have already noted (section 1) some novel aspects of our methodology. We begin with the display of data (e.g., scenes and/or words) where visualization of relationships is greatly facilitated by having a Euclidean embedding. We show how Correspondence Analysis furnishes such a metric space embedding of the information present in the film script text, and furthermore how this facilitates an ultrametric (i.e. hierarchical) embedding that takes account of the temporal, semantic dynamic of the film script narrative."}
{"pdf_id": "0805.3799", "content": "Correspondence Analysis [18] takes input data in the form of frequencies of occurrence, or counts, and other forms of data, and produces such a Euclidean embedding. The Appendix provides a short introduction to Correspondence Analysis and hierarchical clustering.We start with a cross-tabulation of a set of observations and a set of at tributes. This starting point is an array of counts of presence versus absence, or frequency of occurrence. From this input data, we can embed the observationsand attributes in a Euclidean space. This factor space is mathematically opti mal in a certain sense (using the least squares criterion, which is also Huyghens' principle of decomposition of inertia). Furthermore a Euclidean space allows for easy visualization that would be more awkward to arrange otherwise."}
{"pdf_id": "0805.3799", "content": "158; we, 151; on, 149; strasser, 135. The numerically high presence of personal names is quite unusual relative to more general texts, and characterizes this film script text. A major reason for this is that character names head up each dialog block. Casablanca is based on a range of miniplots. This occasions considerable variety. Miniplots include: love story, political drama, action sequences, urbane drama, and aspects of a musical. The composition of Casablanca is said by McKee [15] to be \"virtually perfect\" (p. 287)."}
{"pdf_id": "0805.3799", "content": "For the Casablanca scene 43, we found the following as particularly sig nificant. We tested the given scene, with its 11 beats, against 999 uniformly randomized sequences of 11 beats. If we so wish, this provides a Monte Carlo significance test of a null hypothesis up to the 0.001 level."}
{"pdf_id": "0805.3799", "content": "• In repeated runs, each of 999 randomizations, we find scene 43 to be par ticularly significant (in 95% of cases) in terms of attribute 2: variabilityof movement from one beat to the next is smaller than randomized alter natives. This may be explained by the successive beats relating to coming together, or drawing apart, of Ilsa and Rick, as we have already noted."}
{"pdf_id": "0805.3799", "content": "• As for the case of beats in scene 43, we find that the entire Casablanca plot is well-characterized by the variability of movement from one scene to the next (attribute 2). Variability of movement from one beat to the next is smaller than randomized alternatives in 82% of cases."}
{"pdf_id": "0805.3799", "content": "We see here scene metadata, characters, dialog, and action information, all of which we use. Frontpiece, preliminary or preceding storyline information, and credits were ignored by us. We took the labeled scenes. The number of scenes in each movie, and the number of unique, 2-characters or more, words used in the movie, are listed in Table 1. All punctuation was ignored. All upper case was converted to lower case. Otherwise there was no pruning of stopwords. The top words and their frequencies of occurrence were:"}
{"pdf_id": "0805.3799", "content": "The basis for accessing semantics in provided by (i) Correspondence Analysis, where each scene is an average of words or other attributes that characterize it, and each attribute is an average of scenes that are characterized; and (ii) in the hierarchical clustering of the sequence of scenes, relative change is modeled by the dendrogram structure."}
{"pdf_id": "0805.3799", "content": "semantics of information expressed by the data. The way it does this is (i) by viewing each observation or row vector as the average of all attributes that are related to it; and by viewing each attribute or column vector as the average of all observations that are related to it; and (ii) by taking into account the clustering and dominance relationships given by the hierarchical clustering. The analysis chain is as follows:"}
{"pdf_id": "0805.3799", "content": "In Correspondence Analysis the factors are ordered by decreasing moments of inertia. The factors are closely related, mathematically, in the decomposition of the overall cloud, NJ(I) and NI(J), inertias. The eigenvalues associated with the factors, identically in the space of observations indexed by set I, and in the space of attributes indexed by set J, are given by the eigenvalues associated with the decomposition of the inertia. The decomposition of the inertia is a principal axis decomposition, which is arrived at through a singular value decomposition."}
{"pdf_id": "0805.3800", "content": "1 g = u1 + u2  2 g = ~u1 + u2  3 g = ~(u1 + u2)  4 g = u1 + ~u2  5 g = u1 * u2  6 g = ~u1 * u2  7 g = ~(u1 * u2)  8 g = u1 * ~u2  9 g = ~u1 * u2 + u1 * ~u2  10 g = ~u1 *~u2 + u1 * u2"}
{"pdf_id": "0805.3800", "content": "Thus, for a reasonably large number l, the above search  procedure can find the solution (Q*, M*).  2.2. Selection of Models  The DMs trained on a small amount of data can be  selected by the number of errors on the training data.  However, such selection favours overfitted DMs with a  poor ability to generalise. To enhance the generalisation"}
{"pdf_id": "0805.3800", "content": "2 and  circulating immune complex (x5) is less than 130 and  articular syndrome (x8) is absent and  anhelation (x11) is absent and  erythema (x13) is absent and  noises in heart (x14) are absent and  hepatomegaly (x15) is absent and  myocarditis (x16) is absent,   then the diagnose is the IE"}
{"pdf_id": "0805.3802", "content": "If the screening tests are  ambiguously interpreted, and information about the severity  of the injury is misleading, the mistake in a decision can be  fatal; the choice of a mild treatment can put a patient at risk  of dying from posttraumatic shock, while the choice of an  overtreatment can also cause death [1]"}
{"pdf_id": "0805.3802", "content": "2.Death. Randomly pick a splitting node with two ter minal nodes and assign it to be one terminal with the  united data points. 3. Change-split. Randomly pick a splitting node and  assign it a new splitting variable and rule drawn  from the corresponding priors.  4.Change-rule. Randomly pick a splitting node and as sign it a new rule drawn from a given prior."}
{"pdf_id": "0805.3802", "content": "number of minimal data instances allowed in DT nodes was  3; the acceptance rate was around 0.25.  Having obtained the ensemble of DTs, we estimated the importance of all 16 variables for the prediction. The estimates were calculated as the posterior probabilities of vari ables used in the DTs ensemble as shown in Fig. 1."}
{"pdf_id": "0805.3802", "content": "Gender: Male = 1, Female = 0. 0,1 Injury type: Blunt = 1, penetrating = 0 0,1 Head injury, no injury = 0 0,1,2,3,4,5,6 Facial injury 0,1,2,3,4 Chest injury 0,1,2,3,4,5,6 Abdominal or pelvic contents injury 0,1,2,3,4,5 Limbs or bony pelvis injury 0,1,2,3,4,5 External injury 0,1,2,3 10 Respiration rate Continuous 11 Systolic blood pressure Continuous 12 Glasgow coma score (GCS) eye response 0,1,2,3,4 13 GCS motor response 0,1,2,3,4,5,6 14 GCS verbal response 0,1,2,3,4,5 15 Oximetry  Continuous 16 Heart rate Continuous 17 Died = 1, living = 0. 0,1"}
{"pdf_id": "0805.3802", "content": "From Fig. 1 we can observe that the posterior probability of variable 9 is the smallest, around 0.005, while the maxi mal value is around 0.16 for variable 6. Therefore we can  assume that the variable 9 makes negligible contribution to  the ensemble's outcome.  To test our assumptions, we aim to discard this variable  from the Trauma data. Table 2 shows the maximal values of  loglikelihoods calculated within 5-fold cross-validation for  two sets including 16 and 16\\9 variables. From this table,  we can observe that the loglikelihood value for the 16\\9 set"}
{"pdf_id": "0805.3802", "content": "becomes greater than that for the set of all 16 variables. However the performance of the ensemble using the set of  16\\9 variables is slightly fewer than that using the set of 16  variables. This can happen because the ensemble using the  set of 16\\9 variables becomes more overfitted to the training  data. Thus, we can conclude that the weakest variable 9  provides better conditions for mitigating the DT ensemble  overfitting."}
{"pdf_id": "0805.3802", "content": "As shown above, the presence of the weakest variable  has the positive effect on mitigating overfitting of the DT  ensemble. This means that the DT ensemble should use all  16 input variables during sampling, but then we can exclude  those DTs which use the weakest variable 9. After such  selection of DTs there is no need to use the variable 9. In our experiments this technique was tested within 5 fold cross-validation and results shown in Table 3 which  compares the performance of the original DT ensemble  using all 16 variables with the performance of the selected  ensemble. This table also shows the number of DTs omitted after the selection."}
{"pdf_id": "0805.3802", "content": "We have expected that discarding weakest attributes can improve the performance of the BDT ensemble. However,in our experiments, the performance has oppositely de creased. We have assumed that this happened because the  discarded weakest attribute was still important for a small  amount of the data. Alternatively, we have assumed that the  weakest attribute makes a noticeable contribution to the  BDT ensemble's outcome. The question was would it be"}
{"pdf_id": "0805.3935", "content": "Abstract - We present in this article a new eval uation method for classification and segmentation of textured images in uncertain environments. In uncertain environments, real classes and boundaries are known with only a partial certainty given by the experts. Most of the time, in many presented papers,only classification or only segmentation are consid ered and evaluated. Here, we propose to take intoaccount both the classification and segmentation re sults according to the certainty given by the experts. We present the results of this method on a fusion ofclassifiers of sonar images for a seabed characteri zation."}
{"pdf_id": "0805.3935", "content": "In this section, we propose an original evaluation approach for classification based on a new confusion matrix taking into account the uncertainty and the possi bility that one unit belongs more than one class. Thisevaluation approach is adapted to the image classifi cation evaluation, but can be used for any classifier evaluation."}
{"pdf_id": "0805.3935", "content": "We propose here a linked study of one well segmented pixel measure and a mis-segmented pixelmeasure. Generally one of these measures is consid ered in the case with an a priori knowledge [2, 8, 9]. The well-segmented pixel measure is a well-detectionboundary measure and the mis-segmented pixel mea sure is a false detection boundary measure. We showhow these two measures can take into account the un certainty of the expert on the position and existence of the boundaries, assuming that each certainty grade is represented by a weight."}
{"pdf_id": "0805.3935", "content": "First, for each found boundary pixel f, search the mini mal distance dfe between f and all the boundary pixelsprovided by the expert e. Hence the pixel e is a func tion of f, and we should note it as ef, but in order to simplify notations, it is referred to as e in the rest of the paper. We take here an Euclidean distance but any other distance can be envisaged. The certainty weight of the pixel e given by the expert is noted as We. We define a well-detection criterion vector by:"}
{"pdf_id": "0805.3935", "content": "Hence, this measure is defined between 0 and 1. In real applications, this criterion remains small even for very good boundary detection, so we can take a = 1/6 in order to accentuate small values. This criterion only takes into account the distance from the found boundary to the contour provided by the expert. However, the reference boundary has alocal direction which is another aspect we have to con sider. Indeed, for instance, a found boundary can crossa given boundary orthogonally: in this case some pix els from the found boundary are very near (in terms of distance) to pixels from the reference boundary but that is not a good detection."}
{"pdf_id": "0805.3935", "content": "presented in [7]. Indeed, underwater environment is avery uncertain environment and it is particularly im portant to classify seabed for numerous applications such as Autonomous Underwater Vehicle navigation. In recent sonar works (e.g. [10, 11]), the classification evaluation is made only by visual comparison of one original image and the classified image. That is notsatisfying in order to correctly evaluate image classifi cation and segmentation."}
{"pdf_id": "0805.3939", "content": "Figure 1 shows the differences between the interpretation and the certainty of two sonar experts trying to differentiate types of sediment (rock, cobbles, sand, ripple, silt) or shadow when the information is invisible (each color corresponds to a kind of sediment and the associated certainty of the expert is expressed in terms of sure, moderately sure and not sure) [2]"}
{"pdf_id": "0805.3939", "content": "where and are calculated in order to get P(y 1/f 0) 0.5. Different approaches have been proposed for the estimation of these parameters (see [24]). [7] uses a one class SVM, introduced by [25]. So the combination can be done only with a one-versus-rest strategy. The decision functions coming from this particular classifier are employed to define some plausibility functions on the singleton wi:"}
{"pdf_id": "0805.3939", "content": "Our database contains 42 sonar images provided by the GESMA (Groupe d'Etudes Sous-Marines de l'Atlantique). These images were obtained with a Klein 5400 lateral sonar with a resolution of 20 to 30 cm in azimuth and 3 cm in range. The sea-bottom depth was between 15 m and 40 m. Some experts have manually segmented these images giving the kind of sediment (rock, cobble, sand, silt, ripple (vertical or at 45 degrees)), shadow or other (typically shipwrecks) parts"}
{"pdf_id": "0805.3939", "content": "The table I shows the results for the SVM classifier with the strategies one-versus-one and one-versus-rest. We note that there are many errors between the sand (C2) and silt (C3), that are two homogeneous sediments. The ripple (C4), the unlearning class, is more heterogeneous than the sand and silt, this why it is more classified as rock (C1). The table II"}
{"pdf_id": "0805.3964", "content": "features. In the software application, the features and it and its order to build he parallel coordinates chart are defined by the user. The cross-validation panel (Figure 1-d) is very similar to the prior.Cross-validation [12] consists in to divide the whole data set in two sub sets: training and test, mutually exclusive, and the user can define the size of both sets. The training set is entered as input to the feature selection algorithm. The classifier designed from the feature selection and the joint probability distributions table labels the test set samples. At the end of the cross-validation process, it is plotted a chart with the results of each execution, and it is possible to visualize the rate of hits and its variation along the executions."}
{"pdf_id": "0805.3964", "content": "Another available option is the generalization of non-observed instances. With this option selected, the instances of the selected feature set not present in the training samples are generalized by a nearest neighbors method [1] with Euclidean distance (see Section 3.5 for more details). This method is also applied to take a decision among classes with tied maximum conditional probability distributions given a certain instance."}
{"pdf_id": "0805.3964", "content": "This section presents the results in two main aspects. Initially the softwarewas applied as feature selection in a biological classification problem to clas sify breast cancer cells in two possible classes: benign and malignant. The biological data used here was obtained from [13] which has 589 instances and 32 features. The results shown figure 3, presents very low variations and high accurate classification achieving 99.96% of accuracy on average."}
{"pdf_id": "0805.3964", "content": "Since it is an open-source and multi-platform software, it is suitable for the user that wants to analyze data and draw some conclusions about it, as well as for the specialist that has as objective to compare several combinations ofapproaches and parameters for each specific data set or to include more fea tures in the software such as a new algorithm or a new criterion function"}
{"pdf_id": "0805.3972", "content": "Imagine a situation where an investigator diagnoses the intelligence data set for the run-down of the wire-puller behind the terrorist attack. Figure 1 illustratesthe situation. The pattern of the communication among perpetrators and a wire puller in the terrorist organization lies in the latent layer. It is the transmission of the innuence on decision-making. The pattern governs that of the collective"}
{"pdf_id": "0805.3972", "content": "The intelligence data set is the input to the method for link inference, node discovery, and visualization. The nodes in an intelligence data do not necessarily form a clique structure, where there are links between every pair of nodes. Assuming that they formed a clique would result in a very densely connected structure in the latent layer. Such a superficial interpretation of the intelligence data set leads to a wrong understanding of the terrorist organization. This is why we need a new computational method. The method is described below."}
{"pdf_id": "0805.3972", "content": "The logarithmic likelihood function [5] is defined by eq.(7). In statistics, a likelihood function is a conditional probability function of the observation given the parameters of a statistical model. It plays a key role in statistical inference such as Bayes' Law. The probability where D occurs for given r is denoted by p(D|r)."}
{"pdf_id": "0805.3972", "content": "Lagrange multipliers can be used to solve eq.(13) analytically. But, at present, computational optimization is suitable to solve a large-scale problem.The hill climbing method is a simple incremental optimization technique. Un suitable selection of the initial condition may lead to the sub-optimal solutions.Advanced meta-heuristic algorithms such as simulated annealing, or genetic al gorithm [10] may be employed to avoid sub-optimal solutions. It is not within the scope of this paper to explore the computational technique to solve eq.(13). The details of the algorithm implementation are not described here."}
{"pdf_id": "0805.3972", "content": "The clues on the covert node in the latent layer are discovered after the topol ogy of the links between the nodes, which appeared in the intelligence data set, is inferred with the maximum likelihood estimation (MLE) in 3.2. The degree of suspiciousness (s(di)) is assigned to the individual intelligence data di. It is defined as the likeliness where the covert node would appear in the intelligence data, if it became overt, or if the wire-puller were observable. The degree ofsuspiciousness is evaluated by eq.(14), where g(x) is a monotonically decreas ing function of the variable x. Larger value in eq.(14) means more suspicious intelligence data."}
{"pdf_id": "0805.3972", "content": "The degree of suspiciousness (s(nj)) can also be assigned to the individual nodes nj. More suspicious node is more likely to be the neighbor node of the covert node. Or, it is more likely to be the perpetrator who is associated with the wire-puller closely. The degree of suspiciousness s(nj) can be evaluated by accumulating the degree of suspiciousness of the intelligence data (s(di)), where the node appears, as in eq.(16). The function w(k) is an appropriate weight function."}
{"pdf_id": "0805.3972", "content": "The 19 perpetrators are listed in Table 1, who are responsible for hijacking the 4 commercial nights in the 9/11 terrorist attack (number: American Airlines AA11 (Boeing 767 from Boston to Los Angeles), AA77 (Boeing 757 from Washington to Los Angeles), United Airlines AA175 (Boeing 767 from Boston to Los Angeles), and UA93 (Boeing 757 from Newark to San Francisco)), and appear in a sample intelligence data set"}
{"pdf_id": "0805.3972", "content": "Al-Hisawi, the intelligence data set on Mustafa A. Al-Hisawi should be collected and added to the diagnosis. Similarly, the investigator can predict the position of the 21st person again from the intelligence data set on the 19 perpetrators and Mustafa A. Al-Hisawi. The method provides the investigator with the intuitively comprehensible direction of potentially fruitful investigation from what is already known toward what is not, but can be known."}
{"pdf_id": "0805.4101", "content": "Modeling dialog as a collaborative activity consists notably in specifying the content of the Conversational Common Groundand the kind of social mental state in volved. In previous work (Saget, 2006), we claim that Collective Acceptance is theproper social attitude for modeling Conversational Common Ground in the par ticular case of goal-oriented dialog. Weprovide a formalization of Collective Acceptance, besides elements in order to in tegrate this attitude in a rational model of dialog are provided; and finally, a model ofreferential acts as being part of a collabo rative activity is provided. The particular case of reference has been chosen in order to exemplify our claims."}
{"pdf_id": "0805.4101", "content": "Considering dialog ascollabora tive activity is commonly admitted (Clark, 1996; Garrod and Pickering, 2004; Cohen and Levesque, 1991; Cohen and Levesque, 1994). Generally speaking,modeling a particular collaborative activity re quires the specification of the collective intention helds by the agents concerned and requires the specification of the Common Ground linked to this activity. Common Ground refers to pertinent knowledge, beliefs and assumptions that are shared among team members (Clark, 1996). Thus, Common Ground is a collection of social mental attitudes."}
{"pdf_id": "0805.4101", "content": "Thus, the Conversational Common Ground, since dialog is a mediated activity, contains allgrounded elements linked to the way to com municate (as the necessary level of clarity of articulation or speech rate) as well as elements of dialog's history such as association between modes of presentation (linguistic objects) and mental representations: associations as conceptual pacts"}
{"pdf_id": "0805.4101", "content": "Ground in the particular case of goal-oriented dialog. In the first part of this paper, we show that such a modelization fits better than stronger mental attitudes (such as shared beliefs or weaker epistemic states based on nested beliefs). Wealso show that this modelization may be consid ered as partly due to the subordinated nature of goal-oriented dialog. Then, in the last part of the paper, a formalization of Collective Acceptance and elements are given in order to integrate this attitude in a rational model of dialog. Finally a model of referential acts as being part of a collaborative activity is provided. The particular case of reference has been chosen in order to exemplify our claims."}
{"pdf_id": "0805.4101", "content": "In order to model dialog ascollabora tion, reference resolution has to be consideredas the \"act identifying what the speaker in tends to be picked out by a noun phrase\"(Cohen and Levesque, 1994). Moreover, the col laborative nature of reference have been brought to the forefront (Clark and Wilkes-Gibbs, 1986). More precisely, reference is not the simple sum ofthe individual acts of generating and understand ing, but is a collaborative activity involving dialog partners. Thus, according to H.H. Clark et al. in (Clark and Bangerter, 2004), these individual acts are motivated by two interrelated goals:"}
{"pdf_id": "0805.4101", "content": "For example,let's imagine that two per sons, Tom and Laura, who have been to the same school. Tom suggests to Laura: \"Shall we meet in front of our ex-school's basketball court\". The choice of the description of the intented place should be explained by the fact that Tom thinks that the following mutual belief is part of their common ground:"}
{"pdf_id": "0805.4101", "content": "The main assumption behind this kind of approach is the rationality and the cooperativeness of dialogue participants. In addition, to infer from the fact that someone utters that p that she must also believe that p is commonly assumed as a general rule (Lee, 1997). Nonetheless, this assumption is difficult to handle in practice, as J.A. Taylor et al. have shown (Taylor et al., 1996), mainly because of the computational complexity involved. Furthermore, they proved that, in most cases, nested beliefs are not necessary beyond"}
{"pdf_id": "0805.4101", "content": "the second level of nesting (ie. what an agent thinks another agent thinks a third agent (possibly the first one) thinks), as long as deception is not involved. In the particular case of reference, deception may be involved, as the following situation exemplify, and then may require the handling of deeply nested belief."}
{"pdf_id": "0805.4101", "content": "• And MBelTom,Laura(name(l) = \" Chez Dominique \".We only treat the particular case of definite reference, which counts as an indica tion to access a mental representation of the intended referent that is supposed to be uniquely identifiable for the hearer. So, it can be viewed as a result of a function."}
{"pdf_id": "0805.4101", "content": "However, to the extend that the success of a subordinated activity is governed by the generalization of the sufficient criterion and on the basis of preceding arguments,one may reasonably assume that agents' rational ity does not strictly imply the coherence between the actions being parts of a subordinated activity and the beliefs states of the involved agents"}
{"pdf_id": "0805.4101", "content": "Studies on dialog modeling as a collaborative activity address the philosophical problem of de termining the type of mental states which couldbe ascribed to team members. Based on the observation that sometimes one may encounter sit uations where one has to make judgements or has to produce utterances that are contrary to ones privately held beliefs, philosophers, such has (Cohen, 1992), have introduced the notion of (Collective) Acceptance, which is an intentional social mental attitude. (Collective) Acceptanceshave the following properties, in contrast with be liefs (Wray, 2001):"}
{"pdf_id": "0805.4101", "content": "Rational models, based on (Cohen and Levesque, 1990), can beconsid ered as a logical reformulation of plan-basedmodels. They integrate, in more, a precise for malization of dialog partners' mental states (their beliefs, choices (or desires) and intentions), of the rational balance which relates mental attitudes between them and relates mental attitudes withagents' acts. Moreover, dialogue acts' precondi tions and effects are expressed in terms of dialog partners' mental states. Thus, this is hopeful to model precisely mental attitudes."}
{"pdf_id": "0805.4101", "content": "In this model, utterance generation and under standing, and thus referential acts are consideredas individual acts. Furthermore, the perlocution ary effects are considered as achieved as soon as the communicative act has been performed. So dialog and reference treatment are not considered as collaborative activities. In order to do so, notably, the set of mental attitudes has to be extended with notions such as collective intention and mutual belief. There is no consensus on the definition of collaboration. We consider that a group of agents is engaged in a collaborative activity as soon as they share a collective intention."}
{"pdf_id": "0805.4101", "content": "This social rule is tran scribed by repeated use through a reaction to the realization of a particular action (on the speaker'spoint of view) and through a reaction to the observation of an event which is the occurrence of a par ticular action (on the addressee's point of view)"}
{"pdf_id": "0805.4101", "content": "In order to integrate Collective Acceptance inreference, we propose an extension of an ex isting model of referential acts based on A. Kronfeld's work in the rational model used (Bretier et al., 1995). The act of reference from anagent i to another agent j, using the conceptual ization x (which corresponds to the semantics of the referential expression) to refer to the object y is formalized as:"}
{"pdf_id": "0805.4101", "content": "Remaining the goal of referential acts (2.1), the choice of the description of the intented place is guided by its capacity to enable Laura to pick out, in her mental state, the mental representation of the correct place. That is, the description enables Laura to isolate the correct mental representation from other possible ones, with sufficient evidence of mutuality. This is a pragmatic (ie. contextual) guideline, which corresponds to the Identification goal."}
{"pdf_id": "0805.4101", "content": "She is obliged to reply to his proposition by the social rule. Besides, the precondition of acceptinga conceptual pact is to have realized the Identifica tion goal; otherwise, the addressee has the choice between other possible reactions. As Laura failed to succeed, she chooses to ask for clarification in (U2):"}
{"pdf_id": "0805.4101", "content": "In order to achieve understanding, by a coopera tive attitude, Tom realizes Laura's request in (U3).Laura is now able to pick out a single mental rep resentation of the place. She likes it, so she agrees. The social goal obliges Laura to react to Tom'snew proposition. As the precondition of accept ing is fulfilled, with uttering (U4), Laura realizes the following intention:"}
{"pdf_id": "0805.4101", "content": "Modeling dialog as collaborative activity consists notably in specifying the content of the Conversational Common Ground and the kind of social mental state involved. Even if mutual beliefs, or weaker forms of belief states, do not rise to inconsistencies, but, are still sufficiently strong for the participants to have successful cooperation or coordination of actions. Epistemic states involve computational treatments with high complexity."}
{"pdf_id": "0805.4101", "content": "We show that modeling the CCG by an epistemic state is neither necessary, nor proper. Considering only genuine conceptual pacts limits the capacity of interaction and may leads to \"real\" communicative errors. We have proposed a formalization of Collective Acceptance, furthermore, elements haven been given in order to integrate this attitude in a rational model of dialog. Finally, a model of referential acts as being part of a collaborative activity has been provided."}
{"pdf_id": "0805.4101", "content": "Further studies will hold on the extension of the general principles proposed to the dialog itself. Moreover, collective acceptance is a particularly interesting attitude because it allows to model reference and dialog itself as situated activities in an elegant manner. Finally, this concept may provide symbolic elements in order to form the grounding criterion, which is a notion especially hard to make up, because this criterion is highly context dependant. Grounding criterion differs depending on the people involved, the domain concerned and so on."}
{"pdf_id": "0805.4508", "content": "Some efforts have  been devoted to learning from loosely annotated images, for instance learning latent  semantic models [1-3], translating from discrete visual features to keywords [4-5], using  cross-media relevance model [6-7], learning a statistic modeling for image annotation  [8-11], image annotation using multiple-instance learning [12], and so on"}
{"pdf_id": "0805.4508", "content": "1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45"}
{"pdf_id": "0805.4508", "content": "1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45"}
{"pdf_id": "0805.4508", "content": "we pick out and associate missing keywords in annotated training images with \"imagined\"  occurrence frequencies by averaging similarity measures between them and annotated  keywords. These retrieved missing keywords are referred to as \"imagined\" annotations.  Then, words-driven probabilistic latent semantic analysis (PLSA-words [3]) is used to  modeling both given and \"imagined\" annotations. At last, learned models are used to  automatically annotate new images. Three example images and three kinds of annotations  are illustrated in Fig. 1, where the second row corresponds to the \"imagined\" annotation of  images."}
{"pdf_id": "0805.4508", "content": "The rest of this paper is organized as follows. In section 2, we formulate the problem  of enriching the incomplete annotation in the framework of automatic image annotation.  The proposed algorithm to solving the problem is presented in section 3. Experimental  results and discussions are given in section 4. Some conclusions are drawn in the last  section."}
{"pdf_id": "0805.4508", "content": "46  47  48  49  50  52  52  53  54  55  56  57  58  59  60  61  62  62  64  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92"}
{"pdf_id": "0805.4508", "content": "46  47  48  49  50  52  52  53  54  55  56  57  58  59  60  61  62  62  64  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92"}
{"pdf_id": "0805.4508", "content": "where p(t | Itest) and p(wk | t) are model parameters to be estimated; the first parameter is a  mixture coefficient of topics in the test image; the second is a distribution over keywords  in the topic t. To estimate these parameters, one might maximize the log-likelihood of  annotated keywords in N training images D"}
{"pdf_id": "0805.4508", "content": "93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137"}
{"pdf_id": "0805.4508", "content": "93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137"}
{"pdf_id": "0805.4508", "content": "The proposed algorithm includes two stages: (1) obtaining \"imagined\" annotations through  approximating conditional probability of missing keywords given training images and  loose annotations; (2) modeling both given and imagined annotations using PLSA-words  [3]. For convenience of expression, we refer to the proposed algorithm as Virtual-Word  driven Probabilistic Latent Semantic Analysis (PLSA-vw). The two stages of PLSA-vw  are detailed in the following two subsections, respectively."}
{"pdf_id": "0805.4508", "content": "138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182"}
{"pdf_id": "0805.4508", "content": "138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182"}
{"pdf_id": "0805.4508", "content": "where wij is the count of keyword wj in image Ii. It is easy to see that the joint probability  p(wj, wk | D) in Eq. (7) is actually approximated by an inner product between two  normalized word-count vectors or a cosine-like similarity measure between keywords.  Let count-matrix W (resp. B) be a set of word- (resp. blob-) histograms where each  row corresponds to an training image. The actual approximation method is an inverse  process from Eq. (6) to (8), as shown in the following four steps:"}
{"pdf_id": "0805.4508", "content": "elements are equal to 1, and the matrix division is performed at every correspondent entry.  Up to now, all non-zero entries in the matrix Wimg correspond to keywords missed in the  loose annotation, which are assumed relevant to semantics of images. All keywords  retrieved from training images are referred to as \"imagined\" annotations, in contrast with  the given annotations in training images."}
{"pdf_id": "0805.4508", "content": "183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  225  226  227"}
{"pdf_id": "0805.4508", "content": "183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  225  226  227"}
{"pdf_id": "0805.4508", "content": "annotation of the third example image includes \" Jet Sky Grass Runway Elephant\", where  the \"Jet\" is obviously irrelevant to the image. As mentioned before, the imagined  annotations are associative and are not checked by human supervision. Therefore, we  simply regard the approximated conditional probability of missing keywords as their  reliability in the imagined annotations. Furthermore, we use the approximated conditional  probability as a real-value word-count. Typically, the real-value word-count is less than  one. Therefore, we can define a new word-count matrix for learning"}
{"pdf_id": "0805.4508", "content": "where W and Wimg are word-count matrixes in given and imagined annotations,  respectively. It can be seen from the process of approximation in subsection 3.1 that  imagined annotations Wimg are obtained bypassing blob-count matrix B. In other words,  these annotations have been imagined without consulting visual features of training  images. To ensure the imagination can be reflected on visual features, we derive a new  blob-count matrix for learning in the same way"}
{"pdf_id": "0805.4508", "content": "In the process, the changes what we need make  include (1) replacing matrixes B and Bimg with W and Wimg, respectively; (2) accordingly,  normalized word-count matrix Wnorm and similarity matrix Wsim would be rewritten as  Bnorm and Bsim; (3) the number of keyword, q, should be replaced with that of blob, p, in  step 3"}
{"pdf_id": "0805.4508", "content": "228  229  230  231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272"}
{"pdf_id": "0805.4508", "content": "228  229  230  231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272"}
{"pdf_id": "0805.4508", "content": "In this section, we evaluate the performance of the proposed algorithm from two aspects.  First, we examine the relative improvement of PLSA-vw over PLSA-words in terms of  image annotation, indexing and retrieval. Second, the proposed method is compared with  three typical discrete annotation methods, i.e., machine translation (MT) [4], translation  table (TT) [5], and cross-media relevance model (CMRM) [6]. Unlike PLSA-vw and  PLSA-words, these methods use image as latent variable, and sum out of all training  images to annotate new images [11]. Therefore, the annotation performance of these  methods is heavily dominated by the empirical distribution of keywords in training images.  As shown in subsection 4.2, these methods are biased to annotate images with frequent  keywords."}
{"pdf_id": "0805.4508", "content": "273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304  305  306  307  308  309  310  311  312  313  314  315  316  317"}
{"pdf_id": "0805.4508", "content": "273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304  305  306  307  308  309  310  311  312  313  314  315  316  317"}
{"pdf_id": "0805.4508", "content": "Table 1 lists the performance of automatic annotation methods used in our experiments,  where the number in brackets is the variance of ten samples. Given an index (a column),  the best and next methods are marked with red and blue color, respectively. Although  PLSA-vw is not the best for any index and only is the second for three of four indexes, it  does, as expected, benefit from two kinds of latent variable models outlined in section 2."}
{"pdf_id": "0805.4508", "content": "318  319  320  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336  337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352  353  354  355  356  357  358  359  360  361  362"}
{"pdf_id": "0805.4508", "content": "318  319  320  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336  337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352  353  354  355  356  357  358  359  360  361  362"}
{"pdf_id": "0805.4508", "content": "363  364  365  366  367  368  369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384  385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407"}
{"pdf_id": "0805.4508", "content": "363  364  365  366  367  368  369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384  385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407"}
{"pdf_id": "0805.4508", "content": "408  409  410  411  412  413  414  415  416  417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432  433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  449  450  451  452  453"}
{"pdf_id": "0805.4508", "content": "408  409  410  411  412  413  414  415  416  417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432  433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  449  450  451  452  453"}
{"pdf_id": "0805.4560", "content": "Due to association of uncertainty and vagueness  with the monitored data set, particularly, resulted  from the in-situ tests (such lugeon test), accounting  relevant approaches such probability, Fuzzy Set  Theory (FST) and Rough Set Theory (RST) to  knowledge acquisition, extraction of rules and  prediction of unknown cases, more than the past  have been distinguished"}
{"pdf_id": "0805.4560", "content": "The  indiscernibility relation (similarity), which is a  mathematical basis of the rough set theory, induces  a partition of the universe in to blocks of  indiscernible objects, called elementary sets, which  can be used to build knowledge about a real or  abstract world"}
{"pdf_id": "0805.4560", "content": "2.1. Self Organizing feature Map (SOM)  Kohonen's SOM algorithm has been well renowned  as an ideal candidate for classifying input data in an unsupervised learning way [8]. Kohonen self organizing networks (Kohonen feature maps or  topology-preserving maps) are competition-based  network paradigm for data clustering. The learning  procedure of Kohonen feature maps is similar to the"}
{"pdf_id": "0805.4560", "content": "competitive learning networks. The main idea  behind competitive learning is simple; the winner  takes all. The competitive transfer function returns  neural outputs of 0 for all neurons except for the  winner which receives the highest net input with  output 1.  SOM changes all weight vectors of neurons in the  near vicinity of the winner neuron towards the input  vector. Due to this property SOM, are used to  reduce the dimensionality of complex data (data  clustering). Competitive layers will automatically  learn to classify input vectors, the classes that the  competitive layer finds are depend only on the  distances between input vectors [8]."}
{"pdf_id": "0805.4560", "content": "surely described by attributes  B [6]. The existing  induction algorithms use one of the following  strategies:  (a) Generation of a minimal set of rules covering all  objects from a decision table;  (b) Generation of an exhaustive set of rules  consisting of all possible rules for a decision table;  (c) Generation of a set of `strong' decision rules,  even partly discriminant, covering relatively many  objects each but not necessarily all objects from the  decision table [11]. In this study we have  developed RST in MatLab7, and on this added  toolbox other appropriate algorithms have been  prepared."}
{"pdf_id": "0805.4560", "content": "In the whole of our algorithms, we use four basic  axioms upon the balancing of the successive  granules:  Step (1): dividing the monitored data into groups of  training and testing data  Step (2): first granulation (crisp) by SOM or other  crisp granulation methods  Step (2-1): selecting the level of granularity"}
{"pdf_id": "0805.4560", "content": "Balancing assumption is satisfied by the close-open  iterations: this process is a guideline to balancing of  crisp and sub fuzzy/rough granules by some  random/regular selection of initial granules or other  optimal structures and increment of supporting rules  (fuzzy partitions or increasing of lower /upper  approximations ), gradually"}
{"pdf_id": "0805.4560", "content": "neurons in SOM;  E is the obtained error (measured  error) from second granulation on the test data and  coefficients must be determined, depend on the used  data set. Obviously, one can employ like  manipulation in the rule (second granulation)  generation part, i.e., number of rules.  Determination of granulation level is controlled  with three main parameters: range of neuron  growth, number of rules and error level. The main  benefit of this algorithm is to looking for best  structure and rules for two known intelligent  system, while in independent situations each of  them has some appropriate problems such: finding of spurious patterns for the large data sets, extra time training of NFIS or SOM."}
{"pdf_id": "0805.4560", "content": "4.1. Permeability assessment in Shivashan dam  site-Iran  Shivashan hydroelectric earth dam is located 45km  north of Sardasht city in northwestern of Iran.  Geological investigation for the site selection of the  Shivashan hydroelectric power plant was made  within an area of about 3 square kilometer. The  width of the V-shaped valley with similarly sloping  flanks, at the elevation of 1185m and 1310m with  respect to sea level are 38m and 467m, respectively."}
{"pdf_id": "0805.4560", "content": "It must be noticed that for unrecognizable objects in  test data (elicited by rules) a fix value such 4 is  ascribed. So for measure part when any object is not  identified, 1 is attributed. This is main reason of such swing of MSE in reduced data set 6 (figure 15 b). Clearly, in data set 7 SORST gains a lowest  error (26 neurons in SOM). The extruded rules in  the optimum case can be purchased in table 2. We  have explained application of SORST in back  analysis in other study [14]."}
{"pdf_id": "0805.4560", "content": "Results of transferring attributes(X, Y, Z and lugeon)  in five categories by 1-D SOM  To finding out of the background on these major  zones, we refer to the clustered data set by 2D SOM  with 7*9 weights in competitive layer (figure 10-c),  on the first set of the attributes"}
{"pdf_id": "0805.4560", "content": "Indeed, with developing of new approaches in  information theory and computational intelligence,  as well as, soft computing approaches, it is  necessary to consider these approaches to better  understand of natural events in rock mass. Under  this view and granulation theory, we proposed two  main algorithms, to complete soft granules  construction in not 1-1 mapping level: Self  Organizing  Neuro-Fuzzy  Inference  System  (Random and Regular neuron growth-SONFIS-R,  SONFIS-AR- and Self Organizing Rough Set  Theory (SORST). So, we used our systems to  analysis of permeability in a dam site, Iran."}
{"pdf_id": "0806.0250", "content": "Requirements about the quality of clinical guidelines can be represented by schemataborrowed from the theory of abductive diagnosis, using temporal logic to model the timeoriented aspects expressed in a guideline. Previously, we have shown that these require ments can be verified using interactive theorem proving techniques. In this paper, weinvestigate how this approach can be mapped to the facilities of a resolution-based the orem prover, otter, and a complementary program that searches for finite models of first-order statements, mace-2. It is shown that the reasoning required for checking the quality of a guideline can be mapped to such fully automated theorem-proving facilities. The medical quality of an actual guideline concerning diabetes mellitus 2 is investigated in this way."}
{"pdf_id": "0806.0250", "content": "The meta-level approach that is used here is particularly important for the designof clinical guidelines, because it corresponds to a type of reasoning that occurs dur ing the guideline development process. Clearly, quality checks are useful during this process; however, the design of a guideline can be seen as a very complex process where formulation of knowledge and construction of conclusions and corresponding recommendations are intermingled. This makes it cumbersome to do interactiveverification of hypotheses concerning the optimal recommendation during the con struction of such a guideline, because guideline developers do not generally have the necessary background in formal methods to construct such proofs interactively.Automated theorem proving could therefore be potentially more beneficial for sup porting the guideline development process."}
{"pdf_id": "0806.0250", "content": "It is part of a real-world guideline for general practitioners about the treatment of diabetes mellitus type 2. Part of this description includes details about dosage of drugs at specific time periods. As we want to reason about the general structure of the guideline, rather than about dosages or specific time periods, we have made an abstraction as shown in Fig. 1. This guideline fragment is used in this paper as a running example.Guidelines can be as large as 100 pages; however, the number of recommenda tions they include are typically few. In complicated diseases, each type of disease"}
{"pdf_id": "0806.0250", "content": "Below we present some ideas on how such knowledge may be formalised using temporal logic (cf. (Lucas 1995) for earlier work in the area of formal modelling of medical knowledge). We are interested in the prescription of drugs, taking into account their mode of action. Abstracting from the dynamics of their pharmacokinetics, this can be formalised in logic as follows:"}
{"pdf_id": "0806.0250", "content": "To determine the global quality of the guideline, the background knowledge itself was only formalised so far as required for investigating the usefulness of the theory of quality checking introduced above. The knowledge that is presented here was acquired with the help of a physician, though this knowledge can be found in many standard textbooks on physiology (e.g., (Ganong 2005; Guyton and Hall 2000))."}
{"pdf_id": "0806.0250", "content": "At some stage in the natural history of diabetes mellitus type 2, the level of glucose in the blood is too high (hyperglycaemia) due to decreased production of insulin by the B cells. A popular hypothesis explaining this phenomenon is that target cells have become insulin resistant, which with a delay causes the production of insulin by the B cells to raise. After some time, the B cells become exhausted, and they are no longer capable of meeting the demands for insulin. As a consequence, hyperglycaemia develops. Treatment of diabetes type 2 consists of:"}
{"pdf_id": "0806.0250", "content": "The consequences of various treatment options can be examined using the method introduced in Section 3. Hypothetical patients for whom it is the intention to reach a normal level of glucose in the blood (normoglycaemia) and one of the steps in the guideline is applicable in the guideline fragment given in Fig. 1, are considered, for example:"}
{"pdf_id": "0806.0250", "content": "In order to prove meta-level properties, it is necessary to reason at the object-level. Object-level properties typically do not contain background knowledge concerning the validity what it being verified. For example, the (M2) property of Section 3 has a clear meaning in terms of clinical guidelines, which would be lost if stated as an object-level property. Moreover, it is not (directly) possible to state that something does not follow at the object level. Fig. 3 summarises the general approach. We will first give a definition for translating the object knowledge to standard logic and then the translation of the meta-level knowledge will follow."}
{"pdf_id": "0806.0250", "content": "In order to reason about a sequence of treatments, additional formalisation is re quired. The background knowledge was developed for reasoning about an individual treatment, and therefore, is parameterised for the treatment that is being applied. We postulate BDM2, parameterised by s, where s is a certain step in the protocol, i.e., s = 1, 2, 3, 4 (cf. Fig. 1; for example s = 1 corresponds to diet). The first axiom is then described by:"}
{"pdf_id": "0806.0250", "content": "has two mode specifications. Either the first two arguments are input arguments resulting in a concatenation of the two lists in the output argument, or, the first two arguments can act as output arguments resulting in the decomposition of the third argument into two lists. In the following, we will write all ground atoms without arguments, e.g., we"}
{"pdf_id": "0806.0250", "content": "This states that, if the completed theory implies that the patient will not have normoglycaemia, then this is consistent conclusion with respect to the original specification, for any specific step described by s. Therefore, there is no reason to assume that T is the correct treatment in step s. This result is applied to the control axiom C as described in Section 5.5.1, i.e., formula 5. If we were to deduce that"}
{"pdf_id": "0806.0250", "content": "To investigate the quality of the treatment sequence, a choice of quality criteria has to be chosen. Similarly to individual treatments, notions of optimality could be studied. Here, we investigate the property that for each patient group, the intention should be reached at some point in the guideline. For the diabetes guideline, this is formalised as follows:"}
{"pdf_id": "0806.0250", "content": "As we restrict ourselves to a particular treatment described in step s, this property is similar to the property proven in Section 5.3. However, it is possible that the control never reaches s for a certain patient group, hence, using the knowledge described in C, it is also important to verify that this step is indeed reachable, i.e.,"}
{"pdf_id": "0806.0250", "content": "i.e., the third step will be reached and in this step the patient will be cured. This was implemented in otter using the translation as discussed in the previous subsection. As the temporal reasoning is easier due to the abstraction that was made, the proofs are reasonably short. For example, in the example above, the proof has length 25 and was found immediately."}
{"pdf_id": "0806.0250", "content": "Furthermore, the representation that we have used in this paper is conceptually relatively simple com pared to representation of guidelines and complex temporal knowledge discussedin for example (Shahar and Cheng 2000), however, in principle all these mecha nisms could be formalised in first-order logic and could be incorporated in this approach"}
{"pdf_id": "0806.0250", "content": "For example, assumption (53) models the capacity of the B cells, i.e., nearly ex hausted at time 0 where the property as shown above should be refuted. Note that some of the clauses are introduced in the translation to propositional logic, for example assumption (2) is due to the fact that that values of the capacity are mutually exclusive. This is consistent with the original formalisation, as functions map to unique elements for element of the domain. Early in the proof, otter deduced that if the capacity of insulin in B cells is nearly-exhausted, then it is not completely exhausted:"}
{"pdf_id": "0806.0526", "content": "From the study of ECOTEC in 2005[6] regarding the critical success  factors in cluster development, the two critical success factors are collaboration in  networking partnership and knowledge creation for innovative technology in the  cluster which are about 78% and 74% of articles mentioned as success criteria accordingly"}
{"pdf_id": "0806.0526", "content": "The feasibility study serves as decision support for an economical, technical and  project feasibility study, in order to select the most promising focus area and target  solution. This phase identifies problems, opportunities and potential solutions for  the organization and environment. Most of the knowledge engineering  methodologies provide the analysis method to analyze the organization before the  knowledge engineering process. This helps the knowledge engineer to understand  the environment of the organization. CommonKADS also provides context levels  in the model suite (figure 1.2) in order to analyze organizational environment and  the corresponding critical success factors for a knowledge system [16]. The  organization model provides five worksheets for analyzing feasibility in the  organization as shown in figure 1.4."}
{"pdf_id": "0806.0526", "content": "from OM are a list of knowledge intensive tasks and agents which are related to  each task. Then, KE could interview experts in each task using TM and AM  worksheets for the next step. Finally, KE validates the result of each module with  knowledge decision makers again to assess impact and changes with the OTA  worksheet."}
{"pdf_id": "0806.0526", "content": "The main objectives of this phase are to check, whether the target ontology suffices  the ontology requirements and whether the ontology based knowledge  management system supports or answers the competency questions, analyzed in  the feasibility and kick off phase of the project. Thus, the ontology should be tested  in the target application environment. A prototype should already show core  functionalities of the target system. Feedbacks from users of the prototype are  valuable input for further refinement of the ontology. [18]"}
{"pdf_id": "0806.0526", "content": "The maintenance and evolution of an ontology-based application is primarily an  organizational process [18]. The knowledge engineers have to update and maintain  the knowledge and ontology in their responsibility. In order to maintain the  knowledge management system, an ontology editor module is developed to help  knowledge engineers."}
{"pdf_id": "0806.0526", "content": "In the case study of a handicraft cluster, one of the knowledge intensive tasks is  about product selection for exporting. Not all handicraft products are exportable  due to their specifications, function, attributes, etc. Moreover, there are many  criteria to select a product to be exported to specific countries. So we defined the  task ontology of the product selection task (see the right side of figure 1.6)."}
{"pdf_id": "0806.0526", "content": "The most important role of ontology in knowledge management is to enable and to  enhance knowledge sharing and reusing. Moreover, it provides a common mode of  communication among the agents and knowledge engineer [14]. However, the  difficulties of ontology creation are claimed in most literature. Thus, this study  focuses on creating ontology by adopting the knowledge engineering methodology  which provides tools to support us for structuring knowledge. Thus, ontology was  applied to help Knowledge Management System (KMS) for the industry cluster to  achieve their goals. The architecture of this system consists of three parts,"}
{"pdf_id": "0806.0689", "content": "In last two decades, many fast block-matching algorithms (BMA) have  been proposed to accelerate the process without degrading the performance of the search algorithms  fatally, such as the three-step search (TSS) algorithm [6], the new three-step search (NTSS) algorithm  [7], the four-step search (4SS) algorithm [8], the block-based gradient descent search (BBGDS)  algorithm [9], the diamond search (DS) algorithm [10], the unrestricted center-biased diamond search  (UCBDS) algorithm [11], the hexagon-based search (HEXBS) algorithm [12], and the cross diamond  search (CDS) algorithm [13], etc"}
{"pdf_id": "0806.0689", "content": "Based on the comprehensive study of MVP distribution and the relationship between the search  pattern and the search result, a directional model of MVP distribution is built in this paper to describe  the real-world sequences more precisely. The conditional distribution of motion vector is brought  forward to show the directional characteristics of MVP distribution for the first time. A novel fast  BMA called the directional cross diamond search (DCDS) algorithm is also proposed here with the  horizontal cross search pattern and directional diamond search patterns. This work is improved from  early versions [14, 15]. In the following section, an in-depth study on MVP distribution will be given"}
{"pdf_id": "0806.0689", "content": "The search pattern with a certain shape and size has significant impact on the efficiency and the  effectiveness of the search algorithm. Therefore, the search pattern is important and must be designed  to fit the characteristics of MVP distribution. In fact, every discovery of the new characteristic of the  MVP distribution is followed by the upgrade of the search pattern and the improvement of the search  algorithm's performance."}
{"pdf_id": "0806.0689", "content": "The uniform MVP distribution model hypothesizes that the MVP is the same not  only in each direction but also on each position in the search window; the square-center-biased model  deems that the MVP distribution is the same in eight directions (two horizontal, two vertical and four  diagonal directions); the cross-center-biased model describes the MVP distribution regularity more  accurately for it is same only in four directions (two horizontal and two vertical directions)"}
{"pdf_id": "0806.0689", "content": "However,  after some more in-depth studies on the statistical data of MVP of 18 common standard video  sequences, we can see that the cross-center-biased model is not the most proper or all-around way to  reflect the essence of the MVP distribution because of the existence of the directional differences"}
{"pdf_id": "0806.0689", "content": "The statistical results of the MVP distribution are tabulated in Table II and III. MVPs accumulated at  the corresponding positions of the one-quarter search window are shown as the 2-D accumulative  distribution in Table II. Four types of 1-D statistics are shown in Table III: the MVP distributions of all  the motion vectors accumulated in the vertical and horizontal directions (Ax(d) and Ay(d)), and the  MVP distributions in the horizontal and vertical directions (Bx(d) and By(d))."}
{"pdf_id": "0806.0689", "content": "The MVP distribution that we focus on in the intermediate search steps should be the conditional MVP  distribution because we will determine the next search direction on condition of the former search  results. There are two conditional MVP distributions, the prior probability distribution and the  posterior probability distribution, and they both have the directional characteristics."}
{"pdf_id": "0806.0689", "content": "The prior probability distribution of MVP is defined as the probability distribution of the global  best-matched point (BMP, it is the position of the corresponding motion vector) in the search window  on condition that the current BMP has been found. Let T denote the set of all the points in the search  window and S denote the set of all the points covered by the search pattern in the former search steps,"}
{"pdf_id": "0806.0689", "content": "The posterior probability distribution of MVP is defined as the probability distribution of the current  BMP on condition that the global BMP has been known. T and S have the same definition, and the  global BMP, Q(xq, yq), is the point with the minimum distortion in T,"}
{"pdf_id": "0806.0689", "content": "The directional model of the MVP distribution can be built easily based on the former analyses: the  motion vector distribution is not equal or same in the different directions, but is  horizontal-center-biased. The MVPs concentrate more heavily in the horizontal directions than in the  vertical. The conditional distribution of MVP has the directional properties so that the direction from  the center to the current BMP gives the rough orientation of the subsequent search. These two  characteristics will help improve the performance of the first and latter search steps in fast BMA."}
{"pdf_id": "0806.0689", "content": "The search patterns in the previous BMAs are symmetrical in all four horizontal and vertical  directions, which do not correspond with the directional characteristics of the MVP distribution.  Therefore, a new kind of search pattern needs to be designed to find the motion vector more quickly  and directly in the proper direction. Based on the horizontal center-biased MVP distribution and  directional characteristics of the conditional distribution of MVP proposed above, the horizontal cross  search pattern (HCSP) and directional diamond search patterns (DDSP), as depicted in Fig. 4, are  proposed in the new BMA, which is termed the directional cross diamond search (DCDS) algorithm."}
{"pdf_id": "0806.0689", "content": "In these patterns, the points with the distance 2 to the  center point are called the distant points and the points with the distance 1 are called the near points;  the part of the pattern in the direction where the distant points are located is called the long wing and  the other part is called the short wing; the points with the distance 1 to the center point on the long  wings are called the middle points (the hollow squares in Fig"}
{"pdf_id": "0806.0689", "content": "The DCDS algorithm is quite different from any other fast BMAs in: 1) the search patterns used in  DCDS have the minimum number of points; 2) the directional search patterns are used; and 3) the  switching strategy of the different search patterns in the middle steps is adopted necessarily. DCDS  exploits the characteristics of the directional model of MVP distribution completely, replacing the  cross search pattern with the HCSP in the first step and the diamond search pattern with HDSP/VDSP  compared to CDS. Below summarizes the DCDS algorithm."}
{"pdf_id": "0806.0689", "content": "Step1: HCSP is centered at the origin of the search window and set as the current search pattern (CSP).  If the current BMP occurs at the center of the CSP, the search process stops and the motion vector is  found on the center; otherwise, go to step2;"}
{"pdf_id": "0806.0689", "content": "Step2: Update the CSP to HDSP or VDSP according to the switching strategy one, put the center of  the CSP on the current BMP, and calculate distortion measure to find the new current BMP. If the  current BMP occurs at the center point, go to step4; otherwise go to step3;"}
{"pdf_id": "0806.0689", "content": "Step3: Update the CSP according to the switching strategy two, put the center of the CSP on the  current BMP, and calculate distortion measure to find the new current BMP. If the current BMP occurs  at the center point, go to step4; otherwise repeat this step continuously;"}
{"pdf_id": "0806.0689", "content": "The uni-modal error surface assumption of the BDM is one ideal condition of the MVP distribution:  the BDM of matching blocks increases monotonically away from the global minimum distortion. It  produces us an identical condition to evaluate the general performance of different algorithms though  it is seldom right to reflect the actual distribution. We set up such an ideal condition: the distortion  between the current block P0 and the best-matched reference block MV(xmv, ymv) is zero, and the  block-matching distortion of other reference block P(xp, yp) is defined as its Euclid distance to the  best-matched block, and then calculate the number of search points on each position of the search  window (as listed in Table VII)."}
{"pdf_id": "0806.0689", "content": "When applied to stationary or quasi-stationary sequence, such as \"Salesman\", DCDS and CDS  algorithm have the similar performance according to the PSNR of the compensated frame while the  search speed (measured by the number of search point) of DCDS is 20.6% faster than that of CDS.  But when applied to the sequence having large motion content and various motion directions, DCDS  can speed up the search progress significantly. Take the sequence \"Coastguard\" as the example, the  NSP of DCDS and CDS are 10.885 and 16.857 respectively, so DCDS achieves 54.9% speed-up with  only 0.021dB of degradation in the quality. Other aspects of DCDS and CDS are all quite similar."}
{"pdf_id": "0806.0689", "content": "Fig. 8 and 9 illustrate the frame-by-frame comparison of PSNR and NSP after applying FS, NTSS,  4SS, DS, HEXBS, CDS and DCDS to \"Salesman\" and \"Coastguard\". They clearly demonstrate the  robust and superior performance of the proposed DCDS algorithm to other BMAs in terms of the  average number of search points with the similar or even better distortion error in terms of PSNR."}
{"pdf_id": "0806.0784", "content": "Abstract—The interface for the next generation of Un manned Vehicle Systems should be an interface with multi-modal displays and input controls. Then, the role of the interface will not be restricted to be a support of the interactions between the ground operator and vehicles. Interface must take part in the interaction management too. In this paper, we show that recent works in pragmatics and philosophy [1] provide a suitable theoretical framework for the next generation of UV System's interface. We concentrate on two main aspects of the collaborative model of interaction based on acceptance: multi-strategy approach for communicative act generation and interpretation and communicative alignment."}
{"pdf_id": "0806.0784", "content": "At the moment, most Unmanned Vehicle (UV) Systems are single vehicle systems whose control mode is teleoperation. Several ground operators are needed in order to operate avehicle. Besides, vehicles have limited autonomous capabili ties. Consequently, controlling vehicle is such a hard task that it may lead to an untractable cognitive load for the ground operator [2]. In order to make this task more feasible and in order to reduce the cost of UV Systems in term of human resource, several areas of renection are explored:"}
{"pdf_id": "0806.0784", "content": "• increasing vehicle's autonomy [4]. As a result, control mode will shift to a more nexible control mode such as control/supervision in the next generation of UV Systems. Moreover, the role of the operator will shift to controlling/supervising a system of several cooperating UVs performing a joint mission i.e. a Multi-Agent System (MAS) [5]."}
{"pdf_id": "0806.0784", "content": "In the same time, current works aim at enhancing the nexibility and the naturalness of interface rather than only improving the mission's realization and control. In particular, human-centered approaches introduce new modalities (gesture, spoken or written language, haptic display, etc.), [2], [6]. The interface for the next generation of UV Systems should be an interface with multi-modal displays and input controls. Actually, multi-modal displays aim at making up for the \"sensory isolation\" of ground operator, as well as reducing cognitive and perceptual demands [6]. This is especially important considering the high visual demand"}
{"pdf_id": "0806.0784", "content": "The collaborative nature of interaction (or dialogue) have been brought into the forefront by research in pragmatics since mid-90s [8]. Basing an interface's interaction management on such a model gives the interface and its users the capacity to interactively refine their understanding until a point of intelligibility is reached. Thus, such interface manages non-understandings1. This approach have been used within"}
{"pdf_id": "0806.0784", "content": "1Non-understanding is commonly set apart misunderstanding. In a mis understanding, the addressee succeeds in communicative act's interpretation,whereas in a non-understanding he fails. But, in a misunderstanding, ad dressee's interpretation is incorrect. For example, mishearing may lead to misunderstanding. Misunderstandings are considered here as the only kind of \"communicative errors\" (c.f. section II-A). Thus, they are handled by a recovery process, which is not supported by the interaction model."}
{"pdf_id": "0806.0784", "content": "the WITAS dialog system [9]. In this paper, we show that recent works in pragmatics and philosophy [1] provide a suitable theoretical framework for the next generation of UV System's interface. We concentrate on two main aspects of the collaborative model of interaction based on acceptance: multi-strategy approach for generation and interpretation of communicative acts and communicative alignment."}
{"pdf_id": "0806.0784", "content": "possible strategies. Existing methods are interpretation based on keyword recognition [12], statistical methods based on heuristics [13], more pragmatics-based approach [14], etc. In this paper (section II-C), we present an interaction modelwhich is coherent with each type of method. Thus, an interac tion manager based on such a model can support multi-strategy methods of communicative acts generation and interpretation."}
{"pdf_id": "0806.0784", "content": "interaction manager. Cognitive models of interaction aim, for instance, at defining a symbolic and explanatory model of interaction, whereas Adjacent Pairs provide a descriptive model of interaction. Cognitive models may be considered as a logical reformulation of plan-based models. Cognitive models integrate, in more, a precise formalization of dialog partners' mental states (their beliefs, choices (or desires) and intentions), of the rational balance which relates mental attitudes between them and relates mental attitudes with agents' acts."}
{"pdf_id": "0806.0784", "content": "Basing interaction management on a collaborative modelof interaction gives the interface the ability to manage non understandings, as shown in the first part of this section. A formal collaborative model of interaction is generally based on a psycholinguistic model of interaction. However, existing psycholinguistic models of interaction do not support multi-strategy approach for communicative act generation and interpretation. We propose to base interaction management, for the next generation of UV Systems, on a formal interaction model supporting such a multi-strategy approach. This formal model mixes and enhances the two main and complementary psycholinguistic models of interaction. The second part of this section introduces these two psycholinguistic models of interaction."}
{"pdf_id": "0806.0784", "content": "In contrast with the traditional view, collaborative model of interaction defines it as a bidirectional process resulting from a single social activity. Interaction is considered as a collaborative activity characterized by the goal of reaching mutual understanding, shared by dialog partners. Mutual understanding is reached through interpretation's negotiation. That is an interactive refinement of understanding until a sufficient point of intelligibility is reached, illustrated by the example shown in Fig. 3."}
{"pdf_id": "0806.0784", "content": "Consequently, the production of a suitable communicative act can be divided between several exchanges and between all dialog partners. The complexity of such process must be less complex than in the traditional view of interaction [21]. Besides, the addressee has an active role, explicit and implicit feedbacks are required in order to publicly signal successful understandings. Finally, non-understandings are here regarded as \"the normal case\", so their management is captured by collaborative model of interaction"}
{"pdf_id": "0806.0784", "content": "1) Clark's Intentional model: Most of formal collaborative models of interaction are based on the psycholinguist H. H. Clark's work [8], [23]. His work highlights the collaborative nature of interaction, its realization through a negotiation process, its success warranted by the use of the common ground (i.e. mutual beliefs) among dialog partners, conceptual pacts (i.e. temporary, partner-specific alignment among dialog partners on the description chosen for a particular object). Basing interaction management on this model is interesting because:1) Designing interaction as a collaborative process en hances mixed-initiative interaction.2) Non-understandings are interactively managed, thus in terface's robustness and nexibility are enhanced. 3) Positive and negative signals of understandings are consistently required, as part of the negotiation process."}
{"pdf_id": "0806.0784", "content": "However, there are several limitations against this model [1]:1) The systematic use of common ground leads to mono strategic and complex generation and interpretation ofcommunicative acts. In Human-Human interactions, di alog partners rely on different strategies. The complexity of the strategy vary depending on the context, depending on time pressure for example. 2) Considering common ground as a set of mutual beliefs leads to computational limitations and paradoxes, as human beings tends to have selfish and self-deceptive attitudes."}
{"pdf_id": "0806.0784", "content": "To sum up, this model is suitable for modeling non understandings management through interpretation negotiation. Nevertheless, interpretation negotiation, as defined in this model, is too restrictive. This is due to systematic use of common ground and defining common ground as a set of mutual beliefs, i.e. a stronger definition of"}
{"pdf_id": "0806.0784", "content": "2) The Interactive Alignment Model: Another model of the collaborative nature of interaction has been proposed by M. J. Pickering and S. Garrod [24]: the Interactive Alignment Model (IAM). IAM claims that dialog partners become aligned at several linguistics aspects. In the particular case of spoken dialog, there is an alignment, for example, of the situation model, of the lexical and the syntactic levels, even of clarity of articulation, of accent and of speech rate.For example, syntactic alignment is frequent in question answer, such as in Fig. 4."}
{"pdf_id": "0806.0784", "content": "These alignments results from automatic processes based on priming. Priming consists in reusing the result of a preceding cognitive process, such as perception or action execution, in a following cognitive process. In the particular case of interaction, priming consists in reusing words or syntactic constructions recently understood or generated. As an automatic process, priming does not induce any cognitive load. Besides, these alignments facilitate communicative act generation and interpretation, as well as facilitate social relationship (confidence, rapport, etc.), [25]."}
{"pdf_id": "0806.0784", "content": "To sum up, this model is suitable for enhancing communicative act generation and interpretation. It allows reusing results of preceding successful interactions for the treatment of following communicative acts. Such results are part of the common ground among dialog partners, i.e. co-construction of \"interactive\" tools during interaction. IAM is viewed here as a complementary model of Clark's work. That is, each model provides an alternative strategy which can be used to generate or interpret a particular communicative act. In addition, negotiation interpretation, as described in Clark's model, manages non-understandings."}
{"pdf_id": "0806.0784", "content": "supposed to be rational while interacting. Their rationality is partly defined by their sincerity, i.e. they have to use (mutually) true statements in order to be understood. This sincerity hypothesis highly limits the set of possible strategies for communicative acts generation and interpretation. Thus, selfish or self-deceptive attitudes are considered as being irrational, automatic processes such as priming are not allowed, etc. In preceding works, the incoherence of the systematic use of the sincerity hypothesis has been demonstrated [1], [26]. In fact, interaction is a goal-oriented process which aims here at transmitting informations and control orders. A particular communicative act aims at contributing to:"}
{"pdf_id": "0806.0784", "content": "The problem with the sincerity hypothesis is not that true statement can not enable to reach these goals. The problem is that there is a confusion between what is the aim of the interaction and what is the suitable strategy to use. Distinguishing these two aspects avoid to impose a particular and single strategy."}
{"pdf_id": "0806.0784", "content": "In order to introduce the distinction in a collaborative model of interaction, the philosophical notion of acceptance is used [1], [26]. Thus, the suitable type of interaction model is cognitive model. Acceptance is the contextual mental attitude underlying a goal-oriented activity, whereas belief is the contextual mental attitude underlying a truth-oriented activity [26]."}
{"pdf_id": "0806.0784", "content": "This is a social law, closed to the notion of negotiation protocol, which models interpretation negotiation handling non-understanding. Based on H.H. Clark's work, this sociallaw provides different ways of reacting following a non understanding. Thus, the model of interaction presented hereprovide multi-strategy approach for communicative act's generation and interpretation, as well as for interaction manage ment."}
{"pdf_id": "0806.0784", "content": "CONCLUSION Interface of the next generation of UV Systems must support multi-strategy approach of communicative act generation and interpretation. Moreover, the interface has to take part to the interaction management through non-understanding handling in particular. Our goal is to provide a suitable theoretical framework for future interaction managers. We present a collaborative model of interaction mixing and enhancing the two main psychological collaborative of interaction."}
{"pdf_id": "0806.0870", "content": "We now pass to the theoretical study of the existence of solutions for the initial value problem (IVP) and boundary value problem (BVP) for measure metamorphosis (with uniqueness in the IVP case). The next two sections are notably more technical than the rest of this paper. They are well isolated from it, however, and it is possible, if desired, to skip directly to section 10."}
{"pdf_id": "0806.0870", "content": "9.1. Remark. Equations (19) have been obtained from general formulae that were derived under the assumption that G is a Lie group, (which isnot the case here). It is important to rigorously recompute the Euler equa tion to reconnect the IVP and the BVP. The variation with respect to u is straightforward and provides the first equation in (19)."}
{"pdf_id": "0806.0870", "content": "The previous theorems provide a rigorous foundation for the consideredmeasure matching approach. However, an important issue needs to be ad dressed. The space N, which has been introduced in order to take advantage of its Hilbert structure, is a very big space that contains distributions that are more singular than measures. Now, when matching two measures n0 and n1, the question naturally arises of whether the optimal evolution, i.e., the measure nt, can turn up being more singular than measures, since the existence theorem only ensures that it belongs to N. The second equation in (19) indicates that this should not be the case, since it says that"}
{"pdf_id": "0806.1246", "content": "Due to the development and dissemination of Information and  Communication Technology (ICT), there are greater opportunities to  publish and access research results and intellectual production at  university institutions. The academic use of these technologies, and in  particular Institutional Repositories (IIRR), is essential to reach goals and  milestones related to the preservation and publication of scientific and"}
{"pdf_id": "0806.1246", "content": "One of the main ideas behind these initiatives is that free  and open access to knowledge generates in turn more knowledge and  benefits for humanity; any kind of control or restrictions on this  knowledge would be an obstacle for the advancement of the sciences  (Guedon, 2002)"}
{"pdf_id": "0806.1246", "content": "According the  digital encyclopedia Wikipedia74, digital preservation can be considered as  the group of processes and activities that ensure the continuous long-term  access to existing information and scientific registries and to cultural  heritage in electronic formats  It could be said that thanks to digital technologies the preservation of  knowledge is an easier process, but it is not so"}
{"pdf_id": "0806.1246", "content": "necessary tools, or a responsible member can be trained in each  community (research unit, department, etc.) to be in charge of adding the  content they generate to the IR. This will depend on the publishing model  chosen for the IR and its services. Personnel whot will add metadata to  the contents and offer service support must also be trained, as well as the  organizing managers and technicians involved. It is important to update  the IR personnel with emerging technologies, new platforms and  programming languages, which will be a good investment at the time  when changes are made to the technological systems that support the  repository."}
{"pdf_id": "0806.1246", "content": "Once the IR is built, it is then critical to communicate the benefits that  it offers to the university community (Barton and Waters, 2004). This can  be achieved in two ways, from top to bottom or bottom to top. The first  implies forming leaders and institution authorities, deans, etc; developing  pilot communities for demonstration purposes before the rest of the  institution. The second means informing the content producers  (researchers and research groups, professors, technical and administrative  personnel, librarians, etc) through direct presentations to the members of  the university community, promotion through institutional and local press,  brochures and posters, and using publicity mediums inside and outside the  university."}
{"pdf_id": "0806.1246", "content": "The development of the SABER-ULA IR (2000-2006) as a  preservation and dissemination tool for the intellectual production of the  members of the university community at the University of Los Andes85,  has occurred in three well-defined phases, each one lasting two years, of  infrastructure building, consolidation of service and acknowledgment on  behalf of the users."}
{"pdf_id": "0806.1246", "content": "Between 2004 and 2006, a regular volume was in the processing of  content (journal articles, pre-prints, event references, etc.). During the  first trimester of this year an average of 500 registries a month were  processed. The number of electronic journals reaches 40 and eight  thousand registries were published in the IR. The users began to  recognize the value of the information held by the IR. Historians from the  institution requested use of the registry to build a memory of the events"}
{"pdf_id": "0806.1246", "content": "that took place in the University.  The ULA reached important visibility of its contents on the Internet  thanks to the quantity and quality of the IR87; however, there was still not  a full institutional recognition that could lead to full financing for  supporting services. At the end of the first trimester of 2006, the ULA  officially declared its commitment to adhere and sign the Berlin  Declaration, which meant a great step forward in the understanding of the  importance of the ideas held by the movement and the initiatives for open  access to information (OAI), in which IIRR play an important role."}
{"pdf_id": "0806.1246", "content": "Since its creation in the year 2000 until March 2006, more than 8  million of searches on documents and information registries have carried out in the IR of the ULA, SABER-ULA. In the last two years (2005 March 2006), as can be seen in the following chart (Figure 1), the increase  in the amount of queries has been notable: only in the first three months  of the year 2006 the number was above the total for the whole year 2004."}
{"pdf_id": "0806.1246", "content": "The next figure (Figure 2) represents how the content of the repository  has increased substantially year to year since it began offering services.  This is a sign of the appropriation and acceptance that the electronic  publishing services have had, mainly among the journal editors of the  institution. This coincides with the international tendencies reported by  Swan and Sheridan (2005). In their annual study on the adoption of Open  Access they point out that auto-archiving the use of institutional  repositories has increased 60% between 2004 and 2005."}
{"pdf_id": "0806.1246", "content": "Figure 2: Number of information registries in the Institutional Repository  SABER-ULA (up to March 31, 2006)  Around 50% of the IR of the ULA follows the \"golden path\" (Suber,  2005) established in the open access initiatives and the Berlin Declaration;  wich means that this important percentage of the IR contents come from  electronic university journals."}
{"pdf_id": "0806.1246", "content": "According to Peset et al (Peset, F. et al., 2005), the changes that  Internet has brought to the communication model reside in the possibility  of offering visibility to the scientific production of an institution or a  country in ways that were unthought of until recently. The IIRR are one  of the main tools to facilitate that change and their appropriation, on  behalf of the communities of authors and users of the information, is  generating an interesting dynamic of creation, preservation and use of"}
{"pdf_id": "0806.1246", "content": "After six years of development at the IR SABER ULA, today we can say that there is an acknowledgment and institutional recognition of free access electronic publishing, and that the adoption of ICT has created a  demand for new services and requests for improvements of the tools  related to electronic publishing"}
{"pdf_id": "0806.1246", "content": "However, although the perceived resistance to the dissemination of the  produced information has decreased, there are still some obstacles, among  which we can name the following:  •  The lack of incentives for electronic publishing, which  makes it difficult to incorporate authors and communities as  collaborators and receptors of the services offered by the  repository"}
{"pdf_id": "0806.1246", "content": "From the  beginning, the work team of the repository has constantly  contributed to the recovery of valuable digital archives with  valuable content to which the author originally did not give  the importance to preserve, as the content had already been  published on paper (in a journal, a book, etc)"}
{"pdf_id": "0806.1246", "content": "Although we have no way to measure this in  quantity, we perceive that this situation has decreased  progressively at the same time that formal and informal  training is offered to the content creators and those involved  in the use of tools and digitalization techniques, file formats,  creation of digital content, etc"}
{"pdf_id": "0806.1246", "content": "Although some researchers say they have  reservations and distrust for the contents available on the  Internet, and thus, don't have an interest in publishing under  this modality; they also express fear that their work may be  plagiarized or used without the credit for the original source"}
{"pdf_id": "0806.1246", "content": "There is also work being done, along with the responsible authorities and  dependencies, to create and adopt formal policies within the University to  promote, or make compulsory, the free dissemination of intellectual  production of the institution through IIRR; as many institutions around the  world are doing in order to comply with the recommendations from the  Berlin Declaration; this will help, in the near future, to overcome some of  the obstacles mentioned previously"}
{"pdf_id": "0806.1246", "content": "along Latin America will increase the impact of the content produced in  the region and will give it a visibility and use until recently difficult to  envision. We are working on proposals for the development of this kind  of initiatives in other institutions in Venezuela and Latin America."}
{"pdf_id": "0806.1246", "content": "Steenbakkers, J.(2003). \"Permanent Archiving of Electronic Publications:  Research & Practice1\". International Summer School on the Digital  Library 2003. Retrieved 15 Feb, 2006, from  http://www.kb.nl/hrd/dd/dd_links_en_publicaties/publicaties/summers choolticer2003.pdf  Suber, P. (2006). \"Open Access Overview\". Retrieved 15 Feb, 2006, from  http://www.earlham.edu/~peters/fos/overview.htm  Swan, A, Brown, S. (2005). \"Open access self-archiving: An author  study.\" Retrieved 15 Jan 2006, from  http://cogprints.org/4385/01/jisc2.pdf"}
{"pdf_id": "0806.1280", "content": "Robot ontology for urban search and rescue: Schlenoff [13] has developed robot ontology to capture relevant  information about robots and their capabilities to assist in the development and testing of effective  technologies for sensing, navigation, planning, integration, and human operator interaction within search and  rescue robot systems"}
{"pdf_id": "0806.1280", "content": "Captured information recognized in three categories: structural characteristics (such as  size, weight, power source, locomotion mechanism, sensors and processors), functional capabilities (such as  weather resistance, degree of autonomy, capabilities of locomotion, sensors and operations, and  communications), and operational considerations (such as human operator training and education)"}
{"pdf_id": "0806.1280", "content": "For example, if an emergency officer needed enough tents and food for 3400 people, deliverable in one day,  first by air to the local city, then by road to the crisis area accompanied by fifteen distribution experts, the parts of this  request would need at present to be broken into separate items"}
{"pdf_id": "0806.1316", "content": "propositions/hypotheses in the light of new evidence lies at the heart of  Bayesian inference. The basic natural assumption, as summarized in van  Fraassen's Reflection Principle ([1984]), would be that in the absence of  new evidence the belief should not change. Yet, there are examples that are  claimed to violate this assumption. The apparent paradox presented by such  examples, if not settled, would demonstrate the inconsistency and/or  incompleteness of the Bayesian approach and without eliminating this  inconsistency, the approach cannot be regarded as scientific."}
{"pdf_id": "0806.1316", "content": "attempts to solve the problem fall into three categories. The first two share  the view that new evidence is absent, but differ about the conclusion of  whether Sleeping Beauty should change her belief or not, and why. The third  category is characterized by the view that, after all, new evidence (although  hidden from the initial view) is involved."}
{"pdf_id": "0806.1316", "content": "2 Strictly speaking, White does not explicitly states that he is a \"halfer\". He proposes a generalized version of the problem, which apparently poses a challenge for \"thirders\", in particular Elga-Dorr Arntzenius arguments, but which does not pose any problems for \"halfers\". Though, Horgan ([2007])  denies that White's argument poses any problem for his approach.  3 Dorr's argument was disputed by Bradley ([2003]). 4 In his earlier article Arntzenius ([2002]) maintained a view that upon awakening SB should not have a  definite belief at all due to her cognitive malfunction."}
{"pdf_id": "0806.1316", "content": "5 Note that including a setup in the definition of an event is different from the conditioning of credence  of the event on evidence. SB does not receive any new evidence upon wakening, yet the credences are not  the same, because the setups, and therefore the events, are different."}
{"pdf_id": "0806.1316", "content": "green ball is picked out from the box' are two different events, and therefore their  probabilities are not necessarily equal. These two events are different because they are  the subject to different experimental setups: one is the coin tossing, other is picking up a  ball at random from the full box7. The probability to put a green ball in the box on each"}
{"pdf_id": "0806.1316", "content": "6 Note that here as well as in the original statement of the paradox, as formulated by Elga ([2000]), the  frequentist definition of probability is used in (b). In subsequent discussions, though, Elga ([2000]) and  other authors based their arguments mainly on the application of the principle of indifference and on Bas  van Fraassen's reflection principle, rather than on frequentist definition of probability. In this article I use  the frequentist definition simply because it does the job perfectly. Moreover, the way I dissolve the  problem implies that application of Bayesian methods will not lead to any contradictions as well."}
{"pdf_id": "0806.1316", "content": "I would like to thank James Ladyman for very helpful and encouraging comments and  support, Jeremy Butterfield for his valuable remark and useful corrections, and  anonymous referee for her/his positive feedback and corrections. I am grateful to Lev  Vaidman, who first pointed my attention to the Sleeping Beauty Problem, for  constructive discussions."}
{"pdf_id": "0806.1446", "content": "We investigate a biologically motivated approach to fast visual classification, directly inspired by the recent work [20]. Specifically, trading-off biological accuracy for computational efficiency, we explore using wavelet andgrouplet-like transforms to parallel the tuning of visual cor tex V1 and V2 cells, alternated with max operations to achieve scale and translation invariance.A feature se lection procedure is applied during learning to accelerate recognition. We introduce a simple attention-like feedbackmechanism, significantly improving recognition and robust ness in multiple-object scenes.In experiments, the proposed algorithm achieves or exceeds state-of-the-art suc cess rate on object recognition, texture and satellite imageclassification, language identification and sound classifica tion."}
{"pdf_id": "0806.1446", "content": "As in [20], the algorithm is hierarchical. In addition, motivated in part by the relative uniformity of cortical anatomy [14, 21], the two layers of the hierarchy are made to be computationally similar, as shown in Fig. 1. Layer one performs a wavelet transform [13] in the S1 unit followed by a local maximum operation in the C1 unit. The transform in the S2 unit in layer two is similar to the grouplet transform [12], and is followed by a global maximum operation in the C2 unit."}
{"pdf_id": "0806.1446", "content": "Object identification While one could recalculate the features of the attended object cropped out from the whole image, i.e., concentrate all the visual cortex resource on a sin gle object, a faster procedure identifies the attended object, say object A, using directly the lower-dimensional feature vector C2A, composed of the C2 coefficients corresponding to A already calculated in the feedforward pathway. This can be implemented by reclassifying C2A using subsets of the C2 coefficients of the training images extracted at the same coordinates of C2A, as shown in Fig. 3-c. Discarding the coordinates that are located on the irrelevant object Bin the test image disambiguates the classification and im proves the recognition of the object A."}
{"pdf_id": "0806.1446", "content": "Figure 3. Feedback in a two-object scene.a. Posi tions of C2 coefficients are marked by crosses. b. C2 coefficients are clustered (represented by circles vs crosses). c. Feature coefficients of the training imagesare grouped, the coordinates being in line with the clus tering of the coefficients of the test image. Rectangles and ellipses represent the two groups."}
{"pdf_id": "0806.1446", "content": "For the object recognition experiments we used 4 data sets that are airplanes, motorcycles, cars (rear) and leaves, plus a background class from the Caltech5 database2, some sample images being shown in Fig. 4. The images are turned to gray-level and rescaled in preserving the aspect ratio so that the minimum side length is of 140 pixels. A set of 50 positive images and 50 negative images were used for training and another set for test."}
{"pdf_id": "0806.1446", "content": "Table 1 summarizes the object recognition. The performance measure reported is the ROC accuracy.3 Results ob tained with the proposed algorithm are superior to previous approaches [2, 24] and comparable to [20] but at a lower computational cost (in Matlab code about 6 times faster with feature selection). Fig. 5-d shows that the performance is improved when the number of C2 features increases and is in general stable with 200 features."}
{"pdf_id": "0806.1446", "content": "Figs. 5-a,b,c and Fig. 6 show respectively 3 pairs of tex tures that were used for binary classification and a group of 10 textures that were used for multiple-class (10-class)classification, all from the Brodatz database4. As summa rized in Table 2, the proposed algorithm achieved perfectresults for binary classification and for the challenging mul tiple class classification its performance was comparable to the state-of-the-art methods [8, 6, 17]. Indeed the randompatch extraction applied in the algorithm is ideal for classi fying stationary patterns such as textures. Fig. 5 shows that stable performance is achieved with as few as 40 features, which confirms the good texture classification results and the robustness of the algorithm."}
{"pdf_id": "0806.1446", "content": "Classifying the whole Brodatz database (111 textures) is a more challenging task. Combining C2 coefficients with the histogram of the wavelet approximation coefficients as features, the proposed algorithm achieved 87.8% accuracy for the 111-texture classification, comparable to the 88.2% accuracy rate reported in [7] obtained with a state-of-the-art texture classification approach."}
{"pdf_id": "0806.1446", "content": "Language identification aims to determine the under lying language of a document in an imaged format, andis often carried out as a preprocessing of optical charac ter recognition (OCR). Based on principles totally different from traditional approaches [10], the proposed algorithm achieved 100% success rate in a 8-language identification task, as shown in Fig 8."}
{"pdf_id": "0806.1446", "content": "The main idea is to directly extend the above algorithmto sound applications is to view time-frequency representa tions of sound as textures. Preliminary experiments suggest this may be a fruitful direction of research. Fig. 9 illustrates 5 types of sounds and samples of their log-spectrograms. 2 minutes excerpts of each sound were collected. The spectrograms were segmented (in time) into segments of 5 seconds. Half were used for training andthe rest for test. A direct application of the proposed algo rithm using the spectrograms as the visual patterns resulted in 100% accuracy in the 5-sound classification."}
{"pdf_id": "0806.1446", "content": "Recognition performance tends to degrade when multi ple stimuli are presented in the receptive field. Fig. 10-a shows an example of a multiple-object scene in which onesearched an object, say an airplane, through a binary classification against a background image. Due to the perturbation from the coexisting stimuli, the feedforward recog nition accuracy is as low as 74%. The feedback procedureintroduced in Subsection 2.3 improves considerably the ac curacy to 98% by focusing attention on each object in turn."}
{"pdf_id": "0806.1640", "content": "The repartition of the connict is important because of the non-idempotency of the rules (except the rule of [17] that can be applied when the dependency between experts is high) and due to the responses of the experts that can be connicting. Hence, we have define the auto-connict [21] in order to quantify the intrinsic connict of a mass and the distribution of the connict according to the number of experts."}
{"pdf_id": "0806.1640", "content": "weights w(X). We have proposed also a parametrized PCR to decrease or increase the innuence of many small values toward one large one. The first way is given by PCR6f, applying a functionon each belief value implied in the partial connict. Any non decreasing positive function f defined on ]0, 1] can be used."}
{"pdf_id": "0806.1640", "content": "IV. DISCUSSION: TOWARD A MORE GENERAL RULE The rules presented in the previous section, propose a repartition of the masses giving a partial connict only (when at most two experts are in discord) and do not take heed of the level of imprecision of the responses of the experts (the nonspecificity of the responses). The imprecision of the responses of each expert is only considered by the mixed and MDPCR rules when there is no connict between the experts. In the mixed rule, if the intersection of the responses of the experts is empty, the best way is not necessarily to transfer the"}
{"pdf_id": "0806.1640", "content": "Formula (32), like most of the formula of this article, seems simpler when expressed through an algorithm instead of a direct expression of m(X). We list all the M-uples of focal elements of the M belief functions. An input belief function e is an association of a list of focal elements and their masses. We write size(e) the number of its focal elements. The focal classes are e[1], e[2], ..., e[size(e)]. The mass associated to a class c is e(c), written with parenthesis. The cardinality of a focal element e[i] is also written size(e[i])."}
{"pdf_id": "0806.1640", "content": "[1] L. Xu, A. Krzyzak and C.Y. Suen, \"Methods of Combining Multiple Classifiers and Their Application to Handwriting Recognition,\" IEEE Transactions on Systems, Man Cybernetics, vol. 22, no. 3, pp. 418-435, May 1992.[2] L. Lam and C.Y. Suen, \"Application of Majority Voting to Pattern Recog nition: An Analysis of Its Behavior and Performance,\" IEEE Transactions on Systems, Man Cybernetics - Part A: Systems and Humans, vol. 27, no. 5, pp. 553-568, September 1997. [3] L. Zadeh, \"Fuzzy sets as a basis for a theory of possibility,\" Fuzzy Sets and Systems, vol. 1, no. 3, pp. 3-28, 1978."}
{"pdf_id": "0806.1796", "content": "3.2.1. Boundary good detection measureThe well segmented pixel measure is a mea sure of how the boundary is well detected andthe mis-segmented pixel measure tries to quantify how many boundaries detected by the al gorithm to benchmark have no physical reality. First, we search the minimal distance dfe between each boundary pixel f found by the algorithm to"}
{"pdf_id": "0806.1796", "content": "benchmark, and all the boundary pixels e provided by the expert. Hence the pixel e is a func tion of f, and we should note it as ef, but in order to simplify notations, it is referred as e inthe rest of paper. We take here an Euclidean dis tance but any other distance can be envisaged. The certainty weight of the pixel e given by the expert is noted as We. We define a well-detection criteria vector by:"}
{"pdf_id": "0806.1796", "content": "The normalization is made in order to obtain a measure defined between 0 and 1. However, in real applications, this criteria remains small even for very good boundary detection. So we take a = 1/6 in order to accentuate small values.This criteria is not completely satisfying be cause it only takes into account the distance from the found boundary to the contour provided by the expert. However, the reference boundary alsohas a local direction which is another informa tion we want to use. A boundary found by the algorithm can come across a boundary given by the expert orthogonally: in this case some pixels"}
{"pdf_id": "0806.1796", "content": "We present here an illustration of our image classification and segmentation evaluation on real sonar images. Indeed, underwater environmentis a very uncertain environment and it is particularly important to classify seabed for numer ous applications such as Autonomous Underwater Vehicle navigation. In recent sonar works (e.g. [26,27]), the classification evaluation is made only by visual comparison of one original image and the classified image. That is not satisfying in order to correctly evaluate image classification and segmentation. First we present our database"}
{"pdf_id": "0806.1796", "content": "The discrete translation invariant wavelet transform is based on the choice of the optimal translation for each decomposition level. Each decomposition level d gives four new images. We choose here a decomposition level d = 2. For each image Ii d (the ith image of the decomposition d) we calculate three features. The energy is given by:"}
{"pdf_id": "0806.1796", "content": "Consequently we obtain 15 features (3+4*3). The chosen classifier is based on a SupportVector Machine. The algorithm used here is described in [28]. It is a one-vs-one multi-class approach, and we take a linear kernel with a con stant C = 1.We have considered only three classes for learn ing and tests:"}
{"pdf_id": "0806.1796", "content": "4.3. Evaluation Figure 5 shows the result of the classification of the same image than the one given in the figure 1. Sand (in red) and rock (in blue) are quite well classified but ripple (in yellow) is not well segmented. The dark blue corresponds to that part of the image that was not considered for the classification."}
{"pdf_id": "0806.1796", "content": "Just by looking this figure 5 we cannot say whether the classification is good or not, and any decision stays very subjective. Moreover, theclassification algorithm could be good for this im age and not for others. So we propose to use our measures. The used weights here for the certitude are respectively 2/3 for sure, 1/2 for moderately sure and 1/3 for not sure. But other weights can be preferred according to the application. The normalized confusion matrix obtained for one randomly partition of the database is given by:"}
{"pdf_id": "0806.1796", "content": "The last line means that there is shadow or other parts classified in class 1, 2 or 3. We can note that a high proportion of the rock or cobble (class 1) is classified as sand or silt (class 3), and most of theripple (class 2) also. Sand and silt, the most com mon kinds of sediments on our images, are very"}
{"pdf_id": "0806.1796", "content": "1.Y.J. Zhang, A survey on evaluation methods for images segmentation, Pattern Recog nition, Vol. 29, No. 8 (1996), 1335-1346. 2. A. Martin, Comparative study of informationfusion methods for sonar images classifica tion, The Eighth International Conference on Information Fusion, Philadelphia, USA, 25-29 July 2005. 3. J.C. Russ, The Image Processing Handbook, CRC Press, 2002. 4. H. Laanaya, A. Martin, D. Aboutajdine, and"}
{"pdf_id": "0806.1796", "content": "cessing, Vol. 4, N 21 (1995), 1667-1673. 25. C. Xu and J.L. Prince, Snakes, Shapes, and Gradient Vector Flow, IEEE Transactions onImage Processing, Vol. 7, Issue 3 (1998), 359 369. 26. G. Le Chenadec, and J.M. Boucher, SonarImage Segmentation using the Angular De pendence of Backscattering Distributions, IEEE OCEANS'05 EUROPE, Brest, France, 20-23 June 2005. 27. M. Lianantonakis, and Y.R. Petillot, Sidescan sonar segmentation using active contours andlevel set methods, IEEE OCEANS'05 EU ROPE, Brest, France, 20-23 June 2005. 28. C.C. Chang, and C.J. Lin,Lib svm: library for supportvec tor machines, Software available at http://www.csie.ntu.edu.tw/cjlin/libsvm, 2001."}
{"pdf_id": "0806.1797", "content": "where CM(X) is the DSm cardinality corresponding to the number of parts of X in the Venn diagram of the problem [5, 15].If the credibility function provides a pessimist decision, the plausibility function is often too optimist. The pignistic probability is often taken as a compro mise. We present the three functions for our models."}
{"pdf_id": "0806.1797", "content": "In order to compare the previous rules in this section, we study the decision on the basic belief assignments obtained by the combination. Hence, we consider here the induced order on the singleton given by the plausibility, credibility, pignistic probability functions, or directly by the masses. Indeed, in order to compare the combination rules, we think that the study on the induced order of these functions is more informative than the obtained masses values. All the combination rules presented here are not idempotent, for instance for the conjunctive non-normalized rule:"}
{"pdf_id": "0806.1798", "content": "Many fusion theories can be used for the experts fusion in image classification such as voting rules [2, 3], possibility theory [4, 5], belief function theory [6, 7]. In our case, experts can express their certitude on their perception. As a result, probabilities theories such as the Bayesian theory or the belief function theory are more adapted. Indeed, the possibility theory is more adapted to imitate the imprecise data whereas probability-based theories is more adapted to imitate the uncertain data. Of course both possibility and probability-based theories can imitate imprecise and uncertain data"}
{"pdf_id": "0806.1798", "content": "In the first section, we discuss and present different belief function models based on the power set and the hyper power set. These models try to answer our problem. We study these models also in the steps of combination and decision of the informationfusion. These models allow, in a second section, to a general discussion on the differ ence between the DSmT and DST in terms of capacity to represent our problem and in terms of decision. Finally, we present an illustration of our proposed experts fusion on real sonar images, which represent a particularly uncertain environment."}
{"pdf_id": "0806.1798", "content": "In this section, we present five models taking into account the possible specificities of the application. First, we recall the principles of the DST and DSmT we apply here. Then we present a numerical example which illustrates the five proposed models presented afterward. The first three models are presented in the context of the DST, the fourth model in the context of the DSmT, and the fifth model in both contexts."}
{"pdf_id": "0806.1798", "content": "where CM(X) is the DSm cardinality corresponding to the number of parts of X in the Venn diagram of the problem [15, 8]. If the credibility function provides a pessimist decision, the plausibility function is often too optimist. The pignistic probability is often taken as a compromise. We present the three functions for our models."}
{"pdf_id": "0806.1798", "content": "Consider two experts providing their opinion on the tile X. The first expert says that on tile X there is some rock A with a certainty equal to 0.6. Hence for this first expert we have : pA = 1, pB = 0, and cA = 0.6. The second expert thinks that there are 50% of rock and 50% of sand on the considered tile X with a respective certainty of 0.6 and 0.4. Hence for the second expert we have: pA = 0.5, pB = 0.5, cA = 0.6 and cB = 0.4. We illustrate all our proposed models with this numerical exemple."}
{"pdf_id": "0806.1798", "content": "If for one expert, the tile contains only A, pA = 1, and m(B) = 0. If for another expert, the tile contains A and B, we take into account the certainty and proportion of the two sediments but not only on one focal element. Consequently, we have simply:"}
{"pdf_id": "0806.1798", "content": "Take another example with this last model M5: The first expert provides: pA = 0.5, pB = 0.5, cA = 0.6 and cB = 0.4, and the second expert provides: pA = 0.5, pB = 0.5, cA = 0.86 and cB = 1. We want take a decision only on A or B. Hence we have:"}
{"pdf_id": "0806.1798", "content": "Thus, for two classes, the subspace where the decision is \"rock\" by consensus rule is very similar to the subspace where the decision is \"rock\" by the PCR5 rule: only 0.6% of the volume differ. For a higher number of classes, the decision obtained by fusing the two experts' opinions is much less stable:"}
{"pdf_id": "0806.1798", "content": "Our database contains 40 sonar images provided by the GESMA (Groupe d'Etudes Sous-Marines de l'Atlantique). These images were obtained with a Klein 5400 lateral sonar with a resolution of 20 to 30 cm in azimuth and 3 cm in range. The sea-bottom depth was between 15 m and 40 m. Two experts have manually segmented these images giving the kind of sediment (rock, cobble, sand, silt, ripple (horizontal, vertical or at 45 degrees)), shadow or other(typically ships) parts on images, helped by the manual segmentation interface pre sented in figure 4. All sediments are given with a certainty level (sure, moderately sure or not sure). Hence, every pixel of every image is labeled as being either a certain type of sediment or a shadow or other."}
{"pdf_id": "0806.1798", "content": "Hence, our application does not present a large connict.We have applied the consensus rule and the PCR5 rule with this model. The de cision is given by the maximum of pignistic probability. In most of the cases the decisions taken by the two rules are the same. We note a difference only on 0.4657% of the tiles. Indeed, we are in the seven classes case with only 0.1209 of connict, the simulation given on the figure 3 show that we have few chance that the decisions differ."}
{"pdf_id": "0806.1806", "content": "the massive use of views in Gecode, it is vital to develop a model that allows us to prove that derived propagators have the desired properties. In this paper, we argue that propagators that are derived using variable views are indeed perfect: they are not only perfect for performance, we prove that they inherit all essential properties such as correctness and completeness from their original propagator. Last but not least, we show common techniques for deriving propagators with views and demonstrate their wide applicability. In Gecode, every propagator implementation is reused 3.6 times on average. Without views, Gecode would feature 140 000 rather than 40 000 lines of propagator implementation to be written, tested, and maintained."}
{"pdf_id": "0806.1806", "content": "Overview. The next section introduces the basic notions we will use. Sect. 3 presents views and derived propagators and proves fundamental properties like correctness and completeness. The following three sections develop techniques forderiving propagators: transformation, generalization, specialization, and channeling. Sect. 7 presents extensions of the model, and Sect. 8 discusses its limita tions. Sect. 9 provides empirical evidence that views are useful in practice."}
{"pdf_id": "0806.1806", "content": "Indexicals. Views that perform arithmetic transformations are related to in dexicals [3, 13]. An indexical is a propagator that prunes a single variable and is defined in terms of range expressions. A view is similar to an indexical with a single input variable. However, views are not used to build propagators directly, but to derive new propagators from existing ones. Allowing the full expressivity of indexicals for views would imply giving up our completeness results. Another related concept are arithmetic expressions, which can be used for modeling in many systems (such as ILOG Solver [10]). In contrast to views, these expressions are not used for propagation directly and, like indexicals, yield no completeness guarantees."}
{"pdf_id": "0806.1806", "content": "Beyond injective views. Views as defined in this paper are required to be injective. This excludes some interesting views, such as a view for the absolute value of a variable, or a view of a variable modulo some constant. None of the basic proofs makes use of injectivity, so non-injective views can be used to derive (bounds) complete, correct propagators. However, event handling changes when views are not injective:"}
{"pdf_id": "0806.1806", "content": "Applicability. The Gecode C++ library [5] makes heavy use of views. Table 2shows the number of generic propagators implemented in Gecode, and the num ber of derived instances. On average, every generic propagator results in 3.59 propagator instances. Propagators in Gecode account for more than 40 000 lines of code and documentation. As a rough estimate, generic propagators with views save around 100 000 lines of code and documentation to be written, tested, and maintained. On the other hand, the views are implemented in less than 8 000 lines of code, yielding a 1250% return on investment."}
{"pdf_id": "0806.1984", "content": "Invariants with respect to (6) may be obtained from invariants with respect to (8) by makingsubstitution (7).1 Invariants with respect to a very general class of actions of continuous finite dimensional groups on manifolds can be computed using Fels-Olver generalization [7] of Cartan'smoving frame method (see also its algebraic reformulation [14]). The method consists of choos ing a cross-section to the orbits and finding the coordinates of the projection along the orbits"}
{"pdf_id": "0806.1984", "content": "The first invariant J1 may be viewed as an extension of the 2D invariant I1 to 3D. Indeed, n1, n2, and n3 represent exactly the same area as the 2D invariant I1(in Figure1) in three coordinate planes. They are extended from 2D area to 3D volume by multiplying by X, Z, and Y respectively. For example, n1X is the volume C under surface F in Figure 2, and n2Z and n3Y are similar volumes obtained by relabelling of X, Y , Z axis. Therefore, the invariant J1 is the summation of two volumes n1X and n2Z minus the volume n3Y . The geometric interpretation of the invariants J2 and J3, however, remains at the present time unclear to us."}
{"pdf_id": "0806.1984", "content": "A global integral signature of a curve is the variation of one independent integral invariant, evaluated on the curve, relative to another. If a curve is mapped to another curve by a group transformation, their signatures coincide independently of the selected parametrization. The global signature, however, does depend on a choice of the initial point."}
{"pdf_id": "0806.2007", "content": "Abstract— The sonar images provide a rapid view of the seabed in order to characterize it. However, in such as uncertain environment, real seabed is unknown and the only information we can obtain, is the interpretation of different human experts, sometimes in connict. In this paper, we propose to manage this connict in order to provide a robust reality for the learning step of classification algorithms. The classification is conducted by a multilayer perceptron, taking into account the uncertainty of the reality in the learning stage. The results of this seabed characterization are presented on real sonar images."}
{"pdf_id": "0806.2007", "content": "sonar image classification methods are usually supervised [2], [3], [1] and can be described into three steps. First, significant features are extracted from these tiles. Generally, a second step in necessary in order to reduce these features, because they are too numerous. In the third step, these features feed classification algorithms. The particularity in considering small tiles in image classification is that sometimes, two or more classes can co-exist on a tile. How to take into account the tiles with more than one sediment?"}
{"pdf_id": "0806.2007", "content": "Many fusion theories can be used for the experts fusion in image classification such as voting rules [4], [5], possibility theory [6], [7], belief function theory [8], [9], [10], [11]. In our case, experts can express their certitude on their perception. As a result, probabilities theories such as the Bayesian theory or the belief function theory are more adapted. Indeed, the possibility theory is more adapted to modelize the imprecise data whereas probability-based theories is more adapted to"}
{"pdf_id": "0806.2007", "content": "consensus rule given by the equation (5). This rule allows a proportional connict redistribution on the subsets from where the connict comes and is equivalent for two experts to the rule given in [16]. This rule will be illustrated on simple examples in the next section. These rules are compared in [17]."}
{"pdf_id": "0806.2007", "content": "For instance, consider two experts providing their opinion on the tile X. The first expert says that on tile X there is some rock A with a certainty equal to 0.6. Hence for this first expert we have : pA = 1, pB = 0, and cA = 0.6. The second expert thinks that there are 50% of rock and 50% of sand on the considered tile X with a respective certainty of 0.6 and 0.4. Hence for the second expert we have: pA = 0.5, pB = 0.5, cA = 0.6 and cB = 0.4. We illustrate all our proposed models with this numerical exemple. Consequently, we have simply:"}
{"pdf_id": "0806.2007", "content": "With the PCR rule, the decision will be also A. Of course, we cannot say on this example which rule is the best, and we can apply these two rules in order to construct a reality taking into account the doubts of different experts. This reality can serve to train a classifier and also to evaluate this classifier. We can use many supervised classifiers. In the next section, we propose to introduced a new classifier: a multilayer perceptron based on belief learning, take into account all the reachness of the belief basic assignment."}
{"pdf_id": "0806.2007", "content": "We propose in this section a new belief multilayer percep tron where the difference between the multilayer perceptron relates to the learning based on a belief learning. In [19], a neural network classifier based on Dempster-Shafer theory is presented. In this work, the neural network consider the bba at each neuron, that is not the case in our approach presented feedforward."}
{"pdf_id": "0806.2007", "content": "The neural network classifiers are today the most used supervised classifiers. The multilayer perceptron (MLP) is a feedforward fully connected neural network. The tile X is described by n features (x1, ..., xn). Each unit of the network is an artificial neuron called perceptron, with the structure given in figure 2. All the neuron outputs of every layer are connected to all the neuron inputs of the next layer weighted by values we have to learn. These weights are first initialized with small random values. In order to learn these values we present to the network the learning vectors and the corresponding desired outputs. The objective of the learning process is to minimize the quadratic error:"}
{"pdf_id": "0806.2007", "content": "Usually the decision is taken considering the maximum of the values on the output layer. These values are between 0 and 1, but the sum is not 1. We can easily normalize them in order to interpret these values as belief basic assignment. For instance the normalization can be made dividing by the sum of the values of the output layer. Hence, the decision can be conducted by the maximum of the pignistic probability, or with other function such as the credibility or the plausibility. Note that if the output layer is composed only with the singletons, to consider the maximum of the values or the maximum of the pignistic probability is the same."}
{"pdf_id": "0806.2007", "content": "In order to obtain a kind of reality for learning task, we first fuse the opinion of the three experts following the presented model. We note A for rock, B for sand, C for cobble, D for silt, E for ripple, F for shadow and G for other, hence we"}
{"pdf_id": "0806.2007", "content": "[1] A. Martin, Comparative study of information fusion methods for sonar images classification, The Eighth International Conference on Information Fusion, Philadelphia, USA, 25-29 July 2005. [2] G. Le Chenadec, and J.M. Boucher, Sonar Image Segmentation using the Angular Dependence of Backscattering Distributions, IEEE Oceans'05 Europe, Brest, France, 20-23 June 2005. [3] M. Lianantonakis, and Y.R. Petillot, Sidescan sonar segmentation using active contours and level set methods, IEEE Oceans'05 Europe, Brest, France, 20-23 June 2005."}
{"pdf_id": "0806.2008", "content": "Seabed characterization serves many useful purposes, e.g. help the navigation of Autonomous Underwater Vehicles or provide data to sedimentologists. In such sonar applications, seabed images are obtained with many imperfections [4]. Indeed, in order to build images, a huge number of physical data (geometry of the device, coordinates of the ship, movements of the sonar, etc.) has to be taken into account, but these data are polluted with a large amount of noises caused by instrumentations. In addition, there are some interferences due to the signal traveling on multiple paths (renection on the bottom or surface), due to speckle, and due to fauna and nora. Therefore, sonar images have a lot of"}
{"pdf_id": "0806.2008", "content": "A and B are exclusive and with the second they are not exclusive. We only study the first case: A and B are exclusive. But on the tile X, the expert can also provide A and B, in this case the two propositions \"the expert believes A\" and \"the expert believes A and B\" are not exclusive."}
{"pdf_id": "0806.2008", "content": "We have proposed five models and studied these models for the fusion of two experts [6]. We present here the three last models for two experts and two classes. In this case the conjunctive rule (1), the mixed rule (2) and the DSmH (3) are similar. We give the obtained results on a real database for the fusion of three experts in sonar."}
{"pdf_id": "0806.2008", "content": "If for one expert, the tile contains only A, pA = 1, and m(B) = 0. If for anotherexpert, the tile contains A and B, we take into account the certainty and pro portion of the two sediments but not only on one focal element. Consequently, we have simply:"}
{"pdf_id": "0806.2008", "content": "with a Klein 5400 lateral sonar with a resolution of 20 to 30 cm in azimuth and 3 cm in range. The sea-bottom depth was between 15 m and 40 m. Three experts have manually segmented these images giving the kind of sediment (rock, cobble, sand, silt, ripple (horizontal, vertical or at 45 degrees)),shadow or other (typically ships) parts on images, helped by the manual segmen tation interface presented in figure 3. All sediments are given with a certainty level (sure, moderately sure or not sure). Hence, each pixel of every image is labeled as being either a certain type of sediment or a shadow or other."}
{"pdf_id": "0806.2008", "content": "The three classifiers used here are the same than in [7]. The first one is a fuzzy K-nearest neighbor classifier, the second one is a multilayer perceptron (MLP) that is a feed forward fully connected neural network. And the third one is the SART (Supervised ART) classifier [8] that uses the principle of prototype generation like the ART neural network, but unlike this one, the prototypes are generated in a supervised manner."}
{"pdf_id": "0806.2008", "content": "are thus generated corresponding to 150 angular positions, from -50 degrees to 69.50 degrees, with an angular increment of 0.50 degrees. The database is randomly divided in a training set (for the three supervised classifiers) and test set (for the evaluation). When all the range profiles are available, the training set is formed by randomly selecting 2/3 of them, the others being considered as the test set."}
{"pdf_id": "0806.2140", "content": "A serious defect with the Halpern-Pearl (HP) definition of causality is repaired by combining a theory of causality with a theory of defaults. In addition, it is shown that (despite a claim to the contrary) a cause according to the HP conditionneed not be a single conjunct. A definition of causality mo tivated by Wright's NESS test is shown to always hold for asingle conjunct. Moreover, conditions that hold for all the examples considered by HP are given that guarantee that causal ity according to (this version) of the NESS test is equivalent to the HP definition."}
{"pdf_id": "0806.2140", "content": "For example, if someone typically leaves work at 5:30 PM and arrives home at 6, but, due to unusually bad traffic, arrives home at 6:10, the bad traffic is typically viewed as the cause of his being late, not the fact that he left at 5:30 (rather than 5:20)"}
{"pdf_id": "0806.2140", "content": "For exam ple, if we are trying to determine whether a forest fire was caused by lightning or an arsonist, we can take the world to be described by three random variables: FF for forest fire, where FF = 1 if there is a forest fire and FF = 0otherwise; L for lightning, where L = 1 if lightning occurred and L = 0 otherwise; M for match (dropped by ar sonist), where M = 1 if the arsonist drops a lit match, and"}
{"pdf_id": "0806.2140", "content": "If we were to explicitly model the amount of oxygen in the air (which certainly might be relevant if we were analyzing fires on Mount Everest), then FWB would also take values of O as an argument, and the presence of sufficient oxygen might well be a cause of the wood burning, and hence the forest burning"}
{"pdf_id": "0806.2140", "content": "• T for Monday's treatment (1 if Billy was treated Monday; 0 otherwise);• TT for Tuesday's treatment (1 if Billy was treated Tues day; 0 otherwise); and • BMC for Billy's medical condition (0 if Billy is fine both Tuesday morning and Wednesday morning; 1 if Billy is sick Tuesday morning, fine Wednesday morning; 2 if Billy is sick both Tuesday and Wednesday morning; 3 ifBilly is fine Tuesday morning and dead Wednesday morn ing)."}
{"pdf_id": "0806.2140", "content": "Example 4.1: Assassin is in possession of a lethal poi son, but has a last-minute change of heart and refrains from putting it in Victim's coffee. Bodyguard puts antidote in the coffee, which would have neutralized the poison had therebeen any. Victim drinks the coffee and survives. Is Body guard's putting in the antidote a cause of Victim surviving? Most people would say no, but according to the preliminary HP definition, it is. For in the contingency where Assassin"}
{"pdf_id": "0806.2140", "content": "Example 4.2: Assistant Bodyguard puts a harmless antidote in Victim's coffee. Buddy then poisons the coffee, using a type of poison that is normally lethal, but is countered by the antidote. Buddy would not have poisoned the coffee if Assistant had not administered the antidote first. (Buddy and Assistant do not really want to harm Victim. They just want to help Assistant get a promotion by making it look like he foiled an assassination attempt.) Victim drinks the coffee and survives."}
{"pdf_id": "0806.2140", "content": "The NESS test, as stated, seems intuitive and simple.Moreover, it deals well with many examples. However, al though the NESS test looks quite formal, it lacks a definition of what it means for a set S of events to be sufficient for B to occur. As I now show, such a definition is sorely needed."}
{"pdf_id": "0806.2140", "content": "DiscussionIt has long been recognized that normality is a key component of causal reasoning. Here I show how it can be incorpo rated into the HP framework in a straightforward way. The HP approach defines causality relative to a causal model. But we may be interested in whether a causal statement follows from some features of the structural equations and some default statements, without knowing the whole causal model. For example, in a scenario with many variables,it may be infeasible (or there might not be enough information) to provide all the structural equations and a com plete ranking function. This suggests it may be of interest to"}
{"pdf_id": "0806.2216", "content": "background is sketched in section II. Section III reviews  existing solutions, section IV is the system overview and  section V and VI discuss the agents involved in greater  detail. The user interface and details on integration is  discussed in section VII. An evaluation of the system is  given in section VIII and finally a conclusion is given in  section IX."}
{"pdf_id": "0806.2216", "content": "The Multi-Agent solution designed and built for the  recommendation problem has two main agents. The first  agent is the recommendation agent and the second is the  information  retrieval  agent.  The  configuration  and  interactions are shown Figure 1. The User Interface is the  gateway to the system for the user, the Recommending  Agent proposes a personalized list of training modules, and  the Information Retrieval Agent searches a predefined list of  service providers' websites for course information and  updates. The actions of the agents are described Figure 2 in  reference to Figure 1. The proceeding sections describe the  agents in further detail."}
{"pdf_id": "0806.2216", "content": "The recommendation agent is a reactive agent that is  responsible for using course information as well as user  profile information to recommend courses to users using a  ranking method. This is done by first searching the course  database using the user profile and then ranking each course  returned from the search. The recommending agent was built  using the Sphinx [13] search engine and the IBM Agent  Building and Learning Environment (ABLE) [14]."}
{"pdf_id": "0806.2216", "content": "A. User Modelling  To collect useful user information, user modelling had to  be carried out [15]. For this to be done properly information  in the domain of career guidance and counselling needed to  be collected. The users of the system have to be modelled so  as to use them in determining what they would consider as  good courses. Through consultation with the Career  Counselling and Development Unit (CCDU) at the  University of the Witwatersrand the user attributes that  would be most useful were found. When assisting students  with their careers, counsellors at the CCDU look at a number  of attributes. The ones chosen for the recommendation"}
{"pdf_id": "0806.2216", "content": "C. Searching the Database  The course database was populated by finding courses  from the different service providers. These courses then  needed to be efficiently searched. For this a search engine  tool was needed. The Sphinx search engine was used to sift  through the databases given search strings. The search  strings used in the searches were constructed from the  professional interests of the user as well as their discipline.  This customised the information returned to the user to their"}
{"pdf_id": "0806.2216", "content": "likes. The search engine indexes the course database each  time a new course is added and is built in C++ for speed.  Speed is rated at an average of 0.1 sec on 2-4 GB of text  data. Thus searching does not take a lot of time and results  can be processed quickly. After this search the courses are  ranked."}
{"pdf_id": "0806.2216", "content": "D. Ranking of Courses  To recommend the courses to users ranking was used.  This ranking used the course keywords as well as selected  profile information. A classification multilayer perceptron  neural network was used for the ranking. This neural  network configuration is shown in Figure 3."}
{"pdf_id": "0806.2216", "content": "The information retrieval agent performs two important  functions. Firstly it uses data mining to extract courses from  service provider websites and then add or update them on  the course database. Secondly it extracts keywords from the  course description and uses the keywords to classify the  course. This agent is based on a simple premise: give it an  example of what you want it to retrieve, set it free and wait  for it to return the results, and update the course database  with any new information from these results. Its task is  therefore  two-fold:  automated  data  extraction  and  integration, that is, what to do with the data once it is  extracted."}
{"pdf_id": "0806.2216", "content": "where freq(P,D) is the number of times P occurs in D,  size(D) is the number of words in D, df(P) is the number of  documents containing P in the global corpus and N the size  of the global corpus.  In the filtering stage, a Naive Bayesian classifier model,  previously trained on manually indexed course documents,  is then used to determine the probability that each word is an  index term or not using the formula [25]:"}
{"pdf_id": "0806.2216", "content": "similarly for P[no], where Y is the number of positive  instances in the training documents, N is the number of  negative instances (candidate phrases that are  not  keyphrases), t is a feature value derived from Equation 2  above and f is the position of the first occurrence of the term.  The overall probability that a candidate phrase is a  keyphrase is then calculated as:"}
{"pdf_id": "0806.2216", "content": "The top ranked candidates are then selected as the  document keywords (a more detailed explanation of the  algorithm is available in [25]). Once keywords have been  extracted, classification is determined via a database look up  that maps keywords to the engineering disciplines. Upon  completion, the agent then checks to see if the particular  course exists and updates the database if it does not."}
{"pdf_id": "0806.2216", "content": "B. User Interface  The user interface was designed so as to be easy for a user to  use. Ease of use in such systems is paramount so that the  user need only focus on the task at hand and not on learning  how to use the system. Through experience with simple user  interfaces, such as that of Google [28], an intuitive interface  was built. When the user has registered and logged on to the  system he/she is met with their profile information,  recommended courses and the search and navigation bar.  This is illustrated in Figure 4."}
{"pdf_id": "0806.2216", "content": "Evaluation  The initial goals for the project were to build functioning  agents, have a functioning system that could be used for  recommendation, a system that is easy to use by the target  user that is stable and robust, and a system that is scalable  and adaptable"}
{"pdf_id": "0806.2216", "content": "engine. Thus if a different ranking algorithm needed to be  used it can easily be replaced as the framework allows for it.  The rules of communication within the agent are the only  attributes that need to be kept. Thus the system is scalable as  well as being adaptable. E.g. for adaptability, only a change  in the user modelling and the courses/subject matter being  investigated is needed. Thus the built system can be adapted  to problem fields such as job searches, academic advising,  business support systems etc. The system cost is low as all  of the tools are open source or free to use."}
{"pdf_id": "0806.2216", "content": "The Multiagent system approach for solving the training  course recommendation problem is successful in reducing  the information overload while recommending relevant  courses to users. The system achieves high accuracy in  ranking using user information and course information. The  final system is scalable and has possibilities for future  modification and adaptability to other problem domains.  Improvements to the system can be made and the system  forms a good platform for future research into the use of  computational intelligence in recommender systems."}
{"pdf_id": "0806.2216", "content": "The author would like to thank Raj Naran from Wits  CCDU for his input on user modelling. The author would  Leon Viljoen from Hatch South Africa, Peter Harris from  ThyssenKrupp Engineering and all of the engineers that took  part in the online Survey for their assistance."}
{"pdf_id": "0806.2216", "content": "[1] L. Schmidt-Thieme, A. Felfernig and G. Friedrich. \"Guest Editor  Introduction: Recommender Systems\", IEEE Intelligent Systems, pp.  18 -21, 2007.  [2] M. Wooldridge. An Introduction to MultiAgent Systems. John Wiley  and Sons, 2004.  [3] I. Rudowsky. \"Intelligent Agents\". Communications of the  Association for Information Systems, Vol. 14, pp. 275-290, 2004.  [4] T. Marwala, E. Hurwitz. \"Multi-Agent Modeling using intelligent  agents in a game of Lerpa\", eprint arXiv:0706.0280, 2007.  [5] B. van Aardt, T. Marwala. \"A Study in a Hybrid Centralised-Swarm  Agent Community\". IEEE 3rd International Conf. on Computational  Cybernetics, Mauritius, pp. 169 - 174, 2005."}
{"pdf_id": "0806.2356", "content": "1. Introduction  Complex systems are often coincided with uncertainty and order-disorder transitions. Apart  of uncertainty, fluctuations forces due to competition of between constructive particles of system  drive the system towards order and disorder. There are numerous examples which their behaviors  show such anomalies in their evolution, i.e., physical systems, biological, financial systems [1].  In other view, in monitoring of most complex systems, there are some generic challenges for  example sparse essence, conflicts in different levels, inaccuracy and limitation of measurements"}
{"pdf_id": "0806.2356", "content": "Based upon the above, hierarchical nature of complex systems [6], developed (developing)  several branches of natural computing (and related limbs) [7], collaborations [13], conflicts [11],  emotions and other features of real complex systems, we propose a general framework of the  known computing methods in the connected (or complex hybrid) shape, so that the aim is to  inferring of the substantial behaviors of intricate and entangled large societies"}
{"pdf_id": "0806.2356", "content": "Complexity of this system, called MAny Connected Intelligent Particles Systems (MACIPS),  add to reactions of particles against information flow, and can open new horizons in studying of  this big query: is there a unified theory for the ways in which elements of a system(or  aggregation of systems) organize themselves to produce a behavior?[8]"}
{"pdf_id": "0806.2890", "content": "Graphs are commonly used as abstract representations for complex structures, including DNA sequences, documents, text, and images. In particular they are extensively used in the field of computer vision, where many problems can be formulated as an attributed graph matching problem. Here the nodes of the graphs correspond to local features of the image and edges correspond to relational aspects between features (both nodes and edges can be attributed, i.e. they can encode feature vectors). Graph matching then consists of finding a correspondence between nodes of the two graphs such that they 'look most similar' when the vertices are labeled according to such a correspondence. Typically, the problem is mathematically formulated as a quadratic assignment problem, which consists of findingthe assignment that maximizes an objective function en"}
{"pdf_id": "0806.2890", "content": "Note that the number of constraints in (9) is given by the number of possible matching matrices |Y| times the number of training instances N. In graph matching the number of possible matches between two graphs grows factorially with their size. In this case it is infeasible to solve (9) exactly. There is however a way out of this problem by using an optimization technique known as column generation [24].Instead of solving (9) directly, one computes the most vi olated constraint in (9) iteratively for the current solution and adds this constraint to the optimization problem. In order to do so, we need to solve"}
{"pdf_id": "0806.2890", "content": "quadratic assignment, we developed a C++ implementation of the well-known Graduated Assignment algorithm [17].However the learning scheme discussed here is indepen dent of which algorithm we use for solving either linear or quadratic assignment. Note that the estimator is but a mere approximation in the case of quadratic assignment: since we are unable to find the most violated constraints of (10), we cannot be sure that the duality gap is properly minimized in the constrained optimization problem."}
{"pdf_id": "0806.2925", "content": "This paper explains neural networks, and then presents an efficient way to speed up visualization  process by semi-automatic transfer function generation. We describe how to use neural networks to  detect distinctive features shown in the 2D histogram of the volume data and how to use this  information for data classification."}
{"pdf_id": "0806.2925", "content": "For visualization and analysis of CT data (or  any other 3D medical scan, like MRI or PET),  the key advantage of direct volume rendering is  the potential to show the three dimensional  structure of a feature of interest, rather than just  a small part of the data by cutting plane. This  helps the viewer's perception to find the  relative 3D positions of the object components  and makes it easier to detect and understand  complex phenomena like coronary stenosis for  diagnostic and operation planning [9]."}
{"pdf_id": "0806.2925", "content": "This paper presents a new approach for two dimensional transfer function generation based  on neural networks. Although this technique is  flexible enough for classification of different  types of CT dataset, in this paper we focus on  heart scan visualization to detect coronary  diseases. As histograms of same scan type (e.g.  heart scans) have similar structures (same basic  shape), neural networks can be trained to  position filters on features of interest according  to the diagnostic target."}
{"pdf_id": "0806.2925", "content": "For the volume rendering of scalar volume data  like CT scans, different approaches exist.  Texture based techniques have proved superior,  combining high quality images and interactive  frame rates. These approaches take advantage  of the hardware support of bilinear and trilinear  interpolation provided by modern graphic  cards, making high quality visualization  available on low cost commercial personal  computers. For these approaches the dataset is  stored in the graphics hardware texture  memory first. If the size of the dataset exceeds  the available memory, bricking can be used to  render the data in multiple steps. The dataset is  then sampled, using hardware interpolation."}
{"pdf_id": "0806.2925", "content": "2D texture-based approaches use three copies  of the volume data which resides in texture  memory. Each copy contains a fixed number of  slices along a major axis of the dataset which  will be addressed depending on the current  view direction. After bilinear interpolation, the  values of the slices will then be classified  through a lookup table, rendered as a planar  polygon and blended into the image plane. This  method often suffers from artifacts caused by  the fixed number of slices and their static  alignment along the major axes. Alternatively,  hardware  extensions  can  be  used  for  intermediate slices along the slice axis to  achieve better visual quality."}
{"pdf_id": "0806.2925", "content": "Modern graphics cards support 3D texture  mapping which allows storing the whole  dataset in one 3D texture. It is then possible to  sample view-aligned slices using trilinear  interpolation. This approach avoids the artifacts  which occur when 2D texture-based techniques  switch between the orthogonal slice stacks and  allows an arbitrary sample rate, which results  in an overall better image quality. Also, no  additional copies of the dataset are necessary,  lowering the requirements of texture memory."}
{"pdf_id": "0806.2925", "content": "rendering process itself, the most important  task is to find a good classification technique  that captures the features of interest while  suppressing insignificant parts. As mentioned  above, classification can be achieved by  transfer functions, which assign renderable  optical properties like color and opacity to the  values of the dataset."}
{"pdf_id": "0806.2925", "content": "2D transfer functions classify the volume not  just on the data values but on a combination of  different  properties  and  therefore  the  boundaries of different structures in the dataset  can be better isolated as with 1D transfer  functions. This is because the structures and  tissue types which are to be separated might lie  within the same interval, making 1D transfer  functions unable to render them in isolation."}
{"pdf_id": "0806.2925", "content": "Figure 2 shows a volume rendering of a CT  scan of the heart and the transfer functions  used. It consists of two gauss filters: The first  one colored in yellow is located between the  regions c) and d) (compare Figure 1) to  visualize the myocardial muscle and the  coronaries (by contrast agent). The second one  resides at the top of the first filter, enhancing  the contrast between myocard and coronaries  by coloring the properties that represent the  boundaries of the contrast agent in red."}
{"pdf_id": "0806.2925", "content": "For an experienced user, the distinctive features  of the distribution shown in the histogram  provide useful information about the features  metrics, thereby guiding the transfer function  generation. But even with these hints, this is a  time-consuming iterative process. The user has  to explore the dataset by defining filters and  move them to possible interesting locations on  the histogram. Once a feature of interest is  identified, the parameters for the filter size,  location, filter kernel shape, opacity and color  have to be optimized to match with the user's  needs until all features of interest are made  visible."}
{"pdf_id": "0806.2925", "content": "A neural network is a structure involving  weighted interconnections among neurons  (which are most often nonlinear scalar  transformations). A neuron is structured to  process multiple inputs, usually including the  unity bias, in a nonlinear way, producing a  single output. Specifically, all inputs to a  neuron are first augmented by multiplicative  weights. These weighted inputs are summed  and then transformed via a nonlinear activation  function. The weights are sometimes referred to  as synaptic strengths. The general purpose of  the Neural Networks can be described to be  function approximation."}
{"pdf_id": "0806.2925", "content": "When input data originates from a function  with real-valued outputs over a continuous  range, the neural network is said to perform a  function approximation. An example of an  approximation problem is when we control  some process parameter by calculating a value  of certain (complex) function. Instead, we  could make a neural network that approximates  that function, and a neural network calculates  output very quickly."}
{"pdf_id": "0806.2925", "content": "Feed-forward networks are advantageous as  they are the fastest models to execute, and are  universal function approximators. One major  disadvantage of this network type is that no fast  and reliable training algorithm has yet been  designed and therefore can be extremely slow  to  train.  Thus,  multilayer  feed-forward  networks should be chosen if rapid execution  rates are required, but slow learning rates are  not a problem."}
{"pdf_id": "0806.2925", "content": "Feed-forward networks usually consist of three  or four layers in which the neurons are  logically arranged. The first and last layers are  the input and output layers respectively and  there are usually one or more hidden layers in  between them. Research indicates that a  minimum of three layers is required to solve  complex problems [6]. The term feed-forward  means that the information is only allowed to  \"travel\" in one direction (there are no loops in  networks). Furthermore, this means that the  output of one layer becomes the input of the  next layer, and so on. In order for this to  happen, each layer is fully connected to next"}
{"pdf_id": "0806.2925", "content": "It is important to say that \"over-training\" of a  network should be avoided, as it lowers  predictive abilities of the network, as it is said  that network learns \"details of the training set\".  Examples that the network is unfamiliar with,  form what is known as the validation set, which  tests the network's capabilities before it is  implemented for use."}
{"pdf_id": "0806.2925", "content": "As stated in transfer functions section, the 2D  histogram showing the distribution of tuples of  attenuation coefficient and gradient magnitude  of a heart dataset contains distinctive features  which can be used to guide the transfer  function setup. These features consist of  circular spots at the bottom of the histogram  representing  homogeneous  materials  and  arches which define material boundaries.  Hence, the poison and size of a filter setup for a  2D transfer function depends on those patterns."}
{"pdf_id": "0806.2925", "content": "Given as an input, the histogram can be used to  train a neural network for pattern recognition.  Therefore the user creates filter setups for a  training  set  manually  according  to  the  diagnostic target. The network is then trained  to associate outputs (filters) with input patterns  in the histogram. This time consuming step has  only to be performed once and can be done  outside clinical practice. Once the network is  properly trained, it can be used to create an  appropriate filter setup automatically."}
{"pdf_id": "0806.2925", "content": "The 2D histogram is basically a grayscale  image with dimensions 256*256. An input of  this size would require a significant amount of  memory for storage (16MB just for weights in  case of 64 neurons in 2nd layer). Also, training  of such a network would be slow, and its  generalization abilities would be presumably  low."}
{"pdf_id": "0806.2925", "content": "coefficient and gradient magnitude which have  only a few voxels of the dataset assigned to  them, they appear to the neural network as  noise. Also, these parts vary a lot between  different datasets. As this affects the lea rate, with noise removed and image size  reduced, the neural network will learn more  easily and will have better generalization  abilities."}
{"pdf_id": "0806.2925", "content": "As two gauss filters are usually used to  visualize heart and its arteries, we decided that  output of our network would be positions and  sizes of those gauss filters. Hence, number of  outputs is 8 (xpos1, ypos1, xsize1, ysize1,  xpos2, ypos2, xsize2, ysize2)."}
{"pdf_id": "0806.2925", "content": "That leaves some variability for layers between. We started with 1 hidden layer with  64 neurons in it. We worked with this  architecture throughout software development  until final training and testing, which is when  we did some experimentation. We reduced size  of the hidden layer first to 32 and then to 16,  and noticed no degradation in results. We kept  16 neurons in hidden layer. We did not  experiment with more than 1 hidden layer (as  there was no need for it)."}
{"pdf_id": "0806.2925", "content": "Trying to see how many samples need to be  used for training in order for neural n be useful, we did some experimentation. For 12  samples, we manually determined positions.  Then 2 samples we marked as control samples  (validation samples), and other 10 were used  for training. We created 5 neural networks, first"}
{"pdf_id": "0806.2925", "content": "coefficient and gradient magnitude which have  only a few voxels of the dataset assigned to  them, they appear to the neural network as  noise. Also, these parts vary a lot between  different datasets. As this affects the learning  rate, with noise removed and image size  reduced, the neural network will learn more  easily and will have better generalization"}
{"pdf_id": "0806.2925", "content": "Trying to see how many samples need to be  used for training in order for neural network to  be useful, we did some experimentation. For 12  samples, we manually determined positions.  Then 2 samples we marked as control samples  (validation samples), and other 10 were used  for training. We created 5 neural networks, first"}
{"pdf_id": "0806.2925", "content": "one trained with 2 samples, second one with 4  samples and fifth one with 10 samples, and on  all of these networks we used 2 control samples  to check for error. On Figure  series, one showing error of networks on  training data, and the other errors on test data.  For all networks except first one (the one  trained with only 2 samples), mean square error  is lower on test set, than on training set. This is  unusual, but can be explained with fact that  positions that we manually provided for  networks, were not all that similar."}
{"pdf_id": "0806.2925", "content": "It is quite clear that even small number of  training samples produces good results. In our  measurements, networks trained on 6, 8, and 10  samples provide nearly the same results as  network trained with just 4 samples. This can  be explained by the fact that histograms have  so typical shape, so just 4 training samples  suffice for good recognition, and all knowledge  gained by additional training is annihi \"overfitting\", so training the network beyond  basic needs achieves very little effect."}
{"pdf_id": "0806.2925", "content": "Also interesting is that training MSE (mean  square error) jumps on network trained with 4  samples, and then gradually decreases with  increased number of trainin be explained with assumption that either on 3 or 4th sample training data was \"radically\"  different from the others, so network could not  easily minimize that errors that its oddity  produces. As the number of samples increase,  relative influence of that sample is reduced and  MSE is lowered."}
{"pdf_id": "0806.2925", "content": "network trained with just 4 samples. This can  be explained by the fact that histograms have  so typical shape, so just 4 training samples  suffice for good recognition, and all knowledge  gained by additional training is annihilated by  \"overfitting\", so training the network beyond  basic needs achieves very little effect."}
{"pdf_id": "0806.2925", "content": "The neural network software built into  VolumeStudio enables to additionally train  existing neural network, and stand-alone  training tool has also be created (with more  features for training than built-in function).  This enables neural network to be retrained, or  created from scratch on new dataset. Creating  new neural network also enables creating  specialized networks for other specific body  scans, like head or the whole body, for example  (we did not have enough samples of those  types to experiment with it ourselves)."}
{"pdf_id": "0806.2925", "content": "The time spent on positioning the filters has  been  cut  down  from  1-3  minutes  to  approximately 10-30 seconds needed for fine  tuning of the parameters after automatic filter  generation, giving doctors more time to analyze  the data. Also, neural networks kick-start  usefulness of VolumeStudio for new users."}
{"pdf_id": "0806.2925", "content": "A number of things could have been done  differently. First, histogram image downscaling  could be by factor of 2, not 4. We did not  change that, because the results are satisfactory  as it is done now. We could have experimented  with different number of layers, to see what  results it would give."}
{"pdf_id": "0806.2925", "content": "One approach to automate this too, is to use an  additional network to classify input samples  into type categories. This network has to have  as many outputs as there are different networks  for different data types. When the user loads  new scan, this data classification network is  used to determine type of scan and after that,  based on the output of the classification  network the appropriate network for filter  positioning is chosen. This approach, however,  has the small drawback that whenever you add  a network for new scan type, you have to  change architecture of the classifier by adding  an additional output and subsequently re-train  it."}
{"pdf_id": "0806.3765", "content": "• Cross-concordances between controlled vocabularies: The different concept systems  are analyzed in a user context and an attempt made to relate intellectually their  conceptualization. This idea should not be confused with the construction of  metathesauri. While establishing cross-concordances, there is no attempt made to  standardize existing concept worlds. Cross-concordance encompasses only partial  union of existing terminological systems. They cover with it the static remaining part  of the transfer problematic. Such concordances mostly offer mappings (see Table 1  and 2) in the sense of synonym or similarity/hierarchy relations but also as a deductive  rule relation."}
{"pdf_id": "0806.3765", "content": "• Quantitative-statistical approaches: The transfer problem can be generally modeled as  a fuzzy problem between two content description languages. For the vagueness  addressed in information retrieval between terms e.g. within the user inquiry and the  data collections, different automatic operations have been suggested (probability  procedures, fuzzy approaches and neuronal networks) that can be used on the transfer  problematic (Hellweg et al., 2001). The individual document can be indexed into  individual documents in two concept schemata or whereby two different and  differently indexed documents can be put in some relation to each other. Procedures of  these types need training data. For the multilingual IR the same text can be in two  languages."}
{"pdf_id": "0806.3765", "content": "For interdisciplinary information systems, semantic integration not only increases the success  chances for distributed searches over collections with different subject metadata schemes but  it also provides a window into a different disciplinary framework and domain-specific  language for the searcher, if the mapped vocabularies are made available (see e"}
{"pdf_id": "0806.3765", "content": "In addition, every relation must be tagged with a relevance rating (high, medium, and low).  The relevance rating is a secondary but weak instrument to adjust the quality of the relations.  They are not used in our current implementations.  Table 2 presents typical unidirectional cross-concordances between two vocabularies A and  B."}
{"pdf_id": "0806.3765", "content": "The project generated cross-concordances between the following controlled vocabularies  (thesauri, descriptor lists, classifications, and subject headings) which all play a role in the  subject specific collections of vascoda. Several cross-concordances from the previous projects  CARMEN10 and infoconnex11 were incorporated.  The vocabularies involved in the project KoMoHe are mostly in German, English (N=8),  Russian (N=1), or multilingual (e.g. AGROVOC, IBLK, DDC). Some vocabularies have  English or German translations of terms (e.g. THESOZ, PSYNDEX, MESH, INION, STW).  Mapped thesauri (N=16):"}
{"pdf_id": "0806.3765", "content": "Figure 2 gives an overview of all 64 crosswalks. The Thesaurus Sozialwissenschaften  (THESOZ) is the vocabulary with the most incoming and outgoing mappings and due to its  centrality the THESOZ is displayed in the middle of the net. Other vocabularies like SWD or  PSYNDEX play central roles for switching into other domains. The mapping DDC-RVK is  the only cross-concordance which is not connected. Possibly, the terminology work done by  the project CRISSCROSS which maps SWD to DDC could be utilized to connect this  disconnected pair. The mapping JEL-STW is one example for a unidirectional (one-way)  cross-concordance from JEL to STW."}
{"pdf_id": "0806.3765", "content": "To search and retrieve terminology data from the database, a web service (called  heterogeneity service or HTS in Figure 4, see Mayr & Walter, 2008) was built to support  cross-concordance searches for individual start terms, mapped terms, start and destination  vocabularies as well as different types of relations"}
{"pdf_id": "0806.3765", "content": "4. Cross-concordance evaluation  4.1 General Questions  Although the need for terminology mappings is generally acknowledged by the community  and many mapping projects are undertaken, the actual effectiveness and usefulness of the  project outcomes is rarely evaluated stringently. Many questions can be asked of the  terminology networks created in these mappings, e.g.:"}
{"pdf_id": "0806.3765", "content": "A quantitative analysis can give some insight into the basic features of a cross-concordance,  but it can not determine the quality improvements gained from using specific mappings in  search. We have devised an information retrieval test with the goal of evaluating the  application of cross-concordances in a real-world search scenario."}
{"pdf_id": "0806.3765", "content": "• Retrieved: average number of retrieved documents (across all search types)  • Relevant: average number of relevant retrieved documents (across all search types)  • Rel_ret: average number of relevant retrieved documents for a particular search type  • Recall: proportion of relevant retrieved documents out of all relevant documents  (averaged across all queries of one search type)"}
{"pdf_id": "0806.3765", "content": "5. Results of the evaluation  5.1 Test 1: Controlled term search  Test 1 evaluated whether the replacement of a query with vocabulary A terms (CT) with  controlled vocabulary terms from vocabulary B (transformation through term mapping) (TT)  would improve retrieval in database B. If the term mapping is imprecise or ambiguous or the  vocabularies overlap, then the translation from the original query to the mapped query could  introduce noise into the query formulation, which could then impede on the quality of the  search.  Table 5 gives an overview of the average results over all 13 tested cross-concordances. The  last line shows the difference in percentage points between the search types:"}
{"pdf_id": "0806.3765", "content": "The search utilizing term transformations doubles the number of retrieved documents, more  documents containing the query terms are found. Recall increases by almost 100%, whereas  precision increases by more than 50%. The use of a cross-concordance in this particular  search finds not only more relevant documents (recall) but is still more accurate (precision)  than a search without the term transformation.  However, this huge improvement is partly due to the translation between English and German  in the bilingual cross-concordance. Whereas monolingual term mappings might be ineffective  because the mapped terms are identical, this will not be the case in translated mapping. Table  6 show the retrieval results when the bilingual cross-concordance is removed from the test set:"}
{"pdf_id": "0806.3765", "content": "Because of term overlap, the retrieval results should be different for cross-concordances  spanning two disciplines (interdisciplinary) or cross-concordances within the same  disciplinary area (intradisciplinary). If the test results are separated by disciplinarity, we can  see significant changes in the retrieval results. For intradisciplinary cross-concordances, recall  and precision increase but not as much. A smaller or negative change in precision should  actually be expected as commonly in information retrieval precision and recall are in an  inverse relationship with each other (if recall rises, precision falls).  Table 7 shows the average recall and precision measures for all and only the monolingual  intradisciplinary cross-concordances. For monolingual intradisciplinary cross-concordances,  precision and recall still increase but much less than for all cross-concordances."}
{"pdf_id": "0806.3765", "content": "Utilizing cross-concordances has more than a positive effect on the controlled term search.  The result set is not only bigger but also more precise. The biggest impact can be observed for  cross-concordances spanning more than one discipline.  5.2 Test 2: Free-text search  Test 2 evaluated whether adding controlled vocabulary terms gained from mapping natural  language query terms to the controlled vocabulary of a database (FT-CK) to a free-text query  (FT) would improve retrieval results. For some of the individual queries in the tests, no  changes to the queries were made because no matching controlled vocabulary terms could be  found. Table 9 shows the retrieval results for all 8 tested cross-concordances:"}
{"pdf_id": "0806.3765", "content": "The results show that not only more but more relevant documents are found. Average recall  still increases by 20%. Generally, controlled terms simply added to a query can still improve  retrieval results. However, a drop in precision is observed, which is nevertheless not as big as  the rise in recall.  Table 10 shows the retrieval results for cross-concordances mapping terms within the same  discipline, whereas table 11 shows the results for 2 interdisciplinary cross-concordances:"}
{"pdf_id": "0806.3885", "content": "Abstract— Adams and Bishop have proposed in 1994 a novel region growing algorithm called seeded region growing by pixels aggregation (SRGPA). This paper introduces a framework to implement an algorithm using SRGPA. This framework is built around two concepts: localization and organization of applied action. This conceptualization gives a quick implementation of algorithms, a direct translation between the mathematical idea and the numerical implementation, and an improvement of algorithms efficiency."}
{"pdf_id": "0806.3885", "content": "1) myself part: We suppose that there is only the nuctuation of Xt i,m between time t and t + 1/2. For all j different of i, Zt+1/2 is equal to Zt j because they do not depend on Xt i,m. If it is a growth Xt+1/2 i,m = Xt i,m + At then"}
{"pdf_id": "0806.3885", "content": "1) an implementation of algorithms using SRGPA with less than fourty lines of codes,2) the application of these algorithms whatever the dimen sion of the image (principally 2D, 3D) and the type of pixel/voxel, 3) the optimization of all algorithms using SRGPA. Since the library has been optimized, all algorithms using this library will benefit from the optimization."}
{"pdf_id": "0806.3885", "content": "In this paper, we have conceptualized the localization and the organization of seed region growing method by pixels aggregation. In the conceptualization part, we define two objects and one procedure to make possible the creation of the library, called Population. The first object, zone of innuence, is associated to each region to localize a zone on the outer boundary region."}
{"pdf_id": "0806.3885", "content": "ACKNOWLEDGMENT I would like to thank my Ph.d supervisor, P. Levitz, for his support and his trust. The author is indebted to P. Calka for valuable discussion and C. Wiejak for critical reading of the manuscript. I express my gratitude to the Association Technique de l'Industrie des Liants Hydrauliques (ATILH) andthe French ANR project \"mipomodim\" No. ANR-05-BLAN 0017 for their financial support."}
{"pdf_id": "0806.3887", "content": "Abstract— In the previous paper, we have conceptualized the localization and the organization of seeded region growing by pixels aggregation (SRGPA) but we do not give the issue when there is a collision between two distinct regions during the growing process. In this paper, we propose two implementations to manage two classical growing processes: one without a boundary region region to divide the other regions and another with. Unfortunately, as noticed by Mehnert and Jakway (1997), this partition depends on the seeded region initialisation order (SRIO). We propose a growing process, invariant about SRIO such as the boundary region is the set of ambiguous pixels."}
{"pdf_id": "0806.3887", "content": "Using this growing process, the localization of final partition is invariant about the SRIO. The outline of the rest of the paper is as follows: in Sec. II, we present the two classical growing processes. In Sec. III, we explain how to implement a growing process invariant about the SRIO. In Sec. V, we make concluding remarks."}
{"pdf_id": "0806.3887", "content": "II. CLASSICAL GROWING PROCESSES This section presents two classical growing processes. For the first, there is no boundary region to divide the other regions. For the second, there is a boundary region to divide the other regions. The geodesic dilatation[4] is used like an example but this approach can be used for the most of algorithms using SRGPA if the algorithm can be reduced in a succession of geodesic dilatations[3]. This section is decomposed in two parts: definition of two distinct partitions and how to get both partitions for algorithms using SRGPA."}
{"pdf_id": "0806.3887", "content": "Whatever the growing process is, the final partition is not invariant about SRIO. The figure 3 shows the case with an ambiguous pixel for the growing process without a boundary region to divide the other regions. The figure 4 shows the case with two ambiguous pixels for the growing process with a boundary region to divide the other regions. The localization of the inner border of each region depends on SRIO. The next section proposes a solution to overcome this limitation."}
{"pdf_id": "0806.3887", "content": "In discrete space, the boundary definition is not oclearly defined. Using the SRGPA, we have proposed two growing processes to do a simple or V-boundary partition. Thesegrowing processes have incertitude on the regions boundary lo calisation. To overcome this problem, we have defined a set of ambiguous points such as in a discrete space, it is impossible to know to which regions they belong. Knowing that, we have defined a growing process with a boundary region localized"}
{"pdf_id": "0806.3887", "content": "The idea of the first article is to define three objects: Zone of Innuence (ZI), System of Queues (SQ) and Population. Thealgorithm implementation using SRGPA is focused on the util isation of these three objects. An object ZI is associated to each region and localizes a zone on the outer boundary of its region. For example, a ZI can be the outer boundary region excluding all other regions. An algorithm using SRGPA is not global (no treatment for a block of pixels) but local (the iteration is applied pixel by pixel belonging to the ZI). To manage the"}
{"pdf_id": "0806.3887", "content": "pixel by pixel organisation, a SQ sorts out all pixels belonging to ZI depending on the metric and the entering time. It gives the possibility to select a pixel following a value of the metric and a condition of the entering time. The object population links all regions/ZI and permits the (de)growth of regions. A pseudo-library, named Population, implements these three objects. An algorithm can be implemented easier and faster with this library, fitted for SRGPA."}
{"pdf_id": "0806.3887", "content": "I would like to thank my Ph.d supervisor, P. Levitz, for his support and his trust. The author is indebted to P. Calka for valuable discussion and C. Wiejak for critical reading of the manuscript. I express my gratitude to the Association Technique de l'Industrie des Liants Hydrauliques (ATILH) andthe French ANR project \"mipomodim\" No. ANR-05-BLAN 0017 for their financial support."}
{"pdf_id": "0806.3928", "content": "Abstract— In the two previous papers of this serie, we have created a library, called Population, dedicated to seeded region growing by pixels aggregation and we have proposed different growing processes to get a partition with or without a boundary region to divide the other regions or to get a partition invariant about the seeded region initialisation order. Using this work, we implement some algorithms belonging to the field of SRGPA using this library and these growing processes."}
{"pdf_id": "0806.3928", "content": "Many fields in computer science, stereovision[12], math ematical morphology[14], use algorithm which principle is Seeded Region Growing by Pixels Aggregation (SRGPA). This method consists in initializing each region with a seed, then processing pixels aggregation on regions, iterating this aggregation until getting a nilpotence [1][10]. The general purpose of this field is to define a metric divided into two distinct categories [3]: the region feature like the tint [1] and region boundary discontinuity[6]. In this article, the aim is not to do an overview of the algorithms using SRGPA but to prove that the framework introduced in the two previous articles[15][16] is generic. Some algorithms using SRGPA are implemented thanks to the library Population:"}
{"pdf_id": "0806.3928", "content": "• distance function, watershed transformation and geodesic reconstruction. The first enhancement is the easiness to implement these algorithms using the objects of the library Population. The second enhancement is the algorithms efficiency. All these algorithms have been applied on 3D image with a size equal to 700*700*700=0.35 Giga pixels. The running time is always less than 3 hours with an Intel(R) Xeon(R) CPU 3.00GH. This is due to1) the library optimisation using the template metaprogram ming1[2]: all algorithms using this library will benefit from this optimization,"}
{"pdf_id": "0806.3928", "content": "1Template metaprogramming is a metaprogramming technique in which templates are used by a compiler to generate temporary source code, which is merged by the compiler with the rest of the source code and then compiled. The output of these templates include compile-time constants, data structures,and complete functions. The use of templates can be thought of as compile time execution."}
{"pdf_id": "0806.3928", "content": "• let V be a neighborhood function (an elementary struc turing element). In the appendice I, the definition of the distance is given. The article understanding depends on the comprehension of the previous articles of this serie. A summary is done in the appendice II. The outline of the rest of the paper is as follows: in Sec. II, we present the algorithms using only one queue in the system of queue (SQ), in Sec. III we present the algorithms using more than one queue, in Sec. IV, we make concluding remarks."}
{"pdf_id": "0806.3928", "content": "If f is seen as a topographic surface, the second line means that the level is the same in each point belonging to si and the third line means that all paths between two points belonging to different elements of S do not have a constant level. In this decomposition, an element s of S is a regional"}
{"pdf_id": "0806.3928", "content": "An efficient segmentation procedure developed in mathe matical morphology is the watershed segmentation [6], usually implemented by a nooding process from labels (seeds). Any greyscale image can be considered as a topographic surface and all boundaries as sharp variations of the grey level. When a gradient is applied to an image, boundaries"}
{"pdf_id": "0806.3928", "content": "The idea of the second article is to give three different growing processes, leading up to three different partitions of the space: 1) one without a boundary region to divide the other regions, 2) another with a boundary region to divide the other regions, 3) the last one does not depend on the seeded region initialisation order"}
{"pdf_id": "0806.3928", "content": "I would like to thank my Ph.d supervisor, P. Levitz, for his support and his trust. The author is indebted to P. Calka for valuable discussion and C. Wiejak for critical reading of themanuscript. I express my gratitude to the Association Tech nique de l'Industrie des Liants Hydrauliques (ATILH) for its financial support and the French ANR project \"mipomodim\" No. ANR-05-BLAN-0017 for their financial support."}
{"pdf_id": "0806.3939", "content": "to achieve this goal. Simple means that this method can be used by anybody who is not a specialist of image processing. Generic means that this method can be applied in a wide range of materials. This method has been applied for granular materials (see figure 1) but its extension to other materials is straightforward. Robust means that the extraction is few sensitive with a \"little\" variation of the parameters. This method has two steps:"}
{"pdf_id": "0806.3939", "content": "Microtomography is a non-destructive 3D-characterisation technic providing a three-dimensional image2. Each voxel of the image is associated to a cube included in the material, under investigation[1]. In first order, its grey-value is the space average of linear X-ray absorption coefficient of the different solids and nuids contained into it. But since more often the tomographic reconstruction amplifies the noise of the projections, and generates artefacts, there is extra-term given impressive images with generally a too weak quality for a quantitative and automatic use. Also, the materials are different in the chemical composition and in the geometrical organisation (see figure 1). Due to the materials variety and the images defects, a generic, simple and robust segmentation procedure has been developed."}
{"pdf_id": "0806.3939", "content": "For every image, the grey-level is coded on one byte (0 255) and a median filter has been applied to minimize the ring artefact and to smooth in keeping the sharpness of the boundary. The images have an uniform illumination and each component on the image has a specific brightness3. For the visualization convenience, the results are sometimes presented in 2D but the method has been applied efficiently in 3D for all materials."}
{"pdf_id": "0806.3939", "content": "For each material, depending on the histogram shape, the classical threshold segmentation can be applied to extract a component, using tint information (1). If the contrast between the component and the background is low and if the boundaryhas to be well localized, the watershed transformation con trolled by labels is applied using the boundary information (2). For the both approaches, a combination of morphological filters has to be applied in order to: 1) match the visual segmentation for (1) (the combination is an opening followed by a closing), 2) localize two labels for (2) (the combination is just an opening).This section is decomposed into two parts: threshold seg mentation using tint information and watershed transformation using boundary information."}
{"pdf_id": "0806.3939", "content": "3This last assumption is not always verified. For example, the large grains with a medium average grey level in the granular B is divided into two components which chemical composition is different and which linear X-ray absorption coefficient is the same. Without more information, we consider these two components as one component."}
{"pdf_id": "0806.3939", "content": "Except the last component, the extraction procedure is: 1) to localize two labels: one included in the component and the other in the component complementary (the next paragraph is dedicated to this task), 2) to apply the Deriche's operator[6] on the initial image to get the gradient image, 3) to apply the watershed transformation controled by labels on this gradient image with these labels (see figure 5)"}
{"pdf_id": "0806.3939", "content": "In this article, the selection of the threshold/opening param eters and are done manually following these constraints (see table I): 1) the material specialist checks if the visual segmentation matches the numerical segmentation, 2) if there is some experimental data about the volume fraction, we impose the correspondence between the experimental value and the numerical value obtained by segmentation. This manual limitation is attenuated by a good property: some small parameters modifications have no consequence on the final segmentation (see subsubsection III-B.3). So it is easy to find the right parameters for a good segmentation because"}
{"pdf_id": "0806.3939", "content": "the range of the right parameters is large. This simple method gives some good results for the four granular materials. The figure 6 shows the different steps for the extraction of one component for the granular A, B and C. The figure 7 shows the 3D visualization of the multi-component extraction. In the next subsubsection, a method to evaluate the robustness is presented, in more this method opens up the opportunity of an automatic evaluation of the parameters."}
{"pdf_id": "0806.3939", "content": "ACKNOWLEDGMENT I would like to thank my Ph.d supervisor, P. Levitz, for his support and his trust. The author is indebted to E. Gallucci, D. Jeulin for valuable discussion and C. Wiejak for critical reading of the manuscript. I express my gratitude to the Association Technique de l'Industrie des Liants Hydrauliques (ATILH) for its financial support and the French ANR project \"mipomodim\" No. ANR-05-BLAN-0017 for their financial support."}
{"pdf_id": "0806.3939", "content": "Fig. 10.Application for the granular material B: for the threshold seg mentation, the threshold value is selected (the value 128 corresponds to the valley on the histogram) and for the double labels watershed, the threshold value to localize the label inside the grains is selected (the value 90 has been chosen manually to give a result matching the visual segmentation). For both distances, the double labels watershed is more stable of one decade than the threshold segmentation."}
{"pdf_id": "0806.3939", "content": "1 1 1 1 1  1 2 2 2 2 1 2 2 1 1 2 2 1 1 2 2 1 1 2 2 1 1 2 2 1 1 1 1 1 1  1 2 2 2 1  1 2 3 2 1  1 2 3 2 1  1 2 3 2 1  1 2 3 2 1  1 1 2 2 1  1 2 1  1 2 1"}
{"pdf_id": "0807.0023", "content": "Metadata is a costly resource to create, maintain, and/or recover manually. There has therefore been significant research on automated metadata generation(e.g. by extracting metadata from the content of re sources). Natural language processing [26] and document image analysis techniques [7, 10, 17, 24] may extract keywords, subject categories, author, and citations (e.g. CiteSeer[29]) from manuscripts. Furthermore, in [9], two metadata generators are demonstrated that successfully harvest and extract metadata from existing"}
{"pdf_id": "0807.0023", "content": "For the reasons outlined above, methods for the gen eration of metadata that do not rely on resource content have generated considerable interest. The recent growth in applications of \"folksonomies\" (i.e. community-based \"tagging\" [8, 18]), has been, to some extent, inspired by the shortcomings of existing metadata generation methods. Unfortunately, human tagging only works well in situations where the number of participants greatly exceeds the number of resources to be tagged and where there is no requirement for controlled vocabularies or standardized metadata formats."}
{"pdf_id": "0807.0023", "content": "In this article, we propose a system for automatedmetadata generation that starts from a common sce nario: a heterogeneous repository contains resources for which varying degrees of metadata are available. Some resources have been imbued with rich, vetted metadata, whereas others have not. However, if it can be assumed that resources that are \"similar\" (e.g. similar in publication venue, authorship, date, citations, etc.)are more likely to have shared meta data, then the problem of metadata generation can be reformulated as one of extrapolating metadatafrom metadata-rich to related, but metadata-poor re sources. This article's experiment focuses on identifying which aspects of metadata similarity are best used to extrapolate resource metadata in a bibliographic dataset."}
{"pdf_id": "0807.0023", "content": "the annotation of personal photograph collections. Oncea user has annotated a photograph its metadata is au tomatically transferred to photographs taken at similar times and locations. For example, a user photographs a group of friends at 3:45PM. Another photograph is made at 3:47PM. Since the second photograph was taken only two minutes after the first, it is likely that it depicts a similar scene. The system therefore transfers metadata from photograph 1 to photograph 2. Similarly, [21] proposes a method of web page metadata propagation using co-citation networks. The general idea is that if two web pages cite other web pages in common, then the probability that they share similar metadata is higher. The user can later correct and augment any transferred metadata."}
{"pdf_id": "0807.0023", "content": "The mentioned systems are strongly related to col laborative filtering [11]. Collaborative filtering systemsare commonly employed in online retail systems to rec ommend items of interest to individual users. Using the principle that similar users are more likely to appreciate similar items, users are recommended items that are missing from their profiles but occur in the profiles of similar users. The collaborative filtering process can thus be regarded as an instance of metadata propagation. If users are considered resources and their profiles are considered \"resource metadata\", it can be said that collaborative filtering systems \"recommend\" metadata from one resource to another based on resource similarity."}
{"pdf_id": "0807.0023", "content": "Such systems for the generation of metadata can be said to operate on a \"Robin Hood\" principle; they take from metadata-rich resources and give to metadata-poor resources, with the exception that metadata is not a zero-sum resource. This mode of operation has a number of desirable properties. First, it reduces the need for the costly generation of metadata; metadata is automatically extrapolated from an existing metadata-rich reference collection to a metadata-poor subset. Second, resource relations can be defined independent of content and metadata extrapolation can thus be implemented for wide range of heterogeneous resources, e.g. audio, video, and images."}
{"pdf_id": "0807.0023", "content": "This paper will first discuss two algorithms to define sets of resource relations and represent these relations in terms of associative networks. It will then formally define a metadata propagation algorithm which can operate on the basis of the generated resource relations. Finally, the proposed metadata generation system is validated using a modified version of the KDD Cup 2003 High-Energy Physics bibliographic dataset (hep-th 2003)[30]. While it is theoretically possible for this method to work on other resource types (e.g. video, audio, etc.) as it doesn't require an analysis of the content of the resources, only their metadata; it is only speculated that the results of such a method would be viable in these other, non-tested, domains."}
{"pdf_id": "0807.0023", "content": "The remainder of this section will describe two asso ciative network construction algorithms. One is based on occurrence metadata where a resource is considered similar to another if there is a direct reference from one resource to the other (e.g. a direct citation). The other algorithm is based on co-occurrence metadata and thus, considers two resources to be similar if they share similar metadata. That is, two resources are deemed similar if the same metadata values occur in both their properties (i.e. same authors, same keywords, same publication venue, etc.). Depending on how the repository represents its metdata some property types will be direct reference properties and others will have to be infered through indirect, co-occurence algorithms."}
{"pdf_id": "0807.0023", "content": "If Algorithm 1 is called recommendMeta(nj, pi) then the full particle propagation algorithm can be described by the pseudo-code in Algorithm 2. The process ofmoving metadata particles through the associative net work and recommending metadata-poor nodes metadata property values continues until some desired t is reached or all particle energy in the network has decayed to 0.0,"}
{"pdf_id": "0807.0023", "content": "By artificially reducing the amount of metadata in the full bibliographic dataset, it is possible to simulate a metadata-poor environment and at the same time still be able to validate the results of the metadata propagation algorithm. The section is outlined as follows. First, the dataset used for this experiment is described. Second, a short review of the validation metrics (precision, recall, and F-score) is presented. Third, the various system parameters are discussed. Finally, the results of the experiment are presented as a validation of the systems use for manuscript-based digital library repositories. Further research into other domains besides manuscripts will demonstrate the validity of this method for other resource types."}
{"pdf_id": "0807.0023", "content": "The dataset used to validate the proposed system is a modified version of the hep-th 2003 bibliographic dataset for high energy physics and theory [19].[31] A modified version of the hep-th dataset, as used in [16], is represented as a semantic network containing manuscripts (29,014), authors (12,755), journals (267), organizations (963), keywords (40), and publication date in year/season pairs (60). These nodes are then connected according to the following semantics:"}
{"pdf_id": "0807.0023", "content": "As can be noticed from Table II, Table III, and Figure 8a, the keyword property performs best in a citationnetwork. A direct reference from one document to an other is a validation of the similarity between documents with respect to subject domain. Therefore, the tendency for citing documents to contains similar keyword values is high. For instance, refer to the citations of this article (references in this manuscript's bibliography). Every cited manuscript is either about automatic metadata generation, bibliographic networks, or network analysis."}
{"pdf_id": "0807.0023", "content": "What has been presented in this study is the results of this algorithm without the intervention of any human components (besides the initial creation of metadata through the hep-th dataset creation process). Futurework that studies this method with the inclusion of hu mans that help to validate and \"clean\" the recommended metadata would be telling of how much this method is able to speed up the process of generating accurate and reliable metadata for metadata-poor resources. Such an analysis is left to future research."}
{"pdf_id": "0807.0023", "content": "This research was financially supported by the Re search Library at the Los Alamos National Laboratory. The modified hep-th 2003 bibliographic dataset was generously provided by Shou-de Lin and Jennifer H. Watkins provided editorial assistance. Finally, the hep-th 2003 database is based on data from the arXiv archive and the Stanford Linear Accelerator Center SPIRES-HEP database provided for the 2003 KDD Cup competition with additional preparation performed by the Knowledge Discovery Laboratory, University of Massachusetts Amherst."}
{"pdf_id": "0807.0517", "content": "Evolution of belief systems has always been in focus of cognitive research. In this paper we  delineate a new model describing belief systems as a network of statements considered true. Testing  the model a small number of parameters enabled us to reproduce a variety of well-known mechanisms ranging from opinion changes to development of psychological problems. The self organizing opinion structure showed a scale-free degree distribution. The novelty of our work lies in  applying a convenient set of definitions allowing us to depict opinion network dynamics in a highly  favorable way, which resulted in a scale-free belief network. As an additional benefit, we listed  several conjectural consequences in a number of areas related to thinking and reasoning."}
{"pdf_id": "0807.0517", "content": "Definition 4: An input is a new point for the network (with non-existing content). Definition 5: At a certain time one and only one point of the network is active (it has a  distinguished role in dynamic processes). Definition 6: A time step is a discrete time interval for elementary changes in the network.  (Detailed elucidation is given below.) Definition 7: In every time step n links randomly vanish. (This random process can be  interpreted as forgetting (Bednorz and Schuster, 2006). Definition 8: A vertex losing all its links vanishes."}
{"pdf_id": "0807.0517", "content": "3. Compatibility factor of a vertex:  ig - gives the probability that the given vertex is in  positive (strengthening) connection with a randomly chosen vertex - a number  between 0 and 1 4. Contradiction factor of a vertex:  ih - gives the probability that the given vertex is in  negative (weakening) connection with a randomly chosen vertex - a number between 0  and 1"}
{"pdf_id": "0807.0517", "content": "there is a statement of unique importance in a network. This leads to a conformation that  determines behavior: the exceptional point gathers a large number of links, most random  walks go that way, and that point will be the absolute center as shown in Fig. 3. (The peak in  the right is not a single point with a probability of 1 but approximately 100 points close to  each other with probabilities of approximately 0.01, as the average of 10000 simulations is  depicted in the figure. Colors indicate different simulations: the ordinal number of the special  point was modified from 1 to 32.)"}
{"pdf_id": "0807.0517", "content": "(It is shown that  emotions play a decisive role in political reasoning, see Westen, Kilts, Blagov, Harenski, and  Hamann, 2006) This is a typical devastating effect of a star shaped subnetwork: new  information are connected to the center and only allowed to remain in the network if there is a  non-negative link between them"}
{"pdf_id": "0807.0517", "content": "Elder, highly qualified people usually have more developed networks as it follows from  the previous arguments about the role of time, so their degree distribution is wider, they have  more vertices with large numbers of links. Obviously, it is not easy for newcomers to attain  such high degrees what is an explanation for the above mentioned experiences. On the other  hand, networks with a smaller number of vertices and less connections are more easily  affected by novelties. Though, there are a number of different ways of change that are under  study in the following three subsections."}
{"pdf_id": "0807.0517", "content": "The  model allows a very special way of vertex integration: if a new part of the network evolves  separately from the former parts of the network and only a few connections are built between  the two parts, then it is possible that contradictions remain undiscovered until enough time is  given for thinking about the new points"}
{"pdf_id": "0807.0517", "content": "This phenomenon is also encompassed in the model: if a vertex drops out and another  is ejected due to the loss of the first (to which it was positively linked) then there will be a  high probability that some vertices loose two positive partners and have to be dropped"}
{"pdf_id": "0807.0517", "content": "We realize network construction in a series of cycles. In each cycle the system processes  only one input point: establishment of new connections between the point and the existing network is endeavored. According to the parameters it will succeed or not. If the input point  joins the network it induces further linking until a new input arrives. The main units of the  process are shown in Fig. 8."}
{"pdf_id": "0807.0517", "content": "As mentioned before new points should follow preferential linking in order to get scale free network structure. Mathematically it means that the probability of a new edge attaching  to a particular vertex (denote this non-neighboring target vertex by t ) is proportional to  tk .  Taking into account our extra parameter referring to the attractiveness of points, one can  formulate the expression"}
{"pdf_id": "0807.0517", "content": "3. If point  n should be removed and point i not: we remove  n and start a checking  mechanism to investigate, whether the removal of  n affected other points as well.  (The falling number of positive links may lead to ejection of new points.) Details are  elucidated in the next section (Self-Consistency Test)."}
{"pdf_id": "0807.0517", "content": "To recall the meaning of the parameter we give short explanations for the letters: H : negativity tolerance factor of the network U : number of prospective edges of the input E : amount of available time steps for a cycle F : number of edges to be forgotten (thus  F E  with the original notation) f : fitness factor a, b and c: relative probabilities for an edge to be positive, negative, or neutral, respectively"}
{"pdf_id": "0807.0517", "content": "Figure 3: As mentioned afore, if we deal with inhomogeneous inputs, then some points may obtain  outstanding significance. In this simulation the fitness factor of a point is different from the  others. (As earlier points usually become big centers, we performed two simulations. In the  first run the special point was the first, in the second run the special point was the 32nd. Thus,  we see that in these simulations conspicuous effects occur mainly due to the changed fitness  factors, and not the early integration.) The network was expanded to 1000 points."}
{"pdf_id": "0807.0517", "content": "Figure 7: We used a basic network of 1000 points and in each run added a different number of new points in 1000 time steps. Fig. 7 shows the final number of points in the network. Standard  deviations are marked to characterize uncertainties. We used a high F parameter (forgetting)  to get this curve. Settings are given in Table 4."}
{"pdf_id": "0807.0627", "content": "Abstract:The textured images' classification assumes to consi der the images in terms of area with the same texture. In uncertain environment, it could be better to take an imprecise decision or to reject the area corresponding to an unlearning class. Moreover, on the areas that are the classification units, we can have more than one texture.These considerations allows us to develop a belief deci sion model permitting to reject an area as unlearning and to decide on unions and intersections of learning classes."}
{"pdf_id": "0807.0908", "content": "Figure 1: Photo shows from left to right: Prof. John Morrison (Director BCRI), Prof. Patrick Fitzpatrick (Director BCRI), Prof. Fionn Murtagh, Dr. James Grannell, Chairman, School of Mathematical Sciences, Prof. Eugene Freuder (Director Cork Constraint Computation Centre). The Annual Boole Lecturewas established and is sponsored by the Boole Centre for Research in Informat ics, the Cork Constraint Computation Centre, the Department of Computer Science, and the School of Mathematical Sciences, at University College Cork.The series in named in honour of George Boole, the first professor of Mathemat ics at UCC, whose seminal work on logic in the mid-1800s is central to modern digital computing."}
{"pdf_id": "0807.0908", "content": "Various aspects of how we respond to these challenges will be discussed in this article, complemented by the Appendix. We will look at how this works, using the Casablanca film script. Then we return to the data mining approach used, to propose that various issues in policy analysis can be addressed by such techniques also."}
{"pdf_id": "0807.0908", "content": "The well known Casablanca movie serves as an example for us. Film scripts, such as for Casablanca, are partially structured texts. Each scene has metadata and the body of the scene contains dialogue and possibly other descriptive data. The Casablanca script was half completed when production began in 1942. The dialogue for some scenes was written while shooting was in progress. Casablanca was based on an unpublished 1940 screenplay [2]. It was scripted by J.J. Epstein, P.G. Epstein and H. Koch. The film was directed by M. Curtiz and produced"}
{"pdf_id": "0807.0908", "content": "Figure 2: Correspondence Analysis of the Casablanca data derived from thescript. The input data is presences/absences for 77 scenes crossed by 12 at tributes. The 77 scenes are located at the dots, which are not labelled here for clarity. For a short review of the analysis methodology, see Appendix."}
{"pdf_id": "0807.0908", "content": "What sort of explanation does this provide for our conundrum? It means that the query is a novel, or anomalous, or unusual \"document\". It is up to us to decide how to treat such new, innovative cases. It raises though the interesting perspective that here we have a way to model and subsequently handle the semantics of anomaly or innocuousness. The strong triangular inequality, or ultrametric inequality, holds for treedistances: see Figure 6. The closest common ancestor distance is such an ultra metric."}
{"pdf_id": "0807.0908", "content": "Figure 7 uses a sequence-constrained complete link agglomerative algorithm. It shows up scenes 9 to 10, and progressing from 39, to 40 and 41, as major changes. The sequence constrained algorithm, i.e. agglomerations are permitted between adjacent segments of scenes only, is described in an Appendix to this article, and in greater detail in [7]. The agglomerative criterion used, that is subject to this sequence constraint, is a complete link one."}
{"pdf_id": "0807.0908", "content": "10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77"}
{"pdf_id": "0807.0908", "content": "The Casablanca script has 77 successive scenes. In total there are 6710 words in these scenes. We define words as consisting of at least two letters. Punctuation is first removed. All upper case is set to lower case. We use from now on all words. We analyze frequencies of occurrence of words in scenes, so the input is a matrix crossing scenes by words."}
{"pdf_id": "0807.0908", "content": "As a basis for a deeper look at Casablanca we have taken comprehensive but qualitative discussion by McKee [4] and sought quantitative and algorithmic implementation. Casablanca is based on a range of miniplots. For McKee its composition is \"virtually perfect\".Following McKee [4] we will carry out an analysis of Casablanca's \"Mid Act Climax\", Scene 43, subdivided into 11 \"beats\". McKee divides this scene, relating to Ilsa and Rick seeking black market exit visas, into 11 \"beats\"."}
{"pdf_id": "0807.0908", "content": "Figure 9: Hierarchical clustering of sequence of beats in scene 43 of Casablanca. Again, a sequence-constrained complete link agglomerative clustering algorithm is used. The input data is based on the full dimensionality Euclidean embedding provided by the Correspondence Analysis. The relative orientations (defined by correlations with the factors) are used as input data."}
{"pdf_id": "0807.0908", "content": "Our aim is to understand the \"big picture\". It is not to replace the varied measures of success that are applied, such as publications, patents, licences, numbers of PhDs completed, company start-ups, and so on. It is instead to appreciate the broader configuration and orientation, and to determine the most salient aspects underlying the data."}
{"pdf_id": "0807.0908", "content": "This categorization scheme can be viewed as the upper level of a concept hierarchy. It can be contrasted with the somewhat more detailed scheme that we used for analysis of articles in the Computer Journal, [9]. CSETs labelled in the Figures are: APC, Alimentary Pharmabiotic Centre;BDI, Biomedical Diagnostics Institute; CRANN, Centre for Research on Adap tive Nanostructures and Nanodevices; CTVR, Centre for Telecommunications Value-Chain Research; DERI, Digital Enterprise Research Institute; LERO,"}
{"pdf_id": "0807.0908", "content": "Overly-preponderant elements (i.e. row or column profiles), or exceptional ele ments (e.g. a sex attribute, given other performance or behavioural attributes) may be placed as supplementary elements. This means that they are given zero mass in the analysis, and their projections are determined using the transitionformulas. This amounts to carrying out a Correspondence Analysis first, with out these elements, and then projecting them into the factor space following the determination of all properties of this space. Here too we have a new approach to fusion of information spaces focusing the projection."}
{"pdf_id": "0807.1494", "content": "While this approach might sound reasonable, it actually ignores the computational cost of the initial training phase: collecting a representative sample of performance data has to be done via solving a set of training problem instances, and each instance is solved repeatedly, at least once for each of the available algorithms, or more if the algorithms are randomized"}
{"pdf_id": "0807.1494", "content": "In a Reinforcement Learning [36] setting, algorithm selection can be formulated as a Markov Decision Process: in [26], thealgorithm set includes sequences of recursive algorithms, formed dynamically at run-time solving a sequen tial decision problem, and a variation of Q-learning is used to find a dynamic algorithm selection policy; the resulting technique is per instance, dynamic and online"}
{"pdf_id": "0807.1494", "content": "our situation, as we would like to avoid any restriction on the sequence of problems: a very hard instance can be met first, followed by an easy one. In this sense, the hypothesis of a constant, but unknown, bound is more suited. In [7], Cesa-Bianchi et al. also introduce an algorithm for loss games with partial information (EXP3LIGHT), which requires losses to be bound, and is particularly effective when the cumulative loss of the best arm is small. In the next section we introduce a variation of this algorithm that allows it to deal with an unknown bound on losses."}
{"pdf_id": "0807.1494", "content": "We presented a bandit problem solver for loss games with partial information and an unknown bound on losses. The solver represents an ideal plug-in for our algorithm selection method GAMBLETA, avoiding the need to set any additional parameter. The choice of the algorithm set and time allocators to use is still left to the user. Any existing selection technique, including oblivious ones, can be included in the set of N allocators, with an impact O("}
{"pdf_id": "0807.1494", "content": "BLETA to allocate multiple CPUs in parallel, in order to obtain a fully distributed algorithm selection framework [17]. Acknowledgments. We would like to thank Nicol`o Cesa-Bianchi for contributing the proofs for EXP3LIGHT and useful remarks on his work, and Faustino Gomez for his comments on a draft of this paper. This work was supported by the Hasler foundation with grant n. 2244."}
{"pdf_id": "0807.1560", "content": "Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study. One possible way to overcome this problem is to summarize a scientific topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others'viewpoint of the target article's contribu tions and the study of its citation summary network using a clustering approach."}
{"pdf_id": "0807.1560", "content": "citation summaries, can be a good resource to un derstand the main contributions of a paper and how that paper affects others. The citation summary of an article (A), as defined in (Elkiss et al., 2008),is a the set of citing sentences pointing to that ar ticle. Thus, this source contains information aboutA from others' point of view. Part of a sample ci tation summary is as follows:"}
{"pdf_id": "0807.1560", "content": "The ACL Anthology is a collection of papers fromthe Computational Linguistics journal, and pro ceedings from ACL conferences and workshops and includes almost 11, 000 papers. To produce the ACL Anthology Network (AAN), (Joseph andRadev, 2007) manually performed some prepro cessing tasks including parsing references and building the network metadata, the citation, and the author collaboration networks.The full AAN includes all citation and collabo ration data within the ACL papers, with the citationnetwork consisting of 8, 898 nodes and 38, 765 di rected edges. 2.1 Clusters"}
{"pdf_id": "0807.1560", "content": "We built our corpus by extracting small clusters from the AAN data.Each cluster includes papers with a specific phrase in the title or con tent.We used a very simple approach to col lect papers of a cluster, which are likely to betalking about the same topic. Each cluster con sists of a set of articles, in which the topic phrase is matched within the title or the contentof papers in AAN. In particular, the five clus ters that we collected this way, are: Dependency Parsing (DP), Phrased Based Machine Translation (PBMT), Text Summarization (Summ), Question Answering (QA), and Textual Entailment (TE). Table 1 shows the number of articles and citations in each cluster. For the evaluation purpose we"}
{"pdf_id": "0807.1560", "content": "After scanning through all sentences in the citation summary, we can come up with a fact dis tribution matrix for an article. The rows of this matrix represent sentences in the citation summaryand the columns show facts. A 1 value in this ma trix means that the sentence covers the fact. The matrix D shows the fact distribution of P99-1065. IDs in each row show the citing article's ACL ID, and the sentence number in the citation summary.These matrices, created using annotations, are par ticularly useful in the evaluation process."}
{"pdf_id": "0807.1560", "content": "We want to build a network with citing sentences as nodes and similarities of two sentences as edge weights. We'd like this network to have a nicecommunity structure, whereby each cluster corre sponds to a fact. So, a similarity measure in which we are interested is the one which results in high values for pairs of sentences that cover the same facts. On the other hand, it should return a lowvalue for pairs that do not share a common contri bution of the target article. The following shows two sample sentences from P99-1065 that cover the same fact and we want the chosen similarity measure to return a high value for them:"}
{"pdf_id": "0807.1560", "content": "similarity: a general IDF, an AAN-specific IDF where IDF values are calculated only using the documents of AAN, and finally DP-specific IDF in which we only used all-DP data set. Table 4 also shows the results for an asymmetric similarity measure, generation probability (Erkan, 2006) aswell as two string edit distances: the longest common substring and the Levenshtein distance (Lev enshtein, 1966). Methodology"}
{"pdf_id": "0807.1560", "content": "• Cluster Round-Robin (C-RR) We start with the largest cluster, and extract sentences in the order they appear in each cluster. So we extract first, the first sentences from each cluster, then the second ones, and so on, until we reach the summary length limit |S|. Previously, we mentioned that factswith higher weights appear in greater number of sentences, and clustering aims to clus ter such fact-sharing sentences in the same"}
{"pdf_id": "0807.1560", "content": "We also conducted experiments with two baseline approaches. To produce the citation summary weused Mead's (Radev et al., 2004) Random Sum mary and Lexrank (Erkan and Radev, 2004) on the entire citation summary network as baseline techniques. Lexrank is proved to work well in multi-document summarization (Erkan and Radev, 2004). It first builds a lexical network, in which"}
{"pdf_id": "0807.1560", "content": "E06-1011:21 5.2 Czech Results For the Czech data, we used the predefined train- ing, development and testing split of the Prague Dependency Treebank (Hajic et al, 2001), and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from Collins et al (1999)."}
{"pdf_id": "0807.1560", "content": "In this work we use the citation summaries to un derstand the main contributions of articles. The citation summary size, in our experiments, ranges from a few sentences to a few hundred, of which we pick merely a few (5 in our experiments) most important ones. As a method of summarizing a scientific paper,we propose a clustering approach where commu nities in the citation summary's lexical networkare formed and sentences are extracted from sep arate clusters. Our experiments show how ourclustering method outperforms one of the current state-of-art multi-document summarizing al gorithms, Lexrank, on this particular problem.A future improvement will be to use a reorder ing approach like Maximal Marginal Relevance"}
{"pdf_id": "0807.1560", "content": "(MMR) (Carbonell and Goldstein, 1998) to re-rank clustered documents within each cluster in orderto reduce the redundancy in a final summary. Another possible approach is to assume the set of sentences in the citation summary as sentences talking about the same event, yet generated in different sources. Then one can apply the method inspired by (Barzilay et al., 1999) to identify com mon phrases across sentences and use language generation to form a more coherent summary. Theultimate goal, however, is to produce a topic sum marizer system in which the query is a scientific topic and the output is a summary of all previous works in that topic, preferably sorted to preserve chronology and topicality. Acknowledgments"}
{"pdf_id": "0807.2047", "content": "The translation of unity length between the two centres of the cameras may be understood as imaging on the unity sphere its center. The translation has only 2 degree of freedom, and for that reason, with the relative orientation, the scale cannot be determined. The equation of the unity sphere is the following :"}
{"pdf_id": "0807.2047", "content": "The rotation matrix (R) in the 3D space has 3 degree of freedom. It is thus pos sible to express it with 3 parameters. However several representations with morethan 3 parameters exist. The algebraic model will depend on the chosen repre sentation. In the following part the main models for the coplanarity constraint are described."}
{"pdf_id": "0807.2047", "content": "Representation of the rotation using quaternions (4 parameters) A quaternion is composed of 4 parameters, q = (a, b, c, d)t, with the vector part being (b, c, d). The quaternions provide a simple representation of the rotation. Indeed with the parameters of a unity quaternion , the matrix of rotation can be expressed in the following manner :"}
{"pdf_id": "0807.2047", "content": "Model with 6 equations While using the Thompson rotation matrix, the rotation is expressed with 3 parameters. The system will have 6 unknowns, considering the three parameters of translation. The polynomial expressing the coplanarity constraint for a couple of homologous points, taking for model the Thompson rotation, is the following :"}
{"pdf_id": "0807.2047", "content": "To quantify the performances of the presented method, synthetic data have been simulated. The parameters used for the simulations, are the same as Nister's ones. The images size is 352 x 288 pixels (CIF). The field of view is 45 degrees wide. The distance to the scene is equal to 1. Several cases have been treated :"}
{"pdf_id": "0807.2047", "content": "Planar Structure and short base . Several surfaces are known as \"dan gerous\" [36] the reason of this appellation is due to the fact that if the points chosen for the evaluation of the relative orientation are on this kind of surface,the configuration becomes degenerate. In the following, one of the most unfavor able configurations has been chosen. We note that the method of the 5 points of Stewenius is not robust in the sideways motion case. Besides, Sarkis [13] has"}
{"pdf_id": "0807.2928", "content": "Consider Fig. 1. Why do we perceive in these visual stimuli a cluster of points, a straight contour and a river? How is the identification performed between a subgroup of stimuli and the perceived objects? These classical questions can be addressed from a variety of point of views, both biological"}
{"pdf_id": "0807.2928", "content": "Many physiological studies, e.g. [12, 17, 23], have shown evidence of grouping in visual cortex. Gestalt psychology [49, 31, 20, 9], an attemptto formalize the laws of visual perception, addresses some grouping princi ples such as proximity, good continuation and color constancy, in order to describe the construction of larger groups from atomic local information in the stimuli."}
{"pdf_id": "0807.2928", "content": "Oscillators i and j are said to be synchronized if xi remains equal to xj. Once the elements are synchronized, the coupling terms disappear, so that each individual elements exhibits its natural, uncoupled behavior, as illustrated in Fig. 2. It is intuitive to see that a larger kij value facilitates and reinforces the synchronization between the oscillators i and j (refer to Appendix for more details)."}
{"pdf_id": "0807.2928", "content": "Recall that a subset of the global state space is called invariant if trajec tories that start in that subset remain in that subset. In our synchronization context, the invariant subsets of interest are linear subspaces, corresponding to some components of the overall state being equal or verifying some linearrelation. Concurrent synchronization analysis quantifies stability and conver gence to invariant linear subspaces. Furthermore, a property of concurrent synchronization analysis, which turns out to be particularly convenient in the context of grouping, is that the actual invariant subset itself need not be know a priori to guarantee stable convergence to it."}
{"pdf_id": "0807.2928", "content": "Traces of synchronized oscillators coincide in time, while those of desyn chronized groups are separated [42]. The identification of synchronization in the oscillation traces (as illustrated in the example of Fig. 4-b) can be realized by thresholding the correlation of the traces or by simply applying a clustering algorithm such as k-means."}
{"pdf_id": "0807.2928", "content": "Fig. 4 illustrates an example in which the points make clearly two clusters. As shown in Fig. 4-b, the oscillator system converges to two concurrently synchronized groups, each corresponding to one cluster, and separated in the time dimension. The identification of the two groups induces the clustering of the underlying points, as shown in Fig. 4-c."}
{"pdf_id": "0807.2928", "content": "Field and his colleagues [12] have shown some interesting experiments, anexample being illustrated in Fig. 6, to test human capacity of contour inte gration, i.e. of identifying a path within a field of randomly-oriented elementsand made some quantitive observations in accordance with the \"good con tinuation\" law [49, 20, 31]:"}
{"pdf_id": "0807.2928", "content": "The proposed image segmentation scheme is based on concurrent synchro nization [37] and follows the general visual grouping algorithm described in section 2.5. In the basic version, the coupling gain between oscillators are again inspired directly from more standard techniques, namely non-local grouping as applied e.g. to in image denoising [3, 4] in addition to the gestaltlaws. Multi-layer neural networks and feedback mechanisms are then introduced to reinforce robustness under strong noise perturbation and to ag gregate the grouping. Experiments on both synthetic and real images are shown."}
{"pdf_id": "0807.2928", "content": "where ui is the pixel gray-level at coordinates i = (i, j) and w adjusts the size of the neighborhood. Pixels with similar grey-levels are coupled more tightly, as suggested by the color constancy gestalt law [49, 20, 31]. Non-local coupling plays an important role in regularizing the image segmentation, with a larger w resulting in more regularized segmentation and higher robustness to noise."}
{"pdf_id": "0807.2928", "content": "segmentation result of the basic algorithm without feedback shown in Fig. 15 b contains a few punctual errors and, more importantly, the contour of thesegmented objected zigzags due to the strong noise perturbation. As illus trated in Fig. 15-c, the feedback procedure corrected the punctual errors and regularized the contour."}
{"pdf_id": "0807.3483", "content": "The proposed codification is more practical for computing union and inter section operations and the DSm cardinality, because only one integer representone of the distinct parts of the Venn diagram. With the Smarandache's codi fication computing union and intersection operations and the DSm cardinality could be very similar than with the practical codification, but adding a routine in order to treat the code of one part of the Venn diagram."}
{"pdf_id": "0807.3483", "content": "% Code Theta for DSmT framework % [Theta,Scod]=codingTheta(n) % Input: % n = cardinality of Theta % Outputs: % Theta = the liste of coded elements in Theta % Scod = the bijection function between the integer of the coded elements in Theta and the Smarandache codification % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% Code ThetaR the reduced form of Theta % taking into account the constraints given by the user % [ThetaR]=addConstraint(constraint,Theta) % Inputs: % constraint = the list of element considered as constraint or '2T' to work on 2^Theta % Theta = the description of Theta after coding % Output: % ThetaR = the description of coded Theta after reduction % taking into account the constraints % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% Code the focal element for DSmT framework % [focalC]=codingFocal(focal,Theta) % Inputs: % focal = the list of focal element for one expert % Theta = the description of Theta after coding % Output: % focalC = the list of coded focal element for one expert % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% [expertC]=codingExpert(expert,Theta) % Inputs: % expert = structure containing the list of focal elements for each expert and the bba corresponding % Theta = the description of Theta after coding % Output: % expertC = structure containing the list of coded focal element for each expert and the bba corresponding % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "The function 7 proposes many combination rules. Most of them are based on the function 8, but for some combination rules we need to keep more information,so we use the function 9 for the conjunctive combination. E.g. in the func tion 10 note the simplicity of the code for the PCR6 combination rule. Other combination rules' codes are not given here for the sake of clarity."}
{"pdf_id": "0807.3483", "content": "% Give the combination of many experts % [res]=combination(expert,constraint,n,criterium) % Inputs: % expertC = containt the structure of the list of focal elements and corresponding bba for all the experts % ThetaR = the coded and reduced discernment space % criterium = is the combination criterium criterium=1 Smets criterium (conjunctive rule in open world) criterium=2 Dempster-Shafer criterium (normalized) (conjunctive rule in closed world) criterium=3 Yager criterium criterium=4 disjunctive combination criterium criterium=5 Florea criterium criterium=6 PCR6 criterium=7 Mean of the bbas"}
{"pdf_id": "0807.3483", "content": "criterium=8 Dubois criterium (normalized and disjunctive combination) criterium=9 Dubois and Prade criterium (mixt combination) criterium=10 Mixt Combination (Martin and Osswald criterium) criterium=11 DPCR (Martin and Osswald criterium) criterium=12 MDPCR (Martin and Osswald criterium) criterium=13 Zhang's rule % Output: % res = containt the structure of the list of focal elements and corresponding bbas for the combinated experts % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% Conjunctive Rule % [res]=conjunctive(expert) % Inputs: % expert = containt the structures of the list of focal element and % corresponding bba for all the experts % Output: % res = is the resulting expert (structure of the list of focal element and corresponding bba) % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% Conjunctive Rule conserving all the focal elements % during the combination % [res,tabInd]=globalConjunctive(expert) % Input: % expert = containt the structures of the list of focal element and % corresponding bba for all the experts % outputs: % res = is the resulting expert (structure of the list of focal element and corresponding bba) % tabInd = table of the indices given the combination % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% PCR6 combination rule % [res]=PCR6(expert) % Input: % expert = containt the structures of the list of focal element and % corresponding bba for all the experts % Output: % res = is the resulting expert (structure of the list of focal element and corresponding bba) % Reference: A. Martin and C. Osswald, ''A new generalization of the proportional conflict redistribution rule stable in terms of decision,'' Applications and Advances of DSmT for Information Fusion, Book 2, American Research Press Rehoboth, F. Smarandache and J. Dezert, pp. 69-88 2006. % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% Give the decision for one expert % [decFocElem]=decision(expert,Theta,criterium) % Inputs: % expert = containt the structure of the list of focal elements and corresponding bba for all the experts % Theta = list of coded (and reduced with constraint) of the elements of the discernement space % criterium = is the combination criterium criterium=0 maximum of the bba criterium=1 maximum of the pignistic probability criterium=2 maximum of the credibility criterium=3 maximum of the credibility with reject criterium=4 maximum of the plausibility criterium=5 DSmP criterium criterium=6 Appriou criterium"}
{"pdf_id": "0807.3483", "content": "criterium=7 Credibility on DTheta criterium criterium=8 pignistic on DTheta criterium % elemDec = list of elements on which we can decide, or A for all, S for singletons only, F for focal elements only, SF for singleton plus focal elements, Cm for given specificity, 2T for only 2^Theta (DST case) % Output: % decFocElem = the retained focal element, 0 in case of rejet, -1 if the decision cannot be taken on elemDec % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "decFocElem=MaxFoc(DSmP,elemDecC,type); case 6 % Appriou criterium [Pl]=plausibility(expertDec); lambda=1; r=0.5; bm=BayesianMass(expertDec,lambda,r); Newbba=Pl.Pl.*bm.bba; % normalization Newbba=Newbba/sum(Newbba); funcDec.focal=Pl.focal; funcDec.bba=Newbba; decFocElem=MaxFoc(funcDec,elemDecC,type); case 7 % Credibility on DTheta criterium [Bel]=credibility(expertDec); lambda=1; r=0.5; bm=BayesianMass(expertDec,lambda,r); Newbba=Bel.Bel.*bm.bba; % normalization Newbba=Newbba/sum(Newbba); funcDec.focal=Bel.focal; funcDec.bba=Newbba; decFocElem=MaxFoc(funcDec,elemDecC,type); case 8 % pignistic on DTheta criterium [BetP]=pignistic(expertDec); lambda=1; r=0.5; bm=BayesianMass(expertDec,lambda,r); Newbba=BetP.BetP.*bm.bba; % normalization Newbba=Newbba/sum(Newbba); funcDec.focal=BetP.focal; funcDec.bba=Newbba; decFocElem=MaxFoc(funcDec,elemDecC,type); otherwise 'Accident: in decision choose of criterium: uncorrect' end end"}
{"pdf_id": "0807.3483", "content": "% Find the element of DTheta with the minium of specifity minSpe % and the maximum maxSpe % [elemDecC]=findFocal(Theta,minSpe,maxSpe) % Input: % Theta = list of coded (and eventually reduced with constraint) of the elements of the discernment space % minSpe = minimum of the wanted specificity % minSpe = maximum of the wanted specificity % Output: % elemDec = list of elements on which we want to decide with the minimum of specifity minSpe and the maximum maxSpe % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% Generalized Pignistic Transformation % [BetP]=pignistic(expert) % Input: % expert = containt the structures of the list of focal element and corresponding bba for all the experts % expert.focal = list of focal elements % expert.bba = matrix of bba % Output: % BetP = containt the structure of the list of focal element and the matrix of the plausibility corresponding % BetP.focal = list of focal elements % BetP.BetP = matrix of the pignistic transformation"}
{"pdf_id": "0807.3483", "content": "% Credibility function % [Bel]=credibility(expert) % Input: % expert = containt the structures of the list of focal element and corresponding bba for all the experts % expert.focal = list of focal elements % expert.bba = matrix of bba % Output: % Bel = containt the structure of the list of focal element and the matrix of the credibility corresponding % Bel.focal = list of focal elements % Bel.Bel = matrix of the credibility"}
{"pdf_id": "0807.3483", "content": "% Plausibility function % [Pl]=plausibility(expert) % Input: % expert = containt the structures of the list of focal element and corresponding bba for all the experts % expert.focal = list of focal elements % expert.bba = matrix of bba % Output: % Pl = containt the structure of the list of focal element and the matrix of the plausibility corresponding % Pl.focal = list of focal elements % Pl.Pl = matrix of the plausibility"}
{"pdf_id": "0807.3483", "content": "% DSmP Transformation % [DSmP]=DSmPep(expert,epsilon) % Inputs: % expert = containt the structures of the list of focal element and corresponding bba for all the experts % expert.focal = list of focal elements % expert.bba = matrix of bba % epsilon = epsilon coefficient % Output: % DSmPep = containt the structure of the list of focal element and the matrix of the plausibility corresponding % DSmPep.focal = list of focal elements % DSmPep.DSmP = matrix of the pignistic transformation % Reference: Dezert & Smarandache, ''A new probbilistic transformation of belief mass assignment'', fusion 2008, Cologne, Germany. % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% for only one after combination) % Theta = list of coded (and reduced with constraint) of the elements of the discernement space % DTheta = list of coded (and reduced with constraint) of the elements of DTheta % Output: % expertDecod = containt the structure of the list of decoded (for human) focal elements and corresponding bba for all the experts % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% The goal of this function is to decode the focal elements % [focalDecod]=decodingFocal(focal,elemDec,Theta) % Inputs: % expert = containt the structure of the list of focal elements after % combination and corresponding bba for all the experts % elemDec = the description of the subset of uncoded elements for decision % Theta = list of coded (and reduced with constraint) of the elements of the discernement space % DTheta = list of coded (and reduced with constraint) of the elements of DTheta, eventually empty if not necessary % Output: % focalDecod = containt the list of decoded (for human) focal elements % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% The goal of this function is to code the focal elements in % expert with the Smarandache's codification from the practical % codification in order to display the expert % [expertDecod]=cod2ScodExpert(expert,Scod) % Inputs: % expert = containt the structure of the list of focal elements after % combination and corresponding bba for all the experts (generally use % for only one after combination) % Scod = list of distinct part of the Venn diagram coded with the Smarandache's codification % Output: % expertDecod = containt the structure of the list of decoded (for human) focal elements and corresponding bba for all the experts % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% expert = containt the structure of the list of focal elements after combination and corresponding bba for all the experts % Scod = list of distinct part of the Venn diagram coded with the Smarandache's codification % Output: % focalDecod = containt the list of decoded (for human) focal elements % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3483", "content": "% Generation of DThetar: modified and adapted code from % Dezert & Smarandache Chapter 2 DSmT book % Vol 1 to generate DTeta % [DTheta]=generationDThetar(Theta) % Input: % Theta = list of coded (and eventually reduced with constraint) of the elements of the discernment space % Output: % DTheta = list of coded (and eventually reduced with constraint in this case some elements can be the same) of the elements of the DTheta % Copyright (c) 2008 Arnaud Martin"}
{"pdf_id": "0807.3669", "content": "Abstract— In this paper, we propose in Dezert-Smarandache Theory (DSmT) framework, a new probabilistic transformation, called DSmP, in order to build a subjective probability measure from any basic belief assignment defined on any model of the frame of discernment. Several examples are given to show how the DSmP transformation works and we compare it to main existing transformations proposed in the literature so far. We show the advantages of DSmP over classical transformations in term of Probabilistic Information Content (PIC). The direct extension of this transformation for dealing with qualitative belief assignments is also presented. Keywords: DSmT, Subjective probability, Probabilistic Information Content, qualitative belief."}
{"pdf_id": "0807.3669", "content": "4For notation convenience and simplicity, we use a different but equivalent notation than the one in [15]. 5For example, f(.) must be replaced by P l(.) in (4) or by Bel(.) in (5). 6i.e. the mass committed to partial and total ignorances, i.e. to disjunctions of elements of the frame."}
{"pdf_id": "0807.3669", "content": "From the extension of the isomorphism between the set of linguistic equidistant labels and a set of numbers in the interval [0, 1], one can built exact operators on linguistic labels which makes possible the extension all the quantitative fusion rulesand probabilistic transformations into their qualitative coun terparts [3]. We brieny remind the main qualitative operators (or q-operators for short) on linguistic labels:"}
{"pdf_id": "0807.3669", "content": "(24)where all operations in (24) are referred to labels, that is q operators on linguistic labels defined in IX-B and not classicaloperators on numbers. In the same manner, due to our con struction of labels and qualitative operators, we can transform any quantitative fusion rule (or arithmetic expression) into a qualitative fusion rule (or qualitative expression)."}
{"pdf_id": "0807.3755", "content": "We thank the Linguistic Data Consortium, University of Pennsylvania and Google, Inc. for providing the \"Web 1T 5-gram Version 1\" dataset. We also thank the WaCky community for providing the ukWaC dataset. Further we would like to thank Thorsten Brants from Google Inc. for promptly answering our emails and helping to clarify questions on the Google N-gram corpus."}
{"pdf_id": "0807.3908", "content": "In this way, an RVM can traverse the Linked Data set not by pulling data to a local environment, but by actually moving between machines and more specifically, moving to those machines that are maintaining the subgraph of the Semantic Web that is of interest to the algorithm at particular points in time"}
{"pdf_id": "0807.3908", "content": "It is important to ensure that poorly or maliciously written RDF code does not destroy the integrity of an RDF data set, does not abuse the computational resources of a publicly available physical machine, and only accesses those aspects of an RDF data set that it has permission to access"}
{"pdf_id": "0807.3908", "content": "is possible for the virtual machine and the compiled code to be relocated by simply downloading the RDF subgraph to another environment. Thus, instead of migrating large amounts of data to a local environment for processing, the RDF virtual machine and code can be migrated to the remote environment. In this way, the process is moved to the data, not the data to the process."}
{"pdf_id": "0807.4417", "content": "Abstract. We discuss metacognitive modelling as an enhancement to cognitive modelling and computing. Metacognitive control mechanisms should enable AI systems to self-renect, reason about their actions, and to adapt to new situations. In this respect, we propose implementationdetails of a knowledge taxonomy and an augmented data mining life cy cle which supports a live integration of obtained models."}
{"pdf_id": "0807.4417", "content": "We view introspective reports as data to be explained, in contrast to the Structuralists' view of introspective reports as descriptions of internal processes; i.e., we regard introspection not as a conduit to the mind but rather as a source of data to be accounted for by postulated internal processes.4"}
{"pdf_id": "0807.4417", "content": "In order to integrate learning schemes—i.e. to learn meta-level action strategies from experience—we propose a meta knowledge taxonomy (figure 1). Consider a world (W) and a modeller (M) who exists in the world, and who can be a human or an intelligent computer agent. A knowledge taxonomy can be constructed to include the modelling of the world and the modeller (according to some articles in [12]). In this paper, we provide the implementations of this knowledge taxonomy by using semantic technologies and machine learning."}
{"pdf_id": "0807.4417", "content": "In subsequent applications of the augmented CRISP cycles, the introspective models can be combined with the models of the former CRISP process. It is important to note that empirical machine learning models are pattern patching systems; we expect the behaviour to be improved by drawing an analogy to a past experience which materialises as patterns to be mined. These patternsdo not necessarily follow logical rules in terms of a higher order logic—but in stead, they should follow at least the causal implications of a propositional logic which helps to implement reactivity based on learned causality. All patterns to be mined can be regarded as introspective reports on the application or business domain."}
{"pdf_id": "0807.4417", "content": "The question we investigated was about the scope and usefulness of a metacogni tive model. In order to develop a computational introspective model, empirical machine learning models can be investigated. This should augment cognitive capabilities of adaptable AI systems, especially in the reasoning phase before action taking, which we believe requires to a great extent metacognitive instead of cognitive capabilities.Similar methodology in computation has received great attention for uncertainty handling, control in decentralised systems, scheduling for planning in real"}
{"pdf_id": "0807.4417", "content": "time, and meta-level reasoning in general [13]. Applications are to be found in the contexts of large-scale natural language processing architectures for texts (e.g., UIMA [14]), and dialogical interactions with the Semantic Web (e.g., SmartWeb [15] integrating extensive ontological groundwork [16] for self-representation ofan information state to be included into a metacognitive model). The metacogni tive control and augmented Data Mining Cycle proposed here will be integrated into a new situation-aware dialogue shell for the Semantic Access to Media and Services in the near future—to handle, fore and foremost, the access to dynamic, heterogeneous information structures."}
{"pdf_id": "0807.4417", "content": "Acknowledgements. This research has been supported in part by the THE SEUS Program in the Core Technology Cluster WP4 Situation Aware Dialogue Shell for the Semantic Access to Media and Services, which is funded by the German Federal Ministry of Economics and Technology under the grant number 01MQ07016. The responsibility for this publication lies with the author."}
{"pdf_id": "0807.4478", "content": "In summary, use of passive video cameras to sense direction and distance results in distinct advantages in a mission  such as OLEV, which has to interact with satellites already in orbit and not equipped with navigational aids to ease  docking, and where low mass and power consumption is a primary requirement"}
{"pdf_id": "0807.4478", "content": "•  the client satellite image (or the part of it selected for autonomous detection/tracking) at the closest limit of the  range is small enough to allow complete viewing in the camera's field of view (FoV) with a safety margin of  around half of the image."}
{"pdf_id": "0807.4478", "content": "The complete cycle of image downloading, processing and transmission of the determined angular position and distance  to the OLEV control system is scheduled to last a maximum of 1 second, which sets an important limitation to the type  of image processing algorithms that could be used, even if the processing is performed with on-ground resources"}
{"pdf_id": "0807.4478", "content": "In  addition, the image processing algorithms have to deal robustly with factors inherent to the operational scenario, such as  noise, presence of a stellar background, variations in illumination and sometimes a considerably cluttered background,  caused by the appearance of bright objects (Earth, Moon) in the camera's FoV"}
{"pdf_id": "0807.4478", "content": "To cope with  these problems, SENER has designed an image processing chain based on the use of morphological gray-filters by  reconstruction [1-4], which have proven an excellent reliability and performance in environments as demanding as that  of automatic mine detection [5], and had been successfully used by SENER on automatic airborne inspection of  electrical power lines"}
{"pdf_id": "0807.4478", "content": "•  Automous Satellite detection  This function is used at the beginning of the RV manoeuvre, to detect and extract the shape of the client  satellite in an image captured by the far RV camera. The extracted shape location, size and attitude are used to  initialize the tracking procedure, which follows the target with sub-pixel precision during the approaching  manoeuvre. The detection procedure could also be applied periodically during tracking to obtain an  independent estimation of the satellite location parameters, for validation purposes."}
{"pdf_id": "0807.4478", "content": "•  Model-based satellite image tracking  Once the location of the client satellite is determined by the detection function, control is transferred to  tracking, which uses a wireframe model of the satellite to determine its location in the image with sub-pixel  precision. The model is translated, rotated and scaled in the framework of an optimization procedure, to obtain  the best possible matching with the perceived contours of the satellite in the image."}
{"pdf_id": "0807.4478", "content": "•  Sub-pixel determination of satellite location parameters  From the parameters (translation, rotation, scaling) of the best fitting model, the angular position, range  distance and relative attitude to the client satellite are determined. The determination of the best-fitting model  transformation parameters with sub-pixel precision is important to ensure an adequate accuracy in the derived  parameters. Particularly, this is the case of range determination from image scale when observing from the  distant limit of the operational range of a RV camera. At this distance, the client satellite image spans a few  pixels, with a large associated quantization error if image scale is determined with accuracy at just the pixel  level."}
{"pdf_id": "0807.4478", "content": "The image obtained by the closing by reconstruction filter could be taken as a background image, where all potential  objects of interest have been removed. Subtracting from this image the input data, we obtain the results of an operation  known as top-hat closing filtering by reconstruction, which here highlights satellite pixels together with those pixels in  the background fulfilling the same constraints in local contrast sign and shape size. The results of the top-hat filtering  are shown in the central panel of fig. 5."}
{"pdf_id": "0807.4478", "content": "•  Target class: formed (in this example) by pixels belonging to objects in the scene that present a negative  contrast with respect to the background and are smaller in size than the applied filter window. Ideally, this  class will comprise pixels contained in the satellite shape, together with pixels of other objects in the  background fulfilling the same criteria."}
{"pdf_id": "0807.4478", "content": "Pixels in the target class are enhanced by the morphological filtering operation and, hence, will appear in principle in  the upper part of the gray-level histogram, whereas the background pixels will form in the histogram a large lobe close  to the origin. In fig. 6 (left panel) is presented the histogram of the top-hat filtered image, where this hypothesis is"}
{"pdf_id": "0807.4478", "content": "In this case, the main components of the target spacecraft are known to present a circular (satellite body) and a  rectangular shape (solar panel) as seen from the approaching trajectory. Hence, pre-selected regions are evaluated using  a measure of circularity, such as compactness, and a measure of rectangularity, such as the ratio of the region's area to  that of the minimum bounding rectangle. In fig. 8 are presented the values of these attributes for the spacecraft  components and for several regions of the background. The significant difference in feature values for both classes  confirms the possibility of performing a final reliable filtering stage based on this criterion."}
{"pdf_id": "0807.4478", "content": "Fig. 9. Automatic detection of the target satellite on imagery captured during the ESA's ATV rendez-vous  manoeuvre with the International Space Station. First column: input images; second column: results after  morphological processing; third column: results after region filtering based on a combined area and contrast  criterion."}
{"pdf_id": "0807.4478", "content": "GNC 2008  7th International ESA Conference on Guidance, Navigation & Control Systems  2-5 June 2008, Tralee, County Kerry, Ireland  Model-based image tracking and parameter determination  Once the image of the client satellite has been detected, the control is handled to the tracking module, which uses a  simplified model of the object to follow its evolution during the video sequence of the approach"}
{"pdf_id": "0807.4478", "content": "A figure of merit of the alignment between model and image is computed in terms of the degree of matching between  projected model lines and image contours. A numerical optimization process using the simplex downhill algorithm is  carried out in the parameter space to bring this alignment measure to a local maximum. The optimal projection  parameters (position, scale, angle) provide the necessary information to compute the angular position of the target and  its distance and orientation relative to the chaser vehicle."}
{"pdf_id": "0807.4478", "content": "Fig. 11. Results of the model-based image tracking in an Orbital Express sequence.  Simulated RV trajectories using image-based navigation  SENER is currently implementing a generic simulator for the rendez-vous and docking manoeuvre to validate the  integration of the data provided by the described image processing module with the control laws and procedures  designed to guide the manoeuvre. In fig. 12 is presented a diagram of the simulator, including modules to describe the  spacecraft dynamics, sensors, Kalman filtering stage [10, 11], actuators and AOCS/GNC control and guidance laws."}
{"pdf_id": "0807.4478", "content": "GNC 2008  7th International ESA Conference on Guidance, Navigation & Control Systems  2-5 June 2008, Tralee, County Kerry, Ireland  [5]  A. Banerji and J. Goutsias, \"A Morphological Approach to Automatic Mine Detection Problems\", IEEE  Transactions on Aerospace and Electronic Systems, vol. 34 (4), pp. 1085-1096, 1998."}
{"pdf_id": "0807.4680", "content": "Figura 1: En el diagrama cada exocomportamiento se representa con un punto cuyo colorexpresa el tipo de exocomportamiento. Los exocomportamientos elementales tienen asig nados los colores primarios. Los exocomportamientos aleatorios se pintan con el color rojo, los posicionales con verde y los sensibles con azul. Los conjuntos de exocomportamiento elementales son disjuntos entre ellos."}
{"pdf_id": "0807.4701", "content": "Abstract Colour and coarseness of skin are visually different. When image processing is involved in the skin analysis, it is important to quantitatively evaluate such differences using texture features. In this paper, we discuss a texture analysis and measurements based on a statistical approach to the pattern recognition. Grain size and anisotropy are evaluated with proper diagrams. The possibility to determine the presence of pattern defects is also discussed."}
{"pdf_id": "0807.4701", "content": "2. Image analysis To each pixel at the arbitrary point  ,x y)  in the image frame we associate a grey tone b ranging from 0 to 255: ( ,x y)  is then a 2-dimensional function representative of the image intensity (brightness) distribution. Starting from function ( ,x y), which gives the pixel grey tone, the following calculation can be performed. First of all, the mean intensity of the pixel tones is determined:"}
{"pdf_id": "0807.4701", "content": "With this kind of characterisation we are then able to define the average values of the moments for the whole image frame. The distribution of pixel tones is then given according to these moments. The tone dispersion turns out to be evaluate by moment with k=2. All integrals can be calculated on the whole image or on a window. In the case of windowing the image, moments  M and  M allow to find position and shape of objects, because the distribution can change for each specific window. In images where, at a first glance no particular objects are present, we can use the same values of the moments  M and"}
{"pdf_id": "0807.4701", "content": "Instead of measuring the homogeneity, by evaluating the histogram's entropy of intensity difference versus distance from a point of the image frame (see for instance [15]), or by calculating the spatial organisation by means of `run-length statistics' [16,17], we compute a set of coherence lengths defined in the following way"}
{"pdf_id": "0807.4701", "content": "lengths\"1  ,x y) l i,  and  ,x y) k i,  of point P in the image frame. The choice of threshold value t depends on the problem under study. In the calculation of the functions  ,x y) l i,  and  ,x y) k i, , the pixels near the image frame boundary are not involved, because in this case it would not be possible to estimate the coherence lengths in all the directions (boundary effect). On the contrary, in standard image processing techniques [18], periodicity of the image, originally present or artificially introduced by replication of the frame, is used to overcome the boundary problem. Let us"}
{"pdf_id": "0807.4701", "content": "stress the fact that moments  ,x y) M i  and  ,x y) M i  are not calculated on a window in the image frame, but on specific directions: therefore the method is different from the standard statistical approach, allowing to take into account, in a natural way, the anisotropy in the problem of texture recognition. In our analysis, we will use the 32 directions of Fig.1. Actually, we can look for anomalous behaviours of vectors  ,x y) l i,  or  ,x y) k i,  as signals of the presence of a defect at the position in the image frame corresponding to given point"}
{"pdf_id": "0807.4701", "content": "If the image frame were strictly homogeneous, such averaged lengths should coincide with the actual local lengths measured for all image points. On the other hand, if the image frame were completely inhomogeneous, the local lengths would be very dispersed around their averages. The same occurs when the image frame is shared in windows, each of them characterised by a different intensity distribution. It is acceptable to average the coherence length over the whole image frame if the image can be considered as characterised by one distribution only, within a reasonable dispersion. The lengths  Lo i,  represent the distance from"}
{"pdf_id": "0807.4701", "content": "a generic point ( ,x y)  along the i-direction, at which the average value of the image intensity is practically reached: this means that the distance is dependent on the threshold level. In Figure 2, the average values  Lo i,  for two images of snake skins from the Brodatz album are"}
{"pdf_id": "0807.4701", "content": "reported. The result is a diagram showing  Lo i,  in the 32 directions of Fig.1. We can define this diagram has the \"coherence length diagram\". In fact, the figure shows two diagrams obtained by fixing two different threshold values. To obtain the inner diagram we use a threshold corresponding to the 50% of ratio  M 2 Mo . The outer diagram is obtained with the 20% of the same ratio. The diagrams reveal preferential directions in the image texture, that is the anisotropy of the texture. In this paper, we consider just  Lo i, , because this is giving the most visually appreciable"}
{"pdf_id": "0807.4701", "content": "With the analysis here discussed, the detection of defects is a comparison of the local coherence lengths  ,x y) l i, , that is of a local unit cell, with the coherence length  Lo i, diagram, the global unit cell, which is shown in the middle of Figure 3"}
{"pdf_id": "0807.5091", "content": "channel access and transmissions in wireless networks. Mes sage passing algorithms provide a promising alternative to current scheduling algorithms. Another, equally important, motivation is the potentialfor obtaining new insights into the performance of exist ing message-passing algorithms, especially on loopy graphs. Tantalizing connections have been established between suchalgorithms and more traditional approaches like linear pro gramming (see [1], [2] [8] and references therein). We consider MWIS problem to understand this connection as it provides a rich (it is NP-hard), yet relatively (analytically) tractable, framework to investigate such connections."}
{"pdf_id": "0807.5091", "content": "We now brieny state some of the well-known properties of the MWIS LP, as these will be used/referred to in the paper. The polytope of the LP is the set of feasible points for the linear program. An extreme point of the polytope is one that cannot be expressed as a convex combination of other points in the polytope. Lemma 2.1: ( [12], Theorem 64.7) The LP polytope has the following properties 1) For any graph, the MWIS LP polytope is half-integral: any extreme point will have each xi = 0, 1 or 1"}
{"pdf_id": "0807.5091", "content": "so that (a) there is no interference, and (b) nodes which have a large amount of data to send are given priority. In particular, it is well known that if each node is given a weight equal to the data it has to transmit, optimal network operation demands scheduling the set of nodes with highest total weight. If a \" connict graph\" is made, with an edge between every pair of interfering nodes, the scheduling problem is exactly the problem of finding the MWIS of the connict graph. The lack of an infrastructure, the fact that nodes often have limited capabilities, and the local nature of communication, all necessitate a lightweight distributed algorithm for solving the MWIS problem."}
{"pdf_id": "0807.5091", "content": "In the last section, we saw that fixed points of Max-product may correspond to optima \"wrong\" linear programs: ones that operate on the same feasible set as LP, but optimize a different linear function. However, there will also be fixed points that correspond to optimizing the correct function. Max-product is a deterministic algorithm, and so which of these fixed points"}
{"pdf_id": "0807.5091", "content": "In Section V we saw that max-product started from the natural initial condition solves the correct LP at the fixed point, if it converges. However, convergence is not guaranteed, indeed it is quite easy to construct examples where it will notconverge. In this section we present a convergent message passing algorithm for finding the MWIS of a graph. It is based on modifying max-product by drawing upon a dual co-ordinate descent and the barrier method. The algorithm retains the iterative and distributed nature of max-product. The algorithm operates in two steps, as described below."}
{"pdf_id": "0807.5091", "content": "Now, consider a version of EST where we check for updating nodes in a round-robin manner. That is, in an iteration we peform O(n) operations. Now, we state a simple bound on running time of EST. Lemma 6.4: The algorithm EST stops after at most O(n) iterations. Proof: The algorithm stops after the iteration in which no more node's status is updated. Since each node can be updated at most once, with the above stopping condition an algorithm can run for at most O(n) iterations. This completes the proof of Lemma 6.4."}
{"pdf_id": "0807.5091", "content": "We believe this paper opens several interesting directions for investigation. In general, the exact relationship between max-product and linear programming is not well understood. Their close similarity for the MWIS problem, along with the reduction of MAP estimation to an MWIS problem, suggests that the MWIS problem may provide a good first step in an investigation of this relationship. Our novel message-passing algorithm and the reduction of MAP estimation to an MWIS problem immediately yields a new message-passing algorithm for general MAP estimation problem. It would be interesting to investigate the power of this algorithm on more general discrete estimation problems."}
{"pdf_id": "0808.0056", "content": "semantics is assigned to an image by a human observer. That is strongly at variance with  the contemporary views on the concept of semantic information.  Following the new information elicitation rules, it is impossible to continue to pretend that  semantics can be extracted from an image, (as for example in (Naphade & Huang, 2002)), or  should be derived from low-level information features (as in (Zhang & Chen, 2003;  Mojsilovic & Rogowitz, 2001), and many other analogous publications). That simply does  not hold any more."}
{"pdf_id": "0808.0056", "content": "Ahissar, M. & Hochstein, S. (2004). The reverse hierarchy theory of visual perceptual  learning, Trends in Cognitive Science, vol. 8, no. 10, pp. 457-464, 2004.  Barsalou, L.W. (1999). Perceptual symbol systems, Behavioral and Brain Sciences, vol. 22, pp.  577-660, 1999.  Biederman, I. (1987). Recognition-by-Components: A Theory of Human Image  Understanding, Psychological Review, vol. 94, no. 2, pp. 115-147, 1987."}
{"pdf_id": "0808.0103", "content": "For the second part of our analysis we zoom in on usage data, to see how reader ship varies per geographical region. In the previous section, we mentioned that our data logs also record the origin of requests. This allows us to determine use as a function of geographical region. Since science and technology depend heavily on budgets, it is particularly interesting to look at the readership in a"}
{"pdf_id": "0808.0103", "content": "• an economic vulnerability criterion, involving a composite Economic Vul nerability Index (EVI) based on indicators of: (a) population size; (b)remoteness; (c) merchandise export concentration; (d) share of agricul ture, forestry and fisheries in gross domestic product; (e) homelessness owing to natural disasters; (f) instability of agricultural production; and (g) instability of exports of goods and services."}
{"pdf_id": "0808.0103", "content": "To be added to the list, a country must satisfy all three criteria. In addition, since the fundamental meaning of the LDC category, i.e. the recognition of structural handicaps, excludes large economies, the population must not exceed 75 million. To become eligible for graduation, a country must reach threshold levels for graduation for at least two of the aforementioned three criteria, or its GNI per capita must exceed at least twice the threshold level, and the likelihood that the level of GNI per capita is sustainable must be deemed high."}
{"pdf_id": "0808.0112", "content": "In order to stress that the necessity of advancing a novel variant of decision theory, the QDT presented here, is not just \"theory-driven\" but is fundamentally \"problem-driven\", with the aim of resolving the existing paradoxes, we describe below some of the most often discussed paradoxes occurring in classical decision making"}
{"pdf_id": "0808.0112", "content": "if classical utility theory is to describe this situation. But, Eqs. (13) and (14) are in contradic tion with each other, which implies that there are no utility functions that would satisfy both these equations simultaneously. Such a paradox does not arise in QDT, as will be proved in Proposition 7."}
{"pdf_id": "0808.0112", "content": "The decision procedure described in the previous section, when applied to composite prospects containing composite actions, results in nontrivial consequences, often connected to the factthat the probability operators (34) for composite prospects correspond to entangling opera tions (Yukalov, 2003a,b,c). Several modes of a composite action can interfere, leading to the appearance of interference terms. The occurrence of several modes of an action implies the existence of uncertainty and of the perception of possible harmful consequences. In contrast, the elementary prospects (21) yield no interference. This is because the states of the elementary prospects are the basic states (25)."}
{"pdf_id": "0808.0112", "content": "Remark 7.1. The notions of \"gain\" and \"loss\" are assumed to have the standard meaning accepted in the literature on decision making. The same concerns the notions of \"being active\" and \"being passive\". The notion \"being active\" implies that the decision maker chooses to accomplish an act. While \"being passive\" means that the decision maker restrains from an action. For instance, in the Hamlet hesitation \"to be or not to be\", the first option \"to be\" implies activity, while the second possibility \"not to be\" means passivity."}
{"pdf_id": "0808.0112", "content": "Remark 7.3. We are careful to distinguish the concept of \"uncertainty or perceived po tential harm\" from \"risk\". Risk involves the combination of the uncertainty of a loss and of the severity or amplitude of that loss. In contrast, uncertainty and perceived potential harm that we consider in QDT emphasize more the subjective pain that a human subject visualizes in his/her mind when considering the available options and making a decision."}
{"pdf_id": "0808.0112", "content": "Remark 7.4. The interference alternation (50) shows that some of the interference terms are positive, while other are negative, so that the total sum of all these terms is zero. This means that the probability of prospects with larger uncertainty and/or perceived potential harm will be suppressed, while that of less uncertain and/or harmful prospects will be enhanced."}
{"pdf_id": "0808.0112", "content": "Let us consider two actions, A and X from the action ring A, with the action A being arbitrary and the action X being composite as in notation (52). By the definition of the action ring A, an action AXj implies joining two actions A and Xj to be accomplished together, with the probability p(AXj). The related conditional probability p(A|Xj) can be introduced in the standard manner (Feller, 1970) through the identity"}
{"pdf_id": "0808.0112", "content": "Definition 8.1. For the actions A and X from the action ring A, where A is arbitrary and X is a composite action given by Eq. (52), the conditional probability p(A|Xj) of A under condition Xj and the conditional probability p(Xj|A) of Xj under condition A are defined by the equations"}
{"pdf_id": "0808.0112", "content": "Remark 8.1. Formula (67) is the generalization of the Bayes' formula of classical proba bility theory (Feller, 1970). Equation (67) reduces to the Bayes formula, provided that there is no interference, when q(AX) is zero, and that the actions pertain to a field where all actions are commutative. However, in QDT, the actions belong to a noncommutative ring A, so that in general p(AXj) and p(XjA) are not equal, since AXj is not the same as XjA. As already mentioned, the noncommutativity of actions is an important feature of QDT."}
{"pdf_id": "0808.0112", "content": "This paradox, first described by Allais (1953), and now known under his name, is a choice problem showing an inconsistency of actual observed choices with the predictions of expected utility theory. It is also often referred to as the violation of the independence axiom of classical utility theory. This paradox is that two decisions which are incompatible in the framework of classical utility theory are nevertheless taken by real human agents. The mathematical structure of the Allais paradox has been presented in Sec. 2. Its explanation in the framework of QDT is as follows. Let us consider two composite actions"}
{"pdf_id": "0808.0112", "content": "Another well-known anomaly in the use of utility theory to account for real human decisions is called the Ellsberg paradox (Ellsberg, 1961). It states that, in some cases, no utility function can be defined at all, so that utility theory fails. The mathematical structure of the Ellsberg paradox is described in Sec. 2. As we show below, such a paradox does not arise in QDT. Let us consider two composite actions"}
{"pdf_id": "0808.0112", "content": "A large set of paradoxes found when applying classical utility theory to the decision making of real human beings are related to the unexpected inversion of choice, when decisions are made in the presence of uncertainty. In other words, the ordering or preference of competing choices according to classical utility theory is reversed by human beings. For this literature, we refer to the numerous citations found in Tversky and Kahneman (1983) and Machina (2008). This anomaly is sometimes called the Rabin paradox (Rabin, 2000)."}
{"pdf_id": "0808.0112", "content": "This paradox was described by Kahneman and Tversky (1979), who pointed out that in somecases utility theory yields the same expected utility outcomes for several prospects, while subjects clearly prefer some prospects to others. The mathematical structure of the Kahneman Tversky paradox is explained in Sec. 2. One considers four composite prospects, as in Eq. (93), under the invariance condition"}
{"pdf_id": "0808.0112", "content": "Proof: It is easy to notice that the Kahneman-Tversky paradox is nothing but a slightly complicated version of the Ellsberg paradox. The Kahneman-Tversky paradox can be treated as a particular case of the inversion paradox. Therefore the proof of Eqs. (98) is the same as in Propositions 7 and 8."}
{"pdf_id": "0808.0112", "content": "(2) We have specified the basic techniques of QDT so that they could be applicable to realdecision processes. In particular, the manifold of intended actions is defined as a noncommuta tive ring, since noncommutativity is a typical property that captures accurately what we believe is an essential property of human decision making. The set of action prospects is characterized as a complete lattice."}
{"pdf_id": "0808.0112", "content": "(3) The point of fundamental importance in our approach is that the action prospects are described as composite objects, formed by composite actions. The composite structure of prospects, together with the entangling properties of probability operators, result in the appearance of decision interferences, which take into account the uncertainties and repulsion to potential harmful consequences associated with the decision procedure."}
{"pdf_id": "0808.0518", "content": "Terminology mappings could support distributed search in several ways. First and foremost,  they should enable seamless search in databases with different subject metadata systems.  Additionally, they can serve as tools for vocabulary expansion in general since they present a  vocabulary network of equivalent, broader, narrower and related term relationships (see examples  in TAB. 1). Thirdly, this vocabulary network of semantic mappings can also be used for query  expansion and reformulation."}
{"pdf_id": "0808.0518", "content": "Starting point of the project was the multidisciplinary science portal vascoda1 which merges  structured, high-quality information collections from more than 40 providers in one search  interface. A concept was needed that tackles the semantic heterogeneity between different  controlled vocabularies (Hellweg et al., 2001, Krause, 2003)."}
{"pdf_id": "0808.0518", "content": "In addition, every relation must be tagged with a relevance rating (high, medium, and low).  The relevance rating is a secondary but weak instrument to adjust the quality of the relations.  They are not used in our current implementations. In our approach it takes approximately 4  minutes to establish one mapping between two concepts. Table 1 presents typical unidirectional  cross-concordances between two vocabularies A and B."}
{"pdf_id": "0808.0518", "content": "In the end, the semantics of the mappings are reviewed by experts and samples are empirically  tested for document recall and precision. Expert reviews focus especially on semantic  correctness, consistency and relevance of equivalence relations which are our most important  relationship type. Sampled mappings are cross-checked and assessed via queries against the  controlled term field of the associated database."}
{"pdf_id": "0808.0518", "content": "A relational database was created to store the cross-concordances for later use. It was found  that the relational structure is able to capture the number of different controlled vocabularies,  terms, term combinations, and relationships appropriately. The vocabularies and terms are  represented in list form, independent from each other and without attention to the syndetic  structure of the involved vocabularies. Orthography and capitalization of controlled vocabulary  terms were normalized. Term combinations (i.e. computers + crime as related combination for the  term hacker) were also stored as separate concepts."}
{"pdf_id": "0808.0518", "content": "application, which uses the equivalence relations5, looks up search terms in the controlled  vocabulary term list and then automatically adds all equivalent terms from all available  vocabularies to the query. If the controlled vocabularies are in different languages, the  heterogeneity service also provides a translation from the original term to the preferred controlled  term in the other language. If the original query contains a Boolean command, it remains intact  after the query expansion (i.e. each query word gets expanded separately). In the results list, a  small icon symbolizes the transformation for the user (see FIG. 2)."}
{"pdf_id": "0808.0518", "content": "Another major issue for a growing terminology network is the scale and overlap of cross concordances. The more vocabularies are mapped to each other, the more terms occur multiple  times in variant mappings6, which makes automatic query expansion more imprecise. On the  other hand, the more vocabularies are added in such a network, the more inferences can be drawn  for additional mappings. Indirect mappings via a pivot vocabulary could help in connecting  vocabularies that haven't been mapped to each other. A sufficiently large network could assist in  reducing the mapping errors introduced by statistical or indirect mappings."}
{"pdf_id": "0808.0518", "content": "5 The other relations, which can lead to imprecise query formulations because they are broader, narrower or  related to the original term, could be leveraged in an interactive search, when the searcher can guide and  direct the selection of search term.  6 For example: term A from vocabulary 1 also occurs in vocabulary 2. A variant mapping exists when term  A from vocabulary 1 is mapped to term B in vocabulary 3, but term A from vocabulary 2 is mapped to term  C in vocabulary 3. This might be the correct mapping because the concepts in the different vocabularies are  differently connotated but most of the time this will introduce noise to the network."}
{"pdf_id": "0808.0518", "content": "The current cross-concordances will be further analyzed and leveraged for distributed search  not only in the sowiport portal but also in the German interdisciplinary science portal vascoda.  The terminology mapping data is made available for research purposes. Some mappings are  already in use for the domain-specific track at the CLEF (Cross-Language Evaluation Forum)  retrieval conference (Petras, Baerisch & Stempfhuber, 2007)."}
{"pdf_id": "0808.0518", "content": "Aside from its application in a distributed search scenario, the semantic web community might  be able to find new and interesting usages for terminology data like this one. The SKOS standard  (Simple Knowledge Organization System)7 contains a section on mapping vocabularies in its  draft version. Once the standard gets stabilized, we plan on transferring the cross-concordance  data to the SKOS format. If more vocabularies and mappings become available in SKOS, then  further research into connecting previously unmapped terminology networks with each other  should be possible."}
{"pdf_id": "0808.0973", "content": "Our framework is somewhat more general than both of these approaches in that we not only improve the quality of making predictions on text data by using prior human concepts and concept-hierarchy, but also are able to make inferences in the reverse direction about concept words and hierarchies given data"}
{"pdf_id": "0808.0973", "content": "The experiments in this paper are based on one large text corpus and two different concept sets. For the text corpus, we used the Touchstone Applied Science Associates (TASA) dataset (Landauer and Dumais, 1997). This corpus consists of D = 37, 651 documents with passages excerpted from educational texts used in curricula from the first year of school to the first year of college. The documents are divided into 9 different educational genres. In this paper, we focus on the documents classified as SCIENCE and SOCIAL STUDIES, consisting of D = 5, 356 and D = 10, 501 documents and 1.7 Million and 3.4 Million word tokens respectively."}
{"pdf_id": "0808.0973", "content": "For human-based concepts the first source we used was a thesaurus from the Cambridge Ad vanced Learner's Dictionary (CALD; http://www.cambridge.org/elt/dictionaries/cald.htm). CALD consists of C = 2, 183 hierarchically organized semantic categories. In contrast to other taxonomies such as WordNet (Fellbaum, 1998), CALD groups words primarily according to semantic topics with the topics hierarchically organized. The hierarchy starts with the concept EVERYTHING whichsplits into 17 concepts at the second level (e.g. SCIENCE, SOCIETY, GENERAL/ABSTRACT, COM"}
{"pdf_id": "0808.0973", "content": "This distribution allows us to traverse the concept tree and exit at any of the C nodes in the tree — given that we are at a concept node c, there are Nc child concepts to choose from and an additional option to choose an \"exit\" child to exit the concept tree at concept node c"}
{"pdf_id": "0808.0973", "content": "In this section, we provide two illustrative examples from the hierarchical concept model trained on the science genre of the TASA document set. Figure 2 shows the 20 highest probability concepts (along with the ancestors of those nodes) for a random subset of 200 documents. The concepts are from the CALD concept set. For each concept, the name of the concept is shown in all caps and the"}
{"pdf_id": "0808.0973", "content": "Figure 3 shows the result of inferring the hierarchical concept mixture for an individual docu ment using both the CALD and the ODP concept sets (Figures 3(b) and 3(c) respectively). For thehierarchy visualization, we selected the 8 concepts with the highest probability and included all an cestors of these concepts when visualizing the tree. This illustration shows that the model is able to give interpretable results for an individual document at multiple levels of granularity. For example, the CALD subtree (Figure 3(b)) highlights the specific semantic themes of FORESTRY, LIGHT, and"}
{"pdf_id": "0808.0973", "content": "PLANT ANATOMY along with the more general themes of SCIENCE and LIFE AND DEATH. For the ODP concept set (Figure 3(c)), the likely concepts focus specifically on CANOPY RESEARCH, CONIFEROPHYTA and more general themes such as ECOLOGY and FLORA AND FAUNA. This shows that different concept sets can each produce interpretable and useful document summaries focusing on different aspects of the document."}
{"pdf_id": "0808.0973", "content": "Perplexity is equivalent to the inverse of the geometric mean of per-word likelihood of the heldout data. It can be interpreted as being proportional to the distance (cross entropy to be precise) between the word distribution learned by a model and the word distribution in an unobserved test document. Lower perplexity scores indicate that the model predicted distribution of heldout data is closer to the true distribution. More details about the perplexity computation are provided in the Appendix B. For each test document, we use a random 50% of words of the document to estimate document specific distributions and measure perplexity on the remaining 50% of words using the estimated distributions."}
{"pdf_id": "0808.1125", "content": "In this article we review standard null-move pruning and introduce our extended version of it, which we call verified null-move pruning. In verified null-move pruning, whenever the shallow null-move search indicates a fail-high, instead of cutting off the search from the current node, the search is continued with reduced depth."}
{"pdf_id": "0808.1125", "content": "Our experiments with verified null-move pruning show that on average, it constructs a smaller search tree with greater tactical strength in comparison to standard null-move pruning. Moreover, unlike standard null-move pruning, which fails badly in zugzwang positions, verified null-move pruning manages to detect most zugzwangs and in such cases conducts a re-search to obtain the correct result. In addition, verified null-move pruning is very easy to implement, and any standard null-move pruning program can use verified null-move pruning by modifying only a few lines of code."}
{"pdf_id": "0808.1125", "content": "Until the mid-1970s most chess programs were trying to search the same way humans think, by generating \"plau sible\" moves. By using extensive chess knowledge at each node, these programs selected a few moves which theyconsidered plausible, and thus pruned large parts of the search tree. However, plausible-move generating pro grams had serious tactical shortcomings, and as soon as brute-force search programs like TECH (Gillogly, 1972) and CHESS 4.X (Slate and Atkin, 1977) managed to reach depths of 5 plies and more, plausible-move generating programs frequently lost to brute-force searchers due to their tactical weaknesses. Brute-force searchers rapidly dominated the computer-chess field."}
{"pdf_id": "0808.1125", "content": "Most brute-force searchers of that time used no selectivity in their full-width search tree, except for some exten sions, consisting mostly of check extensions and recaptures. The most successful of these brute-force programs were BELLE (Condon and Thompson, 1983a,b), DEEP THOUGHT (Hsu, Anantharaman, Campbell, and Nowatzyk, 1990), HITECH (Berliner and Ebeling, 1990; Berliner, 1987; Ebeling, 1986), and CRAY BLITZ (Hyatt, Gower, and Nelson, 1990), which for the first time managed to compete successfully against humans."}
{"pdf_id": "0808.1125", "content": "In this article we introduce our new verified null-move pruning method, and demonstrate empirically its improved performance in comparison with standard null-move pruning. This is renected in its reduced search tree size, as well as its greater tactical strength. In Section 2 we review standard null-move pruning, and in Section 3 we introduce verified null-move pruning. Section 4 presents our experimental results, and Section 5 contains concluding remarks."}
{"pdf_id": "0808.1125", "content": "As mentioned earlier, brute-force programs refrained from pruning any nodes in the full-width part of the search tree, deeming the risks of doing so as being too high. Null-move (Beal, 1989; Goetsch and Campbell, 1990; Donninger, 1993) introduced a new pruning scheme which based its cutoff decisions on dynamic criteria, and thus gained greater tactical strength in comparison with the static forward pruning methods that were in use at that time."}
{"pdf_id": "0808.1125", "content": "There are positions in chess where any move will deteriorate the position, so that not making a move is the best option. These positions are called zugzwang positions. While zugzwang positions are rare in the middle game, they are not an exception in endgames, especially endgames in which one or both sides are left with King and Pawns. Null-move pruning will fail badly in zugzwang positions since the basic assumption behind the method does not hold. In fact, the null-move search's value is an upper bound in such cases. As a result, null-move pruning is avoided in such endgame positions."}
{"pdf_id": "0808.1125", "content": "As previously noted, the major benefit of null-move pruning stems from the depth reduction in the null-move searches. However, these reduced-depth searches are liable to tactical weaknesses due to the horizon effect (Berliner, 1974). A horizon effect results whenever the reduced-depth search misses a tactical threat. Such a threat would not have been missed, had we conducted a search without any depth reduction. The greater the depth reduction R, the greater the tactical risk due to the horizon effect. So, the saving resulting from null-move pruning depends on the depth reduction factor, since a shallower search (i.e., a greater R) will result in faster null-move searches and an overall smaller search tree."}
{"pdf_id": "0808.1125", "content": "Experiments conducted by Heinz (1999), in his article on adaptive null-move pruning, suggest that using R = 3 in upper parts of the search tree and R = 2 in its lower parts can save 10 to 30 percent of the search effort in comparison with a fixed R = 2, while maintaining overall tactical strength"}
{"pdf_id": "0808.1125", "content": "Cutoffs based on a shallow null-move search can be too risky at some points, especially in zugzwang positions. Goetsch and Campbell (1990) hinted at continuing the search with reduced depth, in case the null-move search indicates a fail-high, in order to substantiate that the value returned from the null-move search is indeed a lower bound on the position. Plenkner (1995) showed that this idea can help prevent errors due to zugzwangs. However, verifying the search in the middle game seems wasteful, as it appears to undermine the basic benefit of null-move pruning, namely that a cutoff is determined by a shallow null-move search."}
{"pdf_id": "0808.1125", "content": "As the experimental results in the next section show, verified null-move pruning constructs a search tree which is close in size to that of standard null-move pruning with R = 3, and whose tactical strength is greater on average than that of standard null-move pruning with R = 2"}
{"pdf_id": "0808.1125", "content": "Implementation of verified null-move search is a matter of adding a few lines of code to standard null-move search, as shown in Figure 3. Regarding the pseudo-code presented, when the search starts at the root level, the nag verify is initialized to true. When the null-move search indicates a fail-high, the remaining depth is reduced by one ply, and verify is given the value false, which will be passed to the children of the current node, indicating that standard null-move pruning will be conducted with respect to the children. Upon a fail-high indication due to the standard null-move search of these children's subtrees, cutoff takes place immediately."}
{"pdf_id": "0808.1125", "content": "In order to obtain an estimate of the search tree, we searched 138 test positions from Test Your Tactical Ability by Yakov Neishtadt (see the Appendix) to depths of 9 and 10 plies, using standard R = 1, R = 2, R = 3, and verified R = 3. Table 1 gives the total node count for each method and the size of the tree in comparison with verified R = 3. Table 2 gives the number of positions that each method solved correctly (i.e., found the correct variation for). Later we will further examine the tactical strength, using additional test suites."}
{"pdf_id": "0808.1125", "content": ", standard R = 2 and R = 3, and verified R = 3), we would like to examine the behavior of verified R = 3 and find out whether its tree size remains between the tree sizes associated with R = 2 and R = 3, or whether it approaches the size of one of"}
{"pdf_id": "0808.1125", "content": "these trees. We therefore conducted a search to a depth of 11 plies, using 869 positions from the Encyclopedia of Chess Middlegames (ECM)4. Table 3 provides the total node counts at depths 9, 10, and 11, using standard R = 2, R = 3, and verified R = 3. See also Figure 4."}
{"pdf_id": "0808.1125", "content": "As Figure 4 clearly indicates, for depth 11 the size of the tree constructed by verified null-move pruning with R = 3 is closer to standard null-move pruning with R = 3. This implies that the saving from verified null-move pruning will be greater as we search more deeply. This can be explained by the fact that the saving from the use of R = 3 in the shallow null-move search far exceeds the verification cost of verified null-move pruning."}
{"pdf_id": "0808.1125", "content": "The results in Tables 5 and 6 indicate that verified null-move pruning solved far more positions than standard null move pruning with depth reductions of R = 2 and R = 3. This demonstrates that not only does verified null-move pruning result in a reduced search effort (the constructed search tree is closer in size to that of standard R = 3), but its tactical strength is greater than that of standard R = 2, which is the common depth reduction value."}
{"pdf_id": "0808.1125", "content": "Finally, to study the overall advantage of verified null-move pruning over standard null-move pruning in practice, we conducted 100 self-play games, using two versions of the GENESIS engine, one with verified R = 3 and the other with standard R = 2. The time control was set to 60 minutes per game. The version using verified R = 3 scored 68.5 out of 100 (see the Appendix), which demonstrates the superiority of verified null-move pruning over the standard version."}
{"pdf_id": "0808.1125", "content": "We showed empirically that verified null-move pruning with a depth reduction of R = 3 constructs a search tree which is closer in size to that of the tree constructed by standard R = 3, and that the saving from the reduced search effort in comparison with standard R = 2 becomes greater as we search more deeply"}
{"pdf_id": "0808.1125", "content": "We considered a number of variants of standard null-move pruning. The first variant was not to cut off at all upon fail-high reports, but rather reduce the depth by 2 plies. We obtained good results with this idea, but its tactical strength was sometimes smaller than that of standard R = 2. We concluded that in order to improve the results, the depth should not be reduced by more than one ply at a time upon fail-high reports. An additional variant was not to cut off at any node, not even in the subtree of a node with a fail-high report, but merely to reduce the depth"}
{"pdf_id": "0808.1125", "content": "by one ply upon a fail-high report. Unfortunately, the size of the resulting search tree exceeded the size of the tree constructed by standard R = 2. Still, another variant was to reduce the depth by one ply upon fail-high reports, and to reduce the depth by two plies upon fail-high reports in that node's subtree, rather than cutting off."}
{"pdf_id": "0808.1125", "content": "Our empirical studies showed that cutting off the search at the subtree of a fail-high reported node does not decrease tactical strength. Indeed, this is the verified null-move pruning version that we studied in this article. In contrast to the standard approach which advocates the use of immediate cutoff, the novel approach taken here uses depth reduction, and delays cutting off the search until further verification. This yields greater tactical strength and a smaller search tree."}
{"pdf_id": "0808.1125", "content": "We would like to thank Shay Bushinsky for his interest in our research, and for promoting the discipline of Com puter Chess in our department. We would also like to thank Dann Corbit for providing the CAP test positions for our empirical studies, and Azriel Rosenfeld for his editorial comments. Finally, we are indebted to Jonathan Schaeffer and Christian Donninger for their enlightening remarks and suggestions."}
{"pdf_id": "0808.1211", "content": "Biographical notes: W. Saba received his PhD in Computer Science from Carleton Uni versity in 1999. He is currently a Principal Software Engineer at the American Institutes for  Research in Washington, DC. Prior to this he was in academia where he taught computer  science at the University of Windsor and the American University of Beirut (AUB). For  over 9 years he was also a consulting software engineer where worked at such places as  AT&T Bell Labs, MetLife and Cognos, Inc. His research interests are in natural language  processing, ontology, the representation of and reasoning with commonsense knowledge,  and intelligent e-commerce agents."}
{"pdf_id": "0808.1211", "content": "In Types and Ontology Fred Sommers (1963) suggested  several years ago that there is a strongly typed ontology that  seems to be implicit in all that we say in ordinary spoken 1 In addition to EAT, a Human can of course also BUY, SELL, MAKE, PRE PARE, WATCH, or HOLD, etc. a Sandwich. Why EAT might be a more salient relation between a Person and a Sandwich is a question we shall pay con siderable attention to below."}
{"pdf_id": "0808.1211", "content": "7 It is perhaps worth investigating the relationship between the number of  meanings of a certain adjective (say in a resource such as WordNet), and  the number of different functions that one would expect to define for the  corresponding adjective. 8 Technically, the reason we can always cast up is that we can always ignore additional information. Casting down, which entails adding informa tion, is however undecidable."}
{"pdf_id": "0808.1211", "content": "In addition to so-called intensional verbs, our proposal  seems to also appropriately handle other situations that, on the surface, seem to be addressing a different issue. For ex ample, consider the following:  9 Note that it is the Trip (event) that did not necessarily happen, not the  planning (Activity) for it."}
{"pdf_id": "0808.1211", "content": "[[ ,1,1 ,...],[ ,1 ,1 ,...],,...] drive ride Since these lists are ordered, the degree to which a property  or a relation is salient is inversely related to the position of  the property or the relation in the list. Thus, for example,  while a Human may drive, ride, make, buy, sell,  build, etc. a Car, drive is a more salient relation between"}
{"pdf_id": "0808.1211", "content": "CIZE, DIRECT, PRODUCE, etc. a Movie. Although this issue is  beyond the scope of the current paper we simply note that  picking out the most salient relation is still decidable due to  tow differences between READ/WRITE and WATCH/DIRECT  (or WATCH/PRODUCE): (i) the number of people that usually read a book (watch a movie) is much greater than the num ber of people that usually write a book (direct/produce) a movie, and saliency is inversely proportional to these num bers; and (ii) our ontology typically has a specific name for those who write a book (author), and those who direct (di rector) or produce (producer) a movie."}
{"pdf_id": "0808.1211", "content": "DEATH (44c); and, finally, jon is going through (GT) a Proc ess called AGING (44d). Finally, consider the following  well-known example (due, we believe, to Barbara Partee):  (45) a. The temperature is 90.  b. The temperature is rising.  c. 90 is rising.  It has been argued that such sentences require an intensional  treatment since a purely extensional treatment would make"}
{"pdf_id": "0808.1211", "content": "If the main business of semantics is to explain how  linguistic constructs relate to the world, then semantic  analysis of natural language text is, indirectly, an attempt at  uncovering the semiotic ontology of commonsense  knowledge, and particularly the background knowledge that  seems to be implicit in all that we say in our everyday  discourse. While this intimate relationship between  language and the world is generally accepted, semantics (in  all its paradigms) has traditionally proceeded in one  direction: by first stipulating an assumed set of ontological"}
{"pdf_id": "0808.1211", "content": "this ontological structure, and, as also argued in Saba  (2007), it is the systematic investigation of how ordinary  language is used in everyday discourse that will help us  discover (as opposed to invent) the ontological structure that  seems to underlie all what we say in our everyday discourse."}
{"pdf_id": "0808.1211", "content": "While any remaining errors and/or shortcomings are our own, the work presented here has benefited from the valu able feedback of the reviewers and attendees of the 13th  Portuguese Conference on Artificial Intelligence (EPIA 2007), as well as those of Romeo Issa of Carleton Univer sity and those of Dr. Graham Katz and his students at  Georgetown University."}
{"pdf_id": "0808.1721", "content": "Abstract. In this paper, we show our results on the bi-directional data exchange  between the F-logic language supported by the Flora2 system and the OWL  language. Most of the TBox and ABox axioms are translated preserving the  semantics between the two representations, such as: proper inclusion, individual  definition, functional properties, while some axioms and restrictions require a  change in the semantics, such as: numbered and qualified cardinality  restrictions. For the second case, we translate the OWL definite style inference  rules into F-logic style constraints. We also describe a set of reasoning  examples using the above translation, including the reasoning in Flora2 of a  variety of ABox queries."}
{"pdf_id": "0808.1721", "content": "The translation into Flora2's format makes possible the evaluation of transactions  over the data in the ontology, making possible the design and execution of workflows  and execution of plans that change facts about individuals while executing Web  workflows. These features cannot be represented with the auto-epistemic K-operator  and the reasoning tasks cannot be solved using the tableau algorithms (see updates of  the ABox in DL-Lite in [4] and representation of supply chains in [5]).  The paper is organized as follows. The basic translations are defined in Section 2.  Section 3 describes applications of the translation into querying and checking the  integrity of ontology, and related work. Section 4 summarizes our contributions and  concludes the paper."}
{"pdf_id": "0808.1753", "content": "The size of RW is by order  of magnitude higher than SEW (number of words, lexemes), though the growth rate of number of pages in SEW was  found to be 14% higher than in Russian, and the rate of acquisition of new words in SEW lexicon was 7% higher during  a period of five months (from September 2007 to February 2008)"}
{"pdf_id": "0808.1753", "content": "In the USA, the 2007 nationwide survey found that more than a third of adult Internet users (36%)  consulted the online encyclopedia Wikipedia [Rainie07]. The popularity of encyclopedia is  probably best explained by the sheer amount of material on the site, the wide coverage of topics and  the freshness of data. Wikipedia (WP) continues to gain popularity among the broad masses  because it has a high rank assigned by search engines. E.g., in March 17, 2007, over 70% of the  visits to Wikipedia came from search engines, according to Hitwise data [Rainie07]. More over, the  search system Koru analyses Wikipedia links to expand query terms [MilneWitten07]."}
{"pdf_id": "0808.1753", "content": "The earlier developed adapted HITS algorithm (AHITS) [Krizhanovsky2006a] searches for related  terms by analysing Wikipedia internal links. There are many algorithms for searching related terms  in Wikipedia, which can do without full text search [Krizhanovsky07a] (Table 3, p. 8). However,  experimental comparison of algorithms [Gabrilovich2007], [Krizhanovsky07a] shows that the best  results were obtained with the statistical text analysis algorithm ESA."}
{"pdf_id": "0808.1753", "content": "ARCHITECTURE OF WIKI INDEXING SYSTEM In the architecture of the wiki indexing system shown in Fig. 1, interactions between the programs  (GATE [Cunningham2005], Lemmatizer [Sokirko01], and Synarcher [Krizhanovsky2006a]) are  presented.7 The result produced by the system is the record level inverted index database8, which  contains a list of references to documents for each word, or rather, for each lemma. The indexing  system requires three groups of input parameters:"}
{"pdf_id": "0808.1753", "content": "1. The Language that defines the language of Wikipedia (one of 254 as of 16 Jan 2008) and the  language of lemmatizing.9 The language of WP should be defined in order to parse wikitext (see  Fig. 1, function \"Convert wiki-format to text\" of the software module \"Wikipedia Application\"). 2. Database location that is a set of parameters (host, port, login, password) for connecting to  the remote database (WP and index). 3. TF-IDF constraints that define the size of the result index DB.10"}
{"pdf_id": "0808.1753", "content": "Number of tables in the index DB, the table's fields and relations between the tables are defined  based on the problem to be solved: search for a document by the word with the help of TF-IDF  formula (see below). Calculations by this formula requires three17 tables18 (Fig. 2)19:"}
{"pdf_id": "0808.1753", "content": "Postfix \"_id\" in the names of tables' fields means that the field contains a unique identifier (Fig. 2). The indexed (for speed) fields are listed below the horizontal line in the frames of tables. An one-to many relation is defined between the tables term and term_page, and between page and term_page."}
{"pdf_id": "0808.1753", "content": "where TF i is the frequency of occurrence of the term t i within a specific document (field  term_page.term_freq, or a value of the field term_freq of the index database table term_page), DF i  is the number of documents containing the term t i (field term.doc_freq), inverse document  frequency (idf) serves to filter out common terms."}
{"pdf_id": "0808.1753", "content": "The articles of Wikipedia are written in wikitexts. There is a need to convert the wikitext with the  aim to strip out the wiki tags and to extract the text part of them. If this step is omitted then the first  hundred of the most frequent words will contain special tags like \"ref\", \"nbsp\", \"br\" and others.24"}
{"pdf_id": "0808.1753", "content": "This wikitext parser was implemented as one of the Java packages of the program  Synarcher [Krizhanovsky2006a]. The Java regular expressions [Friedl2001] are widely used to  transform elements of wikitext. The fragment of the Simple Wikipedia29 article \"Sakura\" is  presented in the left column of Table 2. The result of parsing this fragment taking into account all  the rules (presented above) is in the right column."}
{"pdf_id": "0808.1753", "content": "Since the API above (and API of Synarcher to work with MediaWiki database) are not suitable for  the indexing DB, it was decided to develop a new API. Thus, an API providing access to the index  Wikipedia database WikIDF has been developed. I). The high level interface allows:34"}
{"pdf_id": "0808.1753", "content": "The developed software for indexing wiki-texts enabled to create an index databases of Simple  English Wikipedia36 (further, denote SEW) and Russian Wikipedia37 (RW) and to carry out  experiments. The statistical data of the source / result databases and the parsing process are  presented in Table 3."}
{"pdf_id": "0808.1753", "content": "20/09/2007 and 20/02/2008) divided by the SEW parameters (at 09/09/2007 and 14/02/2008) in  2007 and 2008 years, respectively, are presented. The parameters that characterize the Russian  Wikipedia are the large quantity of lexemes (1.43 M38) and the total number of words in the corpus  (32.93 M)."}
{"pdf_id": "0808.1753", "content": "31 See http://api.futef.com/apidocs.html. 32 See http://json.org/. 33 See http://modis.ispras.ru/sedna/ and http://wikixmldb.dyndns.org/help/use-cases/. 34 See an example of usage of these functions in the file: synarcher/wikidf/src/wikidf/ExampleAPI.java. 35 See Table 4 with the result returned by this function (in Appendix, p. 15). 36 Most frequent 1000 words found in English Simple Wikipedia (14 Feb 2008) are listed with frequencies, see"}
{"pdf_id": "0808.1753", "content": "the English word frequencies decreased with faster lowering frequencies. This could be explained  by several facts. Firstly, the size of Russian Wikipedia is an order of magnitude larger than Simple  Wikipedia and hence a richer lexicon is used in order to explain more number of concepts.  Secondly, the authors of Simple Wikipedia try to use the limited number of English words."}
{"pdf_id": "0808.1753", "content": "with a log-log scale could be approximated good enough by a straight line. At this time, the law  holds better for Simple Wikipedia (0.20)44 than for Russian Wikipedia (0.23). This could be  explained by simplified language characteristics or by differences between English and Russian. A  definitive answer to this question will require a solving of an industrial scale problem that is the  indexing of the huge English Wikipedia."}
{"pdf_id": "0808.1753", "content": "presented in the paper. The interaction of the programs GATE, Lemmatizer, and Synarcher during  the indexing process is described. The result of the indexing process is a list of lemmas and  frequencies of lexemes stored to a database. The design of this inverted file index database is  presented. The rules of converting from wiki markup to NL text are proposed and implemented in  the indexing system."}
{"pdf_id": "0808.1753", "content": "http://www.cs.waikato.ac.nz/~dnk2/publications/nzcsrsc07.pdf [MilneWitten07]. Milne D., Witten I.H., Nichols D.M. A knowledge-based search engine powered by Wikipedia. In  Proc. of the ACM Conference on Information and Knowledge Management (CIKM'2007). Portugal, Lisbon, 2007.  http://www.cs.waikato.ac.nz/~dnk2/publications/cikm07.pdf [Ollivier2007]. Ollivier Y., Senellart P. Finding related pages using Green measures: an illustration with Wikipedia. In  Association for the Advancement of Artificial Intelligence.Vancouver, Canada, 2007."}
{"pdf_id": "0808.2227", "content": "Abstract— The compound models of clutter statistics are foundsuitable to describe the nonstationary nature of radar backscat tering from high-resolution observations. In this letter, we show that the properties of Mellin transform can be utilized to generate higher order moments of simple and compound models of clutter statistics in a compact manner."}
{"pdf_id": "0808.2227", "content": "I. INTRODUCTION ADAR backscattering from ground or sea surfaces are wide-sense stationary for low-resolution observations as expectations of clutter statistics or moments are assumed to be independent of spatio-temporal changes. For high-resolution observations, such surfaces reveal heterogeneous structures such as swell in sea waves or winds blowing over the canopy of grasslands that result in nonstationary clutter statistics [1], [2], [4]. The compound models of probability density functions (pdf) incorporate the variation in the parameters of clutter in such cases. Traditionally higher order moments of a continuous random variable (rv) X are generated from higher order derivatives of its characteristic function defined as"}
{"pdf_id": "0808.2227", "content": "The underlying mean of speckle component of clutter vary widely in the compound models of amplitude or power statistics resulting in long-tailed distributions. Speckle arises from randomness in the distribution of backscattering elementsin the resolution cell, the number of such scatterers is nonstationary for high-resolution observations. The pdf of high resolution clutter is described by taking into account of a rv Z signifying randomness in the mean of clutter."}
{"pdf_id": "0808.2227", "content": "V. CONCLUSION The utility of Mellin transform properties to generate higher order moments of simple and compound models of clutter in both amplitude and power domain is shown in this letter. The second kind characteristic function and its properties provide compact analytical expressions for higher order moments that are useful to interpret texture properties of high-resolution clutter."}
{"pdf_id": "0808.2246", "content": "We are  addressing the basic question \"What are the pros and cons of human and automatic mapping and  how can they complement each other?\" By pointing out the difficulties in specific cases or groups  of cases and grouping the sample into simple and difficult types of mappings, we show the  limitations of current automatic methods and come up with some basic recommendations on what  approach to use when"}
{"pdf_id": "0808.2246", "content": "Mapping major thesauri and other knowledge organization systems in specific domains of  interest can therefore greatly enhance the access to information in these domains. System  developers for library search applications can programmatically incorporate mapping files into  the search applications. The mappings can hence be utilized at query time to translate a user"}
{"pdf_id": "0808.2246", "content": "•  AGROVOC2 is a multilingual, structured and controlled vocabulary designed to cover  the terminology of all subject fields in agriculture, forestry, fisheries, food and related  domains (e.g. environment). The AGROVOC Thesaurus was developed by the Food  and Agriculture Organization of the United Nations (FAO) and the European  Commission, in the early 1980s. It is currently available online in 17 languages (more  are under development) and contains 28,718 descriptors and 10,928 non-descriptors in  the English version."}
{"pdf_id": "0808.2246", "content": "•  The NAL Thesaurus3 (NALT) is a thesaurus developed by the National Agricultural  Library (NAL) of the United States Department of Agriculture and was first released  in 2002. It contains 42,326 descriptors and 25,985 non-descriptors organized into 17  subject categories and is currently available in two languages (English and Spanish).  Its scope is very similar to that of AGROVOC. Some areas such as economical and  social aspects of rural economies are described in more detail."}
{"pdf_id": "0808.2246", "content": "•  The Schlagwortnormdatei4 (SWD) is a subject authority file maintained by the  German National Library and cooperating libraries. Its scope is that of a universal  vocabulary. The SWD contains around 650,000 keywords and 160,000 relations  between terms. The controlled terms cover all disciplines and are classified within 36  subject categories. The agricultural part of the SWD contains around 5,350 terms."}
{"pdf_id": "0808.2246", "content": "Many thesauri, amongst which AGROVOC and the Aquatic Sciences and Fisheries Abstracts  Thesaurus (ASFA) 7 are being converted into ontologies, in order to enhance their expressiveness  and take advantage of the tools made available by the semantic web community. Therefore, great  attention is being dedicated also to mapping ontologies. An example is the Networked Ontologies  project (NeOn)8, where mappings are one of the ways to connect ontologies in networks."}
{"pdf_id": "0808.2246", "content": "Cases like this clearly show how beneficial it would be  to gain a clear understanding of when manual mapping is more advisable than automatic mapping  (as in the case of the AGROVOC- ASFA mapping) or the other way around (as in the case of the  AGROVOC - NALT mapping analyzed in this paper)"}
{"pdf_id": "0808.2246", "content": "Another major mapping exercise was carried out mapping AGROVOC to the Chinese  Agricultural Thesaurus (CAT) described in (Liang et al., 2006). The mapping has been carried  out using the SKOS Mapping Vocabulary10 (version 2004) and addresses another very important  issue in mapping thesauri and other KOS: multilinguality. AGROVOC has been translated from"}
{"pdf_id": "0808.2246", "content": "6 The project was funded by BMBF, grant no. 01C5953.  http://www.gesis.org/en/research/information_technology/komohe.htm.  7 http://www4.fao.org/asfa/asfa.htm.  8 http://neon-project.org.  9 In particular, a problem could be the different level of details of the two resources, as ASFA tends to be  very specific on fisheries related terms.  10 http://www.w3.org/2004/02/skos/mapping/spec/."}
{"pdf_id": "0808.2246", "content": "The system that performed best at the OAEI 2007 food task was Falcon-AO. It found around  80% of all equivalence relations using lexical matching techniques. However, it was unable to  find any hierarchical relations. Also, it did not find relations that required background knowledge  to discover. This led to a recall score of around 50%. The SCARLET system was the only system  that found hierarchical relations using the semantic web search engine Watson12 (Sabou et al.,  2007). Many of the mappings returned by SCARLET were objectively speaking valid, but more  generic than any human would suggest. This led to a very low recall score."}
{"pdf_id": "0808.2246", "content": "The AGROVOC-SWD mapping is a fully human generated bilateral mapping that involves  major parts of the vocabularies (see Table 2). Both vocabularies were analysed in terms of topical  and syntactical overlap before the mapping started. All mappings in the GESIS-IZ approach are  established by researchers, terminology experts, domain experts, and postgraduates. Essential for  a successful mapping is the complete understanding of the meaning and semantics of the terms  and the intensive use of the internal relations of the vocabularies concerned. This includes  performing lots of simple syntactic checks of word stems but also semantic knowledge, i.e. to  lookup synonyms and other related or associated terms."}
{"pdf_id": "0808.2246", "content": "In the end the semantics of the mappings are reviewed by experts and samples are empirically  tested for document recall and precision (classical information retrieval definition). Some  examples of the rules in the KoMoHe approach can be found in (Mayr & Petras, 2008a, to be  published)."}
{"pdf_id": "0808.2246", "content": "Given these two approaches, one completely carried out by human subject experts and the  other by machines trying to simulate the human task, the basic questions are: who performs more  efficiently in a certain domain?, what are the differences?, and where are the limits? In order to  draw some conclusions, a qualitative assessment is needed."}
{"pdf_id": "0808.2246", "content": "We first \"aligned\" the mappings for the overlapping AGROVOC terms that have been mapped  both to NALT and to SWD. For this we aligned the AGROVOC term with the mapped NALT  terms (in English) and the mapped SWD term (in German): about 5,000 AGROVOC terms have  been mapped in both approaches. For the AGROVOC-NALT mapping, we took the entire set of  suggestions made by five systems participating in OAEI 2007. We also listed the number of  systems that have suggested the mapping between the AGROVOC and the NALT term (between"}
{"pdf_id": "0808.2246", "content": "This was done in order to be able to draw more detailed conclusions on the difficulty of  mappings based on the terminology group a particular mapping falls into. These groups were  chosen in order to be more specific on whom to contact to evaluate the respective mappings. This  will give an indication on what kind of knowledge is generally harder for automatic computer  systems to map and what kind of background knowledge might also be needed to solve the more  difficult cases."}
{"pdf_id": "0808.2246", "content": "Out of the about 5,000 mappings, we chose a representative sample of 644 mappings to be  manually assessed. The mappings for the sample have been picked systematically in such a way  that each of the groups is represented. We then assigned one of the following 6 difficulty ratings  once for each of the mappings, AGROVOC-NALT and AGROVOC-SWD respectively. The  assessments were done by Gudrun Johannsen and Willem Robert van Hage. Table 3 summarizes  our rating."}
{"pdf_id": "0808.2246", "content": "The assessment of the sample selection of 644 mappings is summarized in Table 4. The table is  grouped by major subject groups: Taxonomic, Biological/Chemical and Miscellaneous. For each  mapping approach (AGROVOC-NALT and AGROVOC-SWD), the table shows, what  percentage of the mappings in the respective group are Simple, Easy Lexical, etc. The numbers in  brackets are the absolute numbers. For example in the group Miscellaneous: 18.12% of the  AGROVOC- SWD mappings in this subject group have been found to be of difficulty 6 (Hard  Background Knowledge), whereas only 1.45% of the AGROVOC-NALT mappings have been  given this rating."}
{"pdf_id": "0808.2246", "content": "13 The Codex Alimentarius Commission was created in 1963 by FAO and WHO to develop food standards,  guidelines and related texts such as codes of practice under the Joint FAO/WHO Food Standards  Programme. The main purposes of this Programme are protecting health of the consumers, ensuring fair  trade practices in the food trade, and promoting coordination of all food standards work undertaken by  international  governmental  and  non-governmental  organizations.  It  is  available  at:  http://www.codexalimentarius.net/web/index_en.jsp."}
{"pdf_id": "0808.2246", "content": "agriculture domain, it might be correct to declare equivalence between these terms.  However, in another domain there might actually be no mapping or at most a related term  mapping. For example, in the business area, marketing strategies differ from marketing  techniques substantially in that the strategies are long term objectives and roadmaps  whereas the marketing techniques are operational techniques used in the marketing of  certain products. For an automatic mapping algorithm, this is difficult to detect and  alternative labels as they are sometimes found in thesauri, might be misleading."}
{"pdf_id": "0808.2246", "content": "The current mappings in the project at GESIS-IZ will be further analyzed and leveraged for  distributed search not only in the sowiport portal but also in the German interdisciplinary science  portal vascoda. Some of these mappings are already in use for the domain-specific track at the  CLEF (Cross-Language Evaluation Forum) retrieval conference. We also plan on leveraging the  mappings for vocabulary help in the initial query formulation process as well as for the ranking of  retrieval results (Mayr, Mutschke & Petras, 2008)."}
{"pdf_id": "0808.2246", "content": "We have seen that automatic mapping can definitely be very helpful and effective in case of  Simple and Easy Lexical mappings. From our results, it appears that groups like Taxonomic  vocabulary, Biological and Chemical Terminology and Geographic concepts fall into this  category, as in general there seems to be more consensus on how to name things than in other  groups. However, we need to be careful in these areas, where often word similarity does not mean  that this is a potential mapping. These can be serious traps for automatic mapping approaches  (like in the case of geopolitical issues)."}
{"pdf_id": "0808.2246", "content": "Things get potentially more difficult in the case of more diversified groups/categories (in our  case just summarized as Miscellaneous). Here, often background knowledge is needed to infer the  correct mapping, and automatic mapping tools are able to identify only very little of these  correctly. Most of the automatic suggestions are simply wrong or should not be equivalence  relationships but broader, narrower or related terms."}
{"pdf_id": "0808.2246", "content": "The bottom line is that for the moment, mapping should not be seen as a monolithic exercise,  but we can take the best of both approaches and use automatic mapping approaches to get to the  simple and easy lexical mappings and then use human knowledge to control the ambiguous cases."}
{"pdf_id": "0808.2246", "content": "We would like to thank Lori Finch at the NAL for her extensive help on the AGROVOC-NALT  mapping and for many discussions that contributed to this work. Van Hage was supported by the Dutch BSIK project Virtual Laboratory for e-science (http://www.vl-e.nl). The project at GESIS IZ was funded by the German Federal Ministry for Education and Research, grant no. 01C5953.  P. Mayr wishes to thank all our project partners and my colleagues in Bonn for their  collaboration."}
{"pdf_id": "0808.2428", "content": "4. model #1 plus Journal Section and Cover Article  5. model #1 plus Journal as a random variable, and Year instead of Months after publication; Phys Genomics  for year 2003 removed  6. model #1 plus Journal as a random variable, and Year instead of Months after publication; PNAS (all  years) and Phys Genomics (2003) removed"}
{"pdf_id": "0808.2428", "content": "Notes: The estimated citation gain over two years is calculated by multiplying the estimate of the open access  effect (a multiplicative effect) by the journal's impact factor (the number of times the average article is cited in a  journal within the first two years after publication). The cost per citation is simply the estimated citation gain  divided by the open access publication costs."}
{"pdf_id": "0808.2670", "content": "Recommender systems use data on past user preferences to predict possible future likes and interests. A key challenge is that while themost useful individual recommendations are to be found among di verse niche objects, the most reliably accurate results are obtainedby methods that recommend objects based on user or object sim ilarity. In this paper we introduce a new algorithm specifically to address the challenge of diversity and show how it can be used to resolve this apparent dilemma when combined in an elegant hybridwith an accuracy-focused algorithm. By tuning the hybrid appro priately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and diversity of recommendations."}
{"pdf_id": "0808.2670", "content": "DiscussionRecommender systems have at their heart some very sim ple and natural social processes. Each one of us looks to others for advice and opinions, learning over time who to trust and whose suggestions to discount. The paradox is that many of the most valuable contributions come not from close friends but from people with whom we have only a limited connection—\"weak ties\" who alert us to possibilities outside our regular experience [31]."}
{"pdf_id": "0808.2670", "content": "ACKNOWLEDGMENTS. We are grateful to Yi-Kuo Yu for useful comments and conversations, and to two anonymous referees for their valuable feedback. This work was supported by Swiss National Science Foundation grant 200020-121848, Swiss State Ministry for Education and Research grant C05.0148 (Physics of Risk), and National Natural Science Foundation of China grants 10635040 and 60744003. We also acknowledge funding from the Liquid Publications and QLectives projects (EU FET-Open grants 213360 and 231200) during the final stages of this work."}
{"pdf_id": "0808.2670", "content": "Fig. 1. The HeatS (a,b,c) and ProbS (d,e,f) algorithms (Eqs. 1 and 2) at work on the bipartite user-object network. Objects are shown as squares, users as circles, with the target user indicated by the shaded circle. While the HeatS algorithm redistributes resource via a nearest-neighbour averaging process, the ProbS algorithm works by an equal distribution of resource among nearest neighbours."}
{"pdf_id": "0808.3109", "content": "with the same  ,  1 and  = .  We can define all 16 Fuzzy Logical Operators with respect to two FL operators: FL  conjunction ( FLC and FL negation ( FLN .  Since in FL the falsehood value is equal to 1- truth value , we can deal with only one  component: the truth value.  The Venn Diagram for two sets X and Y  1"}
{"pdf_id": "0808.3109", "content": "= part = intersection of negation of x and y ;  ( 2) ( ), ) FL P .  = part = intersection of negation of x and the negation of y ;  ( 0) ( ), ( )) FL P x n ,  and for normalization we set the condition:  ( , ) ( ( ), ( ) ( ), ( ) (1,0) x y n x x n x n ."}
{"pdf_id": "0808.3109", "content": "and y are normalized, then   is also normalized. Of course, the reader can redefine the  neutrosophic conjunction operator, depending on application, in a different way, for example in a  more optimistic way, i.e. I  or T prevails with respect to I , then we get:  1 2 1 2 2 1 1 2 1 2 1 2 2 1 2 1 ( , ) cITF x y TT T I T I I I F F F I FT F T F I ."}
{"pdf_id": "0808.3109", "content": "by interchanging the truth T and falsehood F vector components.  Then:  1 2 1 2 1 2 2 1 1 2 1 2 2 1 2 1 ( 12) NL P TT I I I T I T F F F I FT F T F I"}
{"pdf_id": "0808.3109", "content": "+ F .  This neutrosophic disjunction operator of disjoint variables allows us to add neutrosophic  truth values of disjoint parts of a shaded area in a Venn Diagram.  Now, we complete Donald E. Knuth's Table of the Sixteen Logical Operators on two  variables with Fuzzy Logical operators on two variables with Fuzzy Logic truth values, and  Neutrosophic Logic truth/indeterminacy/false values (for the case T )."}
{"pdf_id": "0808.3296", "content": "but this study does not test or show anything at all about the causal role of QB (or of  any of the other potential causal factors, such as Accessibility Advantage, AA,  Competitive Advantage, CA, Download Advantage, DA, Early Advantage, EA, and  Quality Advantage, QA). The author also suggests that paid OA is not worth the cost,  per extra citation. This is probably true, but with OA self-archiving, both the OA and  the extra citations are free."}
{"pdf_id": "0808.3296", "content": "higher-quality articles (the Quality Bias, QB) is the primary causal factor underlying the  observed OA Advantage, in fact this study does not test or show anything at all about the causal  role of QB (or of any of the other potential causal factors underlying the OA Advantage, such as  Accessibility Advantage, AA, Competitive Advantage, CA, Download Advantage, DA, Early  Advantage, EA, and Quality Advantage, QA; Hajjem & Harnad 2007b).  The following 5 further analyses of the data are necessary. The size and pattern of the observed  results, as well as their interpretations, could all be significantly altered (as well as deepened) by  their outcome:"}
{"pdf_id": "0808.3296", "content": "The natural interpretation of Figure 1 accordingly seems to be the exact opposite of the one the  author makes: Not that the size of the OA Advantage shrinks from 2004-2007, but that the size  of the OA Advantage grows from 2007-2004 (as articles get older and their citations grow)"}
{"pdf_id": "0808.3296", "content": "It is undoubtedly true that better authors are more likely to make their articles OA, and that authors in general are more likely to make their better articles OA. This Quality or \"Self Selection\" Bias (QB) is one of the probable causes of the OA Advantage.  However, no study has shown that QB is the only cause of the OA Advantage, nor even that it is  the biggest cause. Three of the studies cited (Kurtz et al., 2005; Kurtz & Henneken, 2007; Moed,  2007) showed that another causal factor is Early Access (EA: providing OA earlier results in  more citations).  There are several other candidate causal factors in the OA Advantage, besides QB and EA"}
{"pdf_id": "0808.3296", "content": "EA and DA,  in contrast, will continue to contribute to the OA advantage even after universal OA is reached,  when all postprints are being made OA immediately upon publication, compared to pre-OA days  (as Kurtz has shown for Astronomy, which has already reached universal post-publication OA)"}
{"pdf_id": "0808.3296", "content": "conflated with QB (Quality Bias):  Ever since Lawrence's original study in 2001, the OA Advantage can be estimated in two  different ways: (1) by comparing the average citations for OA and non-OA articles (log citation  ratios within the same journal and year, or regression analyses like Davis's) and (2) by  comparing the proportion of OA articles in different \"citation brackets\" (0, 1, 2, 3-4, 5-8, 9-16,  17+ citations)"}
{"pdf_id": "0808.3296", "content": "Hence both QB and QA are likely to be causal components in the OA Advantage, and the only  way to tease them apart and estimate their individual contributions is to control for the QB effect  by imposing the OA instead of allowing it to be determined by self-selection"}
{"pdf_id": "0808.3296", "content": "No OA advantage at all was observed in that 1-year  interval, and this too agrees with the many existing studies on the OA Advantage, some based on  far larger samples of journals, articles and fields: Most of those studies (none of them  randomized) likewise detected no OA citation advantage at all in the first year: It is simply too  early"}
{"pdf_id": "0808.3296", "content": ")  The only way the absence of a significant OA advantage in a sample with randomized OA can  be used to demonstrate that the OA Advantage is only or mostly just a self-selection bias (QB) is  by also demonstrating the presence of a significant OA advantage in the same (or comparable)  sample with nonrandomized (i"}
{"pdf_id": "0808.3296", "content": "But Davis et al. did not do this control comparison (Harnad 2008b). Finding no OA Advantage  with randomized OA after one year merely confirms the (widely observed) finding that one year is usually too early to detect any OA Advantage; but it shows nothing whatsoever about self selection QB."}
{"pdf_id": "0808.3296", "content": "Both analyses are, of course, a good idea to do, but why was Journal Impact Factor (JIF) not  tested as one of the predictor variables in the cross-journal analyses (Hajjem & Harnad 2007a)?  Surely JIF, too, correlates with citations: Indeed, the Davis study assumes as much, as it later  uses JIF as the multiplier factor in calculating the cost per extra citation for author-choice OA  (see below)"}
{"pdf_id": "0808.3296", "content": "But the other possibility is that length is a valid causal factor  in quality! If length is indeed an artifact, then longer articles are being cited more just because  they are longer, rather than because they are better, and this length bias needs to be subtracted  out of citation counts as measures of quality"}
{"pdf_id": "0808.3296", "content": "It is a reasonable, valid strategy, to analyze across journals. Yet this study still persists in  drawing individual-journal level conclusions, despite having indicated (correctly) that its sample  may be too small to have the power to detect individual-journal level differences (see below).  (On the other hand, it is not clear whether all the OA/non-OA citation comparisons were always  within-journal, within-year, as they ought to be; no data are presented for the percentage of OA  articles per year, per journal. OA/non-OA comparisons must always be within-journal/year  comparisons, to be sure to compare like with like.)"}
{"pdf_id": "0808.3296", "content": "Yes, but one could probably tell a Just-So story either way about the direction of that difference:  paying for OA because one thinks one's article is better, or paying for OA because one thinks  one's article is worse! Moreover, this is AC-OA, which costs money; the stakes are different  with SA-OA, which only costs a few keystrokes. But this analysis omitted to identify or measure  SA-OA."}
{"pdf_id": "0808.3296", "content": "(1) Compare the above with what is stated earlier: \"Because we may lack the statistical power to  detect small significant differences for individual journals, we also analyze our data on an  aggregate level.\"  (2) Davis found an OA Advantage across the entire sample of 11 journals, whereas the individual  journal samples were too small. Why state this as if it were some sort of an empirical effect?"}
{"pdf_id": "0808.3296", "content": "This reasoning can appeal only if one has a confirmation bias: PNAS is also the journal with the  biggest sample (of which only a fraction was used); and it is also the highest impact journal of  the 11 sampled, hence the most likely to show benefits from a Quality Advantage (QA) that  generates more citations for higher citation-bracket articles. If the objective had not been to  demonstrate that there is little or no OA Advantage (and that what little there is is just due to  QB), PNAS would have been analyzed more closely and fully, rather than being minimized and  excluded."}
{"pdf_id": "0808.3296", "content": "\"When other explanatory predictors of citations (number of authors,  pages, section, etc.) are included in the full model, only two of the  eleven journals show positive and significant open access effects.  Analyzing all journals together, we estimate a 17% citation advantage,  which reduces to 11% if we exclude PNAS.\""}
{"pdf_id": "0808.3296", "content": "If there were not this strong confirmation bent on the author's part, the data would be treated in a  rather different way: The fact that a journal with a bigger sample enhances the OA Advantage  would be treated as a plus rather than a minus, suggesting that still bigger samples might have  the power to detect still bigger OA Advantages"}
{"pdf_id": "0808.3296", "content": "What is certain is  that a 1-year-old 2007 article differs from a 4-year-old 2004 article not just in its total cumulative  citations in June 2008, but in that the estimate of its citations per year is based on a much smaller  sample, again reducing the power of the statistic: This analysis is not based on 2005 citations to  2004 articles, plus 2006 citations to 2005 articles, plus 2007 citations to 2006 articles, etc"}
{"pdf_id": "0808.3296", "content": "Hence it is not clear what the  Age/OA interaction in Table S2 really means: Has (1) the OA advantage for articles really been  shrinking across those 4 years, or are citation rates for younger articles simply noisier, because  based on smaller citation spans, hence (2) the OA Advantage grows more detectable as articles  get older?)  From what is described and depicted in Figure 1, the natural interpretation of the Age/OA interaction seems to be the latter: As we move from one-year-old articles (2007) toward four year-old articles, three things are happening: non-OA citations are growing with time, OA  citations are growing with time, and the OA/non-OA Advantage is emerging with time"}
{"pdf_id": "0808.3296", "content": "Although these costs are probably overestimated (because the OA Advantage is underestimated,  and there is no decline but rather an increase) the thrust of these figures is reasonable: It is not  worth paying for AC-OA for the sake of the AC-OA Advantage: It makes far more sense to get  the OA Advantage for free, through OA Self-Archiving"}
{"pdf_id": "0808.3296", "content": "not estimated the size of its contribution, relative to many other factors (AA, CA, DA, EA, QA).  It has simply shown that some of the same factors that influence citation counts, influence the  OA citation Advantage too.  By failing to test and control for the Quality Advantage in particular (by not testing JIFs in the  full regression equation, by not taking percentage OA per journal/year into account, by  restricting the sample-size for the highest impact, largest-sample journal, PNAS, by overlooking  OA self-archiving and crediting it to non-OA, by not testing citation-brackets of JIF quartiles),  the article needlessly misses the opportunity to analyze the factors contributing to the OA  Advantage far more rigorously."}
{"pdf_id": "0808.3296", "content": "There is some circularity in this, but it is correct to say that this correlation is compatible with  both QB and QA, and probably both are contributing factors. But none of the prior studies nor  this one actually estimate their relative contributions (nor those of AA, CA, DA and EA)."}
{"pdf_id": "0808.3296", "content": "It is not that CA (Competitive Advantage) disappears simply because time elapses: CA only  disappears if the competitors provide OA too! The same is true of QB (Quality Bias), which also  disappears once everyone is providing OA. But at 20%, we are nowhere near 100% OA yet;  hence there is still plenty of scope for a competitive edge."}
{"pdf_id": "0808.3296", "content": "The syntax here makes it a little difficult to interpret, but if what is meant is that Davis et al's  prior study has shown that the OA Advantage found in the present study was more likely to be a  result of QB than of QA, AA, CA, DA, or EA, then it has to be replied that that prior study  showed nothing of the sort (Harnad 2008b)"}
{"pdf_id": "0808.3296", "content": "A \"prospective\"  analysis, taking citing dates as well as cited dates into account, would be welcome (and is far  more likely to show that the size of the OA Advantage is, if anything, growing, rather than  confirming the author's interpretation, unwarranted on the present data, that it is shrinking)"}
{"pdf_id": "0808.3296", "content": "\"all of the journals under investigation make their articles freely  available after an initial period of time [hence] any [OA Advantage]  would be during these initial months in which there exists an access  differential between open access and subscription-access articles. We  would expect therefore that the effect of open access would therefore be  strongest in the earlier years of the life of the article and decline over  time. In other words, we would expect our trend (Figure 1) to operate  in the reverse direction.\""}
{"pdf_id": "0808.3296", "content": "\" But even in a  fast-moving field like Astronomy, the effect is not immediate! There is no way to predict from  the data for Astronomy how quickly an EA effect for nonsubscribers during the embargo year in  Biomedicine should make itself felt in citations, but it is a safe bet that, as with citation latency  itself, and the latency of the OA citation Advantage, the \"EmA\" (\"Embargo Access\") counterpart  of the EA effect in access-embargoed Biomedical journals will need a latency of a few years to  become detectable"}
{"pdf_id": "0808.3296", "content": "There is no monotonic decline to explain. Just (a) low power in initial years, (b) cumulative data  not analyzed to equate citing/cited year spans, (c) the failure to test for QA citation-bracket  effects, and (d) the failure to reckon self-archiving OA into the OA Advantage (treating it instead  as non-OA).  If this had been a JASIST referee report, I would have recommended performing several further  analyses taking into account:"}
{"pdf_id": "0808.3296", "content": "Full Disclosure: I am an OA advocate. And although I hope that I do not have a  selective confirmation bias favoring QA, AA, CA, DA & EA, and against the Quality  Bias (QB), I do think it is particularly important to ensure that QB is not given more  weight than it has been empirically demonstrated to be able to bear.   Davis writes:"}
{"pdf_id": "0808.3296", "content": "Lawrence, S. (2001) Free online availability substantially increases a paper's impact Nature 31  May 2001  Moed, H. F. (2007). The effect of 'Open Access' upon citation impact: An analysis of ArXiv's  Condensed Matter Section. Journal of the American Society for Information Science and  Technology 58(13): 2047-2054  Seglen, P. O. (1992). The Skewness of Science. Journal of the American Society for Information  Science 43(9): 628-638"}
{"pdf_id": "0809.0406", "content": "Real world problems often comprise several points of view that from a decision makers perspective have to be taken simultaneously into consideration. Multi-objective optimization approaches play in this context an increasingly important role, tackling applications in numerous areas. Due to the complexity of mostproblems however, problem resolution has to rely in many cases on modern heuristics that provide fast re sults without necessarily identifying an optimal solution. Here, local search approaches like e. g. Simulated Annealing, Evolutionary Algorithms, and Tabu Search play a dominant role. Depending on the application area, more and more refined version and adaptations of local search metaheuristics have been proposed with increasing success in recent years."}
{"pdf_id": "0809.0406", "content": "Scheduling is one of the most active areas of research, with applications in numerous areas of manufac turing, computer systems/grid scheduling, sports/tournament scheduling, and airline/neet scheduling, to mention a few. Many of the mentioned problems are of multi-criteria nature, and considerable effort has been made to solve these often NP-hard problems. While metaheuristics often lead to acceptable results,room for improvements can still be identified, especially as modern metaheuristics tend to require increas ingly complex parameter settings."}
{"pdf_id": "0809.0406", "content": "The current paper describes an local search heuristic for the effective resolution of multi-objective opti mization problems, based on the local search paradigm. An application of the approach is presented to the multi-objective permutation now shop scheduling problem. The article is organized as follows. Section 2first introduces the considered problem and brieny reviews heuristic solution approaches known from lit erature. The Pareto Iterated Local Search algorithm is then presented in Section 3. An application of the metaheuristic to the discussed problem is given in the following Section 4, and conclusions are drawn in Section 5."}
{"pdf_id": "0809.0406", "content": "Others express violations of due dates dj of jobs Jj. A due date dj defines a latest point of time until a job Jj should be finished as the assembled product has to be delivered to the customer on this date. The computation of an occurring tardiness Tj of a job Jj is given in Expression (3). A possible optimality criteria based on tardiness of jobs is e. g. the total tardiness Tsum as given in Expression (4)."}
{"pdf_id": "0809.0406", "content": "Flow shop scheduling problems with three objectives are studied by (Ishibuchi and Murata, 1998), and (Ishibuchi, Yoshida and Murata, 2003). The authors minimize the maximum completion time, the totalcompletion time, and the maximum tardiness at once. A similar problem minimizing the maximum com pletion time, the average now time, and the average tardiness is then tackled by (Bagchi, 1999; Bagchi, 2001)."}
{"pdf_id": "0809.0406", "content": "The main principle of the algorithm is sketched in Figure 1. Starting from an initial solution x1, an im proving, intensifying search is performed until a set of locally optimal alternatives is identified, stored in a set P approx representing the approximation of the true Pareto set P. No further improvements are possible from this point. In this initial step, a set of neighborhoods ensures that all identified alternatives are locallyoptimal not only to a single but to a set of neighborhoods. This principle, known from Variable Neighbor hood Search, promises to lead to better results as it is known that all global optima are also locally optimal with respect to all possible neighborhoods while this is not necessarily the case for local optima."}
{"pdf_id": "0809.0406", "content": "The PILS metaheuristic may be formalized as given in Algorithm 1. The intensification of the algorithm, illustrated in the steps (1) and (3) of Figure 1 is within the lines 6 to 21, the description of the diversification, given in step (2) of Figure 1 is within the lines 22 to 26."}
{"pdf_id": "0809.0406", "content": "In the following, the Pareto Iterated Local Search is applied to a set of benchmark instances of the multi objective permutation now shop scheduling problem. They have been provided by (Basseur, Seynhaeve and Talbi, 2002), who first defined due dates for the well-known instances of (Taillard, 1993). The instances range from n = 20 jobs that have to be processed on m = 5 machines to n = 100, m = 20. All of them are solved under the simultaneous consideration of the minimization of the maximum completion time Cmax and the total tardiness Tsum."}
{"pdf_id": "0809.0406", "content": "An implementation of the algorithm has been made available within the MOOPPS computer system, a software for the resolution of multi-objective scheduling problems using metaheuristics. The system is equipped with an extensive user interface that allows an interaction with a decision maker and is able to visualize the obtained results in alternative and outcome space. The system also allows the comparison of results obtained by different metaheuristics. For a first analysis, we compare the results obtained by PILS to the approximations of a multi-objective multi-operator search algorithm MOS, described in Algorithm 2."}
{"pdf_id": "0809.0406", "content": "The MOS Algorithm is based on the concept of Variable Neighborhood Search, extending the general idea of several neighborhood operators by adding an archive P approx towards the optimization of multi-objective problems. For a fair comparison, the same neighborhood operators are used as in the PILS algorithm. After the termination criterion is met in step 10, we restart search while keeping the approximation P approx for the final analysis of the quality of the obtained solutions."}
{"pdf_id": "0809.0406", "content": "When analyzing the convergence of local search heuristics toward the globally Pareto front as well as towards locally optimal alternatives, the question arises how many local search steps are necessary until a locally optimal alternative is identified. From a different point of view, this problem is discussed in the context of computational complexity of local search (Johnson, Papadimitriou and Yannakakis, 1988). It might be worth investigating this behavior in quantitative terms. Table 3 gives the average number of evaluations that have been necessary to reach a locally optimal alternative from some randomly generated initial solution. The analysis reveals that the computational effort grows exponentially with the number of jobs n."}
{"pdf_id": "0809.0406", "content": "In the past years, considerable progress has been made in the resolution of complex multi-objective optimization problems. Effective metaheuristics have been developed, providing the possibility of computing approximations to problems with numerous objectives and complex side constraints. While many ap proaches are of increasingly effectiveness, complex parameter settings are however required to tune the"}
{"pdf_id": "0809.0406", "content": "After an initial introduction to the problem domain of now shop scheduling under multiple objectives, theintroduced PILS algorithm has been applied to a set of scheduling benchmark instances taken from litera ture. We have been able to obtain encouraging results, despite the simplicity of the algorithmic approach. A comparison of the approximations of the Pareto sets has been given with a multi-operator local search approach, and as a conclusion PILS was able to lead to consistently better results.The presented approach seems to be a promising tool for the effective resolution of multi-objective opti mization problems. After first tests on problems from the domain of scheduling, the resolution behavior on"}
{"pdf_id": "0809.0410", "content": "The vehicle routing problem with soft time windows can be described as fol lows: A known number of customers have to be delivered from a depot with aknown amount of goods for which an unlimited number of homogeneous ve hicles is available. It is assumed that each customer is visited by exactly one vehicle and a loading and a travelling constraint exists for the vehicles. A soft time window is associated with each customer, defining a desired earliest and a latest time of service. Violation of these time windows does not lead to infeasibility of the solution. With respect to the soft nature of the time windows, it is assumed that service is done immediately after the arrival of"}
{"pdf_id": "0809.0410", "content": "the vehicle. The objective of the problem is to maximize quality of service and to minimize cost, such that the requirements of the customers and the side-constraints are met. It is obvious, that the violation of the time windows has to be minimized in order to achieve a high quality of service. This can be done by minimizing the number of time window violations and the time window violations itself, measured in time units. The cost consist of a fixed part, induced by the number of used vehicles and a variable part, caused by the route length and the travel time."}
{"pdf_id": "0809.0410", "content": "As our goal is to minimize the distance between the obtained approximationsP approx and the reference set P ref, the distances d1 and d2 are to be mini mized. To come to stable and reliable conclusions, average values of d1 and d2 of several test runs with the same configuration are computed."}
{"pdf_id": "0809.0410", "content": "the studied crossover operators themselves are comparable weak for the multi objective formulation of the problem as they do not recombine the desirablestructures of the underlying model. Nevertheless, specific formulations of par ticular multi-objective operators are still missing. A combination of genetic operators with local search heuristics is consequently a logical conclusion of the obtained results."}
{"pdf_id": "0809.0416", "content": "For example the alternative with the shortest routes is compared to the alternative having the lowest time window violations. The windows show the routes, travelled by the vehicles from the depot to the customers. The time window violations are visualized with vertical bars at each customer. Red: The vehicle is too late, green: the truck arrives too early."}
{"pdf_id": "0809.0458", "content": "AGENT MODELS OF POLITICAL INTERACTIONS  Eric Engle  AGENT MODELS OF POLITICAL INTERACTIONS.................................... 1  INTRODUCTION .................................................................................................................1  I. SOCIAL SCIENCE............................................................................................................1  A. Emergence in Social Sciences...............................................................................1  B. The contemporary international system ................................................................5  II. COMPUTER SCIENCE ...................................................................................................5  A. AI in Game Theory ...............................................................................................5  1. Game Theory..............................................................................................5  2. Coalitions...................................................................................................6  3. Coalitional Game Theory...........................................................................6  4. Opponent Modeling ...................................................................................7  B. Existing Research..................................................................................................9  1. Scenarios....................................................................................................10  2. Technologies..............................................................................................11  3. Implementations.........................................................................................11  RISK...................................................................................................11  DIPLOMACY....................................................................................12  BALANCE OF POWER....................................................................12  CONSIM ............................................................................................12  Critique...................................................................................14  III. IMPLEMENTATION .....................................................................................................15  A. Agent Strategies....................................................................................................16  B. Agent Intentions....................................................................................................17  C. Learning Functions................................................................................................17  D. Results ..................................................................................................................17  E. Paths for future research........................................................................................17  CONCLUSIONS ...................................................................................................................18  BIBLIOGRAPHY..................................................................................................................20 Eric Engle"}
{"pdf_id": "0809.0458", "content": "persons living in it. Out of these individual transactions of real persons an artificial person  2  See, Adam Smith, On the Nature and Causes of the Wealth of Nations (1776)  http://www.econlib.org/library/Smith/smWN.html  3  Id. Book I, Chapter I note 39.  4  David Ricardo, On The Principles of Political Economy and Taxation, Ch. 7 (1817)  http://www.marxists.org/reference/subject/economics/ricardo/tax/ch07.htm"}
{"pdf_id": "0809.0458", "content": "8  Andrew Grosso, The Demise of Sovereignty, 44/3 Communications of the ACM (2001) p. 102.  9  \"The main trend in the postwar international system is proliferating complexity in all dimensions of  analysis and a parallel information explosion.\" John Mallery, \"Thinking about Foreign Policy: Finding an  Appropriate Role for Artificially Intelligent Computers\", 1998 Annual Meeting of the International Studies  Association (1988)"}
{"pdf_id": "0809.0458", "content": "Further, they were in fact very unequal powers in  terms of their disposable wealth and military capacity (the US had an absolute advantage as to the former and  a relative advantage as to the later after 1949) and also in their ability to appeal to third parties (where the  USSR had a potential advantage)"}
{"pdf_id": "0809.0458", "content": "Gavin Duffy, Seth Tucker, Investigation of the Potential Contribution of AI Methods  to the Avoidance of Crises and Wars, Social Science Computing Review (Spring, 1995)  15  Gary King, Brent Heeringa, David Westbrook, Joe Catalano, Paul Cohen, \"Models of Defeat\",  Proceedings of the 2002 Winter Simulation Conference (2002) p"}
{"pdf_id": "0809.0458", "content": "\" Gavin Duffy, Seth Tucker, Investigation of the  Potential Contribution of AI Methods to the Avoidance of Crises and Wars, Social Science Computing  Review (Spring, 1995)  26  \"In reviewing the main AI applications in political science, we confess our inability to categorize  these efforts neatly"}
{"pdf_id": "0809.0458", "content": "\" Gavin Duffy, Seth Tucker,  Investigation of the Potential Contribution of AI Methods to the Avoidance of Crises and Wars, Social  Science Computing Review (Spring, 1995)  27  Gavin Duffy, Seth Tucker, Investigation of the Potential Contribution of AI Methods to the  Avoidance of Crises and Wars, Social Science Computing Review (Spring, 1995)"}
{"pdf_id": "0809.0458", "content": "44  John Mallery, Thinking about Foreign Policy: Finding an Appropriate Role for Artificially Intelligent  Computers, 1998 Annual Meeting of the International Studies Association (1988)  45  \"As the time required to take actions and react decreases, the rate at which actions and reactions can  occur increases. This increases the gain in the system which in turn, increases the probability of non-linear  amplification of small intiial perturbations in strategic systems. Thus, even if the AI system works correctly,  the presence of these systems can increase gain, and therefore, lower the stability of international security  sytems.\" Id.  46  Id.  47  Id."}
{"pdf_id": "0809.0610", "content": "Unfortunately, most problems of this domain are NP-hard. As a result, heuristics and more recently metaheuristics have been developed with increasing success [5]. In order to improve known results, more and more refined techniques have been proposed that are able to solve, or at least approximate very closely, a large number of established benchmark instances. With the increasing specialization of techniques goes however a decrease in generality of the resolution approaches."}
{"pdf_id": "0809.0610", "content": "A solutions is constructed by placing the orders on the marketplace, collecting bids from the vehicle agents, and assigning orders to vehicles while constantly updating the bids. Route construction by the vehicle agents is done in parallel using local search heuristics so that a route can be identified that maximizes the preferences of the decision maker."}
{"pdf_id": "0809.0610", "content": "The decider assigns orders to vehicles such that the maximum regret when not assigning the order to a particular vehicle, and therefore having to assign it to some other vehicle, is minimized. It also analyzes the progress of the improvement procedures. Given no improvement for a certain number of iterations, the decider forces the vehicle agents to place back orders on the market such that they may be reallocated."}
{"pdf_id": "0809.0610", "content": "We simulated a decision maker changing the relative importance wDIST during the optimization procedure. First, a decision maker starting with a wDIST = 1 and successively decreasing it to 0, second a decision maker starting with a wDIST = 0 and increasing it to 1, and third a decision maker starting with a wDIST = 0.5, increasing it to 1 and decreasing it again to 0. Between adjusting the values of wDIST in steps of 0.1, enough time for computations has been given to the system to allow a convergence to (at least) a local optimum. Figure 2 plots the results obtained during the test runs."}
{"pdf_id": "0809.0610", "content": "The first decision maker starts with DIST = 975, TARDY = 6246 and moves to DIST = 1412, TARDY = 0 while the second starts with DIST = 2953, TARDY = 0 and moves to DIST = 1326, TARDY = 3654. Clearly, the first strategy outperforms the second. While an initial value of wDIST = 0 allows the identification of a solution with zero tardiness, it tends to construct routes that, when decreasing the relative importance of the tardiness, turn out to be hard to adapt. In comparison to the strategy starting with a wDIST = 1, the clustering of orders turns out the be prohibitive for a later improvement."}
{"pdf_id": "0809.0610", "content": "When comparing the third strategy of starting with a wDIST = 0.5, it becomes obvious that this outperforms both other ways of interacting with the system. Here, the solutions start with DIST = 1245, TARDY = 63, go to DIST = 946, TARDY = 4342, and finally to DIST = 1335, TARDY = 0. Apparently, starting with a compromise solution is beneficial even for both extreme values of DIST and TARDY ."}
{"pdf_id": "0809.0610", "content": "Future developments are manifold. First, other ways of representing preferences than a weighted sum approach may be beneficial to investigate. While the comparable easy interaction with the GUI by means of a slider bar enables the user to directly change the relative importance of the objective functions, it prohibits the definition of more complex preference information, e. g. involving aspiration levels."}
{"pdf_id": "0809.0723", "content": "Data integration is recently the center issue among the infor mation management communities. Because data integration is intended to overcome the phenomena of information nooding, and on the other the information islands. The second one refers to a condition of separating data pools, though within the same topic, which are not well connected nor integrated each other. Both obscure the potential users to access and to efficiently use the available data. Although data archiving is an important aspect of information and knowledge management since long time ago, it would unfortunately not benefit the stakeholders without improving the accessibility to the data itself. There are several methods to establish either real or virtual, and partial or total data integration. Some widely implemented methods can be listed as follows :"}
{"pdf_id": "0809.0723", "content": "• Electronic integration over dedicated network : In this system all participating databases remain at theiroriginal places, but all of them are connected and inte grated at real-time basis through a secure private network. This method is rather costy, relies highly on the reliability of network, requiring a uniform platform and applications among the participating databases. Though, it would keep the accuracy as the conventional method."}
{"pdf_id": "0809.0723", "content": "• Conventional search engine : This method is categorized as virtual data integration. Because it integrates the data through the index databases updated in a regular basis. The severe problem is the data retrieval is done through indiscriminate crawlings of any web pages in relevant sites. It pays the ease with much less accuracy. Moreover, the results often generate another type of information nooding."}
{"pdf_id": "0809.0723", "content": "• Federated search : This is recently developed approach to provide a single gateway of search engine enabling simultaneous search at multiple online databases. It is actually an emerging feature of automated, web-based library and information retrieval systems. However, this requires well connectedand online databases. Also the system should be established under official agreements among participating insti tutions, and requires some modifications at each database to allow query requests from the gateway. Regardless a need for data integration is obvious, in reality there are many non-technical obstacles to realize it. We point out some of them :"}
{"pdf_id": "0809.0723", "content": "• Moreover, in that case requirement of modifications or deploying universal standard at each site would increase refusal, since each institution has developed their own system with some uniqueness that might not be able to be accommodated under universal standard. Worsely, there might in some cases be contradictory requirements among them."}
{"pdf_id": "0809.0723", "content": "• Data integration over distributed databases requires nu merous number of skilled human resources to maintain. Therefore, no matter how good the idea of data integration is, in most cases it doesn't work as expected. More importantly, the issues are less technical like the data format, etc. So we should find any intermediate solutions to overcome the problem and to realize data intregation in an efficient manner. For the sake of simplicity, let us focus on the topical"}
{"pdf_id": "0809.0723", "content": "data integration. Also by its nature, the data integration is mostly relevant only for topical integration. In this paper wepropose a new method based on the so-called focused web harvesting. After explaining its concept in the next section, we discuss in detail the general architecture. After introducing its implementation to the Indonesian Scientific Index (ISI), we finish the paper with conclusion and some comments on future developments."}
{"pdf_id": "0809.0723", "content": "• A centralized infrastructure : There should be a centralized infrastructure hosted and maintained by a leading institution or consortium in the topic. Because once a data integration gateway started providing the service, it would grow very fast and soonrequires more financial backup for maintenance and fur ther expansion along with increasing traffics, spaces and memories to handle properly all data."}
{"pdf_id": "0809.0723", "content": "Actually the first point is consistent with recent facts that suc cessful topical data storages which de-facto integrate all data in some fields are pioneered and hosted in a centralized manner by a leading institution. For example the Astrophysics Data System by SAO [1], the preprint repository arXiv pioneered by LANL [2], the Protein Data Bank by RCSB [3] and the DBRiptek by KRT [4]. Yet, all of them are based on either voluntary or incentive-driven submission by the data owners."}
{"pdf_id": "0809.0723", "content": "In order to improve the accuracy and avoid wasting the resources to crawl irrelevant web pages, we have adopted the conventional web-harvesting with more human-guidance parameters setup. The whole mechanism is renected in the following initial procedure for each target and should be done by the administrators of participating institutions over web :"}
{"pdf_id": "0809.0723", "content": "The same procedure should be done done for each type of contents maintained by the institutions. We should emphasize that this procedure is handed over to the administrator of each institution to keep the parameter set of each targeted URL to be accurate. It also avoids unnecessary delay of knowing design or any other detail changes at the"}
{"pdf_id": "0809.0723", "content": "harvested targets, and provides a freedom for the institution to decide what and how their contents are crawled. The nowchart of harvesting mechanism is depicted in Fig. 1. As shown in the figure, in principle the full human guidance targeted URL can be complemented with machine guidance by adopting text-mining based self-learning system in the harvesting mechanism. Through the above-mentioned procedure, it is clear that the human-guided parameters would reduce significantly crawling of irrelevant information. Also the mechanism gets rid ofsome policies commonly concerned in regular or focused web crawlings like :"}
{"pdf_id": "0809.0723", "content": "• Selection policy : This policy is not more relevant in our approach, since all targeted URLs are well-defined and automatically already filtered in some sense. In other word all pages are considered important. Also, no need to concern about restricting followed links in crawled pages and how to deal with path-ascending crawling, focused crawling and the deep web."}
{"pdf_id": "0809.0723", "content": "• Intellectual property right (paten, copyright, etc). The total targeted URLs for all types of contents reaches more than a hundred with few tenth thousands indexed pages. During the first beta running till March 2008, the algorithms performs perfectly as expected. This might be due to full human-guided parameters setup through the web interface as seen in Fig. 4. We have yet not complemented with the automated machine guidance using self-learning systems."}
{"pdf_id": "0809.0723", "content": "[1] Smithsonian Astrophysical Observatory, The Astrophysics Data System, http://adsabs.harvard.edu. [2] Los Alamos National Laboratory, arXiv, http://www.arxiv.org. [3] Research Collaboratory for Structural Bioinformatics, Protein Data Bank, http://www.rcsb.org/pdb/. [4] Indonesian Ministry of Research and Technology, Database Riset, Ilmu Pengetahuan dan Teknologi, http://www.dbriptek.ristek.go.id. [5] F. Menczer, ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery, Proc. of the 14th International Conference on Machine Learning (ICML97). Morgan Kaufmann, 1997. [6] F. Menczer and R.K. Belew, Adaptive Information Agents in Distributed Textual Environments, Proc. of the 2nd International Conference on Autonomous Agents (Agents '98), ACM Press, 1998."}
{"pdf_id": "0809.0753", "content": "Abstract— The article presents an approach to interactivelysolve multi-objective optimization problems. While the iden tification of efficient solutions is supported by computational intelligence techniques on the basis of local search, the search is directed by partial preference information obtained from the decision maker.An application of the approach to biobjective portfolio op timization, modeled as the well-known knapsack problem, is reported, and experimental results are reported for benchmarkinstances taken from the literature. In brief, we obtain encour aging results that show the applicability of the approach to the described problem."}
{"pdf_id": "0809.0753", "content": "1) Search for optimal alternatives (the Pareto set P), sup ported by an optimization approach. In comparison to single-objective optimization approaches, the notion of optimality is here generalized with respect to the set of simultaneously considered optimality criteria. 2) Choice of a most-preferred solution by the decisionmaker of the particular situation. While in singleobjective optimization problems, the choice of the most preferred solution naturally follows the identification of the (single) optimal solution, in multi-objective problems an individual tradeoff between connicting criteria has to be resolved in a decision making procedure."}
{"pdf_id": "0809.0753", "content": "1) A priori approaches reduce the multi-objective problem into a single-objective problem by constructing a utility function for the decision maker. The resolution of the problem then lies in the identification of the solution which maximizes the chosen utility function. 2) A posteriori approaches first identify the Pareto set P (or a close and representative approximation) and then resolve the choice of a most-preferred solution within an interactive decision making procedure. 3) Interactive approaches combine search and decisionmaking, presenting one or several solutions to the deci sion maker and collecting preference information which is then used to further guide the search for higher preferred alternatives."}
{"pdf_id": "0809.0753", "content": "Recent approaches of computational intelligence techniques implement interactive problem resolution procedures, e. g. on the basis of Evolutionary Algorithms [3], involving a decisionmaker during search. While in these approaches the set of cri teria remains fixed during search, other concepts also include the possibility of dynamically changing the relevant criteria when searching for a most-preferred solution [4]. Research in interactive computational techniques is however a rather new field, and the precise way of how to integrate articulated preferences in the search process is still to be investigated in more detail."}
{"pdf_id": "0809.0753", "content": "In this article, we aim to contribute to the development of interactive computational intelligence techniques for the resolution of multi-objective optimization problems. While thesearch for Pareto-optimal alternatives is done by metaheuris tics on the basis of local search, individual preferences guide the search in a particular direction with the goal of identifying a subset of P that is considered to be of interest to the decision maker. While the idea is generic, it is tested on a particular application."}
{"pdf_id": "0809.0753", "content": "The article is organized as follows. In the following Section II, the biobjective portfolio optimization problem is intro duced and a quantitative optimization model is presented. We also brieny review existing approaches from the literature that have been used to solve this problem. An interactive procedure to solve the problem is proposed in Section III. Experimental investigations on benchmark instances taken from literature follow in Section IV, and conclusions are drawn in Section V."}
{"pdf_id": "0809.0753", "content": "Based on the data gathered in the experiments, the arithmetic mean values of M have been computed, depending num ber of evaluations of the metaheuristic. These average values, given in Figure 3, clearly show that the iPILS metaheuristic successfully identified the Pareto-optimal alternatives in the particular areas of the reference points. However, there does not turn out to be a consistent difference for the three chosen reference points within the same instance."}
{"pdf_id": "0809.0755", "content": "Abstract— The article proposes a heuristic approximation ap proach to the bin packing problem under multiple objectives. In addition to the traditional objective of minimizing the number of bins, the heterogeneousness of the elements in each bin is minimized, leading to a biobjective formulation of the problemwith a tradeoff between the number of bins and their heteroge neousness. An extension of the Best-Fit approximation algorithm is presented to solve the problem. Experimental investigations have been carried out on benchmark instances of different size, ranging from 100 to 1000 items. Encouraging results have been obtained, showing the applicability of the heuristic approach to the described problem."}
{"pdf_id": "0809.0755", "content": "Expression (1) minimizes the number of bins. The secondobjective given in (2) minimizes the average heterogeneous ness of the bins. To do this, the number of distinct attributes ui is counted for each bin i. Unused bins (yi = 0) have a value of ui = 0. Used bins (yi = 1) have a possible minimum value of ui = 1. This is the case when all items in the particular bin have the identical nominal attribute. The values of ui are bounded by either the number of items assigned to a bin or the number of distinct attributes over all items i."}
{"pdf_id": "0809.0755", "content": "The experimental investigations revealed that only few effi cient outcomes exist for the instances. Instead of plotting the outcomes in figures, we chose to give the data of all found best vectors Z(x) = (z1(x), z2(x)). The following Table I shows the results for the smallest instance with n = 100. It can be seen, that both Best-Fit and Random-Fit perform comparably good given a decreasing or random order of the items."}
{"pdf_id": "0809.0757", "content": "Abstract The article presents a local search approach for the solution of timetablingproblems in general, with a particular implementation for competition track 3 of the In ternational Timetabling Competition 2007 (ITC 2007). The heuristic search procedure is based on Threshold Accepting to overcome local optima. A stochastic neighborhood is proposed and implemented, randomly removing and reassigning events from the current solution. The overall concept has been incrementally obtained from a series of experiments, which we describe in each (sub)section of the paper. In result, we successfully derived a potential candidate solution approach for the finals of track 3 of the ITC 2007."}
{"pdf_id": "0809.0757", "content": "1. A room capacity soft constraint tries to ensure that the number of students attend ing a lecture does not exceed the room capacity. 2. Lectures must be spread into a minimum number of days, penalizing timetables in which lectures appear in too few distinct days. 3. The curricula should be compact, meaning that isolated lectures, that is lectures without another adjacent lecture, should be avoided. 4. All lectures of a course should be held in exactly one room."}
{"pdf_id": "0809.0757", "content": "The overall evaluation of the timetables is then based on a weighted sum approach, combining all four criteria in a single evaluation function. While we adopt this approach in the current article, is should be mentioned that Pareto-based approaches may be used as an alternative way to handle the multi-criteria nature of the problem."}
{"pdf_id": "0809.0757", "content": "It should be noticed that the behavior of the approach for the other benchmarkinstances is similar. This observation is however less important, as a repetitive applica tion of the simple constructive approach will increase the percentage of cases in which a feasible solution is reached, too. For instance comp05.ctt, where not a single feasible solution is found after the first loop, this does not hold."}
{"pdf_id": "0809.0757", "content": "Obviously, the Threshold Accepting algorithm did not converge after only 375 sec onds. Rather big improvements can be seen for most instances, sometimes improving the best solution by 25% (comp10.ctt). For the instances with large values of sc,comp05.ctt and comp12.ctt, improvements are possible, but the absolute values re main rather high. We suspect that these instances possess properties that complicate the identification of timetables with small soft constraint violations. Recalling that instance comp05.ctt was problematic with respect to the identification of a feasible assignment in the initial experiments, this is however not surprising. No improvements are possible for instance comp01.ctt, and of course for instance comp11.ctt."}
{"pdf_id": "0809.0788", "content": "AbstractThis paper studies peek arc consistency, a reasoning technique that extends the well known arc consistency technique for constraint satisfaction. In contrast to other more costly extensions of arc consistency that have been studied in the literature, peek arc consistency requires only linear space and quadratic time and can be parallelized in a straightforward way such that it runs in linear time with a linear number of processors. We demonstrate that for various constraint languages, peek arc consistency gives a polynomial-time decisionprocedure for the constraint satisfaction problem. We also present an algebraic characteriza tion of those constraint languages that can be solved by peek arc consistency, and study the robustness of the algorithm."}
{"pdf_id": "0809.0922", "content": "Superposition is a sound and refutationally complete calculus for the standard semantics |=. In this paper, we develop a sound and refutationally complete calculus for |=F. Given a clause set N and a purely existentially quantified conjecture, standard superposition is also complete for |=F. The problem arises with universally quantified conjectures that become existentially quantified after negation. Then, as soon as these existentially quantified variables are Skolemized, the standard"}
{"pdf_id": "0809.0922", "content": "In this section, we will present a saturation procedure for sets of constrained clauses over a domain T (F) and show how it is possible to decide whether a saturated constrained clause set possesses a Herbrand model over F. The calculus extends the superposition calculus of Bachmair and Ganzinger [Bachmair and Ganzinger 1994]. Before we come to the actual inference rules, let us review the semantics of constrained clauses by means of a simple example. Consider the constrained clause set"}
{"pdf_id": "0809.0922", "content": "These propositions can also be proved using agruments from model theory. The shown proofs using superposition or SFD, respectively, notably the argument aboutthe lack of new productive clauses, illustrate recurring crucial concepts of super position-based inductive theorem proving. We will see in Example 4.4 that other superposition-based algorithms often fail because they cannot obviate the derivation of productive clauses."}
{"pdf_id": "0809.0922", "content": "Using Proposition 4.2, we can employ the calculus SFD for fixed domain reasoning to also decide properties of minimal models. This is even possible in cases for which neither the approach of Ganzinger and Stuber [Ganzinger and Stuber 1992] nor the one of Comon and Nieuwenhuis [Comon and Nieuwenhuis 2000] works."}
{"pdf_id": "0809.0922", "content": "The notation of the rules is taken from [Comon 1991]. Almost all rules are reduction or simplification rules. The only exception is the explosion rule E(x) which performs a signature-based case distinction on the possible instantiations for the variable x: either x = 0 or x = s(t) for some term t. No rule is applicable to the last formula, but there is still a universal quantifier left. Hence the quantifier elimination is not successful."}
{"pdf_id": "0809.0922", "content": "The given version of this rule is in general not sound for |=F but glued to the currently considered model IN; however, analogous results hold for every Herbrand model of N over F and even for arbitrary sets of such models, in particular for the set of all Herbrand models of N over F"}
{"pdf_id": "0809.0922", "content": "Some examples will demonstrate the power of the extended calculus IS(H). In these examples, there will always be a unique (non-empty) set H satisfying the side conditions of the induction rule, and we will write IS instead of IS(H).The induction rule will often allow to derive an unbounded number of conclu sions. So the application of this rule in all possible ways is clearly unfeasible. It seems appropriate to employ it only when a conclusion can directly be used for a superposition inference simplifying another constrained clause. We will use this heuristic in the examples below."}
{"pdf_id": "0809.0922", "content": "We have presented the superposition calculi SFD and SFD+, which are sound and refutationally complete for a fixed domain semantics for first-order logic. Compared to other approaches in model building over fixed domains, our approach is applicable to a larger class of clause sets. We showed that standard first-order and fixed domain superposition-based reasoning, respectively, delivers minimal model results for some cases. Moreover, we presented a way to prove the validity of minimal model properties by use of the calculus IS(H), combining SFD and a specific induction rule."}
{"pdf_id": "0809.0961", "content": "The resolution of multi objective scheduling problems is supported by a procedure consisting of two stages. First, Pareto optimal alternatives or an approximation Pa of the Pareto set P are computed using the chosen metaheuristics. Second, an interactive search in the obtained results is performed by the decision maker."}
{"pdf_id": "0809.1618", "content": "This document (\"ECOLANG_v_1_3c_Eng.doc\") describes the communication language used  in one multi-agent systems environment for ecological simulations, based on the EcoDynamo  simulator application (Pereira and Duarte 2005) linked with several intelligent agents and  visualisation applications and extends the initial definition of the language (Pereira et al.  2005)."}
{"pdf_id": "0809.1618", "content": "2.1 Connection messages  Connection messages define the start and the finish of the communications sessions between  applications. In this group there are also messages to ask the agents known by the other  partner of the session. This allows the establishment of links between multiple applications,  facilitating the expansion of the communications and knowledge network."}
{"pdf_id": "0809.1618", "content": "To deposit (seed), the agent indicates the region, the time, the characteristics of the species of  molluscs to deposit and the total weight seeded. The two real values indicated in the message  may have different meanings, depending on molluscs in question. By example, for the oysters  and scallops, the first value indicates the individual weight of the shell and the second  indicates the individual weight of meat; for clams, the first value indicates the individual dry  weight, and the second indicates the individual weight."}
{"pdf_id": "0809.1618", "content": "Any agent / application can act over the simulator choosing the model it wants to simulate,  controlling the parameterization of the model - gathering / changing parameters of the  simulated classes and collecting / recording the results of the simulation. Messages can be  divided into four different types:"}
{"pdf_id": "0809.1618", "content": "The response to the seed action of the agent may be positive or negative (in the case such  action is denied). In response to the inspection action the agent receives a message with the  bivalve's characteristics in the region. The resulting harvest is negative or positive, and in this  case, it is indicated the total weight harvested."}
{"pdf_id": "0809.1618", "content": "The communication between the simulator (EcoDynamo application) and the other actors  present in the simulation system is usually of the type handshake - a message-type action  expects to receive an answer from the destination application; that response comes in the form  of a perception type message."}
{"pdf_id": "0809.1618", "content": "The first message of each agent for the simulator must be connected (connect). The reception  of a positive acceptance message (to accept ok result) indicates that the agent was registered  in the simulator as an agent interested in obtaining results from the simulations. When the  agent leaves the system it must send the message to disconnect from the simulator."}
{"pdf_id": "0809.1618", "content": "1 This is the answer while there were messages to send from morphology: morphology of each message  has, at most, 750 elements.  2 This is the answer indicating end of morphology messages.  3 This is the answer while there were messages to send from benthic species: each benthic species  message has, at most, 150 elements.  4 This is the answer indicating end of benthic species messages."}
{"pdf_id": "0809.1618", "content": "4.1 Header Files  The header files contain the definition of the EcoDynProtocol class, the message  symbols and the data structures used.  Folder: DLLs/ECDProtocol  Files:  EcoDynProtocol.h,  ECDPMessages.h,  ECDPAgents.h,  AgentsTable.h  e  Region.h.  Note: the file EcoDynProtocol.h includes the other ones."}
{"pdf_id": "0809.1686", "content": "Many mathematical models used in the fields of ecol ogy, economics and environmental science are based on  a body of knowledge formed with not generally  accepted theories, debatable or controversial hypothesis,  questionable simplifications and a bundle of implicit or  ambiguous assumptions, i.e., based on an imperfect  understanding of the dynamics of the object systems.  This leads to highly uncertain model results because of  the uncertainty associated with model parameters and inputs and, sometimes, the uncertainty in model struc ture [1]."}
{"pdf_id": "0809.1686", "content": "When an ecological model is built, those uncertain ties are intrinsic to the model and the major problem is  to quantify the quality of the simulations in order to recognize if a modification of the concepts, laws simulating the processes or model parameters would im prove it [2].If the concepts and laws of the simulated processes are well established, attention must be di rected to deciding parameter values. Calibration of these  parameters, i.e., defining appropriate values for each  parameter in the simulation in order to approximate simulation results to reality, is a task of major impor tance."}
{"pdf_id": "0809.1686", "content": "Several procedures for automatic calibration and validation are available in the literature, like the Con trolled Random Search (CRS) method [1][3] or linear  regression techniques [2]. However, these procedures  do not capture the complexity of human reasoning in the calibration process. They are based on the system atic and exhaustive generation of parameter vectors and  require a large number of model runs, demanding heavy  computationally search operations. In addition, when  the model is very complex, those procedures demand  large computational time."}
{"pdf_id": "0809.1686", "content": "The traditional calibration is oriented, i.e., the \"mod eller\" analyses the results and, in face of his knowledge about the behaviour of different mathematical relation ships, some common sense reasoning is used to choose new values for each parameter. The systematic ap proach described in [4] argues that the ultimate use of  the model should be explicitly acknowledged in the  calibration process. These procedures raise the question: \"Is it possible to implement that common sense reason ing in an automatic calibration system when the model  is very complex?\" Being able to answer this question  raises an even more challengeable one: \"Is it possible to  implement a generic automatic calibration system that  learns for itself and is self-adaptable to any model?\""}
{"pdf_id": "0809.1686", "content": "This paper introduces a new approach to answer these two questions: an agent-based calibration software. The architecture for the calibration system described herein is based on the \"intelligent agents\" approach [5][6][7][8]. An agent may be defined as a self contained software program, specialized in achieving a set of goals, by autonomously performing tasks on be half of users or other agents. Agents are particularly"}
{"pdf_id": "0809.1686", "content": "The approach presented in this study is based on a  software agent, called Calibration Agent that builds the  inter-variable relationships and analyses variable's sensitivity to different parameter changes. The Calibra tion Agent executes the simulation model iteratively,  measuring the lack of fit, adequacy and reliability [1][3] at each round, until some predefined convergence crite ria is attained. At each simulation iteration, the agent changes values of selected parameters trying to mini mize the lack of fit of the results achieved to real data,  thus improving the reliability of the model without  reducing the adequacy too much [1][3]."}
{"pdf_id": "0809.1686", "content": "This paper is organized as follows. Section II de scribes the type of ecological modelling problems under  analysis in this study and refers some examples. The  next section briefly describes the simulation system  built under this project, EcoDyn application and its main features. The calibration agent approach is de scribed in section IV. The paper concludes with project  state and pointers to future work."}
{"pdf_id": "0809.1686", "content": "Ecological models are simplified views of nature  used to solve scientific or management problems. These  models only contain the characteristic features that are  essential in the context of the problem to be solved or described. Ecological models may be considered a synthesis of what is known about the ecosystem with reference to the considered problem, as opposed to a statisti cal analysis - a model is able to translate our knowledge  about the system processes, formulated in mathematical  equations, and component relationships and not only  relationships between data [9]."}
{"pdf_id": "0809.1686", "content": "Spatial  grids acceptable for physical and chemical processes (10 to 100 metres) are very detailed for biological proc esses, and similarly, minutes or hours are good time  scales for physical and chemical processes, but hours,  days and months may be appropriate time scales for biotic components of an ecosystem [9]"}
{"pdf_id": "0809.1686", "content": "Unlike the chemical and physical parameters that are  almost known as exact values, it is rather unusual to know exact values for most biological parameters. Al most all literature about this subject presents biological parameters as approximate values or intervals [9]. Un der this context, it is obvious that there is a particular need for parameter estimation methods for most bio logical parameters. Thus, the need for calibration is  therefore \"intrinsic\" to ecological models [9]."}
{"pdf_id": "0809.1686", "content": "The authors are particularly concerned with coastal  lagoons and ecosystems. Located between land and  open sea, these ecosystems receive fresh water inputs, rich in organic and mineral nutrients derived from ur ban, agricultural and industrial effluents and domestic  sewage. Furthermore, coastal ecosystems are subject to  strong anthropogenic pressures due to tourism and  shellfish/fish farming. These factors are responsible for  important ecosystem changes characterized by eutrophic conditions, algal blooms, oxygen depletion and hydro gen sulphide production [10]. Examples of ecological  models can be found in [7][12][13]."}
{"pdf_id": "0809.1686", "content": "EcoDyn is an application built to enable physical and  biogeochemical simulation of aquatic ecosystems. It's  an object oriented program application, built in C++  language, with a shell that manages the graphical user  interface (Figure 3), the communications between  classes and the output devices where the simulation  results are saved. The simulated processes include:"}
{"pdf_id": "0809.1686", "content": "enced by variables of the inquired one. The later method  is used when the invoking class influences variables  belonging to the invoked class. All communication  between classes occurs through the EcoDyn shell. The  invoking and the invoked classes are identified by a  name and a code."}
{"pdf_id": "0809.1686", "content": "This application has an interface module that enables remote control from external/remote applications (typically the Agents). The remote application can do every thing the user can (start/stop the EcoDyn application  and control the model simulation runs: start, stop,  pause, restart and step) and, additionally, can \"spy\" the  simulation activity and change the values of the EcoDyn  parameters. When EcoDyn is under the remote control  the user interface can be activated only for information. The remote control has precedence over the user con trol."}
{"pdf_id": "0809.1686", "content": "Model calibration is performed by comparing ob served with predicted data and is a crucial phase in the  modelling process. It's an iterative and interactive task  in which, after each simulation, the \"modeller\" analyses the results and changes one or more equation parame ters trying to tune the model. This \"tuning\" procedure  requires a good understanding of the effect of different  parameters over different variables."}
{"pdf_id": "0809.1686", "content": "Evaluation of the result's quality is an easy task with simple algorithms (ex. linear regression between pre dicted and observed data), the system can classify the  results quality in a qualitative or quantitative scale. A more complex problem is the selection of new parameter values to use in the next iteration by the model equa tions, trying to maximize the model quality of fit."}
{"pdf_id": "0809.1686", "content": "One way of doing this is to give to the software agent a list with all changeable equation parameters, all possi ble ranges for those parameters and let it exhaustively  search through all available parameter combinations  until it finds the optimal one. This is a very intensive  computation process due to its uninformed (and thus not intelligent) search through the system's tens or hun dreds of equations and parameters. Research on this matter should therefore be focused on devising intelligent search techniques that may be able to use the mod eller's knowledge to guide the search."}
{"pdf_id": "0809.1686", "content": "Knowledge about the behaviour of all system proc esses, possessed by the \"modeller\" in the traditional calibration processes, shall be used to guide the selec tion of the new values for the parameters contained in  different mathematical relationships. In the present  system, the intelligent agent learns this knowledge in  three phases:"}
{"pdf_id": "0809.1686", "content": "From the example presented in Table I (model described in [13]) it follows that class TAdriaticAirTemperature influences classes TWaterTemperatureTwoDimensionalForSango and TLight, class TSan goResuspendDeposit  influences  classes  TLight, TSangoPhytoplankton, TSangoNutrients, TChlamysFarreriV8 and TCrassostreaGigas7, class TSangoPhyto plankton influences classes TSangoResuspendDeposit, TSangoNutrients, TSangoZooplankton, TChlamysFar reriV8 and TCrassostreaGigas7, and so on"}
{"pdf_id": "0809.1686", "content": "Secondly, the inter-class sensitivity is analysed (sen sitivity of each variable of each class is analysed with respect to all variables of each class by which it is influ enced). During this step, the model runs (\"Training  sensitivity simulation\" box) keeping all variables and  parameters constant except those directly involved in  sensitivity analysis."}
{"pdf_id": "0809.1686", "content": "The calibration system architecture with the Calibra tion Agent, EcoDyn application and data (observed data  and model database) is shown in Figure 6. The user  manages the agent actions and the EcoDyn activity and  can manipulate the data present in the system, as the  calibration process proceeds."}
{"pdf_id": "0809.1686", "content": "[12]Hawkins, A. J. S., Duarte, P., Fang, J. G., Pascoe, P. L., Zhang, J. H., Zhang, X. L. & M. Zhu., A func tional simulation of responsive filter-feeding and  growth in bivalve shellfish, configured and validated  for the scallop Chlamys farreri during culture in  China. Journal of Experimental Marine Biology and  Ecology 281: 13-40, 2002."}
{"pdf_id": "0809.1802", "content": "1. INTRODUCTION A wide variety of quantitative information is summarized and visually presented using 2-D plots, including scientific results, business performance reports, time series, etc. The embedded information is invaluable in that once extracted, the data can be indexed and the end-user has the ability to query the data, and operate directly on the data. However, in order to extract information from figures without manual"}
{"pdf_id": "0809.1802", "content": "2. RELATED WORKThe image categorization portion of our work bears a simi larity to image understanding, however, we focus on decidingwhether a given image contains a 2-D plot. Li et.al. [6] de veloped wavelet transform, context sensitive algorithms to perform texture based analysis of an image, in separating camera taken pictures from non-pictures. Building on thisframework, Lu et.al. [8] developed an automatic categorization image system for digital library documents which cat egorizes the images into multiple classes within non-picture class e.g. diagram, 2-D figures, 3-D figures, diagrams andother. We find significant improvements in detecting 2-D fig ures by substituting certain features used in [8]. [7] presentsimage-processing-based techniques to extract the data rep resented by lines in 2-D plots.However, [7] does not ex"}
{"pdf_id": "0809.1802", "content": "3. PRELIMINARY Our algorithm segments a 2-D figure into three regions: 1) X-axis region containing X-axis labels and numerical units,i.e., area below the horizontal axis in Fig 1., 2) Y-axis containing labels and numerical units i.e. area to the left of ver tical axis in Fig 1. and, 3) plotting region, which contains legend text, data points, and lines. A 2-D figure depicts a functional distribution of the form yi = fi(x) with conditions wi where Y-axis and X-axis labels contain the description for y and x data. The legend with textual content provides theparticulars for conditions w, and the values for these func tions are represented by the data points or the lines in the plot."}
{"pdf_id": "0809.1802", "content": "Axes Features: 2-D figures range from curve-fitted plots to histograms and pie-charts. We are primarily interested in 2-D plots that graph the variation of a variable with respect to another variable and the presence of co-ordinate axes is certainly a distinguishing feature of such plots. We apply the Hough transform [4] on the binarized image to obtain the positional information of the longest straight lines, including their mutual angles (eg., X-Y axes are othogonal) and use these as features."}
{"pdf_id": "0809.1802", "content": "Text Features: From our observations, we found that au thors tend to employ certain terms in writing captions for 2-D plots that are used less frequently in captions for othertypes of figures. For instance, re-occurring sets of words in clude distribution, slope, axes, plot, range, etc. We use these words to form boolean features while training our classifier."}
{"pdf_id": "0809.1802", "content": "5. EXPERIMENTS In this section, we report the results obtained by evaluating the new features for 2-D plot identification and data point disambiguation algorithms. The data set that we used for our experiments is randomly selected publications crawled from the web site of Royal Society of Chemistry www.rsc.org and randomly selected computer science publications from the CiteSeer digital library [5] for scientific publications."}
{"pdf_id": "0809.1802", "content": "5.1 2-D figure Classification For our classification experiments, we extracted the imagesfrom the afore-mentioned documents and had them manu ally tagged by two volunteers as 2-D or non 2-D. Our set consists of 2494 images, out of which 734 images are 2-D plots. As mentioned previously, we train a linear SVM(with C = 1.0) on this dataset."}
{"pdf_id": "0809.1802", "content": "5.1.1 Feature extractionTable 1 shows the 3-fold cross-validation accuracies with different combinations of features. We use the following abbre viations: IS for image segmentation, CT for caption text, CAfor the coordinate axes. The confusion matrix over a sam ple test set is shown in Table 3. For comparison purposes, we have also shown the confusion matrix over the training set in Table 2. The libSVM software was used for support vector classification [3]."}
{"pdf_id": "0809.1802", "content": "6. CONCLUSIONS AND FURTHER WORK We have outlined a system that can identify 2-D plots indigital documents and extract data from the identified doc uments. Overlapping data points present a major challengein reconstructing data series from within the plotting re gion, once lines are filtered from 2-D plots. We present anunsupervised machine-learning algorithm to segregate overlapping data points and identify their exact shape and loca tion. The work presented here is currently being integratedinto the overall figure extraction system. In addition, at tention is being given to improving the quality of extracted textual information, to assist in indexing of figures."}
{"pdf_id": "0809.2421", "content": "These modules facilitate  electricity demand and consumption proper planning, because they allow knowing the behavior  of the hourly demand and the consumption patterns of the plant, including the bill components,  but also energy deficiencies and opportunities for improvement, based on analysis of  information about equipments, processes and production plans, as well as maintenance  programs"}
{"pdf_id": "0809.2553", "content": "The typical data mining algorithm uses explicitly given features of the data to assess their similarity and discover patterns among them. It also comes with many parameters for the user to tune to specific needs according to the domain at hand. In this chapter, by contrast, we are discussing algorithms that neither use features of the data nor provide any parameters to be tuned, but that nevertheless often outperform algorithms of the aforementioned kind. In addition, the methods presented here are not just heuristics that happen to work, but they are founded in the mathematical theory of Kolmogorov complexity. The problems discussed in this chapter will mostly, yet not exclusively, be clustering tasks, in which naturally the notion of distance between objects plays a dominant role."}
{"pdf_id": "0809.2553", "content": "understanding of the underlying algorithm. Setting them incorrectly can result in missing the right patterns or, perhaps worse, in detecting false ones. Moreover, comparing two parametrized algorithms is difficult because different parameter settings can give a wrong impression that one algorithm is better than another, when in fact one is simply adjusted poorly. Comparisons using the optimal parameter settings for each algorithm are of little help because these settings are hardly ever known in real situations. Lastly, tweaking parameters might tempt users to impose their assumptions and expectations on the algorithm."}
{"pdf_id": "0809.2553", "content": "allow us to tweak its parameters to help it do the right thing? Of course, parameter and feature free algorithms cannot mind read, so if we a priori know the features, how to extract them, and how to combine them into exactly the distance measure we want, we should do just that. For example, if we have a list of cars with their color, motor rating, etc. and want to cluster them by color, we can easily do that in a straightforward way."}
{"pdf_id": "0809.2553", "content": "tion. Asymmetry refers to the fact that, after all, the inverse transformation of the Mona Lisa into a blank canvas can be described rather simply. Normalization refers to the fact that the transformation description size must be seen in relation to the size of the participating objects. Section 3.2 details how these and other issues are dealt with and explains in which sense the resulting information distance measure is universal. The formulation of this distance measure will involve the mathematical theory of Kolmogorov complexity, which is generally concerned with shortest effective descriptions."}
{"pdf_id": "0809.2553", "content": "one can still use its theoretical idea and approximate it with practical methods. Two such approaches are discussed in subsequent sections. They differ in which property of the Kolmogorov complexity they use and to what kind of objects they apply. The first approach, presented in Sect. 3.3, exploits the relation between Kolmogorov complexity and data compression and consequently employs common compression algorithms to measure distances between objects. This method is applicable whenever the data to be clustered are given in a compressible form, for instance, as a text or other literal description."}
{"pdf_id": "0809.2553", "content": "To what extent can the information required to compute x from y be made to overlap with that required to compute y from x? In some simple cases, complete overlap can be achieved, so that the same minimal program suffices to compute x from y as to compute y from x."}
{"pdf_id": "0809.2553", "content": "Let us assume we want to quantify how much some given objects differ with respect to a specific feature, for instance, the length of files in bits, the number of beats per second in music pieces, or the number of occurrences of some base in genomes. Every specific feature induces a specific distance measure, and conversely every distance measure can be viewed as a quantification of a feature difference."}
{"pdf_id": "0809.2553", "content": "white pictures with binary strings. There are many distances defined for binary strings, for example, the Hamming distance and the Euclidean distance. Such distances are sometimes appropriate. For instance, if we take a binary picture and change a few bits on that picture, then the changed and unchanged pictures have small Hamming or Euclidean distance, and they do look similar."}
{"pdf_id": "0809.2553", "content": "as unduly restrictive. More precisely, only upper semicomputability of D will be required. This is reasonable: as we have more and more time to process x and y we may discover newer and newer similarities among them, and thus may revise our upper bound on their distance. The next definition summarizes the class of distance measures we are concerned with."}
{"pdf_id": "0809.2553", "content": "in relative ones. For example, if two strings of 1, 000, 000 bits differ by 1, 000 bits, then we are inclined to think that those strings are relatively similar. But if two strings of 1, 000 bits differ by 1, 000 bits, then we find them very different."}
{"pdf_id": "0809.2553", "content": "Example 3.3. Consider the problem of comparing genomes. The E. coli genome is about 4.8 megabase long, whereas H. innuenza, a sister species of E. coli, has genome length only 1.8 megabase. The information distance E between the two genomes is dominated by their length difference rather than the amount of information they share. Such a measure will trivially classify H. innuenza as being closer to a more remote species of similar genome length such as A. fulgidus (2.18 megabase), rather than with E. coli. In order to deal with such problems, we need to normalize."}
{"pdf_id": "0809.2553", "content": "It is paramount that the normalized version of the universal information distance metric is also a metric. Were it not, then the relative relations between the objects in the space would be disrupted and this could lead to anomalies, if, for instance, the triangle inequality would be violated for the normalized version."}
{"pdf_id": "0809.2553", "content": "with only the non-conditional terms K( x) , K( y) , K( xy) . This comes in handy if we interpret K( x) as the length of the string x after being maximally compressed. With this in mind, it is an obvious idea to approximate K( x) with the length of the string x under an efficient real-world compressor. Any correct and lossless data compression program can provide an upper-bound approximation to K( x) , and most good compressors detect a large number of statistical regularities."}
{"pdf_id": "0809.2553", "content": "where Z( x) denotes the binary length of the compressed version of the string x compressed with compressor Z. The distance eZ is actually a family of distances parametrized with the compressor Z. The better Z is, the closer eZ approaches the normalized information distance, the better the results are expected to be."}
{"pdf_id": "0809.2553", "content": "parameter-free and feature-free data mining tool on a large variety of sequence benchmarks. Comparing the NCD method with 51 major parameter-loaded methods found in the eight major data-mining conferences (SIGKDD, SIGMOD, ICDM, ICDE, SSDB, VLDB, PKDD, and PAKDD) in the last decade, on all data bases of time sequences used, ranging from heart beat signals to stock market curves, they established clear superiority of the NCD method for clustering heterogeneous data, and for anomaly detection, and competitiveness in clustering domain data."}
{"pdf_id": "0809.2553", "content": "believed that (Rodents, (Ferungulates, Primates)) renects the true evolutionary history. We confirm this from the whole genome perspective using the distance eZ. We use the complete mitochondrial genome sequences from following 20 species: rat (Rattus norvegicus), house mouse (Mus musculus), gray seal (Halichoerus grypus), harbor seal (Phoca vitulina), cat (Felis catus), white rhino (Ceratotherium simum), horse (Equus caballus), finback whale (Balaenoptera physalus), blue whale (Balaenoptera musculus), cow (Bos taurus), gibbon (Hylobates lar), gorilla (Gorilla gorilla), human (Homo sapiens), chimpanzee (Pan troglodytes), pygmy chimpanzee (Pan paniscus), orangutan (Pongo pygmaeus), Sumatran orangutan (Pongo pygmaeus abelii), with opossum (Didelphis virginiana), wallaroo (Macropus robustus) and platypus (Ornithorhynchus anatinus) as the outgroup."}
{"pdf_id": "0809.2553", "content": "The similarity between languages can, to some extent, be determined by the similarity of their vocabulary. This means that given two translations of the same text in different languages, one can estimate the similarity of the languages by the similarity of the words occurring in the translations. This has been exploited by Benedetto et al. [2], who use a compression method related to NCD to construct a language tree of 52 Euroasian languages from translations of the Universal Declaration of Human Rights [1]."}
{"pdf_id": "0809.2553", "content": "resulting matrix of distances, the tree in Fig. 3.2 has been generated. It shows the three main language groups, only Dendi and Somali are somewhat too close to the American languages. Also, the classification of English as a Romance language is erroneous from a historic perspective and is due to the English vocabulary being heavily innuenced by French and Latin. Therefore the vocabulary, on which the approach discussed here is based, is indeed to a large part Romance."}
{"pdf_id": "0809.2553", "content": "It is a common observation in university courses with programming assignments that some programs are plagiarized from others. That means that large portions are copied from other programs. What makes this hard to detect is that it is relatively easy to change a program syntactically without changing its semantics, for example, by renaming variables and functions, inserting dummy statements or comments, or reordering obviously independent statements. Nevertheless a plagiarized program is somehow close to its source and therefore the idea of using a distance measure on programs in order to uncover plagiarism is obvious."}
{"pdf_id": "0809.2553", "content": "one, in which the set of musical pieces comprises four preludes from Chopin's Opus 28, two preludes and two fugues from Bach's \"Das wohltemperierte Klavier,\" and the four movements from Debussy's \"Suite Bergamesque.\" After preprocessing the MIDI files as described above, the pairwise eZ values, with bzip2 as compressor, are computed. To generate the final hierarchical clustering as shown in Fig. 3.3, a special quartet method [9; 10] is used."}
{"pdf_id": "0809.2553", "content": "The NCD is universal, in a mathematical sense as approximation of the universal NID, but also in a practical sense, as witnessed by the wide range of successful applications. Nevertheless the practical universality is of a different navor because the NCD is a family of distance measures parametrized by a compressor. This means that one has to pick a suitable compressor for the application domain at hand. It does, however, not mean that one has to know the relevant features of the objects in that domain beforehand. Rather, using a good compressor for objects in a certain domain, makes it more likely that the compressor does indeed"}
{"pdf_id": "0809.2553", "content": "The normalized compression distance can only be applied to objects that are strings or that at least can be naturally represented as such. Abstract concepts or ideas, on the other hand, are not amenable to the NCD method. In this section, we present a realization of NID overcoming that limitation by taking advantage of the World Wide Web."}
{"pdf_id": "0809.2553", "content": "Example 3.4. We describe an experiment, using a popular search engine, performed in the year 2004, at which time it indexed N = 8, 058, 044, 651 pages. A search for \"horse\" returns a page count of 46,700,000. A search for \"rider\" returns a page count of 12,200,000. A search for both \"horse\" and \"rider\" returns a page count of 2,630,000. Thus eG( horse, rider) = 0. 443. It is interesting to note that this number stayed relatively fixed as the number of pages indexed by the used search engine increased."}
{"pdf_id": "0809.2553", "content": "pages indexed by the search engine grows sufficiently large, the number of pages containing a given search term goes to a fixed fraction of N, and so does the number of pages containing conjunctions of search terms. This means that if N doubles, then so do the f-frequencies. For the NWD to give us an objective semantic relation between search terms, it needs to become stable when the number N of indexed pages grows. Some evidence that this actually happens was given in Example 3.4."}
{"pdf_id": "0809.2553", "content": "slowing down. Therefore search engine databases represent the largest publicly-available single corpus of aggregate statistical and indexing information so far created, and it seems that even rudimentary analysis thereof yields a variety of intriguing possibilities. It is unlikely, however, that this approach can ever achieve 100% accuracy like in principle deductive logic can, because the Web mirrors humankind's own imperfect and varied nature. But, as we will see below, in practical terms the NWD can offer an easy way to provide results that are good enough for many applications, and which would be far too much work if not impossible to program in a deductive way."}
{"pdf_id": "0809.2553", "content": "To perform the experiments in this section, we used the CompLearn software tool [8], the same tool that has been used in Sect. 3.3 to construct trees representing hierarchical clusters of objects in an unsupervised way. However, now we use the normalized Web distance (NWD) instead of the normalized compression distance (NCD). Recapitulating, the method works by first calculating a distance matrix using NWD among all pairsof terms in the input list. Then it calculates a best-matching unrooted ternary tree using a novel quartet method style heuristic based on randomized hill-climbing using a new fitness objective function optimizing the summed costs of all quartet topologies embedded in candidate trees [9]."}
{"pdf_id": "0809.2553", "content": "In the first example [11], the objects to be clustered are search terms consisting of the names of colors, numbers, and some tricky words. The program automatically organized the colors towards one side of the tree and the numbers towards the other, Fig. 3.5. It arranges the terms which have as only meaning a color or a number, and nothing else, on the farthest reach of the color side and the number side, respectively. It puts the more general terms black and white, and zero, one, and two, towards the center, thus indicating their more ambiguous interpretation. Also, things which were not exactly colors or numbers are also put towards the center, like the word \"small.\" We may consider this an example of automatic ontology creation."}
{"pdf_id": "0809.2553", "content": "In the example of Fig. 3.6, the names of fifteen paintings by Steen, Rembrandt, and Bol were entered [11]. The names of the associated painters were not included in the input, however they were added to the tree display afterwards to demonstrate the separation according to painters. This type of problem has attracted a great deal of attention [22]. A more classical solution would use a domain-specific database for similar ends. The present automatic oblivious method obtains results that compare favorably with the latter feature-driven method."}
{"pdf_id": "0809.2553", "content": "Fig. 3.7 Names of several Chinese people, political parties, regions, and others. The nodes and solid lines constitute a tree constructed by a hierarchical clustering method based on the normalized Web distances between all names. The numbers at the perimeter of the tree represent NWD values between the nodes pointed to by the dotted lines. For an explanation of the names, refer to Fig. 3.8"}
{"pdf_id": "0809.2553", "content": "Fig. 3.9 NWD-SVM learning of prime numbers. All examples, i. e.,numbers, were converted into vectors containing the NWD values between that number and a fixed set of anchor concepts. The classification was then carried out on these vectors using a support vector machine. The only error made is classifying 110 as a prime"}
{"pdf_id": "0809.2553", "content": "The next example (see the preliminary version of [11]) has been created using WordNet [12], which is a semantic concordance of English. It also attempts to focus on the meaning of words instead of the word itself. The category we want to learn here is termed \"religious\" and represents anything that may pertain to religion. The negative examples are constituted by simply everything else (see Fig. 3.10). Negative examples were chosen randomly and uniformly from a dictionary of English words. This category represents a typical expansion of a node in the WordNet hierarchy. The accuracy on the test set is 88.89%."}
{"pdf_id": "0809.2553", "content": "1. hyponym: X is a hyponym of Y if X is a (kind of) Y. 2. part meronym: X is a part meronym of Y if X is a part of Y. 3. member meronym: X is a member meronym of Y if X is a member of Y. 4. attribute: A noun synset for which adjectives express values. The noun weight is an attribute, for which"}
{"pdf_id": "0809.2553", "content": "pointer (or edge) of one of the types above is chosen from the WordNet database. Next, the source synset node of this pointer is used as a root. Finally, we traverse outward in a breadth first order starting at this root and following only edges that have an identical semantic pointer type; that is, if the original semantic pointer was a hyponym, then we would only follow hyponym pointers in constructing the category. Thus, if we were to pick a hyponym link initially that says a tiger is a cat, we may then continue to follow further hyponym relationships in order to continue to get more specific types of cats. See the WordNet homepage [20] documentation for specific definitions of these technical terms."}
{"pdf_id": "0809.2553", "content": "Further experiments comparing the results when filtering out WordNet images on the Web suggest that this problem does not usually affect the results obtained, except when one of the anchor terms happens to be very rare and thus receives a non-negligible contribution towards its page count from WordNet views"}
{"pdf_id": "0809.2553", "content": "NWD method turns out to agree well with the WordNet semantic concordance made by human experts. The mean of the accuracies of agreements is 0.8725. The variance is approximately 0. 01367, which gives a standard deviation of approximately 0. 1169. Thus, it is rare to find agreement less than 75%."}
{"pdf_id": "0809.2553", "content": "method does not use an individual word in isolation, but instead uses an ordered list of its NWD relationships with fixed anchors. Therefore nothing can be attached to the isolated interpretation of a literal term, but only to the ordered list by which it is represented. That is to say, the inputs to our SVM are not directly search terms, but instead an image of the search term through the lens of the Web distribution, and relative to other fixed terms which serve as a grounding for the term. In most schools of ontological thought, and indeed"}
{"pdf_id": "0809.2553", "content": "in the WordNet database, there is imagined a two-level structure that characterizes language: a many-to many relationship between word-forms or utterances and their many possible meanings. Each link in this association will be represented in the Web distribution with strength proportional to how common that usage is found on the Web. The NWD then amplifies and separates the many contributions towards the aggregate page count sum, thereby revealing some components of the latent semantic Web. In almost every informal theory of cognition we have the idea of connectedness of different concepts in a network, and this is precisely the structure that the NWD experiments attempt to explore."}
{"pdf_id": "0809.2553", "content": "A typical procedure for finding an answer on the World Wide Web consists in entering some terms regarding the question into a Web search engine and then browsing the search results in search for the answer. This is particularly inconvenient when one uses a mobile device with a slow internet connection and small display. Question-answer (QA) systems attempt to solve this problem. They allow the user to enter a question in natural language and generate an answer by searching the Web autonomously."}
{"pdf_id": "0809.2553", "content": "many answers, among them Seattle, Bellevue, or Dallas. The first two cities are correct answers, but the preferred answer would be Seattle as the more well-known city. In a straightforward attempt to finding the right answer using the normalized Web distance we could compute eG( Lake Washington, Bellevue) , eG( Lake Washington, Seattle) and eG( Lake Washington, Dallas) and pick the city with the least distance. An experiment performed in February 2008 with a popular Web search engine yielded"}
{"pdf_id": "0809.2553", "content": "so that Bellevue would have been chosen. Without normalization the respective distance values are 6. 33, 7. 54 and 10. 95. Intuitively, the reason for Seattle being relatively far away from Lake Washington (in terms of eG) is that, due to Seattle's size and popularity, it has many concepts in its neighborhood not all of which can be close. For the less known city of Bellevue, however, Lake Washington is relatively more important. Put differently, the concept \"Seattle\" contains a lot of information that is irrelevant for its being situated at Lake Washington. Symmetrically, Lake Washington encompasses much information unrelated to Seattle. A variation of (3.1) that accounts for possible irrelevant information is then"}
{"pdf_id": "0809.2553", "content": "bilities compare favorably with other QA systems [25]. The beneficial properties of emin can perhaps best seen in comparison to other measures such as the normalized max distance e or the unnormalized distances E and Emin. Replacing emin with e results in answers that are still technically correct but often less popular and therefore less \"good.\" We already mentioned Bellevue being preferred over Seattle as a city located at Lake Washington. Another example is the question \"When was CERN founded?,\" which would be answered by e with \"52 years ago,\" correct in 2006, whereas emin responds more accurately with \"1954.\""}
{"pdf_id": "0809.2553", "content": "greatest scientist of all?\" would be answered with \"God,\" whereas emin would give \"Newton,\" the reason for this discrepancy being that, in terms of Web pages, God is much more popular than Newton. More generally, experiments have shown [25] that Emin and E perform about 8% worse than emin."}
{"pdf_id": "0809.2553", "content": "derive them. The derivations of NCD and NWD are special instances of this process, which can roughly bebroken into three steps: (1) devising an abstract distance notion, (2) transforming it inside the abstract math ematical realm into an equivalent, yet more easily realizable, formula, and (3) using real-world algorithms or data to practically realize the theoretically conceived measure. That this approach does not work by chance just for the information distance, is demonstrated by the derivation of the minimum distance, which employs the same three step process, just with different starting requirements for the distance measure."}
{"pdf_id": "0809.2553", "content": "versality and the use of absolute measures of information content to achieve this universality. From these principles it follows naturally that the resulting distance measures are independent of fixed feature sets and do not require parameters for tuning. They can thus be used to build feature- and parameter-free methods that are suited for many tasks in exploratory data mining, alignment-free genomics, and elsewhere."}
{"pdf_id": "0809.2818", "content": "Some of them are presented in Figure 1, showing the distribution of the publishing authors in the year of 1994. The publishing communities in 1994 are mainly characterized by a central star consisting of many author nodes. Generally, the observations we have done, produced a diverse number patterns that are often quite similar and that base on very simple geometric structures. Most interestingly, patterns went away but appeared again, they stayed stableor disappeared forever. For example, the big star (Figure 1) has not been ex isting before 1955, but appeared several times afterwards, for example in 1994, disappeared temporarily, and appeared again in 2006 (visiting pattern). Simple"}
{"pdf_id": "0809.2818", "content": "Following our observations, we typify each associative pattern to their funda mental structure, and - since these structures are evocative of chemical basic modules - we label them in almost the same manner. Each author node i corresponds to an atomic author nucleus, owning a certainactivation acti and a number of atomic bonds with other nuclei. In the follow ing model description, we keep these bonds unvalued although the strengthen between the adjacent atomic author nuclei exists per se."}
{"pdf_id": "0809.2818", "content": "With the defined predicates and functions we are then able to decompose molecular structures. In this sense, molecular stars can be seen communities that con sist of an arbitrary number of triggers and reactors; and a molecular diamond is nothing else than a composition of bridges. Furthermore, a decomposition of molecular structures can then be performed quite easily, leaving to a number of descriptive attributes like shown in Table 1."}
{"pdf_id": "0809.2818", "content": "Using such a data table for clustering, we may then get groups of socialsub-networks being similar. This is a simplification of existing molecular com munities. For example, while taking the raw attributes data SB (number of singlebonds), BR (number of bridges), DI (number of diamonds), NU (number of nu clei), RE (number of reactor nodes), and TR (number of triggers nodes) (1) for"}
{"pdf_id": "0809.2818", "content": "With this decomposition to n-ary molecules, we demand on decomposing each publishing community and to describe a publishing community by the molecular attributes. Applying such a data table containing a description for molecular structures with clustering, we may then get groups of molecular structures being similar. The advantage of such an analytical performance is a simplification of existing molecular communities in respect to their structure."}
{"pdf_id": "0809.2818", "content": "The immediate identification of roles in social communities is shown in Figure 9: here, we may observe molecular diamonds and molecular stars, having Micha Sharir as molecular trigger for seven other authors. Furthermore, Carlos Sanchez is both a molecular trigger and a molecular reactor, whereas Eric Dubois, Phillipe Dubois, and Michael Petit form a molecular diamond."}
{"pdf_id": "0809.2818", "content": "Initially, we observe very simple molecules in the years before 1950, because less publications have been made. The first molecular bridge can be observed in 1953, the first more complex structure in 1954. The evolvement remembers to cell division operations of natural processes, leading to a first big star in 1960. Interestingly, the molecular noise (pairwise, but disjunctive publication, not sharing publications with others) is present the whole time, continuously"}
{"pdf_id": "0809.2818", "content": "A yearly night over the association landscape between 1970 and 1999 yields on a results as presented in Figure 11. The first years are characterized by an alternating appearance of the big star (two consecutive years) and one year of restructuring. This is, for example, in 1975, 1978, and 1981. Interestingly, the research years where Artificial Intelligence had become significantly could be characterized by the social communities between 1982 and 1990, dominating the publication landscape with less space for other social communities. In contrast to this, the social communities in the 1990's not generally concern with one social domain but stay manifold and distributed, sharing more simple molecular structures than in the years before."}
{"pdf_id": "0809.2818", "content": "We have focused on entries of the bibliographic communities DBLP and charac terized communities through a simple typifying description model. We have set a publication as a transaction between its associated authors, the general ideais to concern with directed associative relationships amongst them, to decom pose each pattern to the fundamental molecular components, and to describe these communities by such atomic and molecular attributes. The decompositionsupports the management of discovered structures towards the use of adaptive incremental mind-maps (Figure 12), being discovered molecular structures at theassociative memory layer and firstly managed in the short-term memory. Un derstanding bibliographic entries as data stream input, this is an important step towards the interpretation of (temporal) social communities as informational and intermediate results."}
{"pdf_id": "0809.2818", "content": "1. Agrawal, R., Imielinski, T., Swami, A.: Mining Association Rules between Sets of Items in Large Databases. Proceedings of ACM SIGMOD International Conference on Management of Data, 1993. 2. Berendt, B., Hotho, A., Mladenic, D., and Semeraro, G.: From Web to Social Web: Discovering and Deploying User and Content Profiles. Workshop on Web Mining, WebMine 2006, Berlin, Germany, September 18, 2006. 3. Grabmeier, J., Rudolph, A.: Techniques of Cluster Algorithms in Data Mining. Kluwer Academic Publishers. 2002. 4. A. Inokuchi, T. Washio, K. Nishimura, H. Motoda, A Fast Algorithm for Mining Frequent Connected Subgraphs, IBM Research, Tokyo Research Laboratory, 2002."}
{"pdf_id": "0809.2851", "content": "intuition that users of the web assess information quality based on source credibility and authority. Authority can be seen on a institutional level e.g., academic or governmental institutions and on a personal level e.g., professional experts. Another interesting finding of this work is that users believe that the web is less authoritative and also less credible than other, more conventional information systems."}
{"pdf_id": "0809.2851", "content": "2.3 Quality of Web DocumentsLim et at. [12] introduce two models to measure the quality of articles from an online community like Wikipedia with out interpreting their content. In the basic model quality isderived from the authority of the contributors of the arti cle and the contributions from each of them (in number of words). The peer review model extends the basic model by a review aspect of the article content. It gives higher quality to words that \"survive\" reviews. An approach to automatically predict information quality is given by Tang et al. [21]. Analyzing news documents they observe an association between users quality score and the occurrence and prevalence of certain textual features like readability and grammar."}
{"pdf_id": "0809.2851", "content": "3.1 Choosing Expert ListsWe chose a variety of topics (2 academic, 2 financial, 2 ath letic and 2 popular culture) as well as choose expert rankings that are well-known. The accuracy, criteria or bias of these rankings may be critiqued, but that is not the purpose of this investigation. We simply accept the rankings as given from the experts. They include (please note that the URLs are likely to change over time):"}
{"pdf_id": "0809.2851", "content": "3.2 Mapping Resources to URLsAfter the expert lists have been chosen, we began the pro cess of mapping their real-world objects to single URLs. For some lists (ARWU, Fortune, US News) this was easily done because each real-world object has a canonical URL. For the IMDB lists, the URLs are not quite canonical, but they do come from two extremely well-known web sites: imdb.com and wikipedia.org. For the other lists (ATP, Billboard, Money, WTA), judgment calls were needed to determine the best URL."}
{"pdf_id": "0809.2851", "content": "3.3 Creating anOrdinal Ranking ofURLs from SE Queries We developed a Perl program that takes a list of URLsand queries search engines to determine their relative order ing of those URLs. We do not determine a search engine's absolute ranking for any particular URL. That is, we do not compute:"}
{"pdf_id": "0809.2851", "content": "We also are not interested in estimating the PageRank (orrelated metrics), independent of SEs, through link neighbor hoods or other means: the SEs are the subject of our study, not the web graph itself. Instead, using a variation of strand sort (illustrated in section 3.3.2), we simply determine that a search engine ranks the URLs in order:"}
{"pdf_id": "0809.2851", "content": "Since our queries consist of URLs only, each with the same modifier and combined with the boolean operator and no keywords added, all search results have theoretically an equal opportunity to be returned as thetop result and \"only\" the search engine's ranking is dictat ing the ranking of the URLs now"}
{"pdf_id": "0809.2851", "content": "Besides the syntax Yahoo also limits the queries to 5000 per day. Due to Yahoo's site: modifier syntax we can not include Wikipedia URLs in our comparison with the Yahoo search engine because all Wikipedia URLs follow the pattern http://en.wikipedia.org/wiki/certain_object where the path of the URL would be dismissed and only the ranking of the English Wikipedia site is compared to all other URLs, resulting in erroneously high score for the URL."}
{"pdf_id": "0809.2851", "content": "4.2 SE Errors Of the 9 tests, we were able to complete only 3 in all configurations: for 3 list (n) sizes, 3 expert-SE comparisons and 3 inter-SE comparisons. These were ARWU (table 1), Billboard (table 3), and Money (table 7). Limitations of the Yahoo site operator (see section 3.3.1) limited Yahoo's inclusion in ATP (table 2), both IMDB tests (tables 5 and 6), US News (table 8), and WTA (table 9). There was a transient error with Yahoo in the Fortune list for n=50 (table 4) that we were unable to resolve on the day of the tests (15 URLs came back as not indexed). This"}
{"pdf_id": "0809.3027", "content": "Each individual term Pr(M(i, u)|G, N) is easy to define. First recall that each entry M(i, u) can take values 0 or 1. The case M(i, u) = 0 occurs when no 1 in the u column of N propagates to row i and N(i, u) = 0. That is,"}
{"pdf_id": "0809.3027", "content": "That is, the probability that Mt(i, u) = 0 is equal to the the probability that i does not get u from any of the nodes that had it at some previous point in time neither did it get it from any of the nodes that initiated u at time t. Naturally, the probability that Mt(i, u) = 1 is"}
{"pdf_id": "0809.3027", "content": "The ecological datasets used for the experiments are available by AICS Research Inc, University Park, New Mexico, and The Field Museum, Chicago. The datasets are available online1 and they have been used for a wide range of ecological studies [3, 6, 18]. We focus our attention on the results we obtained by applying our method to a single such dataset; the Rocky Mountain dataset. The dataset shows the presence/absence of Boreal and boreo-cordilleran species of mammals in the Southern Rocky Mountains, and has been used as a dataset of reference for many ecological studies, see for example [3]. The dataset itself is rendered in Figure 1.2"}
{"pdf_id": "0809.3027", "content": "Thus the task is the same as before. The only term in the above formula that depends on the propagation model is the term Pr(M|G, N). However, since G and N are known, matrix Mp = P(G, N) is also known. Therefore for some constant c we can substitute Pr(M|G, N) with"}
{"pdf_id": "0809.3027", "content": "We can very easily observe, that this problem is already hard for many well-known information propagation models, like for example the linear threshold (LT) and the independent cascade (IC) model described in [11]. Here we are not giving a detailed description of these two propagation models, we refer to [11] for this. For the rest of the discussion we can treat them as a black-box, bearing in mind that they are non-deterministic. We state the hardness result of the Minimum Initiation problem in the observation below and we discuss it immediately after."}
{"pdf_id": "0809.3352", "content": "This paper generalizes the traditional statistical concept of prediction intervals for arbitrary probability density functions in highdimensional feature spaces by introducing significance level distribu tions, which provides interval-independent probabilities for continuousrandom variables. The advantage of the transformation of a proba bility density function into a significance level distribution is that it enables one-class classification or outlier detection in a direct manner."}
{"pdf_id": "0809.3352", "content": "The significance level distribution is in the true sense of the word a \"prob ability distribution\" because it provides a probability (the significance level)for every continuous realization x. Unfortunately, the term \"probability dis tribution\" is already used for probability density functions, which do notprovide probabilities but probability density values. Note that the significance level distribution does not deliver the probability for a single realiza tion x itself, but the probability for all even more unlikely realizations than x. Nevertheless, bX(x) provides valuable information for the assessment of the realization x and allows to decide if it is sure, probable, or only possible."}
{"pdf_id": "0809.3352", "content": "For simple standard distributions, such as the Gaussian distribution or the Cauchy distribution, the significance level distribution can be given in closed form. Note that for a symmetric and unimodal distribution the significance level distribution and the prediction interval is identically (see Fig. 1). For more complex distributions this is usually not valid and it is here seldom"}
{"pdf_id": "0809.3352", "content": "possible to give the significance level distribution in closed form. In these cases it is reasonable to estimate the cumulative distribution function FY . The next section 4 proposes a method and investigates its convergence speed. Figure 2 shows an example of a significance level distribution for a non-trivial probability density function. Please note that significance level distributions are not restricted to the one-dimensional case."}
{"pdf_id": "0809.3352", "content": "In this article, I have shown that it is always possible to compute prediction regions as generalization of prediction intervals, no matter if the generating density is high-dimensional or multimodal. Only the density has to be known or estimated. The idea was to define the integration borders indirectly by a zero level set with the probability density function as level set function. This has lead"}
{"pdf_id": "0809.3618", "content": "Matching shapes in images has many applications, includ ing image retrieval, alignment, and registration [1, 2, 3, 4]. Typically, matching is approached by selecting features for a set of landmark points in both images; a correspondence between the two is then chosen such that some distance measure between these features is minimised. A great deal of attention has been devoted to defining complex features which are robust to changes in rotation, scale etc. [5,6].1"}
{"pdf_id": "0809.3618", "content": "(3) This however induces an NP-hard problem for general c2 (quadratic assignment).Discriminative structured learn ing has recently been applied to models of both linear and quadratic assignment (eq. (1) and eq. (3)) in [18]. Herewe exploit the structure of c2 that arises from the near isometric shape matching problem in order to make such a problem tractable."}
{"pdf_id": "0809.3618", "content": "In isometric matching settings, one may suspect that it may not be necessary to include all pairwise relations in quadratic assignment. In fact a recent paper [14] has shown that if only the distances as encoded by the graphical modeldepicted in figure 1 (top) are taken into account (nodes rep resent points in S and states represent points in U), exactprobabilistic inference in such a model can solve the isomet ric problem optimally. That is, an energy function of the"}
{"pdf_id": "0809.3618", "content": "Although the model of [14] solves isometric matching prob lems optimally, it provides no guarantees for near-isometric problems, as it only considers those compatibilities which form cliques in our graphical model. However, we are often only interested in the boundary of the object: if we look at the instance of the model depicted in figure 2, it seemsto capture exactly the important dependencies; adding ad ditional dependencies between distant points (such as the duck's tail and head) would be unlikely to contribute to this model. With this in mind, we introduce three new features (for brevity we use the shorthand yi = y(si)):"}
{"pdf_id": "0809.3618", "content": "Figure 5: The running time and performance of our method, compared to those of [18] (note that the method of [14] has running time identical to our method). Our method is run from 1 to 20 iterations of belief propagation, although the method appears to converge in fewer than 5 iterations."}
{"pdf_id": "0809.3618", "content": "We achieve much better performance using this method, and also observe a significant improvement after learning. Figure 9 shows an example match using both the unary and higher-order techniques. Finally, figure 6 (right) shows the weights learned for this model. Interestingly, the first-order term during the second stage of learning has almost zero weight. This must not"}
{"pdf_id": "0809.3618", "content": "It would also be possible to allow for shapes which are rigid in some parts, but less so in others. For instance,although the handlebars, wheels, and pedals appear in sim ilar locations on all bicycles, the seat and crossbar do not; we could allow for this discrepancy by learning a separate weight vector for each clique."}
{"pdf_id": "0809.3618", "content": "We have presented a model for near-isometric shape match ing which is robust to typical additional variations of the shape. This is achieved by performing structured learningin a graphical model that encodes features with several dif ferent types of invariances, so that we can directly learn a \"compound invariance\" instead of taking for granted theexclusive assumption of isometric invariance. Our experi ments revealed that structured learning with a principled graphical model that encodes both the rigid shape as well as non-isometric variations gives substantial improvements, while still maintaining competitive performance in terms of running time."}
{"pdf_id": "0809.3690", "content": "by the Associate function does not innuence this knowledge-saving process. Hence, the framework is highly similar to a database: the probability density contains the knowledge and the Associate function makes it available. We will illustrate this with a short, theoretical example. Let us assume that we have an arbitrary classification problem, which we want to solve. Usually, a training dataset is given"}
{"pdf_id": "0809.3690", "content": "are modeled in the same way by a common distribution. In the following, we will demonstrate this using the powertrain example. The sample problem is to accelerate and decelerate the car to attain a given target speed. Because the target speed can be changed discontinuously and the car cannot reach arbitrary accelerations due to its inertia, not all target speed graphs can be realized. But it is desired that the car reaches the target speed as fast as technically possible."}
{"pdf_id": "0809.3690", "content": "Figure 5: The results of our controller experiment: The left plot shows thatthe controller achieves the desired speed nearly perfectly. The error is every where less than 1 km/h. The right side shows the results of increasing the car mass from 1800 kg to 2800 kg. The controller still works, the deviations are only the result of physical limitations."}
{"pdf_id": "0809.3690", "content": "shows that the speed attained corresponds almost exactly to the desired speed. Also, a sudden increasing of the car mass from 1800 kg to 2800 kg does not lead to problems, despite that no longer every desired speed is realizable. For instance, the engine is for speeds over 100 km/h simply not powerful enough, although the accelerator pedal is opened completely. Furthermore, the car cannot brake fast enough sometimes. This control error cannot be avoided."}
{"pdf_id": "0809.4501", "content": "Time-frequency representations of audio signals often resemble texture images. This paper derives a simple audio clas sification algorithm based on treating sound spectrograms as texture images. The algorithm is inspired by an earlier visualclassification scheme particularly efficient at classifying tex tures. While solely based on time-frequency texture features,the algorithm achieves surprisingly good performance in mu sical instrument classification experiments."}
{"pdf_id": "0809.4501", "content": "cific patterns can be found repeatedly in the sound spectro gram of a given instrument, renecting in part the physics ofsound generation. By contrast, the spectrograms of differ ent instruments, observed like different textures, can easily be distinguished from one another. One may thus expect to classify audio signals in the visual domain by treating their time-frequency representations as texture images."}
{"pdf_id": "0809.4501", "content": "In the literature, little attention seems to have been puton audio classification in the visual domain. To our knowl edge, the only work of this kind is that of Deshpande and his colleges [3]. To classify music into three categories (rock, classical, jazz) they consider the spectrograms and MFCCsof the sounds as visual patterns. However, the recursive fil tering algorithm that they apply seems not to fully capture the texture-like properties of the audio signal time-frequency representation, limiting performance."}
{"pdf_id": "0809.4501", "content": "In this paper, we investigate an audio classification algorithm purely in the visual domain, with time-frequency rep resentations of audio signals considered as texture images.Inspired by the recent biologically-motivated work on ob ject recognition by Poggio, Serre and their colleagues [14], and more specifically on its variant [19] which has been shown to be particularly efficient for texture classification, wepropose a simple feature extraction scheme based on time frequency block matching (the effectiveness of application of time-frequency blocks in audio processing has been shown in previous work [17, 18]). Despite its simplicity, the proposedalgorithm relying only on visual texture features achieves surprisingly good performance in musical instrument classifica tion experiments."}
{"pdf_id": "0809.4501", "content": "The idea of treating instrument timbres just as one wouldtreat visual textures is consistent with basic results in neuroscience, which emphasize the cortex's anatomical uniformity [9, 7] and its functional plasticity, demonstrated exper imentally for the visual and auditory domains in [15]. From that point of view it is not particularly surprising that somecommon algorithms may be used in both vision and audition, particularly as the cochlea generates a (highly redun dant) time-frequency representation of sound."}
{"pdf_id": "0809.4501", "content": "The algorithm consists of three steps, as shown in Fig. 1.After transforming the signal in time-frequency representation, feature extraction is performed by matching the timefrequency plane with a number of time-frequency blocks pre viously learned. The minimum matching energy of the blocksmakes a feature vector of the audio signal and is sent to a clas sifier."}
{"pdf_id": "0809.4501", "content": "structures denser. Flute pieces are usually soft and smooth.Their time-frequency representations contain hardly any vertical structures, and the horizontal structures include rapid vibrations. Such textural properties can be easily learned with out explicit detailed analysis of the corresponding patterns. As human perception of sound intensity is logarithmic [20], the classification is based on log-spectrogram"}
{"pdf_id": "0809.4501", "content": "(3) E[l, k, m] measures the degree of resemblance between thepatch Bm and log-spectrogram S at position [l, k]. A min imum operation is then performed on the map E[l, k, m] to extract the highest degree of resemblance locally between S and Bm: C[m] = min l,k E[l, k, m]. (4)"}
{"pdf_id": "0809.4501", "content": "The audio classification scheme is evaluated through musi cal instrument recognition. Solo phrases of eight instruments from different families, namely nute, trumpet, tuba, violin,cello, harpsichord, piano and drum, were considered. Mul tiple instruments from the same family, violin and cello forexample, were used to avoid over-simplification of the prob lem.To prepare the experiments, great effort has been dedicated to collect data from divers sources with enough varia tion, as few databases are publicly available. Sound samples were mainly excerpted from classical music CD recordings of personal collections. A few were collected from internet. For each instrument at least 822-second sounds were assembled from more than 11 recordings, as summarized in Table 1. All"}
{"pdf_id": "0809.4501", "content": "fication scheme, particularly efficient at classifying textures.In experiments, this simple algorithm relying purely on timefrequency texture features achieves surprisingly good perfor mance at musical instrument classification. In future work, such image features could be combined with more classical acoustic features. In particular, the stilllargely unsolved problem of instrument separation in poly phonic music may be simplified using this new tool."}
{"pdf_id": "0809.4530", "content": "It focuses on research that extracts and makes  use of the concepts, relations, facts and descriptions found in Wikipedia, and organizes the work into four broad  categories: applying Wikipedia to natural language processing; using it to facilitate information retrieval and  information extraction; and as a resource for ontology building"}
{"pdf_id": "0809.4582", "content": "output of P . use notations Ati(P) and Ato(P) for referring to the input signatur and the output signatur e O resp e tiv ely The hidden atoms in Ath(P) = H = At(P) \\ Atv(P) are used to formalize some auxiliary  on epts of P whi  ma not"}
{"pdf_id": "0809.4668", "content": "In collaborative tagging systems, users assign keywords or tags to their uploaded content, or bookmarks, in order to improve future navigation, filter ing or searching (see, e.g., Marlow et al. [MNBD06]). These systems generate a categorization of content commonly known as a folksonomy.An example is the collaborative URL tagging sys tem Delicious [Del], which was analyzed in depth"}
{"pdf_id": "0809.4668", "content": "the construction of a larger multigraph using the hy perlink graph with each vertex corresponding to a pair webpage-concept and each edge to a hyperlinkassociated with a concept. Subgraph ideas are sug gested by them: \"It might be faster to simply runPageRank on sub-graphs pertaining to each individ ual concept (assuming there are a small number of concepts).\" Although DeLong et al. [DMS06] obtain good ranking results for single-keyword facets, they do not support multi-keyword queries."}
{"pdf_id": "0809.4668", "content": "Query-dependent PageRank calculation was intro duced in Richarson and Domingos [RD02] to extracta weighted probability per keyword for each webpage. These probabilities are summed up to gener ate a query-dependent result. They also show that this faceted ranking has, for thousands of keywords, computation and storage requirements that are only approximately 100-200 times greater than that of a single query-independent PageRank. As we show in Section 4.8, our facet-dependent ranking algorithms have similar time complexity."}
{"pdf_id": "0809.4668", "content": "In this section, we present two examples of collabo rative tagging systems where content is tagged and recommendations are made. These systems actuallyrank content according to the number of visits, rec ommendations or relevance of the text accompanying the content. However, to our knowledge, no use of graph-based faceted ranking is made. The taxonomy of tagging systems in Marlow et al. [MNBD06] allows us to classify YouTube [You]"}
{"pdf_id": "0809.4668", "content": "is a non-vanishing probability of finding a vertex with an arbitrary high indegree.Clearly, in any realworld network, the total number of vertices is a nat ural upper-bound to the greatest possible indegree. However, experience with Internet related networks shows that the power-law distribution of the indegree does not change significantly as the network grows and, hence, the probability of finding a vertex with an arbitrary degree eventually becomes non-zero (formore details see, e.g., Pastor-Satorras and Vespig nani [PSV04])."}
{"pdf_id": "0809.4668", "content": "Since recommendation lists are made by individual users, vertex outdegree does not show the same kind of scale-free behavior than vertex indegree. On the contrary, each user recommends only 20 to 30 otherusers on average (see Figure 3). Moreover, since ver tex outdegree is mostly controlled by human users, we do not expect its average to change significantly as the network grows."}
{"pdf_id": "0809.4668", "content": "The correlation of indegree of in-neighbors withvertex indegree (see Figure 4) indicates the existence of assortative (positive slope) or disassorta tive behavior (negative slope). Assortativeness iscommonly observed in social networks, where peo ple with many connections relates to people which isalso well-connected. Disassortativeness is more com mon in other kinds of networks, such as information,technological and biological networks (see, e.g., New man [New02]). In the favorite videos network there is no clear correlation (small or no slope), but the photo network there is a slight assortativeness indicating a biased preference of vertices with high indegree for vertices with high indegree (see Figure 4)."}
{"pdf_id": "0809.4668", "content": "We also computed the PageRank of the sample graphs, removing dangling vertices with indegree 1 and out degree 0, because most of them correspond to vertices which have not been expanded by thecrawler (BFS), having the lowest PageRank (a simi lar approach is taken in [PBMW98]). Figure 5 shows that PageRank distributions are also scale-free, i.e., they can be approximated by power law distributions. Note that the power law exponents are very similar for the complete tagged graph and subgraphs, on each network."}
{"pdf_id": "0809.4668", "content": "Given a set of tags, a ranking may be calculatedby computing the centrality measure of the sub graph corresponding to the recommendation edges which include all the tags. This approach, called E-intersection, cannot be implemented for onlinequeries, as explained above, but serves as a reason able standard of comparison because we use the exactinformation available for the PageRank in a conjunc tive query."}
{"pdf_id": "0809.4668", "content": "Notice that in this sum we are using as centrality thesum of ranking positions in a reverse order, and ac cording to the R-sum algorithm, the ranking of nodes in the example of Table 3 is b, a and c. The complexity of this algorithm is similar to that of PR-product."}
{"pdf_id": "0809.4668", "content": "then the algorithms in Sections 4.5, 4.6 and 4.7 arescalable, linear on the number of edges of the com plete tagged graph. This can be verified empirically on Figure 7, showing that distribution of tags per edges falls quickly, having a mean of 9.26 tags per edge for the YouTube tagged graph and 13.37 for the Flickr tagged graph. These are not heavy-tailed distributions and, since tags are manually added toeach uploaded content, we do not expect the aver age number of tags per recommendation to increase significantly with network growth."}
{"pdf_id": "0809.4668", "content": "In our experiments the computation of all the faceted singleton tag rankings (104, 927 tags) for the video network sample took 211.4 times more time than the single ranking for the complete tagged graph. Meanwhile the photo network sample (283, 093 tags) took 1744.9 times more time. Our merging algorithms work in real-time because they use only the top w results, where w is a smallfixed number like 500 or 1000. Choosing an appropri ate w for an application6 will enable it to store only the w top elements of each single-tag facet."}
{"pdf_id": "0809.4668", "content": "E-union/N-intersection) and the y-axis to the topnumber n of vertices used to compute the similari ties. The similarity results (between 0 and 1) falling in each of the log-log ranges were averaged. Observe that darker tones correspond to values closer to 1, i.e., more similar results. White spaces correspond to cases for which there are no data, e.g., whenever the y coordinate is greater than intersection size."}
{"pdf_id": "0809.4668", "content": "Experiments with Flickr were similar, top 99 tags paired to form 4851 tag pairs. A small sample of the top 99 tags is: bw, portrait, nature, bravo, sky, blue, water, soe, nower, light, clouds, sunset, red, film, macro, white, landscape, green, girl, blackandwhite. Table 5 as well as Figures 10 and 11 summarize the results."}
{"pdf_id": "0809.4784", "content": "The results achieved  showed that the strategies based on temperamental decision  mechanism strongly influence the system performance and there are  evident dependency between emotional state of the agents and  their temperamental type, as well as the dependency between the  team performance and the temperamental configuration of the team  members, and this enable us to conclude that the modular approach  to emotional programming based on temperamental theory is the  good choice to develop computational mind models for emotional  behavioral Multi-Agent systems"}
{"pdf_id": "0809.4784", "content": "Emotions are part of our every day lifes. They help us focus  attention, remember, prioritize, understand and  communicate. The possibility of computation of emotions  has interested researchers for many years. The emotions  influence decision-making processes, socialization,  communication, learning and many other important issues of  our life. Implementation of emotions in an artificial organism  is an important step for different areas of intervention, since  academical inquiry [1-10], education [13-15], communication [11, 16], entertainment and others [12, 17 19, 29, 30]. Researchers have focused on the functions of  emotion for computational models trying to describe some of"}
{"pdf_id": "0809.4784", "content": "behavioral responses to reinforcing signals, communications  which transmit the internal states or social bonding between  individuals, which could increase fitness in the context of  evolution. Among some models of emotions that are  described through the computational process exists different  approaches to the proper concept of emotion. Each model  results of the definition that is given to the emotional  process. Since analysis of needs/satisfactions of the human  being [24, 25], passing through the analysis of characteristics  of the superior nervous system [26, 28], physiological  changes [23, 31], neurobiological processes [27], appraisal  mechanism and analysis of the psychology of individual  personality [20, 21]."}
{"pdf_id": "0809.4784", "content": "The classical definition for \"Temperament\" follows: it is a  specific feature of Man, which determines the dynamics of  his mental activity and behaviour. Two basic indexes of the  dynamics of mental processes and behaviours at present are  distinguishable: activity and emotionality. In this project we  will analyze and develop an emotional model for the agents  with temperament. We will use a complex approach to  emotion/temperament concepts: based on physiological  (CNS) characteristics and on psychological characteristics of  the agents."}
{"pdf_id": "0809.4784", "content": "appraisal theory and on superior nervous system  characteristics. Most appraisal theories [32, 33] assume that  beliefs, desires and intentions are the basis of reasoning and  thus of emotional evaluation of the agents situation. In order  to create a more flexible and efficient emotion-based  behavior system, the appraisal model is implemented in  mixture with Pavlov's temperamental theory [28] which  studies the basic reasons for different temperamental  behaviors and  Eysenck's [26] neurophysiological"}
{"pdf_id": "0809.4784", "content": "As we already have refered, for constructing our emotional  model we studied two subjects: emotional states which  characterize the immediate emotional condition of the agent  and emotional trait (temperament) which define the  personality characteristics and behaviors of the agent and  influence his emotional state changes. We decided to  approach the study of emotions from different perspectives:  physiological and psychical, creating double layer  architecture for emotional model to increase the system  performance. Let us examine each perspective of our  approach."}
{"pdf_id": "0809.4784", "content": "psychological types of temperaments isolated with it and  revealed their complete similarity. Thus, temperament is a  manifestation of the type of nervous system into the activity.  As a result the relationship of the types of nervous system  and temperaments appears as follows (fig. 1):"}
{"pdf_id": "0809.4784", "content": "Eysenck methodology One of the things Pavlov tried with his dogs [37] was  conflicting conditioning - ringing a bell that signalled food at  the same time as another bell that signalled the end of the  meal. Some dogs took it well, and maintain their  cheerfulness. Some got angry and barked like crazy. Some  just laid down and fell asleep. And some whimpered and  whined and seemed to have a nervous breakdown."}
{"pdf_id": "0809.4784", "content": "personality types with two dimensions: On the one hand  there is the overall level of arousal (called excitation) that  the dogs' brains had available. On the other, there was the  ability the dogs' brains had of changing their level of arousal  - i.e. the level of inhibition that their brains had available."}
{"pdf_id": "0809.4784", "content": "Analysis of personality factors in terms of the  PAD temperamental model Analysis of emotional states leads to the conclusion that the  human emotions such as anger, fear, depression, elation, etc.  are discrete and we need to define some kind of measures to  have a basic framework to describe each emotional state  using the same scale. After studing the appraisal theory we"}
{"pdf_id": "0809.4784", "content": "find Mehrabian model [20, 21] more suitable for  computational needs since it defines three dimensions to  describe each emotional state and provides an extensive list  of emotional labels for points in the PAD space (Fig 3) gives  an impression of the emotional meaning of combinations of  Pleasure, Arousal and Dominance (PAD)."}
{"pdf_id": "0809.4784", "content": "define a three-dimensional space where individuals are  represented as points, personality types are represented as  regions and personality scales are represented as straight  lines passing through the intersection point of the three axes.  Mehrabian uses +P, +A and +D to refer pleasant, arousable  and dominant temperament. Respectively, and by using -P,"}
{"pdf_id": "0809.4784", "content": "where some types of applications communicate among each  other, nominated, a simulator, an application for each agent  and a viewer application. The architecture is client-server,  where the simulator acts as the server and both the agents  and the viewer, acts as clients. This architecture is similar to  the Simulation League of RoboCup [36]."}
{"pdf_id": "0809.4784", "content": "hardware and the labyrinth. The simulation is executed in  discrete time, cycle by cycle. In the beginning of each cycle  of simulation the simulator sends to all robotic agents in test,  the measures of its sensors, and to all viewers the positions  and robots information. The agents can answer with the  power values to apply to the engines that command the  wheels."}
{"pdf_id": "0809.4784", "content": "measured by all its sensors and must decide which power to  apply in each motor. The perception that a robotic agent has  from the exterior environment is limited and noisy  transforming him into the most appropriate tool to perform  our work with almost realistic precision."}
{"pdf_id": "0809.4784", "content": "agent's temperamental state and agent's emotional state.  Temperament, as we already defined, is the steady  characteristics of the agent which is \"innate\" and do not  suffer alterations during the agent's life. On the other side,  the emotional state of the agent is the dynamic set of values  which depends on the external influences, and on the agent's  temperament."}
{"pdf_id": "0809.4784", "content": "and the same emotional states on some temporal period,  which receive the same external input will have different  responses on both, the physiological and the psychical  mechanism. We also define different sets of needs and  motivations for each temperamental type by the influence of  the agent's performance and stimuli on the team work. This  modular, but complementary approach, is the core of the  innovation of our emotional system and our aspiration of its  usability."}
{"pdf_id": "0809.4784", "content": "implemented any of dependency between physiological and  psychical layers and we are trying to discover some kind of  influence that one layer could have on the other layer  through the temperamental configurations or common goals  implementation. Psychical layer controls the emotional state  of the agent through PAD values, and the physiological layer  control the engine configuration (motors, sensors, etc...) and  the group interaction, based on temperamental needs of the  agent (like extroversion/introversion or emotional stability)."}
{"pdf_id": "0809.4784", "content": "Physiological layer As we show on previous chapter, Pavlov's theory defines the  temperamental model based on characteristics of the superior  nervous system, but at the same time there are no pure  temperamental types in nature, but there are mixtures of  different properties which characterize one or another unique  temperamental type. So, as we see, one person can have all  temperamental types in different ratios. The different  proportion of values: force, mobility and steadiness of  processes of excitation and braking defines the unique  temperamental type for each person. Based on this"}
{"pdf_id": "0809.4784", "content": "uncertainty we use Fuzzy Logic to describe and monitorize  the temperamental types in our project [39]. In the beginning  of the simulation we generate the values which will define  the unique combination of temperamental type of the agent,  but then these characteristics are changing in run-time in  order to adapt the agent state to the external influences. We  define the fuzzy intervals for each temperamental variable  which define the temperamental characteristics (Force,  Mobility, ...) and the value of this variable increases in  stressful situations (close threat, wall-shock, etc...) and  decreases in calm situations. The speed of the increase and  decrease depends on agent's Arousal."}
{"pdf_id": "0809.4784", "content": "Steadiness The steadiness of the agent is the velocity of his emotional  state variation. For example, more balanced agents have a  slow variation of emotional state. For this we introduce the  variable called Anxiety which is used to increase or decrease  the Pleasure variable. The value of Anxiety depends on the  temperament of the agent. We choose the values for anxiety  based on the Eysenck test [26]."}
{"pdf_id": "0809.4784", "content": "Emotional receptivity This variables were based on the Eysenck test described on  the second section. The Melancholic and Phlegmatic  temperamental types are included in the Introverts group and  the Sanguine and Choleric types are included in the  Extroverts group. We will evaluate how they performance to  reach the beacon, conditioned by their temperamental needs."}
{"pdf_id": "0809.4784", "content": "argues that any emotion can be expressed in terms of values  on these three dimensions, and provides extensive evidence  for this claim [20]. This makes his three dimensions suitable  for a computational approach. Mehrabian also provides an  extensive list of emotional labels for points in the PAD space  [21] and gives an impression of the emotional meaning of the  combinations of Pleasure, Arousal and Dominance. The  emotional-state of an agent can thus be understood as a  continuously moving point in an n-dimensional space of  appraisal dimensions."}
{"pdf_id": "0809.4784", "content": "Appraisal Banks The appraisal bank defines the needs, motivations and  stimulus of the agent as a set of subjective measures, called  appraisal dimensions. First, a simple instrumentation based  on appraisal bank that emotionally evaluates events related  to survival. Second, a more complex instrumentation based  on two appraisal banks, one related to survival the other  related to reach the beacon and satisfy temperamental needs.  In both banks we have used event-encoding to simulate  emotional meaning of events. We now describe how events  are interpreted by the two appraisal banks."}
{"pdf_id": "0809.4784", "content": "performance on reaching the goal. We also evaluated the  appraisal values modifications during the simulation time.  We performed the evaluation of an entire team of nine  agents, in order to compare their performance with other  teams of agents. During these evaluations we tried to analyse  the difference between distinct temperamental teams and  compare them in general terms (PAD scale and emotion  valence), as well as their performance on reaching the goal.  We perform the evaluation on three different simulation  scenarios:"}
{"pdf_id": "0809.4784", "content": "Performance and Emotional State of the agents. In our  architecture the performance of the agents doesn't depend on  appraisal mechanism which only controls the psychical layer  of the agent and only influences his PAD values and the  emotional state. The agents performance only depends on  temperamental (physiological) configuration of the agent  (motors, sensors, anxiety, etc..) and his decision layer based  on extrovert/introvert characteristics. So, we can see that the  temperamental decision mechanism clearly influence the  emotional state of the agent during the simulation."}
{"pdf_id": "0809.4784", "content": "Also we can analyse the influence of the system goals  on the Emotional State of the agent (from the Appraisal  Bank), and as we describe in Section 4, the decision  temperamental mechanism works in order to accomplish  some of these goals (avoid the walls or reach the beacon, for  instance)"}
{"pdf_id": "0809.4784", "content": "results showing the dependence between two different layers  (physiological and psychical) which where implemented  independently. So, as it already has been proved theoretically  from psychological perspective, which define that our  emotional process are dependent on our temperamental type,  we could state that our architecture is consistent and show  the same dependence between two layers. This let us a large  room for future improvement and research on this area."}
{"pdf_id": "0809.4784", "content": "promising way to integrate emotions into multi-agent  systems with different goals and configurations.  Temperament helps to support agent's decision making and  with proper use can improve the agent's performance and the  global teamwork. Also, our system helps us analyse the  configurations we could choose to implement the personality  in the system with different and particular characteristics,  helping us to select the variables and functions of personality  with better fitness to the specific system."}
{"pdf_id": "0809.4784", "content": "search algorithms for evaluate the impact of emotions and  temperament on search strategies. Other development is the  introduction of visual emotional feedback using the face  expressions such as proposed by the Russel [19]. Also we  are aiming at the introduction of additional objects in the  simulation environment with different degree of  thread/satisfaction."}
{"pdf_id": "0809.4834", "content": "In practice, there are three fundamental  aspects to be taken into account that make this task difficult:  • The diversity of applications for digital images;  • The diversity of image users with different perspectives, making the problem  of requirement definition extremely complex;  • The limitation, within current state of the art, of science and technology to  mimic the human capacity of image understanding and description"}
{"pdf_id": "0809.4834", "content": "The purpose for which the images are required typically determines user needs  and behaviour when searching for images. It is widely accepted that present day  society is much more dependent on the use of visual information in both forms: still  and moving images. Visual information is useless if it cannot be obtained in an  efficient and effective way. It is of recognised importance that the user needs should  be an important part of the requirements used to develop image retrieval systems.  Since the first quarter of the 20th-century, developments in photography led to  the widespread use of photograph in the worldwide press. Subsequently, several  institutions were concerned with archiving visual material in order to support services"}
{"pdf_id": "0809.4834", "content": "As for textual documents, one can state that nowadays it is easy to generate  visual documents, not so easy to gain physical access to them, and even more difficult  to retrieve or access those few visual documents which satisfy a specific information  need (Enser, 1995)"}
{"pdf_id": "0809.4834", "content": "In order to  do this, the first task is to identify and classify the different categories of image users,  not only the users that depend on the use of images in their professional activity but  also those who deal with images for entertainment or recreational purposes"}
{"pdf_id": "0809.4834", "content": "The following categories are not exhaustive but could  be interpreted as a description of some of the most representative professional activity  types that, in some way, depend on the use of images: Medicine; Crime prevention;  Fashion and graphic design; Advertising; Architectural and engineering design;  Historical research; Education; Publishing industry and the press"}
{"pdf_id": "0809.4834", "content": "Relevance Feedback constitutes the process of refining the results returned by  the CBIR system in a given iteration of an interaction session. The user performs  some sort of evaluation over the results returned in the last iteration and this  evaluation is fed back to the system (Figure 1).  End User"}
{"pdf_id": "0809.4834", "content": "The refinement is possible since the CBIR relates this information with the  information from the original query and from other refinements in previous iterations.  According to Croft (1995) the process of relevance feedback is one of the preferred  characteristics mentioned by users of information retrieval systems.  The two more popular approaches for relevance feedback presented below are  classified in Ishikawa et al. (1998) as query-point movement and re-weighting. These"}
{"pdf_id": "0809.4834", "content": "Low-level features and conventional distance functions, usually, are not  sufficient to support the correct discrimination of conceptual similarity between  distinct visual regions.  VOIR framework implements a two-layer model separating conceptual  categories at the upper layer from the visual layer composed by the low-level feature  points. The visual layer is partitioned into visual categories, Vj. Each conceptual  category, Ci, can be related with several visual categories. Each visual category is  composed of several regions. The regions sharing the same visual category are"}
{"pdf_id": "0809.4834", "content": "textual thesaurus. The more the system learns, the more accurate and faster are the  subsequent query sessions.  In the implementation used to carry out the experiments, the visual categories,  used in the concept learning process, were defined off-line using a clustering  algorithm that took low-level features extracted from each region as its input data.  The automatic updating of the associations between term and visual item is done  periodically after the query sessions or following new manually added associations.  The updating process affects all the visual items that belong to the same visual  category as the visual item whose situation was changed either because was explicitly  associated with a keyword or because was evaluated during a query iteration."}
{"pdf_id": "0809.4834", "content": "One of our experimental requirements was that the subjects should be exposed to  a simulated work task situation in which their information needs would evolve, in just  the same dynamic manner as such needs might be observed to do so in subjects' real  working lives.  In the instructions given, each subject was asked to simulate that was creating a  leaflet for the promotion of an event, to be held at school, whose generic theme was  science/nature. The three imagined events were: \"The Tree day\", \"The World Water"}
{"pdf_id": "0809.4834", "content": "To the question \"What is your preferred way for selecting images from a  collection\", 3 subjects selected the option \"Keyword based search system for  specifying queries made up of search terms\", 2 selected \"Unordered sequence of  small thumbnail images for browsing through\", 4 selected both of the options and  none indicated alternating ways not mentioned"}
{"pdf_id": "0809.4834", "content": "Following an approach similar to Jose et al. (1998) we adopted a two part  structure for this questionnaire: (i) a set of semantic differential questions, and (ii) a  set of Likert scales questions.  In the semantic differential part, the set of 16, 7-point semantic differential,  questions was used to characterize the following four aspects (Table 2):  • First question was dedicated to the task that had been set.  • Two questions focused on the search process carried out by the subject.  • Two focused on the retrieved image set.  • The last 11 questions focused on the system used in the retrieval task."}
{"pdf_id": "0809.4834", "content": "The final questionnaire was given after each user had completed all his tasks. In  this questionnaire, the subjects were asked to rank the three systems in terms of (i)  enjoyableness and (ii) helpfulness. Also, they were asked why they chose to rank that  way. The results after applying the non-parametric Fisher sign test (Weisstein, 2006)  to the three pairs of versions are listed in table 5."}
{"pdf_id": "0809.4834", "content": "This paper described a Visual Object Information Retrieval system implementing conceptual image retrieval with two layers: conceptual and visual. VOIR uses region based relevance feedback to improve the quality of the results in each query session  and to discover new associations between text and image.  The system was validated through a user-centred and task-oriented evaluation,  comparing it with previous versions without relevance feedback and only with  relevance feedback at the image level. The results achieved showed clearly the  usefulness of our region based relevance feedback approach."}
{"pdf_id": "0809.4834", "content": "Armitage, L. & Enser, P. G. B. (1997). Analysis of user need in image archives.  Journal of Information Science, 23, 287-299.  Barthes, R. (1977). Rhetoric of the Image. In R.Barthes (Ed.), Image, music, text /  trans. by Stephen Heath (pp. 32-51). London: Fontana.  Croft, W. B. (1995). What Do People Want From Information Retrieval? D-Lib  Magazine (www.dlib.org).  Eakins, J. P. & Graham, M. E. (1999). Content-based Image Retrieval - A report to  the JISC Technology Applications Programme."}
{"pdf_id": "0810.0139", "content": "Most research related to unithood were conducted as part of a larger effort for the determination of termhood. Consequently, nov elties are rare in this small sub-field of term extraction. In addition, existing work were mostly empirically motivated and derived. We propose a new probabilistically-derived measure, independent of any innuences of termhood, that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from Googlesearch engine for the measurement of unit hood. Our comparative study using 1, 825test cases against an existing empirically derived function revealed an improvement in terms of precision, recall and accuracy."}
{"pdf_id": "0810.0139", "content": "Automatic term recognition, also referred to asterm extraction or terminology mining, is the process of extracting lexical units from text and fil tering them for the purpose of identifying terms which characterise certain domains of interest. This process involves the determination of two factors: unithood and termhood. Unithood concerns withwhether or not a sequence of words should be com bined to form a more stable lexical unit. On the other hand, termhood measures the degree to whichthese stable lexical units are related to domainspecific concepts. Unithood is only relevant to com plex terms (i.e. multi-word terms) while termhood (Wong et al., 2007a) deals with both simple terms"}
{"pdf_id": "0810.0139", "content": "(i.e. single-word terms) and complex terms. Recent reviews by (Wong et al., 2007b) show that ex isting research on unithood are mostly carried out as a prerequisite to the determination of termhood. As a result, there is only a small number of existing measures dedicated to determining unithood.Be sides the lack of dedicated attention in this sub-fieldof term extraction, the existing measures are usu ally derived from term or document frequency, and are modified as per need. As such, the significance of the different weights that compose the measures usually assume an empirical viewpoint. Obviously, such methods are at most inspired by, but not derived from formal models (Kageura and Umino, 1996)."}
{"pdf_id": "0810.0139", "content": "The three objectives of this paper are (1) to separate the measurement of unithood from the determination of termhood, (2) to devise a probabilisticallyderived measure which requires only one threshold for determining the unithood of word se quences using non-static textual resources, and (3) to demonstrate the superior performance of the new probabilistically-derived measure against existing empirical measures"}
{"pdf_id": "0810.0139", "content": "lated to the use of static corpora. Moreover, only one threshold, namely, OUT is required to controlthe functioning of OU. Regarding the third objective, we will compare our new OU against an ex isting empirically-derived measure called Unithood(UH) (Wong et al., 2007b) in terms of their preci sion, recall and accuracy. In Section 2, we provide a brief review on some ofexisting techniques for measuring unithood. In Sec tion 3, we present our new probabilistic approach,the measures involved, and the theoretical and intuitive justification behind every aspect of our mea sures. In Section 4, we summarize some findingsfrom our evaluations. Finally, we conclude this pa per with an outlook to future work in Section 5."}
{"pdf_id": "0810.0139", "content": "Some of the most common measures of unit hood include pointwise mutual information (MI) (Church and Hanks, 1990) and log-likelihood ratio (Dunning, 1994).In mutual information, the cooccurrence frequencies of the constituents of com plex terms are utilised to measure their dependency.The mutual information for two words a and b is de fined as:"}
{"pdf_id": "0810.0139", "content": "are thresholds for determining mergeability decisions, and MI(ax, ay) is the mutual information be tween ax and ay, while ID(ax, s), ID(ay, s) and IDR(ax, ay) are measures of lexical independence of ax and ay from s. For brevity, let z be either ax or ay, and the independence measure ID(z, s) is then defined as:"}
{"pdf_id": "0810.0139", "content": "(Frantzi, 1997) proposed a measure known as Cvalue for extracting complex terms. The measure is based upon the claim that a substring of a termcandidate is a candidate itself given that it demonstrates adequate independence from the longer version it appears in. For example, \"E. coli food poisoning\", \"E. coli\" and \"food poisoning\" are accept able as valid complex term candidates. However, \"E. coli food\" is not. Therefore, some measuresare required to gauge the strength of word combina tions to decide whether two word sequences should be merged or not. Given a word sequence a to be examined for unithood, the Cvalue is defined as:"}
{"pdf_id": "0810.0139", "content": "where U is the event that s is a stable lexical unit and E is the evidences belonging to s. P(U|E) is the posterior probability that s is a stable unit given the evidence E. P(U) is the prior probability that s is a unit without any evidence, and P(E) is the prior probability of evidences held by s. As we shall see later, these two prior probabilities will be immaterial"}
{"pdf_id": "0810.0139", "content": "In this paper, we highlighted the significance of unit hood and that its measurement should be given equalattention by researchers in term extraction. We fo cused on the development of a new approach thatis independent of innuences of termhood measure ment. We proposed a new probabilistically-derivedmeasure which provide a dedicated way to deter mine the unithood of word sequences. We refer to this measure as the Odds of Unithood (OU). OU is derived using Bayes Theorem and is founded upon two evidences, namely, local occurrence and globaloccurrence. Elementary probabilities estimated us ing page counts from the Google search engine are utilised to quantify the two evidences. The newprobabilistically-derived measure OU is then eval"}
{"pdf_id": "0810.0156", "content": "Most works related to unithood were conducted as part of a larger effort for the de termination of termhood. Consequently, the number of independent research that study the notion of unithood and producededicated techniques for measuring unit hood is extremely small. We proposea new approach, independent of any innuences of termhood, that provides dedicated measures to gather linguistic evidence from parsed text and statistical ev idence from Google search engine for the measurement of unithood.Our evalua tions revealed a precision and recall of 98.68% and 91.82% respectively with anaccuracy at 95.42% in measuring the unit hood of 1005 test cases."}
{"pdf_id": "0810.0156", "content": "where p(a) and p(b) are the probabilities of oc currence of a and b.Many measures that ap ply statistical techniques assuming strict normal distribution, and independence between the word occurrences do not fare well.For handling extremely uncommon words or small sized corpus, log-likelihood ratio delivers the best preci sion (Kurz and Xu (2002); Franz (1997)).Log likelihood ratio attempts to quantify how much more likely one pair of words is to occur compared to the others. Despite its potential, \"How to apply"}
{"pdf_id": "0810.0156", "content": "this statistic measure to quantify structural depen dency of a word sequence remains an interesting issue to explore.\" (Kit (2002)). Frantzi (1997) proposed a measure known asCvalue for extracting complex terms. The mea sure is based upon the claim that a substring of a term candidate is a candidate itself given that it demonstrates adequate independence from the longer version it appears in. For example, \"E.coli food poisoning\", \"E. coli\" and \"food poisoning\" are acceptable as valid complex term candi dates. However, \"E. coli food\" is not. Therefore, some measures are required to gauge the strength of word combinations to decide whether two word sequences should be merged or not. Given a word sequence a to be examined for unithood, the Cvalue is defined as:"}
{"pdf_id": "0810.0156", "content": "ford Parser. Formally, given that s = axbay where b is any preposition, the conjunction \"and\" or an empty string, the problem is to determine whether to accept s as an independent lexical unit (i.e. a term candidate) or leave ax and ay as separateunits. In order to decide on the merge, we need ad equate evidence that s will form a stable unit andhence, a better term candidate than ax and ay sep arated. It is worth mentioning that the size (i.e. number of words) of ax and ay is not limited to1. For example, we can have ax=\"National In stitutes\", b=\"of\" and ay=\"Allergy and Infectious Diseases\". In addition, the size of ax and ay shouldhave no effect on the determination of their unit hood."}
{"pdf_id": "0810.0156", "content": "the commonness of ax and ay, we employ another measure of independence. In such situation, wewill still accept s as a valid unit if it can be demon strated that the extremely high independence of the individual unit ax and ay is the cause behind the low MI(ax,ay). For this purpose, we modifythe Cvalue described in Equation 2 to accommo date the use of page counts rather than frequency.In addition, we remove the multiplier log2 |a| be cause the number of words in ax and ay does not play a role in determining their independence froms. Consequently, we define the measure of Inde pendence (ID) for ax and ay from s as:"}
{"pdf_id": "0810.0156", "content": "where nax, nay and ns is the Google page count for the unit ax, ay and s, respectively. As the lexical unit ax occurs more than its longer counterpart s, its independence ID(ax,s) grows. Only when the number of occurrences of ax is less than those of s, its independence from s becomes ID(ax,s) =0. This means that we will not be able to wit ness ax without encountering s. The same can be said about the measure of independence for ay, ID(ay,s). In short, extremely high independence of ax and ay relative to s will be renected through high ID(ax,s) and ID(ay,s)."}
{"pdf_id": "0810.0156", "content": "Consequently, the decision to merge ax and ay to form s depends on both the mutual informationbetween ax and ay, namely, MI(ax,ay), and the in dependence of ax and ay from s, namely, ID(ax,s) and ID(ay,s). This decision is organised into a Boolean function known as Unithood (UH), and we define it as:"}
{"pdf_id": "0810.0156", "content": "Due to the lack of existing dedicated techniquesfor measuring unithood, we were unable to per form a comparative study. Nonetheless, the highaccuracy and F-score presented during our evalu ation, and our analysis on the false positives and the false negatives revealed the potentials of ournew measures in terms of high precision and recall, portability across domains, and configurabil ity of the performance."}
{"pdf_id": "0810.0156", "content": "cessing especially named-entity recognition. Theabsence of any predefined resources in our ap proach will solve all the problems highlighted in the previous paragraph. Using our UH(ax,ay)function, named-entity recogniser can easily de termine whether or not parts of proper names should be merged together without ever relying onunreliable heuristics, and domain-restricted pat terns and dictionaries."}
{"pdf_id": "0810.0332", "content": "An increasing number of approaches for ontol ogy engineering from text are gearing towardsthe use of online sources such as company in tranet and the World Wide Web. Despite such rise, not much work can be found in aspects ofpreprocessing and cleaning dirty texts from online sources. This paper presents an enhance ment of an Integrated Scoring for Spelling error correction, Abbreviation expansion and Case restoration (ISSAC). ISSAC is implemented aspart of a text preprocessing phase in an ontology engineering system. New evaluations per formed on the enhanced ISSAC using 700 chat records reveal an improved accuracy of 98% as compared to 96.5% and 71% based on the use of only basic ISSAC and of Aspell, respectively.Keywords: Spelling error correction, abbrevi ation expansion, case restoration"}
{"pdf_id": "0810.0332", "content": "Enhancement of ISSAC The list of suggestions and the initial ranks providedby Aspell are integral parts of ISSAC. Table 1 sum marizes the accuracy of basic ISSAC obtained from the previous evaluations [Wong et al., 2006] on four sets ofchat records. The achievement of 74.4% accuracy by As pell from the previous evaluations, given the extremely poor nature of the texts, demonstrates the strength of the Metaphone algorithm and near-miss strategy. Thefurther increase of 22% in accuracy using basic IS SAC demonstrates the potential of the combined weights NS(sj,i)."}
{"pdf_id": "0810.0332", "content": "S produced by Aspell. About 2% of wrong replacements is due to the absence of the correct replacement from the list of suggestions produced by As pell.For example, the error \"prder\" in the con text of \"The prder number\" was wrongfully replaced by both Aspell and basic ISSAC as \"parader\" and\"prder\" respectively. After a look into the evalu ation log, we realized that the correct replacement \"order\" was not in S."}
{"pdf_id": "0810.0332", "content": "After a careful evaluation of all replacements sug gested by Aspell and by enhanced ISSAC for all 3313 errors, we discovered a further improvement in accuracy using the latter. As shown in Table 3a and 3b, the use of the first suggestion by Aspell as replacement for spelling errors yields an average of 71%, which is a decrease from 74.4% in the previous evaluations due to the additional dirtiness in the extra three sets of chat records. Withthe addition of the various weights that form basic IS SAC, an average increase of 22% was achieved, resulting to an improved accuracy of 96.5%. As predicted, the enhanced ISSAC score a much better accuracy at 98%."}
{"pdf_id": "0810.0332", "content": "We proposed three modifications to the basicISSAC, namely, 1) the use of Google spellcheck for com pensating the inadequacy of Aspell, 2) the incorporationof Google spellcheck for determining if a word is erro neous, and 3) the alteration of the reuse factor RS byshifting from the use of a history list to a spelling dictio nary"}
{"pdf_id": "0810.1186", "content": "We present a domain-independent algorithm that computes macros in a novel way. Our algorithm computes macros \"on-the-ny\" for a given set of states and does not require previously learned or inferred information, nor prior domain knowledge. The algorithm is used to define new domain-independent tractable classes of classical planning that are proved to include Blocksworld-arm and Towers of Hanoi."}
{"pdf_id": "0810.1186", "content": "Macros have long been studied in AI planning [9, 18]. Many domain-dependent ap plications of macros have been exhibited and studied [15, 17, 12]; also, a number of domain-independent methods for learning, inferring, filtering, and applying macros have been the topic of research continuing up to the present [2, 7, 20]. In this paper, we present a domain-independent algorithm that computes macros in a novel way. Our algorithm computes macros \"on-the-ny\" for a given set of states and does not require previously learned or inferred information, nor does it need anyprior domain knowledge. We exhibit the power of our algorithm by using it to de fine new domain-independent tractable classes of classical planning that strictly extend previously defined such classes [6], and can be proved to include Blocksworld-arm"}
{"pdf_id": "0810.1186", "content": "Indeed, these two transformations depend on and feed off of each other: the first trans formation introduces increasingly powerful macros, which in turn can be used by the second to increase the set of pairs, which in turn permits the first to derive yet more powerful macros, and so forth"}
{"pdf_id": "0810.1186", "content": "Definition 7 We define two algorithmic functions apply(G, A, a, s) and transitive(G, s1, s2, s3). Type-wise, the function apply(G, A, a, s) requires that G is an action graph, A is a set of actions, a is an action, and s is a vertex of G. The pseudocode for apply(G, A, a, s) is as follows:"}
{"pdf_id": "0810.1186", "content": "Definition 21 A planning instance (V, init, goal, A) has macro persistent Hamming width k (for short, MPH width k) if no plan exists, or for every reachable state s dominating the initial state init, there exists a plan over (H(s, k), A)-derivable actions improving s that stays within Hamming distance k of s."}
{"pdf_id": "0810.1186", "content": "Theorem 22 Let C be a set of planning instances having MPH width k. The plan generation problem for C is solvable in polynomial time via the following algorithm, in time O(n3k+2d3k(a + (nd)2k)). Here, n denotes the number of variables, d denotes the maximum size of a domain, and a denotes the number of actions."}
{"pdf_id": "0810.1732", "content": "The time period that we all live in is often described as the beginning of an information age, since the  world's economic focus has started to shift away from the production of physical goods and instead is  growing to revolve around the production and processing of information"}
{"pdf_id": "0810.1732", "content": "While almost no one will argue that the old adage \"knowledge is power\"  holds true now more than ever, the ever increasing amounts of information being made available have  led to new sets of challenges, with the main one being how does an individual or company separate out  the information that is needed from the information that is not?"}
{"pdf_id": "0810.1732", "content": "Human recognition of a phone number has to do with our ability to recognize the pattern of numbers  that comprise a typical phone number. For example, within the U.S. all phone numbers follow some  variant of the convention (XXX) XXX-XXXX, where X can be any digit. The key to the problem is to thus  find a way for computers to be able to interpret and match textual patterns in the same way that they"}
{"pdf_id": "0810.1732", "content": "are able to match keywords. Luckily, most modern programming languages already come equipped to  this, by supporting regular expressions, which allow the development of text patterns for use in pattern  matching. For example, using the Perl 5 regular expression syntax (most modern regular expressions  syntaxes are derivatives of this) a phone number could be matched with the expression:"}
{"pdf_id": "0810.1732", "content": "Since the Z symbol did not lead to an accepted state, the next character in the string (X) will be read into  the state machine. Since X meets the first condition of the state machine, the next character (Y) is read  in as well, which also meets the next condition of the state machine. Finally a third symbol (X) is read  into the state machine which does not meet the final condition of the state machine and results in a  failure to reach an accepted state (Figure 2)."}
{"pdf_id": "0810.1732", "content": "Now that the portion of the string starting with second character failed to reach an accepted state, the  third character of the string (Y) is used as a start symbol for the state machine. In this case the Y symbol  fails to match the first condition of the start machine and results in a failure as well (Figure 3)."}
{"pdf_id": "0810.1732", "content": "Finally, the next symbol in line (X) will be read into the state machine, which will successfully match the  first condition. The remaining symbols Y and Z now meet the remaining conditions of the state machine  and as such a condition of acceptance is reached by the state machine indicating a successful match of a  string of characters to the regular expression (Figure 4)."}
{"pdf_id": "0810.1732", "content": "More advanced state machines can be created by using the or operator (|) or parenthesis which allow  for sub-patterns to be specified. For example, the regular expression XYZ|AB(C|c) would result in a  state machine in which multiple branches could be used to produce an acceptance state (Figure 5)."}
{"pdf_id": "0810.1732", "content": "This regular expression would thus allow the strings XYZ, ABC, or ABc to successfully match. The (C|c)  portion of the expression is what allows for either an uppercase or lowercase C to be accepted following  the letters AB. While the XYZ|AB(C|C) portion of the expression allows for the acceptance of either XYZ  or AB(C|c) (Frenz, 2005; Freidl, 2006)."}
{"pdf_id": "0810.1732", "content": "When using quantifiers, one important thing to note is that by default Perl's regular expression engine is  designed to be greedy in that it will always seek to find the biggest possible match so that if a match of  the regular expression X[A-Z]*X was being performed against the string XABCXABCX, the regular  expression would match the whole string and not just XABCX. This behavior can be changed by placing a  ? after the quantifier, which will allow you to find the smallest possible match rather than the largest  one. Thus if we sought to match XABCX the expression X[A-Z]*?X should instead be used."}
{"pdf_id": "0810.1732", "content": "One question that may arise is that the quantifiers could potentially be symbols that one is interested in  matching as a part of a regular expression, and thus how could someone use a ? for instance as a part of  a regular expression? By default Perl treats characters, such as ?, as metacharacters in that they have a  special meaning to the regular expression engine. In order to turn off this behavior a metacharacter  should be preceded by a backslash. Thus, adding \\? to an expression would allow the ? to be considered  part of the text pattern and not as a quantifier."}
{"pdf_id": "0810.1732", "content": "The basics of how regular expressions work has now been defined, as well as various ways to ease the  development of regular expressions via quantifiers and predefined sub-patterns, but there is another  useful purpose regular expressions can be used for beyond simple pattern matching, and that purpose is  substring capturing"}
{"pdf_id": "0810.1732", "content": "the first set of parenthesis would assign the entire number to $1, the second set would assign the area  code to $2, and the third set would assign the remainder of the phone number to $3. When processing  text-based data, substring capturing is often a highly useful ability in that it can allow pertinent  information to be extracted from Web pages and other data sources (Frenz, 2005)."}
{"pdf_id": "0810.1732", "content": "While the example given above was from the domain of bioinformatics, this approach to searching is  readily suitable for use with other search engines as well. Many search engines offer API's or other  interfaces that allow search results to be directly downloaded into applications for further processing,  such as Yahoo! Search Web Services (http://developer.yahoo.com/search/) or the Google AJAX Search  API (http://code.google.com/apis/ajaxsearch/). These interfaces thus allow search results to be  downloaded to a custom application where they can be further processed by regular expression based  pattern matching and as such help to further refine the search results presented to the application user  in ways that are not easily implemented using keywords alone."}
{"pdf_id": "0810.1732", "content": "When performing such regular expression-based search refinement, however, there are several  potential caveats that one should consider when creating keywords for querying the search engine and  when designing the regular expressions to be used for refinement. One such caveat is that it is  important to ensure that the keywords used are broad enough to return all documents that are"}
{"pdf_id": "0810.2046", "content": "Step (4): extraction of knowledge rules  Balancing assumption is satisfied by the close-open iterations: this process is a guideline to balancing  of crisp and sub fuzzy/rough granules by some random/regular selection of initial granules or other  optimal structures and increment of supporting rules (fuzzy partitions or increasing of lower /upper  approximations ), gradually"}
{"pdf_id": "0810.2046", "content": "In this part of paper, we ensue our algorithms on the \"lugeon data set\" [9], [10]. To evaluate the  interactions, we follow two procedures where phase transition measure is upon the crisp granules  (here NG): 1) second layer takes a few rules , extracted by using NFIS; 2) considering elicited  rules by RST and under an approximated progress (with changing of scaling)."}
{"pdf_id": "0810.2311", "content": "NMF is a dimensionality reduction method of much recent interest which can, for some common kinds ofdata, sometimes yield results which are more meaningful than those returned by the classical method of Prin cipal Component Analysis (PCA), for example (thoughit will not in general yield better dimensionality reduc tion than PCA, as we'll illustrate later)"}
{"pdf_id": "0810.2311", "content": "For data of significant interest such as images (pixel intensities) ortext (presence/absence of words) or astronomical spec tra (magnitude in various frequencies), where the data values are non-negative, NMF can produce components which can themselves be interpreted as objects of thesame type as the data which are added together to pro duce the observed data"}
{"pdf_id": "0810.2311", "content": "2.1 Solving the optimization problem of NMF. Although in the current literature it is widely believedthat NMF is a non-convex problem and only local minima can be found, we will show in the following subsec tions that a convex formulation does exist. Despite the existence of the convex formulation, we also show thata formulation of the problem as a generalized geomet ric program, which is non-convex, could give a better approach for finding the global optimum."}
{"pdf_id": "0810.2311", "content": "After determining W, H, W and H can be recovered by CP factorization of W, H, which again is not an easy problem. In fact there is no practical barrier function known yet for the CP cone so that Interior Point Methods can be employed. Finding a practical description of the CP cone is an open problem. So although the problem is convex, there is no algorithm known for solving it."}
{"pdf_id": "0810.2311", "content": "w2 11 w11w12 w11w21 w11w22 w11h11 w11h21 w11h12 w11h22 w11h13 w11h23 w12w11 w2 12 w12w21 w12w22 w12h11 w12h21 w12h12 w12h22 w12h13 w12h23 w21w11 w21w12 w2 21 w21w22 w21h11 w21h21 w21h12 w21h22 w21h13 w21h23 w22w11 w22w12 w22w21 w2 22 w22h11 w22h21 w22h12 w22h22 w22h13 w22h23 h11w11 h11w12 h11w21 h11w22 h2 11 h11h21 h11h12 h11h22 h11h13 h11h23 h21w11 h21w12 h21w21 h21w22 h21h11 h2 21 h21h12 h21h22 h21h13 h21h23 h12w11 h12w12 h12w21 h12w22 h12h11 h12h21 h2 12 h12h22 h12h13 h12h23 h22w11 h22w12 h22w21 h22w22 h22h11 h22h21 h22h12 h2 22 h22h13 h22h23 h13w11 h13w12 h13w21 h13w22 h13h11 h13h21 h13h12 h13h22 h2 13 h13h23 h23w11 h23w12 h23w21 h23w22 h23h11 h23h21 h23h12 h23h22 h23h13 h2 23"}
{"pdf_id": "0810.2311", "content": "2.2.5 Local solution of the non-convex problem.In the previous sections we gave several convex formulations and relaxations of the NMF problem that unfor tunately are either unsolvable or they give trivial rank one solutions that are not useful at all. In practice the non-convex formulation of eq. 2.2.2 (classic NMF objective) along with other like the KL distance between V and WH are used in practice [22]. All of them are non-convex and several methods have been recommended, such as alternating least squares, gradient decent or active set methods [18]. In our experiments we used the L-BFGS method that scales very well for large matrices."}
{"pdf_id": "0810.2311", "content": "the algorithm proposed in [8] can be employed. The above algorithm uses a branch and bound scheme that is impractical for high dimensional optimization problems as it requires too many iterations to converge. It isworthwhile though to compare it with thelocal non convex NMF solver on a small matrix. We tried to do NMF of order 2 on the following random matrix:"}
{"pdf_id": "0810.2311", "content": "Gradient descent is a possible way to solve the mini mization of the Lagrangian, but it is rather slow. The Newton method is also prohibitive. The Hessian of this problem is a sparse matrix although the cost of the inversion might be high it is worth investigating. Inour experiments we used the limited memory BFGS (L BFGS) method [23, 27] that is known to give a goodrate for convergence. MFNU in this non-convex formulation behaves much better than MVU. In the experi ments presented in [25], MFNU tends to find more often the global optimum, than MVU. The experiments also showed that the method scales well up to 100K points."}
{"pdf_id": "0810.2311", "content": "4.3Computing the local neighborhoods. As al ready discussed in previous section MFNU and isoNMF require the computation of all-nearest and all-furthest neighbors. The all-nearest neighbor problem is a special case of a more general class of problems called N-body problems [10]. In the following sections we give a sort description of the nearest neighbor computation. The actual algorithm is a four-way recursion. More details can be found in [10]."}
{"pdf_id": "0810.2311", "content": "son why most of the times the dual-tree algorithm can prune larger portions of the tree than the single tree algorithm. The complexity of the dual-tree algorithm is empirically O(N). If the dataset is pathological then the algorithm can be of quadratic complexity too. The pseudo-code for the algorithm is described in fig. 1."}
{"pdf_id": "0810.2311", "content": "when it is being preprocessed. This is mainly because the preprocessing distorts the images and spoils the manifold structure. If we don't do the preprocessing fig. 4(f), the reconstruction error of NMF and isoNMF are almost the same. We would also like to point that isoNMF scales equally well with the classic NMF. Moreover they are seem to show the same sensitivity to the initial conditions.In fig. 6 we see a comparison of the energy spectrums of classic NMF and isoNMF. We define the spec trum as"}
{"pdf_id": "0810.2311", "content": "Figure 6: In this set of figures we show the spectrum of classic NMF (solid line) and Isometric NMF (dashed line) for the three datasets (a)cbcl face (b)isomap statue(c)orl faces. Although isoNMF gives much more com pact spectrum we have to point that the basis functions are not orthogonal, so this figure is not comparable to SVD type spectrums"}
{"pdf_id": "0810.2311", "content": "to nonlinear dimensionality reduction by maximum variance unfolding. Proceedings of the Twenty FirstNational Conference on Artificial Intelligence (AAAI 06), 2006. [34] K.Q. Weinberger, F. Sha, and L.K. Saul. Learning a kernel matrix for nonlinear dimensionality reduction.In Proceedings of the twenty-first international confer ence on Machine learning. ACM New York, NY, USA, 2004."}
{"pdf_id": "0810.2861", "content": "The unique optimal solution of this problem is bbb (an abbreviation for x = y = z = b). Its preference is 0.5.The semiring-based formalism allows one to model also optimization prob lems with several criteria. This is done by simply considering SCSPs defined on c-semirings which are the Cartesian product of linearly ordered c-semirings. For example, the c-semiring"}
{"pdf_id": "0810.2861", "content": "Then aaa is a solution, so the CSP is consistent. But bbb is not an optimal solution, while it is a Nash equilibrium of the resulting game. So for consistent CSPs our mapping L yields games in which the set of Nash equilibria is a, possibly strict, superset of the set of solutions of the CSP. However, there are ways to relate CSPs and games so that the solutions and the Nash equilibria coincide. This is what is done in [5], where the mapping is from the strategic games to CSPs. Notice that our mapping goes in the opposite direction and it is not the reverse of the one in [5]. In fact, the mapping in [5] is not reversible."}
{"pdf_id": "0810.2861", "content": "Since there is one constraint, the mappings L and GL coincide. Thus we have that aa is a Nash equilibrium of GL(P) but is not an optimal solution of P. While the mapping defined in this section has the advantage of providing a precise subset relationship between optimal solutions and Nash equilibria, as Theorem 2 states, it has an obvious disadvantage from the computational point of view, since it requires to consider all the complete assignments of the SCSP."}
{"pdf_id": "0810.3418", "content": "Abstract. The purpose of this paper is to introduce an algorithm that can detect the most unusual part of a digital image. The most unusual part of a given shape is defined as a part of the image that has the maximal distance to all non intersecting shapes with the same form. The method can be used to scan image databases with no clear model of the interesting part or large image databases, as for example medical databases."}
{"pdf_id": "0810.3418", "content": "The pitfall of the consideration in the previous subsection is that the detected blocks are rare in absolute sense, e.g. in respect to all figures that satisfy the power law or similar distribution of the projections. Actually this is not desirable. If for example in X-ray image appear several spinal segments, although these can"}
{"pdf_id": "0810.3451", "content": "optimal policies. R-max collects statistics about transitions and rewards. When visits to a state enable high precision estimations of real transition probabilities and rewards then state is declared known. R-max also maintains an approximate model of the environment. Initially, the model assumes that all actions in all states lead to a (hypothetical) maximum-reward absorbing state. The model is updated each time when a state becomes known. The optimal policy of themodel is either the near-optimal policy in the real environment or enters a not yet-known state and collects new information."}
{"pdf_id": "0810.3451", "content": "The first two benchmark problems, RiverSwim and SixArms, were taken from ? (?). The RiverSwim MDP has 6 states, representing the position of the agent in a river. The agent has two possible actions: she can swim either upstream or downstream. Swimming down is always successful, but swimming up succeeds"}
{"pdf_id": "0810.3451", "content": "We proposed a new algorithm for exploration and reinforcement learning inMarkov decision processes. The algorithm integrates concepts from other advanced exploration methods. The key component of our algorithm is an op timistic initial model. The optimal policy according to the agent's model will either explore new information that helps to make the model more accurate, or follows a near-optimal path. The extent of optimism regulates the amount of exploration. We have shown that with a suitably optimistic initialization, our algorithm finds a near-optimal policy in polynomial time. Experiments were conducted on a number of benchmark MDPs. According to the experimental results our novel method is robust and compares favorably to other methods."}
{"pdf_id": "0810.3451", "content": "Unifying the two requirements for m completes the proof of the lemma. The following is a minor modification of [KS] lemma 4, and [SL] Lemma 1. The result tells that if the parameters of two MDPs are very close to each other, then the value functions in the two MDPs will also be similar."}
{"pdf_id": "0810.3451", "content": "Let us introduce a modified version of OIM that behaves exactly like the old one, except that in each (x, a) pairs, it performs at most m updates. If a pair is visited more than m times, the modified algorithm leaves the counters unchanged. The following result is a modification of [SL]'s Lemma 7."}
{"pdf_id": "0810.3451", "content": "Let us introduce a modified version of OIM that behaves exactly like the old one, except that in each (x, a) pairs, it performs at most m updates. If a pair is visited more than m times, the modified algorithm leaves the counters unchanged. The following result is a modification of [SL]'s Lemma 7."}
{"pdf_id": "0810.3474", "content": "allowed to perform actions in that environment. Humans  learn by interacting with each other. Lessons are learned from  being rewarded or punished after performing an action. This  is different from supervised learning [3]. In supervised  learning, a learning algorithm is given test cases that have  inputs and the corresponding correct outputs. This for  example, can be in the form of function approximation as  shown in equation (1)."}
{"pdf_id": "0810.3474", "content": "Where x can be a vector of multiple inputs and y is a vector  that is composed of multiple outputs. Thus the learning  algorithm  tries  to  approximate  the  function  f(.).  Reinforcement learning can be categorized as unsupervised  learning. An agent is placed in an environment. It performs  actions in that environment and perceives the effects of the  actions in that environment through its sensors/receptors. The  agent also receives a reward/punishment given the change the  action has made in the environment. This reward can be  extrinsic (from the environment) or intrinsic (from within the  agent) [9]. This is illustrated in Figure 1."}
{"pdf_id": "0810.3474", "content": "environment are not normally provided or known. Thus a  challenge in reinforcement learning is modelling an  environments dynamics within the agent. To do this the  concept of the value of a state is introduced. This is done  through the introduction of Value Function and Action Value  functions. Through these functions one can evaluate the  policy that the agent is taking. The value function is defined  in (2) as:"}
{"pdf_id": "0810.3474", "content": "The being or in this case agent must be able to [12]:  • Pay attention to the what is being observed  • Remember the observations  • Be able to replicate the behavior  • Be motivated to demonstrate what they have learnt  Thus learning by observing involves four processes:  attention,  retention,  production  and  motivation"}
{"pdf_id": "0810.3474", "content": "Humans play and learn board games in groups. This  community of players imparts knowledge on each other. If  one looks at communities of chess or Scrabble [16] players  one can see that very experienced players mentor weaker  players. To simulate a social learning environment such as  this, multiple agents need be created. In this paper each agent  is given its own identity in that they have different  initialization parameters. The agents have the same learning  algorithm but have different initialization options. This is  shown in Table 1."}
{"pdf_id": "0810.3474", "content": "Two training configurations are used in training the agents  in the social setting. The two methods are derived from  tournament styles. A modified Swiss [17] and a Round Robin  system are used and compared. In the modified Swiss  configuration, agents are paired up to play one round of a  game which is a full episode. When the game is finished there  is either a winner or a loser or there is a draw. A tournament  like structure was utilised for the agents to play in. The  structure is shown in Figure 3."}
{"pdf_id": "0810.3474", "content": "A. Tic Tac Toe  Tic-Tac-Toe [18] is a 3 x 3 board game. Two players place  pieces on the board trying to connect three of their own pieces  in a row. Figure 4 illustrates the player with the noughts  defeating the player with the crosses."}
{"pdf_id": "0810.3474", "content": "If two great players play a game of Tic-Tac-Toe it should  always end with a draw [2]. The game has been modeled with  reinforcement learning in the past [5]. It has been recorded  that agents take 50000 learning episodes [19] to be able to  play at a beginner level. In this experiment this is the amount  of iterations used for the training of the agents."}
{"pdf_id": "0810.3474", "content": "The games are managed by a game controller. The  controller allocates who has to play next and also keeps track  of game statistics such as wins, test results and how many  times each agent has played games. It also matches winners  and losers and thus implements the social frameworks  described in section III. The agents are initialized with  different learning parameters. Thus the agents play against  non-stationary opponents. This stimulates the emergence of  more robust agents. The opponents policies are also changing  and thus a learner will have to adjust its policy to be a policy  that can play against more than one stationary opponent."}
{"pdf_id": "0810.3474", "content": "The second test the agents take is taking part in a league.  All of the agents are allowed to play with all the other agents.  The wins, losses and draws are recorded. This is used to find  which of the agents are the strongest. 5000 games are played  by the agents against each other. This was applied to the best  modified Swiss agents and Self-Play agents."}
{"pdf_id": "0810.3474", "content": "first size is 4, then 6 and then 8. Each of these was tested 5  different times with the board test (meaning they have been  trained differently 5 times) and then 5 times with the play  test. The results are presented in the following section."}
{"pdf_id": "0810.3474", "content": "increase in the number of intermediate agents in one  generation. This is more evident in the Swiss tournament  setting as opposed to the Round Robin configuration. Both  configurations were tested with 16 and 32 agent sized  populations. When the populations are increased with the  modified Swiss configuration more than one intermediate  agent emerges. In some stages up to 6 intermediate agents  emerge. With the Round Robin configuration 2 intermediate  playing agents have emerged.  By introducing multiple different agents as opponents in  the training phases, one has been able to create agents that  are superior to the S-P agent."}
{"pdf_id": "0810.3474", "content": "thousands of players in any sport.   In the play tests the beginner level of the agents is further  shown as they all have higher chances of winning if they start  the game first. The social agents have made it possible to  create agents that are superior to the best self-play agents.  This is a positive result and merits the potential for the use of  social methods in agent learning."}
{"pdf_id": "0810.3579", "content": "Abstract. Graph kernels methods are based on an implicit embeddingof graphs within a vector space of large dimension. This implicit embed ding allows to apply to graphs methods which where until recently solely reserved to numerical data. Within the shape classification framework, graphs are often produced by a skeletonization step which is sensitiveto noise. We propose in this paper to integrate the robustness to struc tural noise by using a kernel based on a bag of path where each path is associated to a hierarchy encoding successive simplifications of the path. Several experiments prove the robustness and the nexibility of our approach compared to alternative shape classification methods."}
{"pdf_id": "0810.3579", "content": "The bag of path approach is based on a decomposition of the complex graph structure into a set of linear objects (paths). Such an approach benefits of recentadvances in both string and vectors kernels. Our graph kernel based on a hier archy of paths is more stable to small perturbations of the shapes than kernels based solely on a bag of paths. Our notion of path's hierarchy is related to the graph edit distance through the successive rewritings of a path. Our kernel is thus related to the ones introduced by Neuhaus and Bunke."}
{"pdf_id": "0810.3579", "content": "Haack J. 33 Haidt D. 11 Hamon O. 28 Handschuh D. 11 Hanlon E.M. 18 Hapke M. 11 Harjes J. 11 Haydar R. 26 Haynes W.J. Hedberg V. 21 Heinzelmann G. 13 Henderson R.C.W. 18 Henschel H. 33 Herynek I. 29 Hildesheim W. 11 Hill P. 11 Hilton C.D. 22 Hoeger K.C. 22 Huet Ph. Hufnagel H. 14 Huot N. 28"}
{"pdf_id": "0810.3605", "content": "In the following both agent and environment are formalized as causal models over I/O sequences. Agent and environment are coupled to exchange symbols following a standard interaction protocol having discrete time, observation and control signals. The treatment of the dynamics are fully probabilistic, and in particular, both actions and observations are random variables, which is in contrast to the decision-theoretic agent formulation treating only observations as random variables (Russell and Norvig, 2003). All proofs are provided in the appendix."}
{"pdf_id": "0810.3605", "content": "In coding theory, the problem of compressing a sequence of observations from an unknown source is known as the adaptive coding problem. This is solved by constructing universal compressors, i.e. codes that adapt on-the-ny to any source within a predefined class. Such codes are obtained by minimizing the average deviation of a predictor from the true source, and then by constructing codewords using the predictor. In this subsection, this procedure will be used to derive an adaptive agent (Ortega and Braun, 2010a)."}
{"pdf_id": "0810.3605", "content": "Formally, the deviation of a predictor P from the a true distribution Pm is measured by the relative entropy2. A first approach would be to construct an agent B so as to minimize the total expected relative entropy to Pm. This is constructed as follows. Define the history-dependent relative entropies over the action at and observation ot as"}
{"pdf_id": "0810.3605", "content": "Following the discussion in the previous section, an adaptive agent P is going to be con structed by minimizing the expected relative entropy to the Pm, but this time treatingactions as interventions. Based on the definition of the conditional probabilities in Equa tion 6, the total expected relative entropy to characterize P using interventions is going to be defined. Assuming the environment is chosen first, and that each symbol depends"}
{"pdf_id": "0810.3605", "content": "Adaptive control is formalized as the problem of designing an agent for an unknown envi ronment chosen from a class of possible environments. If the environment-specific agents are known, then the Bayesian control rule allows constructing an adaptive agent by combining these agents. The resulting adaptive agent is universal with respect to the environment class. In this context, the constituent agents are called the operation modes of the adaptiveagent. They are represented by causal models over the interaction sequences, i.e. condi tional probabilities P(at|m, ao"}
{"pdf_id": "0810.3605", "content": "where rj and fj are the counts of the number of times a reward has been obtained from pulling lever j and the number of times no reward was obtained respectively. Observe that here the summation over discrete operation modes has been replaced by an integral over the continuous space of configurations. In the last expression we see that the posterior distribution over the lever biases is given by a product of N Beta distributions. Thus, sampling an action amounts to first sample an operation mode m by obtaining each bias mj from a Beta distribution with parameters rj +1 and fj +1, and then choosing the action corresponding to the highest bias i = arg maxj mj."}
{"pdf_id": "0810.3605", "content": "The key idea of this work is to extend the minimum relative entropy principle, i.e. the variational principle underlying Bayesian estimation, to the problem of adaptive control. From a coding point of view, this work extends the idea of maximal compression of the observation stream to the whole experience of the agent containing both the agent's actions and observations. This not only minimizes the amount of bits to write when saving/encoding"}
{"pdf_id": "0810.3605", "content": "• Compression principles. In the literature, there is an important amount of work relating compression to intelligence (MacKay, 2003; Hutter, 2004a). In particular, it has been even proposed that compression ratio is an objective quantitative measure of intelligence (Mahoney, 1999). Compression has also been used as a basis for a theory of curiosity, creativity and beauty (Schmidhuber, 2009)."}
{"pdf_id": "0810.3605", "content": "• Mixture of experts.Passive sequence prediction by mixing experts has been stud ied extensively in the literature (Cesa-Bianchi and Lugosi, 2006). In (Hutter, 2004b), Bayes-optimal predictors are mixed.Bayes-mixtures can also be used for univer sal prediction (Hutter, 2003). For the control case, the idea of using mixtures of expert-controllers has been previously evoked in models like the MOSAIC-architecture (Haruno et al., 2001). Universal learning with Bayes mixtures of experts in reactive environments has been studied in (Poland and Hutter, 2005; Hutter, 2002)."}
{"pdf_id": "0810.3605", "content": "• Stochastic action selection. Other stochastic action selection approaches are foundin Wyatt (1997) who examines exploration strategies for (PO)MDPs, in learning au tomata (Narendra and Thathachar, 1974) and in probability matching (R.O. Duda, 2001) amongst others. In particular, Wyatt (1997) discusses theoretical properties of an extension to probability matching in the context of multi-armed bandit problems. There, it is proposed to choose a lever according to how likely it is to be optimal and it is shown that this strategy converges, thus providing a simple method for guiding exploration."}
{"pdf_id": "0810.3605", "content": "This work introduces the Bayesian control rule, a Bayesian rule for adaptive control. The key feature of this rule is the special treatment of actions based on causal calculus and thedecomposition of an adaptive agent into a mixture of operation modes, i.e. environment specific agents. The rule is derived by minimizing the expected relative entropy from thetrue operation mode and by carefully distinguishing between actions and observations. Fur thermore, the Bayesian control rule turns out to be exactly the predictive distribution over the next action given the past interactions that one would obtain by using only probability and causal calculus. Furthermore, it is shown that agents constructed with the Bayesian"}
{"pdf_id": "0810.3865", "content": "gives better generalization. Therefore, a study on the size  of the ensemble was done as to find the optimal size that  can be used for the investigation. The methods for  measuring structural diversity are to be devised and  implemented. Moreover, the outcome diversity of  structurally different classifiers is critical to be measured.  This is because it is essential to show how correlated the  outcomes of the structurally different classifiers is. Hence,  the limitations of accuracy in the structural diversity are  to be justified."}
{"pdf_id": "0810.3865", "content": "Different methods for creating diversity such as bagging  and boosting have been explored [1, 3]. However, the  aggregation methods are to be used to combine the  ensemble predictions. Methods of voting and averaging  have been found to be popular [9, 10] and hence are used  in this study."}
{"pdf_id": "0810.3865", "content": "The paper first discusses the background in section 2.  Analysis of the data used for this study is presented in  section 3. The accuracy measure and structural measures  of diversity used are discussed in section 4 and section 5.  The methodologies used in investigating the effect of  diversity on generalization are presented in section 6. The  results and future work are then discussed in section 7."}
{"pdf_id": "0810.3865", "content": "Neural Networks (NN) are computational models that  have the ability to learn and model linear and non-linear  systems [11]. There are many types of neural networks  but the most common neural network architecture is the  multilayer perceptron (MLP) [11]. The neural network  architecture that is used in this paper is a MLP network as  shown in Figure 1. The MLP network has the input layer,  the hidden layer and the output layer. An MLP network"}
{"pdf_id": "0810.3865", "content": "The inputs into the neural network are the demographic  data attributes from the HIV antenatal survey and the  output is the HIV status of the individual where 0  represents negative and 1 represents positive. The weights  of the NN are updated using a back propagation algorithm  during the training stage [11].The threshold of 0.5 is used  in order to achieve a zero or one solution from the neural  network. This means that any value less than 0.5 is  converted to 0 and any value more than 0.5 is converted  to 1."}
{"pdf_id": "0810.3865", "content": "The genetic algorithms (GA) are computational models  that are based on the evolution of biological population  [2]. Potential solutions are encoded as the chromosomes  of some individual. These individuals are initially  generated randomly. The individuals are evaluated  through the defined fitness function. Each preceding  generation is populated by the fitness solution (members)  of the previous generation and their offspring. The  offsprings are created through crossover and mutation.  The crossover process combines genetic information of"}
{"pdf_id": "0810.3865", "content": "The dataset used for the study is from antenatal clinics in  South Africa and it was collected by the department of  health in 2001. The features in the data include the age,  gravidity, parity, education, etc. The demographic data  used in the study is shown in table 1 below. The province  was provided as a string so it was converted to an integer  from 1 to 9."}
{"pdf_id": "0810.3865", "content": "2  Education  integer  0-13  3  Parity  integer  0-9  4  Gravidity  integer  1-12  5  Province  integer  1-9  6  Age of father  integer  14-60  7  HIV status  binary  0-1"}
{"pdf_id": "0810.3865", "content": "The data preprocessing is necessary in order to eliminate  impossible situations such as parity being greater than  gravidity because it is not possible for the mother to give  birth without falling pregnant. The pre-processing of the  data resulted in a reduction of the dataset. To use the  dataset for training, it needs to be normalized because  some of the data variables with larger variances will  influence the result more than others. This ensures that all  variables can contribute to the final network weights of  the prediction model [13]. Therefore, all the data is to be  normalized between 0 and 1 using (2)."}
{"pdf_id": "0810.3865", "content": "Regression problems mostly focus on using the mean  square error between the actual outcome and the predicted  outcome as a measure of how well neural networks are  performing. In classification problems, the accuracy can  be measured using the confusion matrix [14]. Analysis of  the dataset that is being used showed that the data is  biased towards the negative HIV status outcomes. Hence,  the data was divided such that there is equal number of  HIV positive and negative cases. The accuracy measure  that is used in this study is given by (3)."}
{"pdf_id": "0810.3865", "content": "Shannon entropy is a diversity measure that was adopted  from ecology and information theory to understand  ensemble diversity [15]. This measure is implemented to  measure structural diversity. The Shannon-Wiener index  is commonly used in information theory to quantify the  uncertainty of the state [15, 16]. If the states are diverse  one becomes uncertain of the outcome. It is also used in  ecology to measure diversity of the species. Instead of  biological species, the species are considered as the  individual base classifiers. The Shannon diversity  measure is given by (4)."}
{"pdf_id": "0810.3865", "content": "Since the focus of the study is the structural diversity, the  activation function, learning rate and the number of  hidden nodes were varied as to induce diversity.  However, varying all the parameters was found to be  ineffective because the classifiers tend to generalize the  same way. Therefore, only hidden nodes and activation  function were varied for this investigation."}
{"pdf_id": "0810.3865", "content": "The classifiers are trained individually using the back  propagation method; where the error is propagated back  so as to adjust the weights accordingly. The data used for  training, validation and testing are the HIV data. All the  features of the input are fed to all the networks. The  classifiers which have the training accuracy of 60% were  accepted. The training accuracy between 60% and 63%  was achieved. The hidden nodes were varied from 7 to 57  and the activation function between the logistics and the  linear function was randomly varied. The classifiers were  trained using quasi-Newton algorithm for 100 cycles at  the same learning rate of 0.01."}
{"pdf_id": "0810.3865", "content": "classification accuracy [17, 18]. This ensures that the  results are based on the consensus decision of the base  classifiers. The base classifiers operate concurrently  during the classification and their outputs are integrated to  obtain the final output [18]. The model for the committee  of classifiers is shown in figure 2."}
{"pdf_id": "0810.3865", "content": "There are many aggregation methods that can be used to  combine the outcomes of classifiers. These were explored  in the preliminary report. The ensemble outcomes were  all aggregated using simple majority voting. This was  chosen because it is popular and easy to implement [9].  The outcomes of each individual from an ensemble are  first converted to 0 or 1 using 0.5 as a threshold. The  majority voting method chooses the prediction that is  mostly predicted by different classifiers [19]. The other  method that was implemented was averaging. All the  outcomes from all the classifiers are taken and averaged."}
{"pdf_id": "0810.3865", "content": "reached, the accuracy tends to remain constant.  Nevertheless, the size of 21 was found to be optimal since  it produced the best accuracy. The results obtained are  found to be concurrent with literature. Currently the  optimal size of an ensemble is 25 [18, 20]. Therefore, an  ensemble size of 21 is used for evaluating the relationship  between diversity and performance of classifiers on HIV  classification."}
{"pdf_id": "0810.3865", "content": "Currently, measuring the outcome diversity had been  popular than measuring the structural diversity [6]. It was  however necessary to measure the outcome diversity for  this study. This is because it is essential to measure the  degree of the agreement and disagreement on the  outcomes of the ensemble. This experiment was useful for  analysing the limitations on structural diversity results.  The diversity measure such as Q statistics was used to  measure diversity."}
{"pdf_id": "0810.3865", "content": "Q statistics evaluate the degree of similarity and  dissimilarity in the outcomes of the classifiers within the  ensemble [8]. The diversity index ranges from -1 to 1  where 0 indicates the highest diversity and 1 indicate  lowest diversity [6]. For all 21 classifiers in an ensemble,  each classifier is paired with every other classifier within  the ensemble. The results from this study show that  outcomes of the structurally diverse classifiers within the  ensemble are highly correlated. This is indicated by a Q  value which is closer to 1. The obtained Q value is from  0.88 to 0.91."}
{"pdf_id": "0810.3865", "content": "The created classifiers were used to investigate the  relationship between the diversity and accuracy. There  were ten base classifiers or species that were selected  from the created classifiers which are all structurally  different based only on the hidden nodes and activation  functions. These networks had different activation  function and hidden nodes were varied from 10 to 55 in"}
{"pdf_id": "0810.3865", "content": "steps 5. The GA has the capabilities to search large spaces  for a global optimal solution [5]. GA was therefore used  to search for 21 classifiers from the 10 base classifiers  using the accuracy as the fitness function. The fittest  function is given by:"}
{"pdf_id": "0810.3865", "content": "In this study, diversity was induced by varying the  parameters of the classifiers that form an ensemble  [5, 16]. The investigation was done on an ensemble of 21  classifiers. Figure 5 shows the obtained results using the  Shannon diversity measure. Figure 6 shows the results  obtained using the Simpson diversity measure."}
{"pdf_id": "0810.3865", "content": "It was however observed that the individual classifiers  within the ensemble were highly correlated in the  outcomes. This had affected the results because very low  and high accuracies could not be attained. It is however  recommended that a strategy of adding classifiers in an  ensemble such that only classifiers that are uncorrelated  are accepted in an ensemble can be adopted. The  experiment focuses on training the classifiers using all the  features of the data. It is however recommended that  different networks can be fed different features of the  data. This might ensure that the outcomes of classifiers  are not highly correlated. Hence, a higher range of  accuracy and diversity index can be attained."}
{"pdf_id": "0810.3865", "content": "The author would like to thank Fulufhelo Netshiongolwe  for his cooperation and contribution during the project as  a project partner. Professor Tshilidzi Marwala is thanked  for supervising the project and additional thanks are  extended to the postgraduate student Lesedi Masisi for his  contribution during implementation of the project."}
{"pdf_id": "0810.4426", "content": "A variety of methods exist for estimating camera distortioncorrection model parameters. Earlier efforts relied on im agery with artificially created structure, either in the form of a test-field, populated with objects having known 3-D world coordinates, or using square calibration grids with lines at constant intervals [13,16,2]. Alternative approaches do not require artificially created structure, but used multiple views of the same scene. The calibration technique makes use ofconstraints due to known camera motion (for instance rota tion) [23], known scene geometry such as planar scenes [21]or general motion and geometry constrained with the epipo lar constraint [24,1,5].These approaches required access to the camera in or der to perform a specific operation, such as acquiring views"}
{"pdf_id": "0810.4426", "content": "We propose a method that is simple and robust to high levels of noise, as shown in the results section. In our algorithm we calculate all image edgels, and then transform these into a one-dimensional Hough space representation of angle. This creates an orientation histogram of the edgel angles. In this form, curved lines will be represented at a variety of angles, while straight lines will be found only at one. Therefore, we optimize the model distortion parameters which minimizethe entropy (or spread) of the Hough space angular repre sentation. The individual steps are:"}
{"pdf_id": "0810.4426", "content": "Note that we do not parameterise the line with a func tion. The line and its normal is known (and used) only at adiscrete set of points, specifically where the edgels are detected. This means that l(t) and n(t) can be evaluated at ev ery value of t we require. Since the edgel detection processalso provides the normals, J is only a function of the distor tion model, and is therefore computed analytically from the definition of D. The derivation of J for the Harris model is given in Appendix A."}
{"pdf_id": "0810.4426", "content": "The radial distortion correction method presented here is motivated by the observation that curved lines map to spreadout peaks in Hough space, while straight lines map to a single bin. Therefore, it is desirable to have an objective func tion that measures this spread. In information theory this quality is represented by entropy [22]. We have therefore normalized the 1-D Hough representation, and treat it as a probability distribution. The objective function is then:"}
{"pdf_id": "0810.4426", "content": "In this paper, we have presented a new, simple and robustmethod for determining the radial distortion of an image us ing the plumb-line constraint. The technique works by first extracting salient edgels and then minimizing the spread ofa 1D angular Hough transform of these edgels. The tech nique is simple and because no edge fitting is performed, thetechnique is very robust to the presence of noise. Further more, the technique is more generally applicable than other plumb-line techniques in that the lines used do not need tobe continuous. The technique works on textures with prin cipal directions, as illustrated by the aerial image of a city,"}
{"pdf_id": "0810.4426", "content": "The proposed algorithm has a number of parameters: the parameters of the tensor voting kernel, the number of binsand the parameters of the optimization. In practice, the se lection of these parameters are not critical, and indeed the same set of parameters was used for the simulated data, the example images and the test images shown."}
{"pdf_id": "0810.4426", "content": "Our method is nexible in that it does not impose con straints beyond the presence of one or more straight edges: it is not a requirement that the edges share vanishing points,or structure of any particular kind. It is not even a require ment that the edgels belong to a related set of images. The technique can be equally applied to edgels from multipleimages of unrelated scenes taken with the same camera pa rameters. Finally, our method is widely applicable because it is, in terms of RMS error, able to produce a calibration to within three percentage points of a technique requiring access to the camera and structured scenes."}
{"pdf_id": "0810.4617", "content": "One may view Problem 1 as a special case of semi-supervised learning [4], where the unlabelled data X(u) represent the multipleobservations with the extra constraint that all unlabelled data exam ples belong to the same (unknown) class. The problem then resides in estimating the single unknown class, while generic semi-supervised learning problems attribute the test examples to different classes."}
{"pdf_id": "0810.4617", "content": "We propose now to build on graph-based algorithms to solve the problem of classification of multiple observation sets. In general, label propagation assumes that the unlabelled examples come from different classes. As Problem 1 presents the specific constraint that all unlabelled data belong to the same class, label propagation does not fit exactly the definition of the problem as it falls short of exploiting its special structure. Therefore, we propose in the sequel a novel graph-based algorithm, which (i) uses the smoothness criterion on"}
{"pdf_id": "0810.4617", "content": "In this section we evaluate our graph-based algorithm in the context of object recognition from multi-view image sets. In this case, the different views are considered as multiple observations of the same object, and the problem is to recognize correctly this object. The proposed MASC method implements Gaussian weights (1) and sets k = 5 in the construction of the k-NN graph. We compare MASC to well-known methods from the literature, which mostly gather algorithms based on either subspace analysis or density estimation (statistical methods):"}
{"pdf_id": "0810.4617", "content": "• MSM. The Mutual Subspace Method [9], [10], which is the most well known representative of the subspace analysis methods. It represents each image set by a subspace spanned by the principal components, i.e., eigenvectors of the covariance matrix. The comparison of a test image set with a training one is then achieved by computing the principal angles [11] between the two subspaces. In our experiments, the number of principal components has been set to nine, which has been found to provide the best performance."}
{"pdf_id": "0810.4617", "content": "• KLD. The KL-divergence algorithm by Shakhnarovich et al [13] is the most popular representative of density-based statistical methods. It formulates the classification from multiple images as a statistical hypothesis testing problem. Under the i.i.d and the Gaussian assumptions on the image sets, the classification problem typically boils down to a computation of the KL divergence between sets, which can be computed in closed form in this case. The energy cut-off, which determines the number of principal components used in the regularization of the covariance matrices, has been set to 0.96."}
{"pdf_id": "0810.4617", "content": "In this section we evaluate our graph-based algorithm in the context of face recognition from video sequences. In this case, the different video frames are considered as multiple observations of the same person, and the problem consists in the correct classification of this person. We evaluate in this section the behavior of the MASC algorithm in realistic conditions, i.e., under variations in head pose, facial expression and illumination. Note in passing that our algorithm does not assume any temporal order between the frames; hence, it is also applicable to the generic problem of face recognition from image sets. We use two publically available databases; the VidTIMIT [15] and the first subset of the Honda/UCSD [16] database. The VidTIMIT"}
{"pdf_id": "0810.4617", "content": "We first study the performance of the MASC algorithm with the VidTIMIT database. Figure 6 shows a few representative images from a sample face manifold in the VidTIMIT database. Observe the presence of large head pose variations. Figure 7 shows the 3D projection of the manifold that is obtained using the ONPP method [18], which has been shown to be an effective tool for data visualization. Notice the four clusters corresponding to the four different head poses i.e., looking left, right, up and down. This indicates that a graph-based method should be able to capture the geometry of the manifold and propagate class labels based on the manifold structure. Since there are three sessions, we use the following metric for evaluating the classification performances"}
{"pdf_id": "0810.4617", "content": "We evaluate the video face recognition performance of all methods for diverse sizes of the training and test sets. The objective is to assess the robustness of the methods with respect to the size of the training and test set. For this reason, each image set is re-sampled as"}
{"pdf_id": "0810.4617", "content": "We further study the video-based face recognition performance on the Honda/UCSD database. Figure 9 shows a few representative images from a sample face manifold in the Honda/UCSD database. Observe the presence of large head pose variations along with facial expressions. The projection of the manifold on the 3D space using ONPP shows again clearly the manifold structure of the data (see Figure 10), which implies that a graph-based method is more suitable for such kind of data."}
{"pdf_id": "0810.4617", "content": "In this paper we have addressed the problem of classification of multiple observations of the same object. We have proposed to exploit the specific structure of this problem in a graph-based algorithm inspired by label propagation. The graph-based algorithm relies on the smoothness assumption of the manifold in order to learn the unknown label matrix, under the constraint that all observations correspond to the same class. We have formulated this process as a discrete optimization problem that can be solved efficiently by a low complexity algorithm. We provide experimental results that illustrate the performance of the proposed solution for the classification of handwritten digits, for object recognition and for video-based face recognition. In the two latter cases, the graph-based solution outperforms state-of-the-art"}
{"pdf_id": "0810.4668", "content": "Example 1: We draw an example of information table  from [18], as shown in Table 1, which is a partial analysis  of papers in proceedings of RSFDGrC 2005 and RSKT  2006. Values in the column \"Theory\" represent Rough Sets  related theories which appear in these papers, while values  in the column \"Application Domain\" represent the related  application domains that these papers refer to. Following is  an example of a concept granule based on Table 1:   (( . ⑷  , , ), , , )) Theory FCA m Theory FCA"}
{"pdf_id": "0810.4668", "content": "Definition 4: (Partial Ordered Relation) Since the  extension of a concept granule corresponds to a set of  elements satisfying its intension, a partial ordered relation  on two concept granules can be defined based on set  inclusion [13]:   ( , ( )) ( , ( )) ( ) ( ) . ⑸"}
{"pdf_id": "0810.4668", "content": "R-A: Rough-Algebra, LR: Logics and Reasoning, RFH:  Rough-Fuzzy Hybridization, FCA: Formal Concept  Analysis, DR: Data Reduction, MS: Medical Science, BI:  Bioinformatics, IP: Image Processing, DT: Decision Table,  RPA: Rough Probabilistic Approach, GC: Granular  Computing, RA: Rough Approximation, IR: Information  Retrieval, MS: Medical Science, IS: Information Security."}
{"pdf_id": "0810.4668", "content": "Relations show how concept granules are connected to  each other [4]. One may define other binary relations  between concept granules. In the context of Artificial  Intelligence and Cognitive Psychology, a composition of  concepts and relations can be used to form a conceptual  graph, which can be used to represent knowledge [1, 4, 8,  9]. From the view point of granular computing, we can use  concept granules and relations among them to describe  granular knowledge structures."}
{"pdf_id": "0810.4668", "content": "A granular knowledge structure emphasizes on how the  concept granules are organized. If concept granules  involved in the granular knowledge structure can be  organized into levels, then the granular knowledge structure  is a hierarchy composed of concept granules. Concept  granules in the same level may share some commonalities.  If they cannot be organized into levels, they may form a  concept  granule  network.  One  can  get  intuitive  understanding of knowledge through different granular  knowledge structures from different views, which can be  induced based on various operations."}
{"pdf_id": "0810.4668", "content": "Definition 6: (Attribute-Value Structure) In an  information table, let an attribute a  and it has a  corresponding set of attribute values, denoted as  . One can generate a set of concept granules  based on equality relations on attribute and attribute values.  A more general concept granule, denoted as"}
{"pdf_id": "0810.4668", "content": "Example 4: With respect to Figure 1(a) and Figure  1(b), the two concept granules [Theory] and [Application  Domain] share the same attribute and attribute value  Discipline, , = Rough Sets) . We consider providing a more general concept granule [Rough Sets] as their super concept granule. The new granular knowledge structure is  shown in Figure 2, which shows an understanding of  Rough Sets from two views, namely, related theories and  application domains."}
{"pdf_id": "0810.4668", "content": "where  . Notice that sub-concept granules which  share the same intention need to be merged together to the  same one. Their corresponding extensions are also grouped  together as the extension of the new one. This operation  helps to understand how a knowledge structure can be  constantly evolving by merging related knowledge source."}
{"pdf_id": "0810.4668", "content": "Example 5: Figure 3(a) and Figure 3(b) are two  granular knowledge structures considering related theories  in proceedings of RSFDGrC 2005 and RSKT 2006. Since  the bottom concept granules of these two structures are all  [Theory], we can use union operation to obtain a unified  structure, which provides a more complete description for  the sub theories of Rough Sets, as shown in Figure 3(c)."}
{"pdf_id": "0810.4668", "content": "Example 6: Considering Figure 4(a) and Figure 4(b),  Since the bottom concept granule of these two structures  are all [Theory], we can use intersection operation to obtain  a new granular knowledge structure, as Figure 4(c), which  shows a partial structure that Figure 4(a) and Figure 4(b)  both have. Since it appears in the analysis results of both  proceedings, the partial structure may reflect hot research  topics in the Rough Sets community."}
{"pdf_id": "0810.4668", "content": "Example 7: Figure 5(a) and Figure 5(b) are granular  knowledge structures representing related theory of Rough  Sets based on proceedings of RSFDGrC 2005 and RSKT  2006. Through the difference operation on these two  structures, we get a new structure, as shown in Figure 5(c),  which shows related theories that Figure 5(a) has while  Figure 5(b) doesn't have, namely, Logic and Reasoning,  and Rough Approximation. This operation helps us to find  the unique topics of a proceeding or a book, which others  may don't contain.  [Theory]  (c) Union operation on (a) and (b)"}
{"pdf_id": "0810.4668", "content": "The concrete meaning of this granular knowledge  structure is as follows: in the bottom level, we just can  conclude that these papers are about Rough Sets. In the  second level, papers are categorized by \"Theory\" and  \"Application Domain\". In the third level, they are classified  by concrete values of \"Theory\" or \"Application Domain\".  In the fourth level, the extension of each concept granule  corresponds to a group of papers which are about an  application domain and meanwhile use a related theory."}
{"pdf_id": "0810.4668", "content": "In granular knowledge structures induced by product  operation, each level represents the concept granule in a  certain degree of granularity. Different levels of concept  granules form a partial ordering. The hierarchical structures  describe the integrated whole of a web of concept granules  from a very high level of abstraction to the very finest  details."}
{"pdf_id": "0810.4668", "content": "Reif and Heller argue that \"effective problem solving in a  realistic domain depends crucially on the content and  structure of the knowledge about the particular domain\" [2].  Hence, the use of granular knowledge structures could help  one solve problems. Selections and switches on levels and  views are two possible practical strategies on how to use  granular knowledge structures."}
{"pdf_id": "0810.4668", "content": "In order to get detailed understanding of a granular  knowledge structure, one may not only view it as an  integrated whole, but also need to investigate concept  granules among levels. For concrete tasks, some specific  levels can be selected. Switching among those levels help"}
{"pdf_id": "0810.4668", "content": "It is emphasized that people with different background  knowledge and purpose will have different understanding  when learning from the same knowledge source [3]. For the  same knowledge source, different views may induce  different granular knowledge structures, and one can get  different understandings of the knowledge source through  each of them. In upper sections of this paper, we examined  concrete examples in the field of scientific literature, and  we provide different granular knowledge structures based  on various operations. Each granular knowledge structure  shows a unique understanding of the papers in those two  proceedings. Even for the same granular knowledge  structure, one can get different understanding when  different viewpoint is selected [3]."}
{"pdf_id": "0810.4668", "content": "Example 9: Figure 7(a) shows an analysis of the 1st 4th China National Rough Sets and Soft Computing  Conference proceedings from the viewpoint of main related  fields, namely, Rough Sets, Fuzzy Sets. The concept  granules [RS] and [FS] form a partial ordering with their  sub-concept granules respectively. We can conclude that  \"data reduction\" and \"machine learning\" are two related  fields for both Rough Sets and Fuzzy Sets. This piece of"}
{"pdf_id": "0810.4668", "content": "knowledge indicates that researchers on Rough Sets and  Fuzzy Sets can work on \"data reduction\" and \"machine  learning\". If we switch to another view to investigate the  picture (as in Figure 7(b)), [ML] and [DR] are all related to  [RS] and [FS], which indicates that both Rough Sets and  Fuzzy Sets are approaches to \"data reduction\" and  \"machine learning\", which tells us that for data reduction  and machine learning researchers, \"Rough Sets\" and  \"Fuzzy Sets\" may be two possible theoretical methods for  their research."}
{"pdf_id": "0810.4668", "content": "In this paper, we provide our understanding on interpreting  knowledge from the viewpoint of granular computing and  examine different granular knowledge structures based on  various operations. Different granular knowledge structures  provide different views of the knowledge source. Each  view provides a unique understanding."}
{"pdf_id": "0810.4668", "content": "Granular knowledge structures provide understandings  of knowledge in two aspects. Firstly, through representation  of a granular knowledge structures based on concept  granules and their relations, they provide an understanding  of knowledge from the set theoretic and logic point of view.  Secondly, through visualized structures, they provide an  easily acceptable way for users to understand knowledge.  In fact, the visualized structure shows how those set  theoretic and logical representations are organized [12]."}
{"pdf_id": "0810.4668", "content": "Examples in this paper has shown some impact of  granular knowledge structures in helping users understand  the knowledge source from multiple levels and multiple  views. Considering its characteristics and expressiveness,  granular knowledge structures may have wider use in other  fields related to human and machine intelligence."}
{"pdf_id": "0810.4668", "content": "This work is supported by National Natural Science  Foundation of China research program (No. 60673015), the  Open Foundation of Beijing Municipal Key Laboratory of  Multimedia and Intelligent Software Technology. The  authors would like to thank Professor Yiyu Yao and Lina  Zhao for their constructive discussion on this paper."}
{"pdf_id": "0810.5057", "content": "Three main remarks follow the above definition: (1) the viewpoint subsets issued from V may  overlap one to another; (2) the union of the different viewpoints can be viewed as the overall  description space of the data; (3) the most suitable basis an for homogeneous management of the  viewpoints is a vectorial description space. As an example, an image can be simultaneously described  using 3 different viewpoints represented by: (1) a key-term vector; (2) color histogram vector; (3) a  feature vector.   The principle of the MultiSOM model is to be constituted by several SOM maps that have been  generated from the same data. Each map is itself issued from a specific viewpoint. The relation"}
{"pdf_id": "0810.5057", "content": "between maps is established through the use of one main communication mechanism. The inter-map  communication mechanism enables to highlight semantic relationships between different topics (i.e.  clusters) belonging to different viewpoints related to the same data. In MultiSOM, this communication  is based on the use of the data that have been projected onto each map as intermediary nodes or  activity transmitters between maps (see Figure 1)."}
{"pdf_id": "0810.5057", "content": "Target Map  The inter-map communication is established by standard Bayesian inference network propagation  algorithm which is used to compute the posterior probabilities of target map's node Tk which inherited  of the activity (evidence Q) transmitted by its associated data nodes. This computation can be carried  out efficiently because of the specific Bayesian inference network topology that can be associated to  the MultiSOM model. Hence, it is possible to compute the probability P(actm|Tk,Q) for an activity of  modality actm on a target map node Tk which is inherited from activities generated on the source map.  This computation is achieved as follows (Al Shehabi & Lamirel. 2004):"}
{"pdf_id": "0810.5057", "content": "and town code, country and town name, the Domain: code, label and related domain codes, the  Inlinks: list of incoming links with their URLs and the number of links coming from these URLs, the  Outlinks: list of outgoing links with their URLs and the number of links going to these URLs"}
{"pdf_id": "0810.5057", "content": "A map is computed for each viewpoint. In order to define the optimum size of that map, different  square maps starting from 9 nodes (3*3) to 400 nodes (20*20) are calculated using the SOM basic  clustering application \"SOM_PACK\" (SOM papers). The choice of the best map is based on an  optimisation algorithm using specific quality criteria (recall, precision and F-measure) derived both  from information retrieval and symbolic learning. This approach is more extensively described in  Lamirel et al. (2004b). Table 2 presents the final results of the whole map construction process, the  optimum number of clusters and the quality values (recall, precision and F-measure) for each"}
{"pdf_id": "0810.5057", "content": "Table 2 highlights very high quality values for Towns and Sub-domains viewpoints, and conversely,  quite low quality values for the Outlinks and Inlinks viewpoints. Hence, in the case of the Towns and  Sub-domains viewpoints, clusters are quite homogeneous and distinct one to another. This distribution  is carried out easily insofar as each website is indexed by a low number of weakly overlapping  properties. As soon as each website presents a relatively significant number of incoming and outgoing  links, overlaps are thus potentially much more significant, this implies relatively moderate quality  values for the Outlinks and Inlinks viewpoints, even after the optimisation process. These preliminary  results will be taken into account in the remaining part of our study."}
{"pdf_id": "0810.5057", "content": "For the viewpoint (1), the map clusters gather websites sharing their geographic location. For the  viewpoint (2), the map clusters gather websites sharing their overall research profile (i.e. combination  of Unesco codes). For the viewpoint (3), the map clusters gather websites sharing their Outlinks: they  are described by the targets of the links. The viewpoint (4) is the equivalent of (3) using the Inlinks:  the maps clusters are described by the targets of the links.   The easiness of interpretation of a map not only depends on the map quality (see section 4.1) but  also on complementary factors, like the granularity of description. Two typical cases of maps are  described hereafter."}
{"pdf_id": "0810.5057", "content": "In a practical way, the propagation consistency takes into account the focalization of the activity  generated by the clusters of the source map on a target map (figure 4). A strong focalization of all the  clusters of a source map on a target map will lead to a high consistency."}
{"pdf_id": "0810.5057", "content": "The MultiSOM inter-map communication mechanism can be used in an interactive mode to highlight  specific relationships between clusters of different maps. For this purpose, an activity is assigned to a  cluster, or to an information area, of a source map. Then, the mechanism of propagation of the activity"}
{"pdf_id": "0810.5057", "content": "Step 1: the propagation of the activity starting from the Munich information area and going towards  the Outlink map concentrates around an information area, which gathers 3 clusters whose profile is  dominated by the URL http://www.tu-muenchen.de/ (figure 5). The activated clusters located around  this information area have the following dominant URLs in their profile:  http://www.uni-passau.de/  http://www.informatik.uni-ulm.de/  http://www.fh-offenburg.de/  http://ls10-www.cs.uni-dortmund.de/   The above mentioned URLs correspond to main websites cited by the websites of Munich  laboratories. They thus summarize the outlinking behaviour of these latter laboratories. This led us to  conclude to a relatively local outlinking behaviour of the Munich laboratories, i.e. referecing towns  mostly located in the South of Germany (Passau, Ulm, Offenburg)."}
{"pdf_id": "0810.5407", "content": "Chapter 4 is dedicated to development of a notion of the quasi-metric spacewith Borel probability measure, or pq-space. The concept of a pq-space is a gen eralisation of a notion of an mm-space from the asymptotic geometric analysis: an mm-space is a metric space with Borel measure that provides the framework"}
{"pdf_id": "0810.5407", "content": "when I started my PhD studies and is now a Professor of Mathematics at the University of Ottawa, and Dr. Bill Jordan, Reader in Biochemistry at Victoria University of Wellington, who have supported me and guided me in all imaginable ways during the course of the study. Dr. Mike Boland from the Fonterra Research"}
{"pdf_id": "0810.5407", "content": "I have enjoyed a generous and consistent support from the Faculty of Science, the School of Mathematical and Computing Sciences and the School of Biological Sciences at the Victoria University of Wellington. Not only have they contributedsignificant funds towards my travels to conferences and to Canada to visit my su"}
{"pdf_id": "0810.5407", "content": "pervisor as well as towards a part of tuition fees, but have provided an excellent environment to work in. I would particularly like to thank Dr. Peter Donelan, who was the head of the School of Mathematical and Computing Sciences for most of the time I was doing my thesis and who signed my progress reports instead of my"}
{"pdf_id": "0810.5407", "content": "accepted me as a visitor on two occasions for four months in total. I thank my colleagues Azat Arslanov and Todd Rangiwhetu who at times shared office with me for encouraging me and proofreading some of my manuscripts. I would like to thank Professor Vitali Milman who, while being a visitor in"}
{"pdf_id": "0810.5407", "content": "Wellington, offered a lot of encouragement and some very helpful advice on how to approach mathematics. A very special thanks goes to Dr. Markus Hegland forconvincing me to learn the Python programming language and ease my program ming burden. Markus was also one of the supervisors (the other being Vladimir"}
{"pdf_id": "0810.5407", "content": "ilarity search as well as to the general theory of indexability of databases for fast similarity search. The biological applications are concentrated to investigations of short protein fragments using a novel tool, called FSIndex, which allows very fast retrieval of similarity based queries of datasets of short protein fragments."}
{"pdf_id": "0810.5407", "content": "believed that secondary, tertiary and quaternary structure are all determined by the amino acid sequence. So far, there has been no solution to the folding problem, which is to determine the conformation solely from the amino acid sequence by computational means. All presently known structures have been determined either"}
{"pdf_id": "0810.5407", "content": "motifs can but need not be associated with biological function. A structural domain is a unit of structure having a specific function which combines several mo tifs and which can fold independently. A protein sequence motif is a amino-acid pattern associated with a biological function. It may, but need not, be associated"}
{"pdf_id": "0810.5407", "content": "where one residue (amino acid in proteins) is substituted for another and indels or insertions and deletions where a residue or a sequence fragment is inserted (in one sequence) or deleted (in the other). Indels are often called gaps and alignments without gaps are called ungapped. Each of the basic transformations is assigned"}
{"pdf_id": "0810.5407", "content": "Improvements to the basic alignment model involve the use of Position SpecificScore Matrices or PSSMs, also known as profiles [78], which assign different substitution scores at different positions. PSI-BLAST [6] uses PSSMs through an it erative technique where the results of each search are used to compute a PSSM for"}
{"pdf_id": "0810.5407", "content": "have physiological activity may also be absorbed. These peptides may modulate neural, endocrine, and immune function [221, 110]. Short peptide motifs may also have a role in disease. For example, it was discovered that one of the proteins encoded by HIV-1 and Ebola viruses contains a conserved short peptide motif"}
{"pdf_id": "0810.5407", "content": "search and provided a simple model of an indexing scheme. The aim of this thesis is to extend their model so that it corresponds more closely to the existing indexingschemes for similarity search and to apply the methods from the asymptotic ge ometric analysis for performance prediction. Sharing the philosophy espoused in"}
{"pdf_id": "0810.5407", "content": "and satisfies the triangle inequality. The theory of metric spaces is very well developed and provides the foundation of many branches of mathematics such as geometry, analysis and topology as well as more applied areas. In many practical applications, it is to a great advantage if the distance function is a metric and"}
{"pdf_id": "0810.5407", "content": "metrics, the most important being the concept of duality. Every quasi-metric has its conjugate quasi-metric which is obtained by reversing the order of each pair of points before computing the distance. Existence of two quasi-metrics, the originalone and its conjugate leads to other dual structures depending on which quasi"}
{"pdf_id": "0810.5407", "content": "section, we construct examples of universal quasi-metric spaces of some classes.A universal quasi-metric space of a given class contains a copy of every quasi metric space of that class and satisfies in addition the ultrahomogeneity property. This notion is a generalisation of a well known concept of a universal metric"}
{"pdf_id": "0810.5407", "content": "[28]), especially in the form of path metric which is the metric associated to thepath quasi-metric of the above Lemma. It naturally leads to consideration of geometric properties of digraphs, as in [35]. The converse is also true: every quasimetric space can be turned into a weighted directed graph such that the quasi"}
{"pdf_id": "0810.5407", "content": "Proof. Universality follows by UQ-universality and the Lemma 2.8.5 while ultra homogeneity is a consequence of the Lemma 2.8.4. Suppose VQ and VQ 1 are twouniversal countable rational quasi-metric spaces. Take any finite rational quasi metric space F. By universality, F embeds isometrically into VQ and VQ 1 and by"}
{"pdf_id": "0810.5407", "content": "matics. The most well known tool (actually a set of tools) is NCBI BLAST (Basic Local Alignment Search Tool) [6] which, given a DNA or protein sequence ofinterest, retrieves all similar sequences from a sequence database. The similar ity measure according to which sequences are compared is based on extension of"}
{"pdf_id": "0810.5407", "content": "a similarity measure on the set of nucleotides in the case of DNA, or the set ofamino acids in the case of proteins to DNA or protein sequences, using a procedure known as alignment. Two types of (pairwise) alignments are usually distinguished: global, between whole sequences and local, between fragments of se"}
{"pdf_id": "0810.5407", "content": "of one character for another, insertions of one character into the first string anddeletions of one character from the first string. It was first mentioned in the pa per by V. Levenstein [122] and is often referred to as the Levenstein distance. In their 1976 paper [203], Waterman, Smith and Beyer introduced the most general"}
{"pdf_id": "0810.5407", "content": "of I for V are more common than substitutions of I for K. It was also argued [178] that indels are more likely to take place by segments than character-by-character and hence that indels of arbitrary segments should take weights smaller than the sum of the weights of indels of single characters comprising each segment."}
{"pdf_id": "0810.5407", "content": "transformations up to and including the previously violating transformation now fully satisfy the conditions. Depending on the particular type of violation, the number of transformations in the new edit script either decreases by one, remains the same or increases by one. The only way it can increase is by inserting an"}
{"pdf_id": "0810.5407", "content": "Computation using a dynamic programming table provides the value of distance but often, especially in biological applications, an optimal edit script (need not be unique) and the corresponding alignment need to be retrieved. This is most easily achieved (at least conceptually) by keeping one or more pointers at each"}
{"pdf_id": "0810.5407", "content": "filled: there must be at least one optimal sequence of transformations which cor responds to a sequence of transformations considered by the Needleman-Wunsch algorithm. This is not always the case in practice (see Section 3.6 below) and one then needs to assume in addition that only those transformations acting on each"}
{"pdf_id": "0810.5407", "content": "The DNA alphabet consists of only 4 letters (nucleotides) and the frequently used similarity measures on it are very simple. The common feature of all general DNA matrices used in practice is that they are symmetric and that self-similarities of all nucleotides are equal. The consequence of this fact is that the distance d resulting"}
{"pdf_id": "0810.5407", "content": "BLOSUM family of matrices was constructed by Steven and Jorja Henikoff in1992 [88] who also showed that one member of the family, the BLOSUM62 ma trix, gave the best search performance amongst all score matrices used at the time. For that reason, BLOSUM62 matrix is the default matrix used by NCBI BLAST"}
{"pdf_id": "0810.5407", "content": "multiple alignments. A multiple alignment between n sequences can be defined in the similar way as a pairwise alignment between two sequences according to the Definition 3.3.12: it is only necessary to replace the sequence of pairs with a sequence of n-tuples and to adjust the remainder of the definition accordingly. The"}
{"pdf_id": "0810.5407", "content": "cluster, it was sufficient for it to share L% identity with one member of the clus ter), resulting in a family of matrices. Thus, the matrix BLOSUM62 corresponds to L = 62 (for BLOSUMN, no clustering was performed). After clustering, the target frequencies were obtained by counting the number of each pair of amino"}
{"pdf_id": "0810.5407", "content": "acids in each column in each block having more than one cluster and normalising by the total number of pairs. The background frequencies were obtained from the amino acid composition of the clustered blocks and log-odds ratios taken. The resulting score matrices are necessarily symmetric since the pair (a, b) cannot be"}
{"pdf_id": "0810.5407", "content": "satisfied and only the triangle inequality presents problems. Where it is not sat isfied, it is either in very small number of cases or for small values of L whichcorrespond to alignments of distantly related proteins and where it is to be ex pected that a transformation from one amino acid to another can arise from more"}
{"pdf_id": "0810.5407", "content": "veloped within the framework of a metric space with measure, we will throughout this chapter state the definitions and results for the metric case first and then give the corresponding statements for the quasi-metric case. The proofs will be given only for the quasi-metric case (as they include the metric case) and where they"}
{"pdf_id": "0810.5407", "content": "We aim to explore the phenomenon of concentration of measure in high di mensional structures in the case where the underlying structure is a quasi-metric space with measure. Many results and proofs can be transferred almost verbatim from the metric case. However, we also develop new results which have no metric"}
{"pdf_id": "0810.5407", "content": "Most of the above concepts and results are generalisations of mm-space results. However, we now develop some results which are trivial in the case of mm-spaces. The main result is that, if both left and right concentration functions drop off sharply, the asymmetry at each pair of point is also very small and the quasi-metric"}
{"pdf_id": "0810.5407", "content": "mostly due to the work of Michel Talagrand [183, 184]. Many of his results are quite general, that is, not restricted to the products of metric spaces, and can beapplied directly to the quasi-metric spaces. Secondly, the space of protein frag ments, the main biological example of this thesis, can be modelled as a product"}
{"pdf_id": "0810.5407", "content": "the underlying similarity measure) and fast growing. One well known example is GenBank [15], the database of all publicly available DNA sequences (Figure 5.1). In this case, the size of queries is much smaller than database size and it is imperative to attempt to avoid scanning the whole dataset in order to retrieve a"}
{"pdf_id": "0810.5407", "content": "queries by enabling elimination of those parts of the dataset which can be certified not to contain any points of the query. There are numerous examples of indexingschemes and access methods, the best known being the B-Tree [42] from the clas sical database theory. However, in order to design new and efficient indexing"}
{"pdf_id": "0810.5407", "content": "The notion of a reduction of one workload to another, allowing creation of new access methods from the existing ones is also suggested. The final sectionsof the present chapter discuss how geometry of high dimensions (asymptotic geo metric analysis) may offer a constructive insight into the performance of indexing"}
{"pdf_id": "0810.5407", "content": "Apart from [87], this work was innuenced by the excellent reviews of sim ilarity search in metric spaces by Chavez, Navarro, Baeza-Yates and Marroquin[36] and by Hjaltason and Samet [93]. While [93] is mostly concerned with de tailed descriptions of each of the existing methods, the main focus of the [36]"}
{"pdf_id": "0810.5407", "content": "plain view, the only way they can be assembled together is by examining concrete datasets of importance and taking one step at a time. Generally, this thesis shares the philosophy espoused by Papadimitriou in [150] that theoretical developments and massive amounts of computational work must proceed in parallel. Indeed, it is"}
{"pdf_id": "0810.5407", "content": "which, while frequently mentioned as generalisations of metric workloads (e.g. in [39]), have been so far been neglected as far the practical indexing schemes are concerned. The main technical result of this Chapter, the Theorem 5.7.11 aboutthe performance of range searches, is stated and proved in terms of the quasi"}
{"pdf_id": "0810.5407", "content": "this stage to turn the domain with the set of queries into a topological space by requiring Q to satisfy the axioms of topology but there is no practical use for that. In the later sections, when we define similarity queries, the queries will become neighbourhoods of points according to some similarity measure (say a metric)"}
{"pdf_id": "0810.5407", "content": "tional requirement that the pair of identical points takes the value 0 (this is differ ent from Remark 2.1.2 where we assume in addition that a distance satisfies the triangle inequality). The justification is that most commonly used (dis)similarity measures are metrics or at least quasi-metrics and that it is almost always possible"}
{"pdf_id": "0810.5407", "content": "structure that determines the way in which a query is processed: for each query we traverse those nodes that have been selected at their parent nodes using the decision functions (Figure 5.2). Each of the bins associated with selected leaf nodes is sequentially scanned for elements of the dataset satisfying the query. The"}
{"pdf_id": "0810.5407", "content": "Clearly, for a consistent indexing scheme, any algorithm which, for any query, starting from the root, visits all branches returned by the decision functions at each node and scans all bins associated with the leaf nodes visited for the members of the query, is an access method. The Algorithm 5.2.1 provides one example."}
{"pdf_id": "0810.5407", "content": "Most existing indexing schemes for similarity search apply to metric similar ity workloads, where a dissimilarity measure on the domain is a metric and thequeries are balls of a given radius. Some indexing schemes apply only to a re stricted class of metric spaces, such as vector spaces, others apply to any metric"}
{"pdf_id": "0810.5407", "content": "space. In most cases we encounter a hierarchical tree index structure where each node is associated with a set covering a portion of the dataset and a certification function which certifies if the query ball does not intersect the covering set, in which case the node is not visited and the whole branch is pruned (Figure 5.4)."}
{"pdf_id": "0810.5407", "content": "concentrate on their overall structures in terms of the above general model and pay less attention to the details of algorithms and implementations, even though they significantly innuence the performance. For many more examples and detailed descriptions the reader is directed to the original references as well as the excellent"}
{"pdf_id": "0810.5407", "content": "fibres need to be merged), it is possible to index into W by indexing data points for each fibre using one of the existing indexing schemes for metric spaces and then collecting the results. We call this scheme a FMTree (Fibre Metric Tree). Some of our attempts to use this scheme to index into datasets of short protein"}
{"pdf_id": "0810.5407", "content": "As in the disjoint sum case, if each Wi is equipped with a consistent indexing scheme, Ii = (Ti, Bi, Fi), then a new consistent indexing scheme for W, denoted I is constructed as follows: the tree T contains all Ti's as branches beginning at the root node, while the families of bins and of decision functions for I contain"}
{"pdf_id": "0810.5407", "content": "is, that all of (T, B, F) are defined.The general goal of indexing is to produce access methods that have time com plexity sublinear in the size of the dataset. Often, the authors of indexing schemes claim to achieve O(log n) time (see for example a summary of space and time"}
{"pdf_id": "0810.5407", "content": "costs) if it is used as well as the cost of any additional data structures used. For example, some algorithms for kNN similarity search [93], which are described in more detail in the context of our indexing scheme for peptide fragments in Chapter 6, make use of priority queue for tree traversal. Under some circumstances, such"}
{"pdf_id": "0810.5407", "content": "costs are explicitly included. The timeB(Q) depends only upon the comparison distance dC (it is exactly the time to evaluate query distances to all points retrieved from the leaf nodes) while the timeF(Q) depends on the index distance dI as well as dC. The authors note that the performance does not depend directly on"}
{"pdf_id": "0810.5407", "content": "tion of the query centres. It has long been observed in the context of relational databases [37] that that it is necessary to consider non-uniform distributions of queries in order to well estimate the query performance and there is no reason to suppose that the same does not hold for similarity-based queries. However, the"}
{"pdf_id": "0810.5407", "content": "[92], function or density estimation [61], signal processing [202] and many oth ers. In all cases the procedures that perform well on two or three dimensional sets fail to do in higher dimensions. We take the paradigm of Pestov [154] thatthe curse of dimensionality is primarily a manifestation of the concentration phe"}
{"pdf_id": "0810.5407", "content": "nomenon. It allows us to use the techniques developed in Chapter 4 to provideestimates of performance of indexing schemes with as few assumptions as possi ble regarding the nature of the dataset. We first outline the previous results for the nearest neighbour queries and then proceed to our contribution for range queries"}
{"pdf_id": "0810.5407", "content": "dimension of the space. They claimed that performance of metric trees could be well approximated in terms of the distance exponent. As a part of his summer research assistantship at the Australian National University in summer 1999/2000, the thesis author performed some experiments to determine the ways of estimating"}
{"pdf_id": "0810.5407", "content": "Our definition of an indexing scheme (Definition 5.2.15) emphasises the three structures which are found in all examples known to us: the set of blocks that cover the dataset, the tree structure supporting an access method and the decisionfunctions. While this setting allows us to directly identify the factors that innu"}
{"pdf_id": "0810.5407", "content": "Consider a tree workload, WT = (T, T, Q) where T is a finite rooted directed weighted tree, such that every edge is assigned a zero weight in the direction towards the root and a positive weight in the opposite direction. The Q is the set of range similarity queries induced by the path quasi-metric (Section 2.7). There"}
{"pdf_id": "0810.5407", "content": "is an obvious access method associated with such workload: traverse the tree starting from the query point and retrieve all nodes closer than the cutoff value. Observe that any metric or quasi-metric indexing scheme where the blocks are pairwise disjoint can be represented as a projective reduction of the original"}
{"pdf_id": "0810.5407", "content": "introduced in [87]. For example, a workload would be higher in the hierarchy if itis more difficult to index and one could decide indexability of any particular work load in reference to some canonical workloads. It is clear that the trivial workload should be on the top of the hierarchy as the most difficult to index."}
{"pdf_id": "0810.5407", "content": "are known, such as in [39] where they correspond to the distance distributions. Ciaccia and Patella also emphasise that their model attests that the performance depends only on the distributions of the index and comparison distances (i.e. the certification functions) and not on the query distance. This is not contrary to our"}
{"pdf_id": "0810.5407", "content": "a structure which allows the user to specify classes of certification functions and an algorithm which fits them to a dataset and produces an indexing scheme. Theinsight gained by the approaches attempting to reduce overlap between the cover ing sets associated with the nodes of a metric tree, such as Slim-trees [189], will"}
{"pdf_id": "0810.5407", "content": "etry of high dimensions and lead to further insights on performance of indexing schemes. While we have not yet reached the stage where asymptotic geometric analysis can give accurate predictions of performance as there exists no algorithm for estimating concentration functions from a dataset, at least it leads to some"}
{"pdf_id": "0810.5407", "content": "ments is that it has been frequently pointed in the literature [32, 143, 99, 100, 103,29, 144, 70] that algorithms for indexing short fragments could be used as sub routines of BLAST-like programs for searches of full sequences. It is hoped that as a part of the future work, the experience gained from indexing short fragment"}
{"pdf_id": "0810.5407", "content": "cluding entries from most other major protein sequence databases (such as SwissProt) as well as the translated coding sequences from GenBank entries (GenPept). Where multiple identical sequences exist, they are consolidated into one entry. The nr dataset is the main dataset searched by NCBI BLAST and the latest version can be"}
{"pdf_id": "0810.5407", "content": "head is the ratio between the sizes of the metric and the quasi-metric ball con taining at least k nearest neighbours with respect to the quasi-metric. If this ratiois close to 1, the metric and the quasi-metric have similar geometry and the re placement of the quasi-metric by a metric is feasible. The average sampled ratios"}
{"pdf_id": "0810.5407", "content": "except for the nearest neighbour searches of very short fragments (length 6) and that it is indeed necessary to develop the theory and algorithms that would allow the use of the intrinsic quasi-metric. This observation was one of the principal motivations behind the development of the theory of quasi-metric trees in Chapter"}
{"pdf_id": "0810.5407", "content": "each generated point the distance to its nearest neighbour in the dataset. If an effi cient indexing scheme is available, such approach is computationally inexpensive. Figure 6.3 shows the results for SwissProt fragment datasets of lengths 6, 9 and 12 using the sample points generated according to Dirichlet mixtures (Subsection"}
{"pdf_id": "0810.5407", "content": "fragments is T1 and therefore the distance of 0 implies identical fragments) and most of the remainder are within one amino acid substitution from a dataset point (Figure 6.10 shows the full BLOSUM62 quasi-metric). In fact, the number of random points belonging to the dataset is much greater than the proportion of the"}
{"pdf_id": "0810.5407", "content": "dataset in the domain from the Figure 6.1 (about 30%), which is essentially based on the counting measure on the domain. This (not surprisingly) indicates that the measure based on Dirichlet mixtures indeed approximates the dataset better than the counting measure. The distributions for the lengths 9 and 12 indicate that a"}
{"pdf_id": "0810.5407", "content": "(in terms of points of the dataset) of a ball of given radius centred at a random point was computed and used to estimate the distance exponent. This approach is justified by the Remark A.1.6, provided the measure induced by the dataset is a good approximation to the measure used to generate the ball centres (i.e. the"}
{"pdf_id": "0810.5407", "content": "It can be seen that both distributions are skewed to the right and that the dis tribution for the length 12 is more spread out, that is, less concentrated. However,if something is to be inferred about the measure concentration and hence index ability from self-similarities, it is necessary to take into account the scale. The"}
{"pdf_id": "0810.5407", "content": "median distance to the nearest neighbour for the length 12 workload is about 23 (Figure 6.3) while it clearly cannot be greater than 10 in length 7 case (the data for length 7 is not available in the Figure 6.3 but it can be inferred from the data for lengths 6 and 9). Thus, if scaled in this way, the distribution for the length 7"}
{"pdf_id": "0810.5407", "content": "Alphanumeric [140]) is a compact representation of a trie where all nodes with one child are merged with their parent. Tries and PATRICIA trees can be easily used for string searches, that is, to find if a string p belongs to X. Such searches take O(n) time where n = |p|."}
{"pdf_id": "0810.5407", "content": "neighbours of a given point in a very efficient and straightforward manner using digital trees or even hashing. For larger lengths, the number of fragments in adataset is generally much smaller than the number of all possible fragments (Fig ure 6.1) and generation of neighbours is not feasible. If it were to be attempted,"}
{"pdf_id": "0810.5407", "content": "most of the computation would be spent generating fragments that do not exist in the dataset. Hence the idea of mapping peptide fragment datasets to smaller, densely and, as much as possible, uniformly packed spaces where the neighbours of a query point can be efficiently generated using a combinatorial algorithm."}
{"pdf_id": "0810.5407", "content": "ously used in sequence pattern matching [176]. In general, substitutions between the members of the same group are more likely to be observed in closely related proteins than substitutions between amino acids of markedly different properties. The widely used similarity score matrices such as PAM [45] or BLOSUM [88]"}
{"pdf_id": "0810.5407", "content": "The FSIndex data structure consists of three arrays: frag, bin and lcp. The array frag contains pointers to each fragment in the dataset and is sorted by bin. The array bin, of size N + 2 is indexed by the rank of each bin and contains the offset of the start of each bin in frag (the N + 1-th entry gives the total number of"}
{"pdf_id": "0810.5407", "content": "of offsets in frag is different because frag is first sorted by bin and then each bin is sorted in lexicographic order. Sorting frag within each bin and constructing and storing the lcp array is not strictly necessary and incurs a significant space and construction time penalty. The benefit is improved search performance for large"}
{"pdf_id": "0810.5407", "content": "N + n log n) on average and O(n + N + n2) in the worst case. Using radix sort [173], the average and worst case running time can both be reduced to O(n + N) with O(n) (or O(log n)) additional space overhead. Another alternative is to use"}
{"pdf_id": "0810.5407", "content": "which returns the farthest data point in the list of hits (Table 6.2 outlines the op erations on priority queue). Most of the code for range search can be reused: it is only necessary to use a different INSERTHIT function involving a priority queue (Algorithm 6.3.6) and to initialise the priority queue in the main search function"}
{"pdf_id": "0810.5407", "content": "ing schemes, datasets and similarity measures. Furthermore, most existing protein datasets are strongly non-homogeneous and the number of points scanned in orderto retrieve a range query for a fixed radius varies greatly compared to the num ber of points scanned in order to retrieve a fixed number of nearest neighbours."}
{"pdf_id": "0810.5407", "content": "queries needed to retrieve 100 nearest neighbours of testing fragments of length 9 were run using the index SPEQ09 which was performing the best for the length 9 in the previous experiment (Figure 6.13). In addition, searches were performed using the PSSMs (Section 3.7) constructed for each test fragment from the results"}
{"pdf_id": "0810.5407", "content": "periments presented in the present Chapter, using the resources from the High Performance Computing Laboratory (HPCVL), a consortium of several Canadian universities that the thesis author had the fortune to access during his visits to University of Ottawa. M-tree was not tested directly but as a part of the FMTree"}
{"pdf_id": "0810.5407", "content": "the other indexing schemes tested but it has proven itself to be very usable in practice: it does not take too much space (5 bytes per residue in the original sequence dataset plus a fixed overhead of the bin array), considerably accelerates common similarity queries and the same index can be used for multiple similarity"}
{"pdf_id": "0810.5407", "content": "ber of bins scanned on the number of actual neighbours retrieved, manifesting as straight lines on the corresponding graphs on log-log scale. For each index, the slopes of of the three graphs (i.e. running time, bins scanned and fragmentsscanned) are very close, implying that the same power law governs the depen"}
{"pdf_id": "0810.5407", "content": "6.13, 6.14 and 6.15 (Subfigure (e) in each case) show that there are two main factors innuencing the proportion of residues scanned out of the total number ofresidues in the fragments belonging to the bins needed to be scanned: the (av erage) size of bins and the number of alphabet partitions at starting positions."}
{"pdf_id": "0810.5407", "content": "would result in many bins being empty. The actual composition of the dataset is also important, as Figure 6.15 (e) attests: although same partitions are used andnr0288K is almost twice as large, SPEQ09 scans fewer characters. The possi ble reason lies in the nature of SwissProt, which, as a human curated database,"}
{"pdf_id": "0810.5407", "content": "for the growth of the number of scanned points (graphs not shown in any figure) is about 0.4, indicating that using PATRICIA-like structure improves scalability. The principal reason for sublinear growth of the number of items needed to be scanned is definitely that search radius decreases with dataset size (Figure 6.15"}
{"pdf_id": "0810.5407", "content": "at least approximately because the same fragment length was used and the size of the yeast proteome dataset used in [131] was very close to the size of SwissProt sample used in our experiment), it appears that there is no more than 10-fold improvement. While this is quite significant, the total performance appears still"}
{"pdf_id": "0810.5407", "content": "Watt and Doyle [204] recently observed that BLAST is not suitable for identi fying shorter sequences with particular constraints and proposed a pattern searchtool to find DNA or protein fragments matching exactly a given sequence or a pat tern2 I propose here an alternative technique, named PFMFind (PFM stands for"}
{"pdf_id": "0810.5407", "content": "with many examples in SwissProt and TrEMBL, thus being particularly suitablefor the PFMFind approach. Histidine kinases are a subset of the class of pro tein kinases while being very distantly related to the remainder of the class. PrPs are involved a well-publicised set of neurological diseases and have a relatively"}
{"pdf_id": "0810.5407", "content": "search to find the set of statistically significant neighbours from a protein fragment dataset with respect to a general similarity scoring matrix such as BLOSUM62.All fragments that have fewer significant neighbours than a given threshold are ex cluded from further iterations. For each fragment where the number of significant"}
{"pdf_id": "0810.5407", "content": "score matrix-based search, are significant under the model from Subsection 7.2.3 at a level usually set in bioinformatics applications of a similar kind (for example,in PSI-BLAST, the inclusion threshold E-value is 0.005) while the hits having E value up to 1.0 clearly belonged to the same protein (in a different species) as the"}
{"pdf_id": "0810.5407", "content": "pute the p-value of each score T, that is the probability that a random score X is greater than T. The number of fragments in the dataset expected by chance to be equal to or exceed T, also known as E-value, is obtained by multiplying the p-value by the size of the dataset. The relationships represented by the search"}
{"pdf_id": "0810.5407", "content": "recode3.20comp mixture as the best to be used with close homologs. After sev eral trials I set the number of hits necessary to proceed with the next iteration to 30 as a compromise between the need to have as large number of hits as possible in order to have a good profile and the average number of neighbours given the"}
{"pdf_id": "0810.5407", "content": "The full PFMFind algorithm was run for the six test sequences. Fragment lengths 8 to 15 were considered for all test proteins except PrP where only fragments of length 8 were considered because of technical limitations: too many hits were encountered and the available memory was insufficient to store all but the length"}
{"pdf_id": "0810.5407", "content": "8 results (there were usually more than 100 hits for each overlapping fragment, sometimes over 1000 hits). The hits were almost exclusively exact matches to fragments of the query sequence or other prion proteins, in the same or different species. PrP is glycine rich and contains several repeats which manifested as"}
{"pdf_id": "0810.5407", "content": "other caseins and other secreted proteins (amelogenin, having a role in biominer alisation of teeth and vitellogenin, a major yolk protein). No hits were found in the mature protein segment (mature protein is the precursor from which the signal peptide and potentially other parts have been cleaved), mainly because the initial"}
{"pdf_id": "0810.5407", "content": "computationally feasible. The aim should be to retain as many of the results while ensuring that the profile does not diverge. One of the reasons for appearance oflow-complexity fragments within the results is the relaxed significance require ments for the first few iterations but one should take care in that respect because"}
{"pdf_id": "0810.5407", "content": "The PrP searches have revealed a further weakness of the current PFMFind al gorithm and implementation. Most of the PrP hits were to the sequence itself and its very close, almost identical homologs. While the numbers of such sequences are not too large, the structure of the PrP itself, containing many aromatic-glycine"}
{"pdf_id": "0810.5407", "content": "tandem repeats was responsible for very large result sets: every PrP homolog ap peared several times (in a different region) as a hit for a single fragment. This made it impossible to proceed because the current implementation of PFMFindstores all results in main memory. The problem should be rectified by better fil"}
{"pdf_id": "0810.5407", "content": "a solution but it is necessary to use weighting that could lower the total weight instead of just redistributing it. An even better approach would be to use other information (structure, function, domains) contained in the databases as well as sequence information. However, the quality of annotations varies considerably"}
{"pdf_id": "0810.5407", "content": "For our work, as a similarity measure, we have chosen the one given by the un gapped global alignment between fragments of fixed length because we believe that gaps do not have major importance in the context of short fragments. One of the important results of the thesis is the discovery that many of the"}
{"pdf_id": "0810.5407", "content": "metrics and partial orders and are well known in topology and theoretical com puter science. The main motif that is encountered with quasi-metrics is duality: the interplay between the quasi-metric, its conjugate and their join, the associatedmetric. The novel contribution of the Chapter 2 is the construction of the uni"}
{"pdf_id": "0810.5407", "content": "classical objects of mathematics, the contribution of the Chapter 4 of this thesis and the corresponding paper in Topology Proc. [181] is only the beginning. Many non-trivial questions are opened by introducing asymmetry, that is, by replacing a metric by a quasi-metric. For example, it would be interesting to generalise"}
{"pdf_id": "0810.5407", "content": "one would want to find out if Vershik's [197] relationships between mm-spaces,measures on sets of infinite matrices and Urysohn spaces, can be extended to mq spaces. Finally, the task of constructing a universal quasi-metric space that is not bicomplete, as well as a universal quasi-metric space complete under different"}
{"pdf_id": "0810.5407", "content": "of domain structure could be of significant help in developing an indexing scheme. FSIndex has shown its usability for searches of protein fragments. Another possible application that ought to be examined is as a subroutine of a full sequence search algorithm. The experiments using the preliminary versions of PFMFind"}
{"pdf_id": "0810.5407", "content": "sion of datasets. By their definition, the distance exponent is the slope of the linearpart of the graph of the distance distribution function on the log-log scale. How ever, a more rigorous definition is necessary, because the power law is only an approximation and it is difficult to ascertain the exact bounds of the linear part."}
{"pdf_id": "0810.5407", "content": "In our experiments, the polynomial fitting approach performed better in the higher dimensions than the estimation from log-log plots. It should be noted that all the datasets tested by Traina, Traina and Faloutsos [188] had the dimension less than 7 (in some cases only estimates were available) so that the underestimation"}
{"pdf_id": "0810.5407", "content": "D. Binns, P. Bradley, P. Bork, P. Bucher, L. Cerutti, R. Copley, E. Courcelle, U. Das, R. Durbin, W. Fleischmann, J. Gough, D. Haft, N. Harte, N. Hulo, D. Kahn, A. Kanapin, M. Krestyaninova, D. Lonsdale, R. Lopez, I. Letunic,M. Madera, J. Maslen, J. McDowall, A. Mitchell, A. N. Nikolskaya, S. Or"}
{"pdf_id": "0810.5428", "content": "In Figure 2 we notice that a user browsing a Web page in the process of gathering information treats the page either as a source of information or as a source of links to other pages. It is therefore appropriate to provide users with links to two kinds of pages:"}
{"pdf_id": "0810.5428", "content": "Additionally it is our contention that as user experience with the Web improves, there will be the realization that people who create Web content and Web links have an understanding of the interrelationships between various pages. And so we suggest that a third kind of page could be useful in the information-gathering process:"}
{"pdf_id": "0810.5428", "content": "Finding witnesses. For both SeekRel and FactRel we have to find witnesses in each Nw. In Figure 5 we describe a simple algorithm that uses breadth-first search from both u and v upto d levels for some value of d to return a sorted list, Sw, of witnesses for SeekRel. Note that we do not just create a set of witnesses, but actually make an ordered list of witnesses. The significance of this will become clear shortly. In order to construct a list of witnesses for FactRel we simply reverse the direction of all the"}
{"pdf_id": "0810.5428", "content": "to a higher score for the pair. But there are cases where this score may be artificially high. Consider the network in Figure 7. E, B, C and G all witness SeekRel for H and I. But the now to B, C and G all goes through E. So these three are redundant, in the sense that the information they provide is already contained in the fact that E is a witness for H and I."}
{"pdf_id": "0810.5428", "content": "It is to prevent these redundant witnesses from artificially innating the relationship score that we reduce the capacity associated with the witness in Step 2e of the now computing algorithm of Figure 6. For SeekRel when we are done computing now to a witness we reduce its incoming capacity before moving on to the next witness in the list. For FactRel the outgoing capacity is reduced. Before we describe the algorithm formally in Figure 8 let us define some notation. For a vertex x let the set of incoming edges be I(x) and the set of outgoing edges be O(x). Let the now routed for vertex u on edge e be fu(e). The capacity of edge e is c(e)."}
{"pdf_id": "0810.5428", "content": "Essentially what reduceSeekCapacity(x) does is remove the amount of now witnessed at x. Since we take the minimum of noww(u, x) and noww(v, x) as the amount of now being witnessed, we remove this amount from the incoming capacity of x. And to ensure we do this fairly for both u and v, we penalize the incoming edges used by both the nows noww(u, x) and noww(v, x) equally by scaling down the larger now to the smaller one before subtracting it from the capacity of the incoming edge."}
{"pdf_id": "0810.5428", "content": "We took the simple subnetwork of Figure 9 and ran our scoring algorithms on it. The table of scores obtained is in Figure 10. For cleanness of presentation all hub values have been scaled by 1000. The now values have been scaled up by maxwt = 815 since we are only considering one subnetwork."}
{"pdf_id": "0810.5428", "content": "And although the node 1 shares many witnesses with 0, the now it can send is limited by its outgoing capacity (which is low because it is not a good hub) and so its SeekRel score is low, though non-zero, and 2 and 3 beat it out in scoring"}
{"pdf_id": "0810.5428", "content": "SimRank related none of the pages to either 0 or 1 whereas our SeekRel is able to detect the fact that 0 can aid in helping the user find links to pages that 2 and 3 can also lead to. Even 1 shares this property as a navigational aid with some of the other pages, a fact that comes up in our scoring."}
{"pdf_id": "0810.5428", "content": "PageSim almost misses 5's relationship to 4 and also scores 5's relationship to 6 quite low. SimRank completely misses the relationship to 4 and scores the relationship to 6 lower than the relationship to 2. On the other hand, a high FactRel score for both of these allows a user to tell that the information available at 4 and 6 are both relevant to people who are interested in 5. Since our FactRel score between 5 and 2 is relatively lower and our SurfRel score between them is high, a user can deduce the nature of the relationship between 5 and 2, a fact also detected by SimRank. We now move on to experiments on real data taken from the Web."}
{"pdf_id": "0810.5428", "content": "if we were looking at the outlinks of a page u which pointed to a core page v, we took only the links on u which were \"around\" the link to v in the sense that we took the 5 links immediately preceding the link to v on the page and the 5 links immediately following v"}
{"pdf_id": "0810.5428", "content": "We presented these 30 URLs in a random order and asked users to answer three yes/no questions: 1) Would you visit this page if you had already visited the target page? 2) Does this page provide similar information to the target page? and 3) Is this page relevant to your information-gathering task? Each such survey was given to between 5 and 8 users"}
{"pdf_id": "0810.5428", "content": "pages in the context of user intent. As part of our future research agenda we want to formulate relationships between pages that can service user intent outside the domain of information-gathering. We also want to test the applicability of our methods in social networking situations and user-generated content scenarios."}
{"pdf_id": "0810.5717", "content": "A lattice-theoretic framework is introducedthat permits the study of the conditional in dependence (CI) implication problem relative to the class of discrete probability measures.Semi-lattices are associated with CI statements and a finite, sound and complete inference system relative to semi-lattice inclu sions is presented. This system is shown to be (1) sound and complete for saturated CIstatements, (2) complete for general CI state ments, and (3) sound and complete for stable CI statements. These results yield a criterion that can be used to falsify instances of the implication problem and several heuristicsare derived that approximate this \"latticeexclusion\" criterion in polynomial time. Fi nally, we provide experimental results that relate our work to results obtained from other existing inference algorithms."}
{"pdf_id": "0810.5717", "content": "Conditional independence is an important concept inmany calculi for dealing with knowledge and uncer tainty in artificial intelligence. The notion plays afundamental role for learning and reasoning in prob abilistic systems which are successfully employed in areas such as computer vision, computational biology,and robotics. Hence, new theoretical findings and al gorithmic improvements have the potential to impact many fields of research.A central issue for reason ing about conditional independence is the probabilistic conditional independence implication problem, that is, to decide whether a CI statement is entailed by a set of other CI statements relative to the class of discrete probability measures. While it remains open whetherthis problem is decidable, it is known that there ex ists no finite, sound and complete inference system"}
{"pdf_id": "0810.5717", "content": "First, we introduce the lattice-theoretic frameworkwhich is at the core of the theory developed in this pa per. The approach we take is made possible through the association of conditional independence statementswith semi-lattices. In this section, we prove that in ference system A is sound and complete relative to specific semi-lattice inclusions. This result forms the backbone of our work on the conditional independence implication problem."}
{"pdf_id": "0810.5717", "content": "The a-satisfaction of a real-valued function for a CI statement can be characterized in terms of an equation involving its density function. This characterization is central in developing our results and is a special case of a more general result by Sayrafi and Van Gucht who used it in their study of the frequent itemset mining problem (Sayrafi and Van Gucht [7])."}
{"pdf_id": "0810.5717", "content": "In what follows, we will only refer to probability measures, keeping their probability models implicit. Definition 6.2. Let I(A, B|C) be a CI statement, andlet P be a probability measure. We say that P m satisfies I(A, B|C), and write |=m P I(A, B|C), if forevery domain vector a, b, and c of A, B, and C, re spectively, P(c)P(a, b, c) = P(a, c)P(b, c)."}
{"pdf_id": "0810.5717", "content": "Proof. The soundness follows directly from Lemma 7.1, Theorem 5.3, and Theorem 6.6. To show completeness, notice that the semi-graphoid axioms are derivable under inference system A.Furthermore, Geiger and Pearl proved that the semi graphoid axioms are complete for the probabilistic conditional independence implication problem for saturated CI statements (Geiger and Pearl [3])."}
{"pdf_id": "0810.5717", "content": "If the falsified implications were, on average, only a small fraction of all those that are falsifiable, the result would be disappointing from a practical point of view. Fortunately, we will not only be able to show that a large number of implications can be falsified bythe \"lattice-exclusion\" criterion identified in Corollary 10.1, but also that polynomial time heuristics ex ist that provide good approximations of said criterion."}
{"pdf_id": "0810.5717", "content": "The falsification algorithm and the heuristics were run on these sets with each of the remaining elementary CI statements as consequence, one at a time. Since there are 80 elementary CI statements for 5 attributes, this resulted in 77000 implication problems for sets with 3 antecedents, 76000 for sets with 4 antecedents, down to 70000 for sets with 10 antecedents."}
{"pdf_id": "0811.0123", "content": "Let us assume a world that produces a series of events. The world contains objects, some of which are alive. Living objects that are able to act on the world are called agents. Agents' actions are a subset of events. An event consists of a type indicator and references to causing object(s) and a target object(s)."}
{"pdf_id": "0811.0123", "content": "The processing loop of the agent is the following: perceive new events, determine their utilities, update object model, perform the action maximizing utility in the current situation. As a new event is perceived, the representation of the causing object is updated to include the utility of the current event. The object representation currently being retrieved and updated is defined as being the target of attention. After evaluating all new objects, the object with the highest absolute utility (of all objects in the model) is taken as a target of attention."}
{"pdf_id": "0811.0123", "content": "This change may then be perceived or not. If it is per ceived, the content of perception is the process of change. In other words, an affect is perceived when the content of the perception is a representation of the body state in transition, associated with the perception of the trigger. This is essentially the idea of Damasio [7]."}
{"pdf_id": "0811.0123", "content": "These differences are however related to triggers only. What makes an experience of fear different from an experience of e.g. hope are the perceived differences in bodily reactions associated with these emotions, i.e. a representation of bodystate associated with one emotion is different from the rep resentation of a representation of another emotion. This is essentially the 'qualia' problem, which in this context would be equal to asking why e.g. fear feels like fear, or what gives fear the quality of fearness. The solution is that the 'quality' of feeling of e.g. fear is just the specific, unique representation of the body state. There cannot be any additional aspects in the experience; what is experienced (i.e. the target of attention) is simply the representation."}
{"pdf_id": "0811.0123", "content": "action that caused a positive event to self or a liked object; events negative for disliked objects are considered positive for self. Shame is targeted towards self when a self-originated action caused a negative event. 4) Events caused by others: Gratitude is targeted towards an agent that caused a positive event towards self or someone who self depends on (i.e. likes). Correspondingly, anger is targeted towards an agent that caused a negative event."}
{"pdf_id": "0811.0123", "content": "G. Affects and time Often mood is thought of as being somehow qualitatively different from emotions. In this paper, the longer duration of mood is thought to be simply a consequence of the stability of the contents of the object model, which in turn depends on the environment. If the environment does not affect the relevant needs, the affective state does not change."}
{"pdf_id": "0811.0131", "content": "Exhaustive  experimentations also help find out the suitable values of  parameter for which the proposed algorithm works best and  from these results we try to ascertain an algebraic relationship  between the parameter set of the algorithm and feature set of  the problem environment"}
{"pdf_id": "0811.0131", "content": "1.  Initialization: 1.Any initial parameters are loaded. 2.  Edges are set with an initial pheromone value. 3. Each  ant is individually placed on a random city.  2. Main Loop:  •  Construct Solution  Each ant constructs a tour by successively applying  the probabilistic choice function:"}
{"pdf_id": "0811.0131", "content": "In this section, we obtain the closed form solution of the ant  system dynamics for determining the condition for stability of  the dynamics.  Case I: For constant deposition rule, the complete solution can  be obtained by adding CF and PI from (5) and (7) respectively  and is given by,"}
{"pdf_id": "0811.0131", "content": "The paper presents a novel approach of stability analysis as  well as a new kind of pheromone deposition rule which  outperforms the traditional approach of pheromone deposition  used so far in all variants of ant system algorithms. Our future  effort is focused in comparing the two kinds of deposition  approach with other models of ant system like Max-Min Ant  System (MMAS) and Rank-Based Ant System and estimate  the optimum parameter setting of proposed deposition  approach for these models."}
{"pdf_id": "0811.0134", "content": "Formally, a context-free grammar is a four-tuple (T,N,S,P),  where T is a set of terminal symbols, describing the allowed  words, N is a set of non-terminals describing sequences of  words and forming constructs. A unique non-terminal S is the  start symbol. P, the set of production rules, describes the"}
{"pdf_id": "0811.0134", "content": "relationship between the non-terminal and terminal symbols,  defining the syntax of the language. A series of regular  expressions can be used to describe the set of allowable words,  and acts as the basis for the description of a scanner, also  called a lexical analyzer."}
{"pdf_id": "0811.0134", "content": "As well as forming the front-end of a compiler, a parser is  also the foundation for many software engineering tools, such  as pretty-printing, automatic generation of documentation,  coding tools such as class browsers, metrication tools and  tools that check coding style. Automatic re-engineering and  maintenance tools, as well as tools to support refactoring and reverse-engineering also typically require a parser as a front end. The amenability of a language's syntax for parser  generation is crucial in the development of such tools."}
{"pdf_id": "0811.0134", "content": "This article deals with a novel parser design algorithm  based on Ant Colony Optimization (ACO) algorithm. The  paper has been structured into 6 sections. In section II, we  present a brief introduction to previous works on parsers.  Section III provides a comprehensive detail of the ACO  metaheuristic. We present our scheme in section IV. Section V  highlights the advantages of our scheme. Finally, the  conclusions are listed in section 6."}
{"pdf_id": "0811.0134", "content": "The automatic generation of parsing programs from a context free grammar is a well-established process, and various  algorithms such as LL (ANTLR and JavaCC) and LALR  (most notably yacc [3]) can be used). Application of software  metrices to the measurement of context-free grammar is  studied in [4]. The construction of a very wide-coverage  probabilistic parsing system for natural language, based on LR  parsing techniques is attempted in [5]."}
{"pdf_id": "0811.0134", "content": "In [6], a design for a reconfigurable frame parser to  translate  radio  protocol  descriptions  to  asynchronous  microprocessor cores is described. [7] presents the design and  implementation  of  a  parser/solver  for  semi-definite  programming problems (SDPs)."}
{"pdf_id": "0811.0134", "content": "The many advantages of the proposed parsing scheme point  towards the fact that this approach will be suitable for parsing  complex expressions, such as those encountered in natural language analysis applications. We use the very basic bottom up approach, so the scheme is conceptually simple. The use of  the ACO metaheuristic ensures that we can use ambiguous and  redundant grammars. In the future, we plan to use the ACO  algorithm to design more advanced parser types."}
{"pdf_id": "0811.0136", "content": "conducted by either the iteration-best ant or the best-so-far ant  and Cbs is the tour length of Tbs. Therefore, in any iteration, only the arcs belonging to the best-so-far ant or the iteration best ant receive pheromone. Now, from the pheromone update  equation of Ant System i.e. from (2), it follows,"}
{"pdf_id": "0811.0136", "content": "tour found in current iteration. Also if pdec be the probability  of choosing a particular solution component at a choice point  and an ant has to make n successive right choices to construct  the best solution, then the probability of selecting the  can be described as pbest= pdec n. In [6], it has been shown that"}
{"pdf_id": "0811.0136", "content": "where the shortest route between two given cities is to be  determined. Now, suppose we have a starting city and a  terminal city in a roadmap. Ants begin their tour at the starting  city and terminate their journey at the destination city. Ant  decides its next position at each intermediate step by a  probability  based  selection  approach.  Suppose  the"}
{"pdf_id": "0811.0136", "content": "A sufficiently complex roadmap of 250 cities is taken as the  first problem environment. Here, 20 ants are employed to  move through the graph for 100 iterations to find out the  optimal path length between the source and destination cities  as highlighted in figure 4. Parameters  over the range 0.5 to 5.0 in steps of 0.5 to find out the"}
{"pdf_id": "0811.0136", "content": "divide the simulation strategy in two levels. In the primary  level, the two competitive algorithms are run on 20 different  city distributions and the range of values of parameters of the  proposed algorithm for which it performs best and  outperforms its classical counterpart by largest extent is  estimated. In section A, we tabulate results for only 3 out of"}
{"pdf_id": "0811.0136", "content": "VII.  CONCLUSIONS AND FUTURE WORK  The stability analysis and pheromone deposition approach  presented in this paper are both entirely novel. The  exponential deposition approach outperformed the classical  one by a large margin and has lead to better solution quality  and algorithm convergence. Our next venture includes  studying the comparative behavior of the two kinds of  deposition approach in other models of extended Ant System  algorithm like the Rank-based Ant System, Ant Colony  System and Elitist Ant System."}
{"pdf_id": "0811.0136", "content": "[3] D.Merkle and M.Middendorf, \"Modeling the dynamics of ant colony  optimization algorithms,\" Evolutionary Computation, vol.10, no. 3, pp.  235-262, 2002. [4] J.L Deneubourge, S. Aron, S. Goss, and J. M Pasteels, \"The Self organizing exploratory patterns of the argentine ant,\" Journal of Insect  Behavior, vol. 3, pp. 159, 1990."}
{"pdf_id": "0811.0136", "content": "[10] T.Stiitzle and M.Dorigo, \"A short convergence proof for a class of ACO  algorithms,\"  IEEE  Transactions  on  Evolutionary  Computation,vol.6,no.4,pp.358-365,2002.  [11] W.J.Gutjahr.\"A graph-based ant system and its convergence,\" Future  Generation Computer Systems, vol. 16, no.9, pp. 873-888, 2000.  [12] W.J.Gutjahr.  \"On  the  finite-time  dynamics  of  Ant  Colony  Optimization,\" Methodology and Computing in Applied Probability,  vol. 8, no. 1, pp. 105-133, 2006.  [13] B. S. Grewal, Higher Engineering Mathematics, Khanna Publisher, New  Delhi, 1996.  [14] http://en.wikipedia.org/wiki/Dijkstra's_algorithm"}
{"pdf_id": "0811.0310", "content": "ABSTRACT The Semantic Web is becoming more and more a reality, as the required technologies have reached an appropriate level of maturity. However, at this stage, it is important to providetools facilitating the use and deployment of these technolo gies by end-users. In this paper, we describe EdHibou, anautomatically generated, ontology-based graphical user in terface that integrates in a semantic portal. The particularityof EdHibou is that it makes use of OWL reasoning capabili ties to provide intelligent features, such as decision support, upon the underlying ontology. We present an application ofEdHibou to medical decision support based on a formaliza tion of clinical guidelines in OWL and show how it can be customized thanks to an ontology of graphical components."}
{"pdf_id": "0811.0310", "content": "1. INTRODUCTION The Kasimir project is a multidisciplinary project which aims at providing oncology practitioners of the Lorraine regionof France with decision support and knowledge management tools. The Kasimir system is a clinical decision sup port system which relies on the formalization of a set of clinical guidelines issued by the regional health network. It uses decision knowledge contained in an OWL ontology to provide decision support to clinicians. In such an ontology O, a class Patient denotes the class of all patients, a class Treatment denotes the class of all treatments and a propertyrecommendation links a class of patients to a class of recom mended treatments. Then to a class P of patients is associated a treatment T by an axiom"}
{"pdf_id": "0811.0310", "content": "EdHibou implements a Model-View-Controller architecture pattern (see figure 2) and was developed using the Google Web Toolkit Java AJAX programming framework. K-OWL, the knowledge server, is a standalone component that plays the role of the model. Though it manages knowledge, and not persistent data, K-OWL has been designed in quite the same spirit as standard database management systems. It stores a set of Java models of OWL ontologies that are created with the Jena Java API coupled to the OWL DL reasoner"}
{"pdf_id": "0811.0310", "content": "5. CONCLUSION EdHibou is a programmatic framework that enables to edit an OWL instance by the means of some user-friendly forms. Itimplements an ontology-driven graphical user interface generation approach and enables to exploit the standard reasoning on the underlying ontologies to provide intelligent behavior. An application of EdHibou is presented in which it is in tegrated in a semantic portal as a user interface for a decisionsupport system in oncology. A first demo is currently avail able online at the URI http://labotalc.loria.fr/Kasimir."}
{"pdf_id": "0811.0335", "content": "Abstract. After presenting the broad context of authority sharing, we outline how introducing more natural interaction in the design of the ground operator interface of UV systems should help in allowing a single operator to manage the complexity of his/her task. Introducing newmodalities is one one of the means in the realization of our vision of next generation GOI. A more fundamental aspect resides in the interaction manager which should help balance the workload of the operator between mission and interaction, notably by applying a multi-strategy approach to generation and interpretation.We intend to apply these principles to the context of the Smaart pro totype, and in this perspective, we illustrate how to characterize the workload associated with a particular operational situation."}
{"pdf_id": "0811.0335", "content": "2. decreasing the cognitive load induced for the ground operator. OperatingUV systems is highly complex. Obviously, shifting to UV Systems with sev eral vehicles will makes mission and vehicles control more complex [6]. In addition, even though increasing vehicles' autonomy aims at decreasing the cognitive load induced by mission control for ground operators, workload mitigation may lead to even higher workload [17, 6]."}
{"pdf_id": "0811.0335", "content": "First, considering \"natural\" input device (i.e. corresponding to a control command from the ground operator to a vehicle), there is a mismatch betweenthe \"natural\" command provided by the operator and the \"operational\" com mand that a vehicle can accept. Then, the ground operator interface must be a semantic bridge, that converts the perceived message in a representation which is suitable for the addressee. That is to say that following the perception of an input on a control input device and following its interpretation, GOI also has to convert the understood control command before transmitting it to the proper vehicle(s). As shown on Fig. 3:"}
{"pdf_id": "0811.0335", "content": "Second, as soon as an interface provides semi-constrained interaction, qualita tive spatial interaction [2], natural (multi-)modality [22], then non-understandings may occur. Non-understanding is commonly set apart misunderstanding. In a misunderstanding, the addressee succeeds in communicative act's interpretation, whereas in a non-understanding he fails. But, in a misunderstanding, addressee'sinterpretation is incorrect. For example, mishearing may lead to misunderstand ing."}
{"pdf_id": "0811.0335", "content": "1. perfect understanding is not required, the level of understanding required is directed by the basic activity (i.e. the mission) and the situational context (e.g. time pressure); 2. as ground operator's cognitive load is \"divided\" between the cognitive loads induced by each activity, the interaction's complexity must vary depending on the complexity involved by the mission, as defined by Mouloua and al. [16]. For example, as time pressure rises, the cognitive load induced by the mission increases. The cognitive load required by the interaction should decrease in order to carry through the mission."}
{"pdf_id": "0811.0335", "content": "continuing his/her global supervising activity of the patrol on the whole airbase. One can detect such a workload level (Patrol with Anomaly) by the action of the operator on an UAV (Subfigure 5b). The two next workload levels are characterized by the presence of alarms. The number of alarms in recent time allows to distinguish low threat Alarm (possible false alarm, Subfigure 5c) from emergency situation (multiple alarms,coordinated Intrusion, Subfigure 5d). In this last situation, the general surveil lance of the airbase is largely jeopardized, as (1) many UAVs are used to pursue the intruders in specific regions, therefore depleting the patrolling vehicles. And, (2) the attention of the operator is largely focused on the intrusions."}
{"pdf_id": "0811.0335", "content": "Based on these criterions, the interaction manager is able to compute a dis crete mission workload level at every moment: either (1) by storing every events (operator action toward UAVs or alarms) and matching with the criterions of table 1, or (2) by updating a continuous workload level by the combination of fixed additive values associated to alarms and orders with a discount temporal factor (see Figure 6). With the latter option, the continuous level is compared to pre-defined thresholds to obtain discrete levels."}
{"pdf_id": "0811.0335", "content": "In the broad context of authority sharing, we have outlined how introducing more natural interaction in the design of the ground operator interface of UV systems should help in allowing a single operator to manage the complexity of his/her task. Introducing new modalities is one one of the means in the realization of our vision of next-generation GOI. A more fundamental aspect resides in the interaction manager which should help balance the workload of the operator between mission and interaction, notably by applying a multi-strategy approach to generation and interpretation. We intend to apply these principles to the context of the Smaart prototype, and in this perspective, we have illustrated how to characterize the workload associated with a particular operational situation."}
{"pdf_id": "0811.0340", "content": "We address here two major challenges presented by dynamic data mining: 1) the stability challenge:  we have implemented a rigorous incremental density-based clustering algorithm, independent from  any initial conditions and ordering of the data-vectors stream, 2) the cognitive challenge: we have  implemented a stringent selection process of association rules between clusters at time t-1 and time t  for directly generating the main conclusions about the dynamics of a data-stream. We illustrate these  points with an application to a two years and 2600 documents scientific information database."}
{"pdf_id": "0811.0340", "content": "Our approach insists on reproducibility and qualitative improvement, mainly for \"weak signals\"  detection and precise tracking of topical evolutions in the framework of information watch: our  GERMEN algorithm exhaustively picks up the whole set of density peaks of the data at time t, by  identifying the local perturbations induced by the current document vector, such as changing cluster  borders, or new/vanishing clusters"}
{"pdf_id": "0811.0340", "content": "However, this is only one side of the medal: on the user side of the problem, it is of the utmost  importance to provide him/her with tools for synthesizing the dynamic information in a humanly  perceptible form, so that he/she may quickly apprehend the main tendencies in the data-flow"}
{"pdf_id": "0811.0340", "content": "PASCAL is a general science bibliographic database edited by CNRS / INIST. We have extracted  2598 records in the field of geotechnics, from 2003 (1541 papers) to 2004 (1057 papers), described by  a vocabulary of 3731 keywords, once eliminated frequent generic or off-topic terms as well as rare  ones.  Our GERMEN algorithm, with parameter K=3, created 179 kernels at the step 2003, 294 at the step  2004. Papers are distributed approximately as follows: 50% in the kernels, of size ranging from 2 to 35"}
{"pdf_id": "0811.0340", "content": "The high support and MIDOVA values show the strong similarity between the two pairs. The higher  confidence in rule (1) is a sign of dissymmetry, the class A03t1526 being a bit more influenced in the  direction of a34t2564.  In the same way, other noticeable examples may be cited:"}
{"pdf_id": "0811.0340", "content": "Beyond the limits of the present options embedded in our algorithms, we have shown that the two  major challenges posed by dynamic data mining could be addressed:  - the stability challenge: we have implemented a rigorous incremental density-based clustering  algorithm, independent from any initial conditions and ordering of the data-vectors stream"}
{"pdf_id": "0811.0603", "content": "In this paper we explore its  ability in integrating the most promising aspects of the studies on query refinement: choice of meaningful text units to cluster  (domain terms), choice of tight semantic relations with which to cluster terms, structuring of terms in a network enabling  abetter perception of domain concepts"}
{"pdf_id": "0811.0603", "content": "We have experimented TermWatch's QR abilities on the 367 645 English abstracts of PASCAL 2005 2006 bibliographic database (http://www.inist.fr) and compared the structured terminological resource  automatically  build  by  TermWatch  to  the  English  segment  of  TermSciences  resource (http://termsciences.inist.fr/) containing 88 211 terms automatically structured by basic clustering and lexico semantic relations."}
{"pdf_id": "0811.0603", "content": "indexing is different from a corpus-based terminology. The difference is huge indeed !  As a consequence, such vocabulary is not adequate as such for text mining/querying. So the next question is :  how can we use TermWatch to refine queries made with the TermSciences vocabularies ?  To answer this question, we compared the two resources, considering TermWatch label components as  possible refinements of TermSciences terms : as the following table shows, 5 070 TermSciences terms have a  left right expansion (LR-exp) in TermWatch (the TS term has to appear as a substring of at least one TW  term).  Table3. Number of terms in TW and TS related by left right expansion (LR-exp)"}
{"pdf_id": "0811.0603", "content": "In Table 4, we can see that among the 5 070 TermSciences terms included in at least one TermWatch  candidate term, there are more exact matchs (80%) than one word expansions (75%). This suggests that  terms of an artificial indexing vocabulary are not adequate starting terms for trivial LR-expansions (substring  occurrence). Taking into account other types of relations (like insertions and WordNet substitutions from  table 1), TermSciences terms can be related to many more TermWatch terms. These terms are likely to be  relevant in QR perspective because, as showed in [13], they belong to clusters that are semantically  homogeneous."}
{"pdf_id": "0811.0603", "content": "Last, we observed that TermSciences uniterms seem to be much \"too generic\" to be considered as queries.  This is because TermSciences vocabulary was meant to be used in a \"post-coordinated\" manner when used  for searching. TermWatch is a useful resource here to show which combinations of uniterms really occur in  corpora. As table 6 shows, a significant number of TermWatch MWT candidate terms (ie. 19 198) include  several TermSciences uniterms and the total number of uniterms involved in TermWatch candidates by this  way is 4 668.  Table6. Number of TW terms that include several TS uniterms."}
{"pdf_id": "0811.0719", "content": "year PY, stored until t1.  3.4.2 Customer Order Factor (COF)  This is the proportion of articles of a journal ordered by Web customers in a period of time from t0 to  t1 by the total number of articles published in this journal and stored until t1."}
{"pdf_id": "0811.0719", "content": "Table 7 - Number of displayed records by users' countries  The country with the greatest number of displayed records is France with 79% of the total. Seven  other countries belonging to the European Union are represented, particularly Belgium with 115  records' visualisations corresponding to 12%. The total number of displayed journals is equal to 82  and Table 8 presents the 10 most often displayed journals as well as their WUF for the year 2002."}
{"pdf_id": "0811.0719", "content": "The algorithm we use is an adaptation of the standard bottom-up single-link clustering in accordance  with readability criteria on the size of the cluster, which is defined as the minimum and maximum  number of items belonging to the cluster, and on the maximum number of associations constructing  the cluster"}
{"pdf_id": "0811.0719", "content": "Let Cl be a cluster and mClin = the number of its internal items; lCl(i) = the number of its  internal items present in the source information unit i; sCl = the number of source information units  contributing to the cluster Cl; L(i) = the number of items present in the source information unit i"}
{"pdf_id": "0811.0719", "content": "In addition, the clusters are characterized by two structural properties respectively called density and  centrality. Cluster density DCl is defined as the mean value of the internal associations (intra-cluster).  The density is an indicator of the cohesiveness of the clusters. Cluster centrality CCl is defined as the  mean value of the external associations (inter-clusters). The centrality is an indicator of the position of  clusters in the network of inter-cluster relationships. Note that these notions of density and centrality"}
{"pdf_id": "0811.0719", "content": "Clusters and maps constitute analytical tools. A cluster is composed of items that are called internal  items. The internal item with the maximal weight value wCl(a) is automatically chosen to be the cluster  label. The clusters are also composed of associations between these items which are also called  internal associations, to distinguish them from external associations which link a cluster with other  clusters.  Figure 3: Cluster graph labelled by B-219249 ordered document  Figure 4: Cluster graph labelled by BEL-ET-1 user-customer"}
{"pdf_id": "0811.0971", "content": "characterized by several biological  traits, that own several modalities.  Our aim is to cluster the plants  according to their common traits and  modalities and to find out the  relations between traits. Galois  lattices are efficient methods for such  an aim, but apply on binary data. In  this article, we detail a few  approaches we used to transform  complex hydrobiological data into  binary data and compare the first  results obtained thanks to Galois  lattices."}
{"pdf_id": "0811.0971", "content": "indices based on the faunistic and  floristic species living in fresh water  (e.g. five indices are used in France  for qualifying running waters). These  indices are useful, but it is difficult to  compare their results from different  areas, since the kind of species living  in a river also depend on regional  characteristics. A promising approach  to avoid this drawback is to  determine functional traits, shared by  different species of different areas,  that can be used to characterize  water quality [8] or other ecosystems  [7]. Currently, these functional traits  have still to be defined for most of the  categories of aquatic living species."}
{"pdf_id": "0811.0971", "content": "First part is the current introduction,  second part introduces the data, third  part presents the methods we used to  convert the data into a suitable  format and the results we obtained  with Galois lattices. The fourth part is  a discussion on related work while  fifth part gives some conclusions and  perspectives of our work."}
{"pdf_id": "0811.0971", "content": "value between 0 and 3 to indicate the  affinity of the plants toward the  modality. 0 means there is no plant  having this modality, 1 means that a  few plants have it, 2 a bit more, and 3  many. For example, the 'potential  size' of Berula erecta (BERE) is given  by the 4-set (1, 2, 3, 0) while it is (0,  1, 2, 2) for Callitriche obtusangula  (CALO), which means, in particular,  that you will never find a berula  erecta plant greater than 1 meter and  no  callitriche obtusangula  plant"}
{"pdf_id": "0811.0971", "content": "For example, the data we deal with  represent about 50 plants, described  by 15 traits and 60 modalities. So,  tools are needed to explore these  data, and especially to cluster the  plants according to their common  traits and modalities and to find out  the relations between various traits  and modalities."}
{"pdf_id": "0811.0971", "content": "Galois connection between the sets E  and F. From this connection, we get a  set of concepts (X, Y), such that  gof(X) = X and Y = f(X), that are  organized within a lattice. Y is a set of  attributes, called intension, and X is a  set of objects, called  extension."}
{"pdf_id": "0811.0971", "content": "levels format of the dataset, we  transform it within a complete  disjunctive table (or binary table)  (Table 2). We denote the new  attributes following a 'Lxx' model.  The letter 'L' denotes a trait ('S' for  potential Size, 'R' for potential of  Regeneration...). The first 'x' is a  number which indicates a modality  and the second 'x' gives an affinity.  For example, S21 means \"few plants  (1) having a  potential size (S)"}
{"pdf_id": "0811.0971", "content": "disjunctive table is shown on Figure 1  (we show a sublattice including three  traits, potential size, perennation and  potential of regeneration). The whole  lattice contains 1401 concepts, i.e.  sets of macrophytes sharing the same  modalities of the same traits with the  same affinity. We have used the  ConExp tool (for Concept Explorer"}
{"pdf_id": "0811.0971", "content": "original data within a disjunctive  table has three main problems. First,  1401 concepts give a lattice too huge  to be readable. Second, the number  of extracted implications is high.  Third, it breaks an information which  is meaningful for hydrobiologists,  namely the distribution of the  affinities of a macrophyte among the  different modalities of a trait. We  tried another approach to overcome  this problem and present it in the  following section."}
{"pdf_id": "0811.0971", "content": "information we would like to  represent. For instance, consider the  plant BERE (Berula erecta), whose  potential size is as follows (1, 2, 3, 0)  according to the four modalities of  this trait. This pattern (1, 2, 3, 0) is  interesting for the hydrobiologists,  because it shows the continuity of the  size distribution of Berula erecta.  Actually, having two plants with  (almost) the same distribution is more  meaningful than having two plants  with the same affinity for one  modality."}
{"pdf_id": "0811.0971", "content": "conversion of the initial dataset. We  have proposed to represent the  distribution of the affinities of a plant  according to the different modalities  of a trait as a unique property, called  a pattern. This pattern is composed  as follows: first comes a letter that  refers to the trait (like 'S' for"}
{"pdf_id": "0811.0971", "content": "-manually built- is shown on Table 3  for the potential size. Looking at this  table, one can see that very few  patterns are common to more than  two individuals. The lattice built from  these data has 76 concepts spread on  6 levels (excepting top and bottom).  The lattice built for the three traits  potential size, perennation and  potential of regeneration, is shown on  Figure 2. We can see that most of the  patterns belong to only one  individual."}
{"pdf_id": "0811.0971", "content": "lattice, 219 implication sets were  extracted with a support under 5.  This means only 5 plants (for the best  result) support these implications.  This is due to the patterns which are  very precise and so few macrophytes  match each of them. To solve this  problem we can decrease the  precision of the pattern, which can be  done simply by grouping affinities.  Either we consider the presence  (affinities 1, 2 and 3 grouped  together) and the lack (the affinity 0)  of the modality, or we consider the  affinity as low (affinities 0 and 1  grouped together) or high (affinities 2  and 3 gathered together)."}
{"pdf_id": "0811.0971", "content": "until now are not very efficient  according to the hydrobiologists  requirement. The first one gives too  much, unstructured information,  while the second one gives very few  but structured information. To  explore further this second approach  we will rely on [10] which proposed  methods to deal with complex data  within the Galois lattice theory.  Actually [10] proposes to build and  compare two lattices :"}
{"pdf_id": "0811.0971", "content": "in defining a new evaluation system  of the quality of water bodies. In this  paper, the main concern with respect  to that problem is to extract  knowledge from data that do not  depend on regional characteristics.  This is an important problem in order  to be able to compare the quality of  water bodies in different regions and  to build a coherent evaluation system  over Europe. Analyzing biological  traits and determining functional  groups is a promising approach for"}
{"pdf_id": "0811.0971", "content": "analysis of biological traits of  macrophytes. In order to determine  functional groups of macrophytes, we  have proposed to use Galois lattices  and have tried to extract groups of  biological traits shared by groups of  species, and to analyze implications  between biological traits."}
{"pdf_id": "0811.0971", "content": "traits data are represented as triples  (trait, modality, affinity) which make  them too complex to directly build a  lattice from them. We have thus  proposed two conversions from those  data to binary ones: building a full  disjunctive table and using patterns  which represent the distributions of  species affinities wrt the modalities of  biological traits. None of these  approaches is really satisfactory. The  first one gives too much,"}
{"pdf_id": "0811.1319", "content": "When a user tags a resource, be it a Web page on the social bookmarking cite Delicious, a scientific paper on CiteULike, or an image on the social photosharing site Flickr, the user is free to select any keyword, or tag, from an uncontrolledpersonal vocabulary to describe the resource"}
{"pdf_id": "0811.1319", "content": "We can use tags to categorize resources, sim ilar to the way documents are categorized using their text, although the usual problems of sparseness (few unique keywords per document), synonymy (different keywords may have the same meaning), and ambiguity (same keyword has multiple meanings), will also bepresent in this domain"}
{"pdf_id": "0811.1319", "content": "In our previous work [Plangprasopchok and Lerman 2007], we proposed a probabilistic model that describes social annotation process, which was extended from probabilistic Latent Semantic Analysis (pLSA) [Hofmann 2001]. However, the model inherited some shortcomings from pLSA. First, the strategy for estimating parameters in both models — the point estimation using EM algorithm — has been criticized as being prone to local maxima [Griffiths and Steyvers 2004; Steyvers and Griffiths 2006]. In addition, there"}
{"pdf_id": "0811.1319", "content": "stable state, it only slightly nuctuates from one iteration to the next, i.e., there is no sys tematic and significant increase and decrease in likelihood. We can use this as a part of thestopping criterion. Specifically, we monitor likelihood changes over a number of consecu tive iterations. If the average of these changes is less than some threshold, the estimation process terminates. More robust approaches to determining the stable state are discussed elsewhere, e.g. [Ritter and Tanner 1992]. The formula for the likelihood is defined as follows."}
{"pdf_id": "0811.1319", "content": "Fig. 5. Performance of different models on the five data sets. X-axis represents the number of retrieved resources; y-axis represents the number of relevant resources (that have the same function as the seed). LDA(80) refers to LDA that is trained with 80 topics. ITM(80/40) refers to ITM that is trained with 80 topics and 40 interests. In wunderground case, we can only run ITM with 30 interests due to the memory limits."}
{"pdf_id": "0811.1319", "content": "Reference topic: reference, database, cheatsheet, Reference, resources, documentation, list, links, sql, lists, resource, useful, mysql —Databases interest: reference, database, documentation, sql, info, databases, faq, technical, reviews, tech, oracle, manuals —Tips & Productivity interest: reference, useful, resources,information, tips, howto, geek, guide, info, produc tivity, daily, computers —Manual & Reference interest: resource, list, guide, resources, collection, help, directory, manual, index, portal, archive, bookmark"}
{"pdf_id": "0811.1319", "content": "In Section 3, we assumed that parameters, such as, NZ and NX (number of topics andinterests respectively), were fixed and known a priori. The choice of values for these pa rameters can conceivably affect the model performance. The traditional way to determine these numbers is to learn the model several times with different values of parameters, and then select those that yield the best performance [Griffiths and Steyvers 2004]."}
{"pdf_id": "0811.1319", "content": "Modeling social annotation is an emerging new field, but it has intellectual roots in two other fields: document modeling and collaborative filtering. It is relevant to the former in that one can view a resource being annotated by users with a set of tags to be analogous to a document, which is composed of words from the document's authors. Usually, the numbers of users involved in creating a document is much less than those involved in annotating a resource. In regard to collaborative rating systems, annotations created by users in a social annotation system are analogous to object ratings in a recommendation system. However,"}
{"pdf_id": "0811.1319", "content": "ACKNOWLEDGMENTSWe would like to thank anonymous reviewers for providing useful comments and sugges tions to improve the manuscript. This material is based in part upon work supported by the National Science Foundation under Grant Numbers CMMI-0753124 and IIS-0812677. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily renect the views of the National Science Foundation."}
{"pdf_id": "0811.1618", "content": "With the objective to minimize the number of conflicts of  any two adjacent aircrafts assigned to the same gate, we build a  mathematical model with logical constraints and the binary  constraints, which can provide an efficient evaluation criterion for  the Airlines to estimate the current gate assignment"}
{"pdf_id": "0811.1618", "content": "We formulate the airport gate assignment problem as the  constraint resource assignment problem where gates serve  as the limited resources and aircrafts play the role of  resource consumers.   The operation constraints consist of two items: 1)  every aircraft must be assigned to one and only one gate.  Namely, for a given gate it can be occupied by one and only"}
{"pdf_id": "0811.1618", "content": "In fact, the airport gate assignment is a very complicated  process; while for the sake of simplifying the problem, we  mainly take into consideration of the following three  factors:  • Number of flights of arriving and departure  • Number of gates available for the coming flight  • The flight arriving and departure time based on the fight  schedule"}
{"pdf_id": "0811.1618", "content": "For example, if an airline authority wants to evaluate the  efficiency of the gate assignment of certain number of  flights (published as timetable or schedule for passengers'  reference) at certain airport, he or she can calculate the  value of the objective function in our proposed model based  on the published schedule"}
{"pdf_id": "0811.1618", "content": "assignment is not good and the authority should consider the  reassignment or modify current flight schedule. However, if  the value is quite small, such as very near to 0, it denotes  that the current gate assignment is almost the desired case in  the scenario that the number of available gate is fixed at  present."}
{"pdf_id": "0811.1618", "content": "Using the Optimization Programming Language we  encode our model into OPLscript as shown in Fig.1 and run  the program in ILOG OPL studio 3.7.1. In the OPLscript of  Figure 1, arrtm, dptm, nbFlt, and nbGate stand for arriving  time, departure time, number of Flight and number of Gate,  respectively.  We run our program on Dell server PE 1850 under the  configuration of Intel(R)Xeon(TM) CPU 3.20GHz, 3.19G  Hz, 2.00G of RAM."}
{"pdf_id": "0811.1618", "content": "In this part we will describe how we conduct all the  experiments and report relevant results. Before starting our  formal experiment we first obtain the raw data and analyze  the data especially due to the large data size. In the  following steps, we run the program and collect the testing  data. At the end of this part we refer to our future research  directions to improve the experiment."}
{"pdf_id": "0811.1618", "content": "B. Experimental Results  In experiment with small data set, the optimal solution  with objective value is 287.0787 indicating that the gate  conflicts are inevitable because of the number of available  gate is too small. When we enlarge the gate number to 6, the  gate conflict decreases dramatically and reaches the value  smaller than 3.8615, which is much better compared to 3  gates."}
{"pdf_id": "0811.1618", "content": "it is a very common phenomenon that  aircrafts always arrive late than the original schedule  because of some uncontrollable factors like the weather  condition; and to search the most robust airport gate  assignment or second most robust airport gate assignment  (considering the time expense) accurately and effectively"}
{"pdf_id": "0811.1618", "content": "During the airline daily operations, assigning the available  gates to the arriving aircrafts based on the fixed schedule is a  very important issue. In this paper, we employ the technique of  constraint programming and integrate it with linear  programming to propose a novel model. The designed  experiments demonstrate that our proposed model is of great  significance to help airline companies to estimate and even  optimize their current flight assignment. Also the experiment  illustrates our model is not only simpler, easy to modify, but  also pragmatic, feasible and sound."}
{"pdf_id": "0811.1711", "content": "Function, Multi-Layer Perception, Committees, and Bayesian Techniques), Support Vector Machines, and Adaptive Neuro Fuzzy Inference Systems. Each of theses AI methods were  investigated and simulated in Matlab, in order to ascertain the  performance of each method as well as its strengths and  weakness when applied to the stated application. The main  performance measures under consideration are the accuracy  obtained, speed of training, and the speed of execution of the  AI system on unseen data.  The paper will first give a basic foundation of the theory of  the AI methods used, and then the implementations and their  results will be presented. Finally, the key findings of the  simulations will be discussed."}
{"pdf_id": "0811.1711", "content": "Neural Networks were originally inspired by the  mechanisms used by the human brain to learn by experience  and processes information. The human brain consists of many  interconnected neurons that form an information processing  network capable of learning and adapting from experience [2,  7]."}
{"pdf_id": "0811.1711", "content": "A neural network learns by example through training  algorithms. Training results in an input/output relationship  being determined for a specific problem. Training can be  supervised or unsupervised. The neural networks discussed  will use supervised training. Supervised training involves  having a training dataset where numerous examples of inputs  and their corresponding outputs (targets) are fed to the  network. The weights and biases of the neural network are  continuously adjusted to minimise the error between the  network's outputs and the target outputs [2, 5, 7]."}
{"pdf_id": "0811.1711", "content": "Multi Layer Perception (MLP) neural networks are a  popular class of feed-forward networks (Figure 2). They were  developed from the mathematical model of the neuron (Figure  1), and consist of a network of neurons or perceptions [2]. An  MLP network consists of an input layer (source data), several"}
{"pdf_id": "0811.1711", "content": "where:  k = number of outputs  yk = the output at the kth node  j = number of hidden neurons  i = number of inputs  fA = activation function of the hidden neurons  f = activation function of the output neurons  xi = the input from the ith input node  wji = weights connecting the input with the hidden   nodes  wjk = weights connecting the hidden with the output   nodes  w0j and w0k = biases  The complexity of the model is related to the number of  hidden units, as the number of free parameters (weights and  biases) available to adjust is directly proportional to the  number of hidden units"}
{"pdf_id": "0811.1711", "content": "stages are relatively fast, therefore, an RBF trains much faster  than an equivalent MLP. The parameters of an RBF can be  determined by supervised training. However, the optimisation  process is no longer linear, resulting in the process being  computationally expensive compared to the two stage training  process.  The main difference between MLPs and RBFs are that an  MLP splits the input space into hyper-planes while an RBF  splits the input space into hyper-spheres [2]."}
{"pdf_id": "0811.1711", "content": "D. Committees  Combining the outputs of several neural networks into a  single solution to gain improved accuracy over an individual  network output is called a committee or ensemble [8]. The  simplest way of combing the outputs of different networks  together is to average the outputs obtained [3]. The averaging  ensemble can be expressed by Equation 5 [3, 8],"}
{"pdf_id": "0811.1711", "content": "where yk is the kth output, yki is the kth output of network i,  and N is the number of networks in the committee. It can be  shown that averaging the prediction of N networks reduces  the sum-of-squares error by a factor of N [3]. However, this  does not take into account that some networks in the  committee may generate better predictions than others[3]. In  this case, a weighted sum can be formulated in which certain  networks contribute more to the final output of the committee  [3]. There are several other committee methods to improve  the accuracy of the prediction obtained, such as Bagging and  Boosting."}
{"pdf_id": "0811.1711", "content": "F. Monte Carlo Methods  In the Bayesian approach to neural networks, integration  plays a significant role as calculations involve evaluating an  integral over the weight space. Monte Carlo is a method of  approximating the integral by using a sample of points from  the function of interest [3]. The integrals that need to be  evaluated are of the form [3],"}
{"pdf_id": "0811.1711", "content": "Using the above conditions, certain of the weight vector  samples will be rejected if they lead to a reduction in the  posterior distribution [3]. This procedure is repeated a  number of times until the necessary number of samples are  produced for the evaluation of the finite sum for the integral.  Due to high correlation in the posterior distribution as a result  of the each successive step being dependent on the previous, a  large number of the new weight vector states will be rejected  [3]. Therefore, a Hybrid Monte Carlo method can be used  instead.  The Hybrid Monte Carlo methods uses information about  the gradient of P(w|D) to ensure that samples through the"}
{"pdf_id": "0811.1711", "content": "where w is the position variable, p is the momentum variable,  H(w,p) is the total energy of the system, E(w) is the potential  energy, and K(p) is the kinetic energy. The positions are  analogous with the weights of a neural network, and potential  energy with the network error [10]. In this equation, the  energies of the system are defined by energy functions  representing the state of the physical system (canonical  distributions) [10]. In order to obtain the posterior  distribution of the network weights, the following distribution  is sampled ignoring the distribution of the momentum vector  [9]."}
{"pdf_id": "0811.1711", "content": "order to model complex relationships. Fuzzy systems use a  more linguistic approach rather than a mathematical  approach, where relationships are described in natural  language using linguistic variables. Fuzzy Logic can deal  with ill-defined, imprecise systems [16], and therefore are a  good tool for system modelling. This section introduces the basics of Fuzzy Logic and then explains Adaptive Neuro Fuzzy Inference Systems that are based on the foundations of  Fuzzy Logic."}
{"pdf_id": "0811.1711", "content": "For example, if a set X is  defined to represent all possible heights of people, one could  define a \"tall\" subset for any person who is above or equal to  a specific height x, and anyone below x doesn't belong to the  \"tall\" set but to a \"short\" subset"}
{"pdf_id": "0811.1711", "content": "of the area under the effected part of the output membership  function. There are other inference methods such as  averaging and sum mean square [19]. Figure 4 shows the  steps involved in creating an input-output mapping using  fuzzy logic [20].  The use of a series of fuzzy rules, and inference methods to  produce a defuzzified output constitute a Fuzzy Inference  System (FIS) [21]. The final manner in which the  aggregation process takes place and the method of  defuzzification can differ depending on the implementation of  the FIS chosen. The approach discussed above is that of the  Mamdani based FIS."}
{"pdf_id": "0811.1711", "content": "The if-then  statement of a Sugeno fuzzy system expresses the output of  each rule as a function of the input variables, and has the  form [1],  if x is A AND y is B then z = f(x,y)  (26)  If the output of each rule is a linear combination of the input  variables plus a constant, then it is known as a first-order  Segeno fuzzy model, and has the form [1]:  z = px + qy + c (27)"}
{"pdf_id": "0811.1711", "content": "Min-Max normalization to allow each variable to have equal  importance. Min-Max normalization uses the maximum and  minimum value of the variable to scale it to a range between  0 and 1, and is given by Equation 28 [22]. The outputs can be  converted back to the original scale without any loss of  accuracy."}
{"pdf_id": "0811.1711", "content": "The training dataset is used during the  supervised training process to adjust the weights and biases to  minimize the error between the network's outputs and the  target outputs as well as for the training of the SVM and  neuro-fuzzy system to adjust their corresponding parameters"}
{"pdf_id": "0811.1711", "content": "The main performance measure that was utilised to evaluate  the prediction ability of the Artificial Intelligence Methods  was the Mean Squared Error (MSE). The Mean Squared  Error is given by Equation 29. This equation allows the  contribution of each output to the total MSE to be calculated."}
{"pdf_id": "0811.1711", "content": "y = predicted value   t = desired target value  Other performance measures that were considered are: the  time taken to train the AI system, the time taken to execute  the AI system, and the complexity of the model produced by  the AI method."}
{"pdf_id": "0811.1711", "content": "comparatively small. Determining the number of training  cycle necessary for RBF was not as easy as it was for the  MLP, as the validation and training error was more \"jumpy\"  than was observed with the MLP. However, the validation  error was relatively steady after a certain point and did not  increase: 150 for 30 hidden nodes and 100 for 50 hidden  nodes."}
{"pdf_id": "0811.1711", "content": "The following performance measures were evaluated for each  of the neural networks implemented: (i) the time taken to  train the network using the training dataset, (ii) the time  taken to execute or forward-propagate through the network  for the testing dataset and (iii) the MSE accuracy obtained by  the network on the testing dataset"}
{"pdf_id": "0811.1711", "content": "E. Bayesian Techniques for Neural Networks  The architectures of the MLP and RBF used for the  Bayesian techniques were the optimum architectures (number  of hidden nodes, number of inputs and outputs, activation  functions) found using the standard approaches discussed in  the previous sections. This allows comparisons to be made  between the results obtained from both approaches.  Table 3: Showing the results for the committee networks using bagging  MLP Committee  (Bagging)  RBF Committee  (Bagging)"}
{"pdf_id": "0811.1711", "content": "The Bayesian Network utilizing Hybrid  Monte Carlo algorithm is implemented using NETLAB by  the following steps: the sampling is executed, each set of  sampled weights obtained are placed into the network in  order to make a prediction, and then the average prediction  is computed from the predicted values obtained from each set  of sampled weights [3]"}
{"pdf_id": "0811.1711", "content": "For the Hybrid Monte Carlo algorithm the following  parameters were adjusted to determine the best set of  parameters to model the dataset: the step size, the number of  steps in each Hybrid Monte Carlo trajectory, the number of  initial states that were discarded, and the number of samples  retained to form the posterior distribution"}
{"pdf_id": "0811.1711", "content": "From, the results in Tables 4 - 6, it can be seen that the  Bayesian MLP gave a better accuracy than the single MLP  implemented using standard approaches. However, it took a  substantial amount more time to train and execute compared  to the single MLP.  The Bayesian techniques using Hybrid Monte Carlo were  attempted  with  an  RBF, however, difficulties were  experienced and no definite results were obtained."}
{"pdf_id": "0811.1711", "content": "The Fuzzy Logic Toolbox has 11 different membership  functions available, of which 8 can be used with the Adaptive  Neuro-Fuzzy System: Triangular function, trapezoidal, 2  different Gaussian functions, bell function, Sigmoidal  Difference function (difference of 2 Sigmoidal functions),  Sigmoidal product function (product of 2 Sigmoidal  functions), and polynomial Pi curves"}
{"pdf_id": "0811.1711", "content": "The same procedure was followed to model the input/output  relationship for Output 2. For the ANFISs trained using the  Sigmoidal and Triangular membership functions, a slight  increase in the validation error was observed after a certain  number of training cycles. However, the validation error for  the ANFIS using the other membership functions rapidly  decreases, and then remains relatively constant. The results  for the ANFIS for Output 2 are shown in Table 9. The  Polynomial Pi Membership function produced the best results,  and didn't take too long to train. Figure 23, shows the Actual  vs. Predicted values for first 60 samples of the test dataset for  Output 2 using a Polynomial Pi membership function."}
{"pdf_id": "0811.1711", "content": "The Polynomial Pi Membership Function produced the most  accurate results for modelling Output 4. The Gaussian  membership function was not appropriate this time as the  validation error actually only increased and didn't decrease at  all. All the ANFISs trained for Output 4 produced  exceptionally accurate results, which could be seem from the  plots of the predicted vs. the actual. Table 11, shows the  performance measures for Output 4, and Figure 25 shows the  Actual vs. Predicted values for first 60 samples of the test  dataset for Output4 using a Polynomial Pi membership  function."}
{"pdf_id": "0811.1711", "content": "The Adaptive Neuro-Fuzzy Inference System was easy to  implement and the results obtained show that it can  accurately model a system as shown by Output 4. The  improvement in the accuracy for Output 4 was significant.  The simulations for the ANFIS produced better accuracy than  the SVMs and had similar training time. However, the  ANFIS executed much faster than the SVMs. Summing the  MSE of each ANFIS to produce the effective error of the 4  ANFIS working as a committee to predict the steam generator  outputs, gives an approximate MSE of 0.06858."}
{"pdf_id": "0811.1711", "content": "The optimum parameters selected probably are not the best  parameters that could be obtained if an exhaustive search was  performed. However, an exhaustive search is computationally  expensive and impractical to perform in reality. Therefore, a  more empirical approach was used to select the free  parameters for each of the AI methods implemented; making  it a difficult task to obtain the optimum combination of the  parameters which produces the best prediction performance."}
{"pdf_id": "0811.1711", "content": "Each  method  had  their  advantages  and  disadvantages in terms of the accuracy obtained, the time  required to train, the time required to execute the AI system,  the number of parameters to be tuned, and the complexity of  the model produced"}
{"pdf_id": "0811.1711", "content": "Last accessed: May 2007  [21]  Fuzzy Logic Toolbox, Matlab Help Files, MathWorks  [22]  Marwala T. Artificial Intelligence Methods.,2005  http://dept.ee.wits.ac.za/_marwala/ai.pdf  Last accessed: may 2007   [23]  Ha K, Cho S, Maclachlan D. Response Models Based on Bagging  Neural Networks, Journal of Interactive Marketing Volume 19,  Number 1, 2005, pp17-33.   [24]  Pelckmans K, Suykens JAK, Van Gestel T, De Brabanter J, Lukas  J, Hamers B, De Moor, Vandewalle J. LS-SVMlab Toolbox User's  Guide, Version 1.5, Department of Electrical Engineering,  Katholieke Universiteit Leuven, Belgium, 2003.  http://www.esat.kuleuven.ac.be/sista/lssvmlab/  Last accessed: 30 April 2007"}
{"pdf_id": "0811.1878", "content": "Statements mentioning no action at all represent laws about the underlying structure of the world, i.e., its possible states (static laws).Several logical frameworks have been proposed to formalize such state ments. Among the most prominent ones are the Situation Calculus [39, 45], the family of Action Languages [16, 30, 17], the Fluent Calculus [49, 50], and the dynamic logic-based approaches [10, 6, 57]. Here we opt to formalize action theories using a version of Propositional Dynamic Logic (PDL) [20]."}
{"pdf_id": "0811.1878", "content": "With PDL we can state laws describing the behavior of actions. One way of doing this is by stating some formulas as global axioms.3 As usually done in the RAA community, we here distinguish three types of laws. The first kind of statements are static laws, which are Boolean formulas that must hold in every possible state of the world."}
{"pdf_id": "0811.1878", "content": "Fortunately correctness of the algorithms w.r.t. our semantics can be guaranteed for those theories whose S is maximal, i.e., the set of static laws in S alone determine what worlds are authorized in the models of the theory. This is the principle of modularity [25] and we brieny review it in the next section."}
{"pdf_id": "0811.1878", "content": "Changing a modular theory should not make it nonmodular. This is not a standard postulate, but we think that as a good property modularity should be preserved across changing an action theory. If so, this means that whether a theory is modular or not can be checked once for all and one does not need to care about it during the future evolution of the action theory, i.e., when other changes will be made on it. Our operators satisfy this postulate and the proof is given in Appendix B."}
{"pdf_id": "0811.1878", "content": "So far we have analyzed the case of contraction: when evolving a theory one realizes that it is too strong and hence it has to be weakened. Let's now take a look at the other way round, i.e., the theory is too liberal and the agent discovers new laws about the world that should be added to her beliefs, which amounts to strengthening them. Suppose the action theory of our scenario example were initially stated as follows:"}
{"pdf_id": "0811.1878", "content": "Contrary to contraction, where we want the negation of some law to become satisfiable, in revision we want to make a new law valid. This means that one has to eliminate all cases satisfying its negation. This depicts the duality between revision and contraction: whereas in the latter one invalidates a formula by making its negation satisfiable, in the former one makes a formula valid by forcing its negation to be unsatisfiable prior to adding the new law to the theory."}
{"pdf_id": "0811.1878", "content": "To the best of our knowledge, the first work on updating an action domaindescription is that by Li and Pereira [33] in a narrative-based action de scription language [16]. Contrary to us, however, they mainly investigatethe problem of updating the narrative with new observed facts and (possi bly) with occurrences of actions that explain those facts. This amounts to updating a given state/configuration of the world (in our terms, what is true in a possible world) and focusing on the models of the narrative in which some actions took place (in our terms, the models of the action theory with a particular sequence of action executions). Clearly the models of the action laws remain the same."}
{"pdf_id": "0811.1878", "content": "In this work we have given a semantics for action theory change in terms of distances between models that captures the notion of minimal change. We have given algorithms to contract a formula from a theory that terminate and are correct w.r.t. the semantics (Corollary 5.1). We have shown the importance that modularity has in this result and in others."}
{"pdf_id": "0811.1878", "content": "We have also extended Varzinczak's studies [52] by defining a semantics for action theory revision based on minimal modifications of models. For the corresponding revision algorithms, the reader is referred to the work by Varzinczak [53]. One of our ongoing researches is on assessing our revision operators' behavior w.r.t. the AGM postulates for revision [1]."}
{"pdf_id": "0811.3055", "content": "Backtracking is a basic strategy to solve constraint satisfaction problems (CSPs). A satisfiable CSP instance is backtrack-free if a solution can be found without encountering any dead-end during a backtracking search, implying that the instance is easy to solve. We prove an exact phase transition of backtrack-free search in some random CSPs, namely in Model RB and in Model RD. This is the first time an exact phase transition of backtrack-free search can be identified on some random CSPs. Our technical results also have interesting implications on the power of greedy algorithms, on the width of superlinear dense random hypergraphs and on the exact satisfiability threshold of random CSPs."}
{"pdf_id": "0811.3055", "content": "A non-zero probability of backtrack-freeness on random instances for a range of parameter values was used by Smith to lower bound the satisfiability threshold [44]. Dyer, Frieze and Molloy obtained a threshold for backtrack-freeness with respect to the parameter of the domain size of binary CSPs with a linear number of constraints [13]. Here we identifyan exact threshold of backtrack-freeness with respect to the density parameter for non binary CSPs with a superlinear number of constraints. This is the first time an exact phase transition of backtrack-freeness can be identified on random CSPs. Before, the exact phase transition results of algorithmic behaviors are rare and mainly about resolution [1, 36]."}
{"pdf_id": "0811.3055", "content": "Our proofs work by first showing a phase transition result about variable-centered consis tency and then estimating the width of a random hypergraph by determining the existence of specific k-cores. As far as we know, this is the first k-core result on k-uniform hypergraphs with rn ln n hyperedges and n vertices. In our case, the width increases smoothly with the density parameter, in sharp contrast to the earlier k-core threshold results in literatures for sparse hypergraphs [4, 34, 41, 13, 8, 35, 22, 23, 27, 28, 9, 42, 31]."}
{"pdf_id": "0811.3055", "content": "Our results have implications on the power of greedy algorithms, since below the backtrack freeness threshold we can find a solution in a greedy manner for almost all instances, while above the threshold we are forced to search with backtracking for almost all instances, even for satisfiable instances. To this end, we define the width of greedy algorithms. Also, our results show that for Model RB/RD, the satisfiability threshold and some local property threshold are linked tightly, so we suggest that a similar link might exist for random 3-SAT."}
{"pdf_id": "0811.3055", "content": "In graph theory, a hypergraph consists of some nodes and some hyperedges. Each hyperedge is a subset of nodes. A hypergraph is k-uniform if every hyperedge contains exact k nodes. Every CSP has an underlying constraint (multi-)hypergraph: each variable corresponds to a node and each constraint corresponds to a hyperedge in a natural way. The constraint hypergraphs of random CSPs are random hypergraphs [29]. The constraint hypergraph of Model RB/RD, denoted by HG(n, rn ln n, k), is a random k-uniform multi-hypergraph with"}
{"pdf_id": "0811.3055", "content": "In this section we determine the width of some random hypergraphs with a superlinear number of hyperedges. We apply a probabilistic method mainly inspired by [13, 35] to detect the existence of k-cores. Denote by HG a random hypergraph from HG(n, rn ln n, k). We show that whp the width of HG, denoted as width(HG), is asymptotically equal to average degree kr ln n, due to high concentration of distribution of node degree in HG."}
{"pdf_id": "0811.3055", "content": "a local property. So our results show an evidence that for random CSPs, the exact threshold of satisfiability might has links to thresholds of some local properties, say local consistency. Based on this evidence, we propose the following two steps to attack the notorious problem of determining the satisfiability threshold for random 3-SAT."}
{"pdf_id": "0811.3137", "content": "This paper reviews the major methods and theories regarding the preservation of new media artifacts such as  videogames, and argues for the importance of collecting and coming to a better understanding of videogame  \"artifacts of creation,\" which will help build a more detailed understanding of the essential qualities of these  culturally significant artifacts. We will also review the major videogame collections in the United States, Europe  and Japan to give an idea of the current state of videogame archives, and argue for a fuller, more  comprehensive coverage of these materials in institutional repositories."}
{"pdf_id": "0811.3137", "content": "The videogame industry is at a critical moment in its history. As videogames are increasingly recognized as  important cultural artifacts, the games are becoming more and more difficult to access and play, videogame  pioneers are getting older and older, and their primary materials are being thrown away as companies go out of  business, or are deteriorating in garages and attics across the nation. The desire to preserve and protect this  material and intellectual culture is growing, as is the need to provide primary source material for the study and  advancement of the industry. As game developer Warren Spector notes,"}
{"pdf_id": "0811.3137", "content": "\"We are faced with the potential disappearance of our cultural heritage if we don't act soon and  act together to preserve digital materials... We have learned from our experience that long-term  preservation of digital content is dependent on influencing decisions of content providers from  the moment of creation.\" ~Laura Campbell, Associate Librarian for Strategic Initiatives at the  Library of Congress"}
{"pdf_id": "0811.3137", "content": "Because new media art generally, and videogames in particular, have a significant digital component, they  tend to rapidly become, at best, inaccessible; and at worst, irretrievably lost. With funding from the NEH and  IMLS, scholars in the related field of new media art have produced numerous theoretical and practical tracts  with which to work, including the development of a notation framework for media art (Rinehart, 2004); a  systematic review of emulation as a strategy for preservation of a multimedia work (Rothenberg, 2006); and the  formulation of agreed upon theories and methods for the preservation of variable media art (Depocas, Ippolito,  & Jones, 2003)."}
{"pdf_id": "0811.3137", "content": "The variable media art community, which includes the videogame industry, currently utilizes four digital  preservation strategies, all focused on the end product. The first three methods have technical origins, and are  based on general digital preservation practices. Related to \"the viewing problem,\" they are: refreshing, the  upgrade of storage mechanisms; migration, the premeditated upgrade of file formats; and emulation, which  focuses on development of Ur-operating systems able to run obsolete media. The fourth option, developed by  and for the new media art community, is re-interpretation (Depocas et al., 2003); a method intimately related to  the presentation, exhibition, and performance of an interactive variable media art object."}
{"pdf_id": "0811.3137", "content": "Whereas we can actually look at the Sistine Ceiling, created five hundred years ago, or play games,  like go invented over a thousand years ago; it is difficult if not impossible to view simple documents on 8-inch  floppy disks created in the last twenty years, even if there has been an immediate, proactive role in preserving  them"}
{"pdf_id": "0811.3137", "content": "Migration and emulation are the two primary methods in managing the problem of obsolete file formats (Waters  & Garrett, 1996). Migration focuses on the files themselves, periodically updating files in new software formats.  With migration, it quickly becomes a question of whether the conservation/preservation community is trying to  preserve access to the physical content of a work, or trying to preserve access to its deeper meaning. It  becomes a very sticky business wherein an archivist or curator has to make major artistic choices specifically  related to format."}
{"pdf_id": "0811.3137", "content": "A recent emulation of Moon Dust, one of the earliest  computer games, was shown to its original designer Jaron Lanier who contended that it was a completely  different game than the one he designed because the pacing was different, and he would not claim authorship  of this new game (Besser, 2001)"}
{"pdf_id": "0811.3137", "content": "The first problem, that of creative intent, is particularly notable, because much of the current thinking on digital  art preservation has an artist questionnaire as one of the first and central means of defense (Ippolito, 2003)  (Rinehart, 2002) (Besser, 2001). However, for the last fifty years, conservators have been debating the  appropriateness of seeking out artistic intent (Lyas, 1983; Wimsatt & Beardsley, 1948). Comprehension of  intent is a very complex process, sometimes not fully understood even by the creator himself (Sloggett, 1998);  it is often ancillary to received wisdom about the piece (Dykstra, 1996); and more often than not, conflicts with"}
{"pdf_id": "0811.3137", "content": "what a conservator is, or should be, willing to do (van de Wetering, 1989). If archivists, curators, and  conservators had a deeper understanding of the general creation behaviors and methods used by new media  artists in general, perhaps discussion of intent would become less important to the preservation framework as  a whole."}
{"pdf_id": "0811.3137", "content": "Although there are few research projects devoted to videogames, there are a number of existing archives and  private collections that focus on them. These run the gamut from physical archives of game hardware and  software, to virtual collections of videogame music, art, and manuals (Game Preservation SIG of the IGDA,  2008). Listed below are the major collections in the United States, Europe and Japan."}
{"pdf_id": "0811.3137", "content": "•  Stephen M. Cabrinety Collection at Stanford University: The Cabrinety Collection on the History of  Microcomputing contains commercially available computer hardware, software, realia and ephemera, and  printed materials documenting the emergence of the microcomputer in the late 1970s until 1995. The  collection specifically documents the emergence of computer games, with a focus on games for Atari,  Commodore, Amiga, Sega, Nintendo, and Apple systems. As such, the software collection documents the increased technical ability of computer software programmers and the growing sophistication of computer generated graphics from the early days of games like Pong to the more contemporary era of game systems  like Nintendo 64. (Stanford University Libraries & Department of Special Collections, 1997)"}
{"pdf_id": "0811.3137", "content": "•  Computer History Museum - The mission of the Computer History Museum is to preserve and present for  posterity the artifacts and stories of the information age. As such, the Museum plays a unique role in the  history of the computing revolution and its worldwide impact on the human experience. While the museum  collection focuses mainly on general hardware and software, it does include some game material.  (Computer History Museum, 2008)"}
{"pdf_id": "0811.3137", "content": "•  Digital Game Archive: The DiGA e.V. was founded to establish a one-of-a-kind digital game archive on the  Internet, which encourages the free download of commercial computer and videogames suitable for any  platform. This Berlin-based organization provides access to nearly 30,000 games. (Digital Game Archive,  2008)"}
{"pdf_id": "0811.3137", "content": "covering the art, entrepreneurs, inventions, and history of the amusement and coin-operated machine  industries.\" (International Arcade Museum, 2008) Additionally, the International Arcade Museum also  maintains the KLOV, or \"Killer List of Videogames;\" an ever growing and comprehensive list, with related  media (images and sound), of videogames."}
{"pdf_id": "0811.3137", "content": "Preservation Society (CAPS), dedicates itself to the preservation of software for the future, namely classic  games. As it is, these items are no longer available from their original suppliers, and are mainly in the  possession of an ever-diminishing community of individual collectors. (Software Preservation Society,  2006)"}
{"pdf_id": "0811.3137", "content": "•  Archive.org Classic Software Preservation: The Internet Archive founded the Classic Software Preservation  Project (CLASP) in January 2004 to help permanently archive classic, obsolete retail software from the late  1970s through the early 1990s. The Archive works to acquire copies of original consumer software of that  era, and, with the help of technical partners, make perfect digital copies of these rapidly decaying floppy"}
{"pdf_id": "0811.3137", "content": "In an attempt to address the situation in videogame collection development, the Center for American History at  the University of Texas at Austin, in collaboration with some of the leading figures in the game industry, has  announced a new archive dedicated to videogames, which will be the first in Texas, and one of the few"}
{"pdf_id": "0811.3137", "content": "institutional archives dedicated to collecting, preserving, and making accessible those materials unique to the  videogame industry. To ensure an archive of scholarly and cultural interest, the Center will gather and make  available for research materials from all sectors of the industry, including developers, publishers, and artists. In  addition to the games themselves, archival materials of interest include:"}
{"pdf_id": "0811.3137", "content": "By creating an institutional-level collection that focuses on all aspects of the game creation and production  process, the creators of the Videogame Archive at the Center for American History at the University of Texas at  Austin hope to be leaders in the field, and to attract large donations from video game pioneers and current  practitioners alike. Collecting these materials will not only provide a scholarly record of videogame history, but  will also enable the development of more relevant and realistic preservation models than exist today."}
{"pdf_id": "0811.3137", "content": "Massively multiplayer online video games are important and significant cultural artifacts. Not only are they  worthy of meticulous and robust collection, representation, and preservation; it will increasingly become more  and more important for collecting institutions to provide access to these materials. The issues involved in  preservation depend on having access to primary documents relating to all aspects of the production process.  Talking to videogame creators, developing models, and collecting primary production materials will support the  industry, as well as facilitate the acceptance of the industry as an important cultural producer."}
{"pdf_id": "0811.3137", "content": "This paper reviewed some of the major obstacles to authentic and reliable preservation of these culturally  significant new media artifacts. By reviewing the major videogame collections in the United States, Europe and  Japan the current state of videogame archives and preservation procedures was revealed. These collections,  while run by knowledgeable and eager individuals, are limited in their ephemerality and their focus on the end  product."}
{"pdf_id": "0811.4186", "content": "Abstract—In this paper, we present an approach to search result clustering, using partitioning of underlying link graph. We define the notion of \"query-induced subgraph\" and formulate the problem of search result clustering as a problem of efficient partitioning of given subgraph into topic-related clusters. Also, we propose a novel algorithm for approximative partitioning of such graph, which results in cluster quality comparable to the one obtained by deterministic algorithms, while operating in more efficient computation time, suitable for practical implementations. Finally, we present a practical clustering search engine developed as a part of this research and use it to get results about real-world performance of proposed concepts. Index Terms—Information Search and Retrieval, Graph Clustering, Randomized Algorithms, Web Measurement"}
{"pdf_id": "0811.4186", "content": "In this paper, we propose a relaxation of the problem of search result clustering from the problem of clustering the entire graph to the domain of query-induced sugraph, representing a subgraph generated by given search query and show the validity of such proposal by determining that the essential structural properties of the entire graph are still preserved in given subgraph"}
{"pdf_id": "0811.4186", "content": "We propose an algorithm for graph clustering using random walks on directed power-law graphs. The algorithm operates by performing a number of independent random walks on the link graph and attempts to exploit the specific structure of common power-law graphs in order to bound the average walk length. For each walk, we record a number of times each node was visited, and obtain partial sets, each containing the nodes visited during the walk and appropriate visit counts. Finally, we use that info in order to perform the merge stage of the algorithm, in which we use pivot nodes (nodes with maximum visit counts), in order to merge the given partial sets into a number of final sets, representing the cluster set for a given graph."}
{"pdf_id": "0811.4186", "content": "As a part of the research, and as a base for obtaining practical results, we have created a cluster ing search engine called RandomNode, accessible at http://www.randomnode.com, which performs query-timeclustering of search results by implementing the Ran dom Walk Clustering algorithm, proposed in section IV, implemented on top of the Lucene search library. Itoperates on 1.1-million node dataset, represents a sig nificant portion of .yu web, generated by performing a crawl starting at the homepage of the Belgrade University (http://www.bg.ac.yu)."}
{"pdf_id": "0811.4186", "content": "We perform analysis using randomNode engine, by performing clustering on 1000 top-scoring keywords in given dataset, varying the approximation coefficient in the (0.1, 1.0) range with 0.1 step and calculating the coverage metric. The results are shown in Figure III, with scatterplotshowing exact coverage values for each of each sample in stance and the average coverage, given by the line segment. We observe that the coverage increases logarithmically with the approximation coefficient, which indicates that the algorithm can provide acceptable approximations, even for the small values of K. Finally, we use the randomNode engine to extract a set of queries, shown in Table II, representing top-scoring clusters, both in terms of results and a cluster coverage, for a given subset of .yu Web."}
{"pdf_id": "0811.4186", "content": "politika 0.999 37473 37417 29 820 pravda 0.967 34688 33556 43 682 rubrike 0.995 33200 33053 13 817 shop 0.967 29440 28482 88 549 nekretnine 0.989 28451 28157 30 535 leasing 0.988 28185 27847 35 272 dekanat 0.947 28783 27264 63 326 banking 0.965 26840 25916 120 211 expo 0.963 26456 24629 69 273 filologija 0.976 23160 22609 39 625"}
{"pdf_id": "0811.4603", "content": "Abstract. Bibliometrics has the ambitious goal of measuring science. To this end, it exploits the way science is disseminated trough scientific publications and the resulting citation network of scientific papers. We survey the main historical contributions to the field, the most interesting bibliometric indicators, and the most popular bibliometric data sources. Moreover, we discuss distributions commonly used to model bibliometric phenomena and give an overview of methods to build bibliometric maps of science."}
{"pdf_id": "0811.4603", "content": "Academic institutions increasingly rely on biblio metric analysis for making decisions regarding hiring, promotion, tenure, andfunding of scholars; authors, librarians, and publishers may use citation indica tors to evaluate journals and to select those of high impact; editors may choosereviewers on the basis of their bibliometric scores on a particular subject of in terest; worldwide college and university rankings, e"}
{"pdf_id": "0811.4603", "content": "processes. Nowadays, the borderlines between the two specialities almost van ished and both terms are used almost as synonyms. The statistical analysis of scientific literature began years before the term bibliometrics was coined. The main contributions are: Lotka's Law of scientific productivity, Bradford's Law of scatter, and Zipf's Law of word occurrence. In 1926, Alfred J. Lotka published a study on the frequency distribution of scientific productivity determined from a decennial index of Chemical Abstracts [4] (see Table 1). Lotka concluded that:"}
{"pdf_id": "0811.4603", "content": "Lotka's Law means that few authors contribute most of the papers and many or most of them contribute few publications. For instance, in the original data of Lotka's study illustrated in Table 1, the most prolific 1350 authors (21% of the total) wrote more than half of the papers (6429 papers, 51% of the total)."}
{"pdf_id": "0811.4603", "content": "A central question is: why bibliometric analysis of research performance? Peer review, that is, the evaluation made by expert peers, undoubtedly is an important procedure of quality judgment. In particular, the results of peer review judgment and those of bibliometric assessment are not completely independent variables. Indeed, peers take some bibliometric aspects into account in their judgment, for instance number of publications in the better journals.But peer review and related expert-based judgments may have serious shortcomings. Subjectivity, i.e., dependence of the outcomes on the choice of individ ual committee members, is one of the major problems. Moreover, peer review is"}
{"pdf_id": "0811.4603", "content": "slow and expensive (at least in terms of hours of volunteer work devoted to ref ereeing). In particular, peer review methodology is practically unfeasible when the number of units to evaluate is consistent, e.g., all papers published by all members of a large department. Bibliometric assessment of research performance is based on the following central assumptions [7]:"}
{"pdf_id": "0811.4603", "content": "Further more, the robustness of citations as a method to evaluate impact is particularlywitnessed by the adoption of a similar approach in several other fields far dif ferent from bibliometrics, including web pages connected by hyperlinks [13,14], patents and corresponding citations [15], published opinions of judges and their citations within and across opinion circuits [16], and even sections of the Bible and the biblical citations they receive in religious texts [17]"}
{"pdf_id": "0811.4603", "content": "Assuming the central bibliometric assumptions mentioned in Section 3, we may design quantitative indicators to assess research quality of an actor. But, what aspects characterize quality of research? Moreover, what are the actors under evaluation? There is a general agreement that research quality is not characterized by a single element of performance. Van Raan [18] claims:"}
{"pdf_id": "0811.4603", "content": "1. it puts newcomers at a disadvantage since both publication output and ci tation rates will be relatively low; 2. it does not account for the number of authors in a paper; 3. it is discipline dependent; 4. it disadvantages small but highly-cited paper sets too strongly; 5. it allows scientists to rest on their laurels (\"your papers do the job for you\") since the index never decreases and it might increase even if no new papers are published."}
{"pdf_id": "0811.4603", "content": "a specific census year is the mean number of citations that occurred in the census year to the articles published in the journal during a target window consisting of the two previous years. Such a measure was devised by Garfield, the founder of the Institute for Scientific Information (ISI). Today, Thomson-Reuters, that acquired the ISI in 1992, computes the the impact factor for journals it tracks and publishes it annually in the Journal Citation Reports (JCR) in separate editions for the sciences and the social sciences. The impact factor has become a standard to evaluate the impact of journals. Nevertheless, the impact factor has many faults [31,20,32]; the most commonly mentioned are:"}
{"pdf_id": "0811.4603", "content": "Moreover, due to the skewness of citation distributions and the fact that the impact factor is essentially a mean value, it is a (common) misuse of the impact factor to predict the importance of an individual publication, and hence of an individual researcher, based on the impact factor of the publication's journal"}
{"pdf_id": "0811.4603", "content": "They show that there exists a steady state period of time specific to each journal such that the number of citations to paper published in the journal in that period will not significantly change in the future: poorly cited papers have stopped accruing citations, while the trickle of citations to highly cited ones issmall when compared to the already accrued citations"}
{"pdf_id": "0811.4603", "content": "Notably, Brin and Page use a similar intuition to design the popular PageRank algorithm that is part of their Google search engine: the importance of a web page is determined by the number of hyperlinks it receives from other pages as well as by the importance of the linking pages [43,14]"}
{"pdf_id": "0811.4603", "content": "Let us fix a census year and let C = (ci,j) be a journal journal citation matrix such that ci,j is the number of citations from articlespublished in journal i in the census year to articles published in journal j dur ing the target window consisting of the five previous years"}
{"pdf_id": "0811.4603", "content": "A dangling node is a journal i that does not cite any other journals; hence, if i is dangling, the ith row of the citation matrix has all 0 entries. The citation matrix C is transformed into a normalized matrix H = (hi,j) such that all rows that are not dangling nodes are normalized by the row sum, that is,"}
{"pdf_id": "0811.4603", "content": "and selects a random journal in proportion to the number of article published by each journal. With this model of research, by virtue of the Ergodic theorem for Markov chains, the innuence weight of a journal corresponds to the relative frequency with which the random researcher visits the journal. The Eigenfactor score is a size-dependent measure of the total innuence of a journal, rather than a measure of innuence per article, like the impact factor. To make the Eigenfactor scores size-independent and comparable to impact factors, we need to divide the journal innuence by the number of articles published in the journal. In fact, this measure, called Article InnuenceTM, is available both at the Eigenfactor web site and at Thomson-Reuters's JCR."}
{"pdf_id": "0811.4603", "content": "The bibliometric databases of the Institute for Scientific Information (ISI) have been the most generally accepted data sources for bibliometric analysis. The ISI was founded by Eugene Garfield in 1960. The ISI was acquired by Thomson in 1992, one of the world's largest information companies. In 2007, the Thomson Corporation reached an agreement with Reuters to combine the two companies under the name Thomson-Reuters (TR).TR maintains Web of Knowledge, an online academic database which pro vides access to many resources, in particular:"}
{"pdf_id": "0811.4603", "content": "The authors studied the distribution of citations by language and found that Google Scholar provides better coverage of non-English language materials (6.9%) with respect to both Web of Science (1.1%) and Scopus (0.7%). Meho and Yang concluded that Web of Science, Scopus, and Google Scholar complement rather than replace each other, so they should be used togetherrather than separately in citation analysis. In particular, although Web of Sci ence remains an indispensable citation database, it should not be used alone for"}
{"pdf_id": "0811.4603", "content": "locating citations, because both Scopus and Google Scholar identify a consider able number of citations not found in Web of Science. Although Google Scholar unique citations are not of the same quality of those found in the two proprietarydatabases, they could be useful in showing evidence of broader international im pact. The authors also concluded that there is an important impact advantage in favor of the articles, and the corresponding journals, that their authors make available online (on personal web pages or on electronic preprints archives like arXiv) since they are more likely discovered by human and automatic agents (like crawlers of Google Scholar), possibly increasing the citation impact."}
{"pdf_id": "0811.4603", "content": "Success seems to breed success. A paper which has been cited many times is more likely to be cited again than one which has been little cited. An author of many papers is more likely to publish again than one who has been less prolific. A journal which has been frequently consulted for somepurpose is more likely to be turned to again than one of previously infre quent use."}
{"pdf_id": "0811.4603", "content": "Once the similarity strength between bibliometric units has been established, bibliometric units are typically represented as graph nodes and the similarity relationship between two units is represented as a weighted edge connecting the units, where weights stand for the similarity intensity. Such visualizations are called bibliometric maps. Such maps are powerful but they are often highly complex. It therefore is helpful to abstract the network into inter-connected modules of nodes. Good abstractions both simplify and highlight the underlying structure and the relationships that they depict. When the units are publications or concepts, the identified modules represent in most cases recognizable research fields. In the rest of this section, we describe three methods for creating these abstractions: clustering, principal component analysis, and information-theoretic abstractions."}
{"pdf_id": "0811.4603", "content": "Informally, clustering is the process of organizing objects into groups whose members are similar in some way [75,76]. A cluster is a collection of objects which are similar between them and are dissimilar to objects belonging to otherclusters. Clustering can be formalized as follows. We are given a weighted undi rected graph G, where the weight function assigns a dissimilarity value to pair of nodes, and an objective function f that assigns a value of merit to any partition of the set of nodes of G. Clustering problems are optimization problems that usually have one of the following forms [77]:"}
{"pdf_id": "0811.4603", "content": "structure can be used to choose the smallest partition among the generated ones (a small subset of all partitions) with objective function value less than or equal to the given threshold. The computational complexity of clustering problems mainly depends on the properties of the weight function that measures the distance between two objects and on the objective function that evaluates the goodness of a given partition of the space. Many exact and approximated clustering problems are known to be hard to solve, in particular NP-hard [77,80]. Hence a polynomial strategy cannot guarantee to find the optimum solution."}
{"pdf_id": "0811.4699", "content": "Abstract: Statistical pattern recognition methods based on the Coherence Length Diagram  (CLD) have been proposed for medical image analyses, such as quantitative characterization  of human skin textures, and for polarized light microscopy of liquid crystal textures. Further  investigations are here made on image maps originated from such diagram and some  examples related to irregularity and anisotropy of microstructures shown. The possibility of  generating a defect map of the image is also proposed."}
{"pdf_id": "0811.4699", "content": "Here we propose a discussion and several examples:  the goal is to explain the nature and some properties of CLD and of four fundamental maps,  which can be generated from it: the Support Map (SMap), the Defect Map (DMap), and the  Directional Defect Map (DDMap) and the Mixed Map (MMap)"}
{"pdf_id": "0811.4699", "content": "The last expression, called the image Support Map (SMap), is less detailed yet better  understandable than the set of single direction support maps. Fig.3 shows a sample of such  map, obtained by laying on the given grayscale image a layer, in which the value of the  average function is represented by the brightness of the added blue component."}
{"pdf_id": "0811.4699", "content": "3. The detection of defects by means of a Defect Map (DMap) As stated in previous sections, both overall and local coherence diagrams are computed when  describing an image. If a comparison between each point's diagram and the CLD is made,  possible out-of-average behaviors can be detected for some points. The technique which can  be used is quite similar to regular gray level methods [10], but applied to the couple"}
{"pdf_id": "0811.4699", "content": "4. The Directional Defect Map (DDMap). The Defect Map described in previous section discriminates between points behaving \"almost  like\" and \"definitely unlike\" the average CLD, but it is not focused on shape differences. A  shape comparison can be made by using a square difference analysis involving the local and  the average coherence length diagram. The sum of square differences"}
{"pdf_id": "0811.4699", "content": "6. Conclusions The paper describes discrete algorithms based on the Coherence Length Diagrams. With these  diagrams it is possible to introduce a defect map (Dmap) which is able to outline defective  areas. Another map, the directional defect map (DDMap) stresses the boundaries of both  sharply and smoothly defined image parts. This different behavior arises from the fact that the  DDMap is sensing the orientation of local CLDs, which shows sudden changes as well as  defined directions at boundaries of shapes. In fact, the DDMap is an improvement with  respect to algorithms for the simple edge detection."}
{"pdf_id": "0811.4717", "content": "In the medical field, digital images are produced in huge quantities and used for direct diagnosis and therapy. Even though the introduction of DICOM*5 medical image format standardization and PACS*6 medical information storage and management systems represent important milestones in the medical field, much effort is needed to use these standards efficiently and effectively for diagnosis assistance, teaching and research.In the same way that PACS expands on the possibilities of a conventional hard-copy medical image storage sys tem by providing capabilities of off-site viewing and reporting (distant education, telediagnosis) and by enablingpractitioners at various physical locations to access the same information simultaneously (teleradiology), Content Based Medical Image Retrieval (CBMIR) opens the gate to the next generation of medical procedures. For"}
{"pdf_id": "0811.4717", "content": "Content-based image retrieval (CBIR) is the application of computer vision to the image retrieval problem, i.e., the problem of searching for digital images in large databases. \"Content-based\" means that the search makes use of the contents of the images themselves, rather than relying on textual annotation or human-input metadata."}
{"pdf_id": "0811.4717", "content": "1)Preprocessing In the clinical practice, a medical case constitutes one or more medical reports and one or more associated medical images. In our approach, we consider decomposition into elementary medical cases c formed by one medical reportand one associated medical image. The combination of the elementary cases can give a reconstruction of the origi nal medical case. The elementary medical case c thus includes indexing of the associated image and medical report:"}
{"pdf_id": "0811.4717", "content": "the MIR*9 database, the smallest, we had 56,000 CUIs). Each medical report (from the 50,000 cases of the CLEF Database) generates an average of about 50 UMLS CUIs. For each medical report, there can be one or more image(s) attached; a medical report along with its attached image(s) is called a \"case\". The medical cases from our database look like the example in Fig. 4. In this case, for the XML file the four images correspond to it. The Figure represents the indexed images and medical reports in the way they are used as input into our system."}
{"pdf_id": "0811.4717", "content": "The alignment method based on the partial media retrieval feedback aims at balancing the two datasets depending on their individual retrieval (recall and precision) performances. As far as we know, this idea is a new and a generic method that can considerably increase the quality of the retrieval. In section 6, we will introduce our results and conclusion about this important topic (see Fig. 6)."}
{"pdf_id": "0811.4717", "content": "3)Fusion Approach There are several fusion methods in literature, depending on the data that is provided and on the final purpose of the fusion. Different classification criteria have been proposed, from the point of view of the nature of the data and respectively from the data quality. Low, Intermediate and High Level"}
{"pdf_id": "0811.4717", "content": "where A is the similarity matrix of feature vectors, being the result of the   operator on the CUIs extracted from the text and the image files (for the common CUIs from the query and the medical case the value in the matrix is 1, for the rest is 0)"}
{"pdf_id": "0811.4717", "content": "In the pre-processing phase we conducted a comparative study on the   (spatial localization fuzzy weight) and (data test feedback or relevance feedback) parameters, using small variations around a theoretically suitable struc ture, composed by the sum fusion operator (simple, commutative, associative and balanced technique) and the Fuzzy Similarity Function (FSF) for the similarity"}
{"pdf_id": "0811.4717", "content": "Considering that the tests on the automatic text retrieval are around 22,55% in MAP and the automatic image retrieval around 6,41% in MAP for the same indexes, the fusion applied here is effective since it gives a result greater than the sum of image and text partial retrieval results"}
{"pdf_id": "0811.4717", "content": "Acknowledgment This work has been done with the support of ONCO-MEDIA*12 ICT Asia project. We would like also to thank our colleagues from IPAL - Caroline Lacoste, Nicolas Vuillemenot, Le Thi Hoang Diem and Jean-Pierre Chevallet - for providing us the text and images separate indexes used for the experimental part. For the financial support for the publication we would like to thank the Kayamori Foundation of Informational Science Advancement."}
{"pdf_id": "0812.0262", "content": "In 2004, the German Federal Ministry for Education and Research funded a major termi nology mapping initiative at the GESIS Social  Science Information Centre in Bonn (GESIS-IZ)  \"Competence Center Modeling and Treatment  of Semantic Heterogeneity\" (KoMoHe), which  concluded in 2007 (see Mayr and Petras, 2008). The task of the KoMoHe project was to organ ise, create and manage \"cross-concordances\"  between major controlled vocabularies and to  evaluate DL models."}
{"pdf_id": "0812.0262", "content": "In the next chapters we try to answer the follow ing research questions:  1) Is a re-ranking of documents according to the  Bradford law (journal productivity) an added  value for users? The re-ranking of content to the  most frequent sources (extracting the nucleus)  can for example be a helpful access mechanism  for browsing (Bates, 2002) and initial search  stages"}
{"pdf_id": "0812.0262", "content": "2) Are the documents in the nucleus of a bradfordized list (core journals show a high produc tivity for a topic) more relevant for a topic than items in succeeding zones with a lower produc tivity? A study by Pontigo and Lancaster (1986)  concluded that less productive journals are not  necessarily of lower quality but mostly less  cited"}
{"pdf_id": "0812.0262", "content": "experts, novice searchers, information scien tists).  3) Can Bradfordizing be applied to document  sources other than journal articles? A paper by  Worthen (1975) and our own analyses show that monograph literature can be successfully brad fordized. But is this a utility? Other document  types (proceedings, grey literature etc.) have to  be equally proven."}
{"pdf_id": "0812.0262", "content": "4) Can Bradfordizing be used to create an al ternative view on search results? Compared to  traditional text-oriented ranking mechanisms, our informetric re-ranking method offers a com pletely new view on results sets (see e.g. Table  1), which have not been implemented and tested in heterogeneous database scenarios with multi ple collections to date."}
{"pdf_id": "0812.0262", "content": "2. Intellectual assessments of document rele vance have been performed following the  classical IR evaluation experiments at  TREC (Harman and Voorhees, 2006) and  Cross-Language  Evaluation  Forum  (CLEF2) (Petras et al., 2007). That followed an empirical analysis of the results for subject-specific topics and questions. We re trieved, analyzed and assessed 164 different  standardized topics which result in more than 96,000 documents from all above do mains (see Table 2 and appendix with a  typical topic and a document, listing 1, 2).  More then 51,000 assessed documents  could be bradfordized."}
{"pdf_id": "0812.0262", "content": "The preliminary results present parts of the re sults. In the following (result 1, 3 and 4) we will  concentrate on one sample (25 topics) from the  domain-specific track at CLEF 2005. The other samples in CLEF and KoMoHe show very simi lar results.  Result 1: Bradford distributions appear in all  subject domains and also for results of scientific literature databases. It follows that Bradfordiz"}
{"pdf_id": "0812.0262", "content": "In Figure 2 each zone (core, zone 2 = z2 and zone 3 = z3) consists of approximately 47 articles. The documents are scattered over 61 jour nals: the highest concentration is in the core  with ~5 journals, z2 consists of ~17 journals and  the 47 articles in z3 are scattered across ~40  journals). In Figure 3 each zone (core, z2 and  z3) consists of approximately 70 monographs.  The documents are scattered over 90 publishers:  the highest concentration is in the core with ~9  publishers, z2 consists of ~30 publishers and the  70 monographs in z3 are scattered across ~52  publishers)."}
{"pdf_id": "0812.0262", "content": "Result 2: The application of informetric meth ods for re-ranking of documents can produce an alternative view of a result set. Intuitively nonexpert users rated this view/re-ordering as positive (compare White, 1981). Positive is gener ally the novelty and insight which comes up  when presenting highly cited papers, papers of  central authors (Mutschke, 2003), articles from  core journals (see Table 1) and the relevance  distribution of the newly organized result set.  Our interviews with experts and non-experts (12  persons) in 24 social sciences topics show  clearly that the presentation of core journals  after Bradfordizing is a value-added for both  types of users.  Result 3: The application of Bradfordizing or  the core journal re-ranking for subject-specific"}
{"pdf_id": "0812.0262", "content": "document sets leads to significant improvements  of the precision between the three Bradford  zones. The core journals cover significantly  more relevant documents than journals in zone 2  or zone 3. The largest increase in precision can  typically be observed between core and zone 3  (see Figure 4)."}
{"pdf_id": "0812.0262", "content": "Result 6: The results show that the journals in  the core appear approximately monthly while journals in the succeeding zones appear bi monthly.  Table 3: Baseline, z3 and improved precision  for articles and monographs in the core. Mean values for 25 topics from the CLEF 2005 data set. The improvements between the zones core and z3 (articles) and core and baseline are statis tically significant (*) based on the Wilcoxon signed-rank test and the paired T-Test. Im provements between core and z3 and core and baseline monographs are positive but not statis tical significant.  Precision Improvement"}
{"pdf_id": "0812.0262", "content": "Table 3 shows precision improvements  (mean values for 25 topics) between different document clusters (baseline and core and additionally z3 and core). Baseline means all docu ments in the sample. The mean precision of all  articles (baseline) is 0.239 whereas precision in  the core is 0.310 and z3 is 0.174. According to  this the core is improving baseline (29.52%) and"}
{"pdf_id": "0812.0262", "content": "The project \"Competence Center Modeling and  Treatment of Semantic Heterogeneity\" at  GESIS-IZ was funded by BMBF, grant no. 01C5953. See project website for more informa tion.  http://www.gesis.org/en/research/information_te chnology/komohe.htm I would like to thank my colleague Vivien Pet ras who pointed me at the assessed topics from  CLEF evaluation 2003-2007 and our assisting  student Dirk Hohmeister who helped with the  assessments and analysis."}
{"pdf_id": "0812.0262", "content": "Mutschke, Peter (2003): Mining Networks and  Central Entities in Digital Libraries: a Graph Theoretic Approach Applied to CoAuthor Networks. pp. 155-166. In: Ber thold, Michael R.; Lenz, Hans-Joachim; Bradley, Elizabeth; Kruse, Rudolf; Borgelt, Christian (eds.): Advances in Intelli gent Data Analysis 5. Proceedings of the  5th International Symposium on Intelligent  Data  Analysis  (IDA  2003).  Berlin:  Springer."}
{"pdf_id": "0812.0340", "content": "Our goal is to find a score to match two polygons P1 and P2 embedded in a rectangle R of the plane, of  height I and width J. Using a pixel based representation of the polygon we find pixel based  representations of the boundary of each polygon, with four matrices representing top, bottom left and  right edges separately. We smooth with a Gaussian kernel, enabling matching of coincident edges and  nearby edges. We match top edges to top edges, left edges to left edges and so on. Not allowing  cancellation between left and right edges, or between top and bottom edges, gives more sensitivity."}
{"pdf_id": "0812.0340", "content": "With an appropriate sign convention as used in oriented boundary integrals with Stokes theorem, the  top edges can be interpreted as horizontal components of oriented curves going to the left, bottom  edges as horizontal components of oriented curve going to the right, left edges as vertical components  of oriented curves pointing down, and right edges as vertical components of oriented curves pointing  up. For unoriented curve matching we only make a distinction between vertical and horizontal  components, requiring only two matrices; all vertical components of a curve are represented with a  positive number in the vertical component matrix. This unoriented case corresponds to a decomposition  into two varifolds[3], one for vertical and one for horizontal."}
{"pdf_id": "0812.0340", "content": "Notice that in (c) the mid left the two polygons share an edge, but for one polygon (a) this is a top edge  for the other (b) it is a bottom edge. Therefore that shared edge does not match in (d) and (e). The two  dots in (e) are from nearby vertical components of the polygonal edges. Notice also that the the two  slender protrusions of the polygons going to the right are matched, even though they do not intersect."}
{"pdf_id": "0812.0659", "content": "This paper develops a declarative language, P-log, that combines logical and probabilistic arguments in its reasoning. AnswerSet Prolog is used as the logical foundation, while causal Bayes nets serve as a probabilistic foundation. We give several non trivial examples and illustrate the use of P-log for knowledge representation and updating of knowledge. We argue that our approach to updates is more appealing than existing approaches. We give sufficiency conditions for the coherency of P-log programs and show that Bayes nets can be easily mapped to coherent P-log programs."}
{"pdf_id": "0812.0659", "content": "By a knowledge representation language, or KR language, we mean a formal language L with an entailment relation E such that (1) statements of L capture the meaning of some class of sentences of natural language, and (2) when a set S of natural language sentences is translated into a set T(S) of statements of L, the formal consequences of T(S) under E are translations of the informal, commonsense consequences of S."}
{"pdf_id": "0812.0659", "content": "One of the best known KR languages is predicate calculus, and this example can be used to illustrate several points. First, a KR language is committed to an entailment relation, but it is not committed to a particular inference algorithm. Research on inference mechanisms for predicate calculus, for example, is still ongoing while predicate calculus itself remains unchanged since the 1920's."}
{"pdf_id": "0812.0659", "content": "Second, the merit of a KR language is partly determined by the class of statements representable in it. Inference in predicate calculus, e.g., is very expensive, but it is an important language because of its ability to formalize a broad class of natural language statements, arguably including mathematical discourse."}
{"pdf_id": "0812.0659", "content": "The example illustrates that the disjunction (6), read as \"believe p(c) to be true or believe p(c) to be false\", is certainly not a tautology. It is often called the awareness axiom (for p(c)). The axiom prohibits the agent from removing truth of falsity of p(c) from consideration. Instead it forces him to consider the consequences of believing p(c) to be true as well as the consequences of believing it to be false."}
{"pdf_id": "0812.0659", "content": "The above intuition about the meaning of logical connectives of ASP1 and that of the rationality principle is formalized in the definition of an answer set of a logic program (see Appendix III). There is a substantial amount of literature on the methodology of using the language of ASP for representing various types of (possibly incomplete) knowledge (Baral 2003)."}
{"pdf_id": "0812.0659", "content": "However, ASP recognizes only three truth values: true, false, and unknown. This paper discusses an augmentation of ASP with constructs for representing varying degrees of belief. The objective of the resulting language is to allow elaboration tolerant representation of commonsense knowledge involving logic and probabilities. P-log was first introduced in (Baral et al. 2004), but much of the material here is new, as discussed in the concluding section of this paper."}
{"pdf_id": "0812.0659", "content": "A prototype implementation of P-log exists and has been used in promising experiments comparing its performance with existing approaches (Gelfond et al. 2006). However, the focus of this paper is not on algorithms, but on precise declarative semantics for P-log, basic mathematical properties of the language, and illustrations of its use. Such semantics are prerequisite for serious research in algorithms related to the language, because they give a definition with respect to which correctness of algorithms can be judged. As a declarative language, P-log stands ready to borrow and combine existing and future algorithms from fields such as answer set programming, satisfiability solvers, and Bayesian networks."}
{"pdf_id": "0812.0659", "content": "P-log extends ASP by adding probabilistic constructs, where probabilities are understood as a measure of the degree of an agent's belief. This extension is natural because the intuitive semantics of an ASP program is given in terms of the beliefs of a rational agent associated with it. In addition to the usual ASP statements, the P-log programmer may declare \"random attributes\" (essentially random variables) of the form a(X ) where X and the value of a(X ) range over finite domains. Probabilistic information about possible values of a is given through causal probability atoms, or pr-atoms. A pr-atom takes roughly the form"}
{"pdf_id": "0812.0659", "content": "The existing implementation of P-log was successfully used for instance in an industrial size applica tion for diagnosing faults in the reactive control system (RCS) of the space shuttle (Balduccini et al. 2001;Balduccini et al. 2002). The RCS is the Shuttle's system that has primary responsibility for maneuvering the air craft while it is in space. It consists of fuel and oxidizer tanks, valves, and other plumbing needed to provide propellant to the maneuvering jets of the Shuttle. It also includes electronic circuitry: both to control the valves in the fuel lines and to prepare the jets to receive firing commands. Overall, the system is rather complex, in that it includes 12 tanks, 44 jets, 66 valves, 33 switches, and around 160 computer commands (computer-generated signals)."}
{"pdf_id": "0812.0659", "content": "We believe that P-log has some distinctive features which can be of interest to those who use probabilities. First, P-log probabilities are defined by their relation to a knowledge base, represented in the form of a P-log program. Hence we give an account of the relationship between probabilistic models and the background knowledge on"}
{"pdf_id": "0812.0659", "content": "which they are based. Second, P-log gives a natural account of how degrees of belief change with the addition of new knowledge. For example, the standard definition of conditional probability in our framework becomes a theorem, relating degrees of belief computed from two different knowledge bases, in the special case where one knowledge base is obtained from the other by the addition of observations which eliminate possible worlds. Moreover, P-log can accommodate updates which add rules to a knowledge base, including defaults and rules introducing new terms."}
{"pdf_id": "0812.0659", "content": "Similar to Answer Set Prolog, a P-log statement containing unbound variables is considered a shorthand for the set of its ground instances, where a ground instance is obtained by replacing unbound occurrences of variables with properly sorted ground terms. Sorts in a program are indicated by the declarations of attributes (see below). In defining semantics of our language we limit our attention to finite programs with no unbound occurrences of variables. We sometimes refer to programs without unbound occurrences of variables as ground."}
{"pdf_id": "0812.0659", "content": "Note that limiting observable formulas to literals is not essential. It is caused by the syntactic restriction of Answer Set Prolog which prohibits the use of arbitrary formulas. The restriction could be lifted if instead of Answer Set Prolog we were to consider, say, its dialect from (Lifschitz et al. 1999). For the sake of simplicity we decided to stay with the original definition of Answer Set Prolog."}
{"pdf_id": "0812.0659", "content": "There are certain reasonableness criteria which we would like our programs to satisfy. These are normally easy to check for P-log programs. However, the conditions are described using quantification over possible worlds, and so cannot be axiomatized in Answer Set Prolog. We will state them as meta-level conditions, as follows (from this point forward we will limit our attention to programs satisfying these criteria):"}
{"pdf_id": "0812.0659", "content": "The justification of Condition 2 is as follows: If the conditions B1 and B2 can possibly both hold, and we do not have v1 = v2, then the intuitive readings of the two pr-atoms are contradictory. On the other hand if v1 = v2, the same information is represented in multiple locations in the program which is bad for maintenance and extension of the program."}
{"pdf_id": "0812.0659", "content": "[Multiple Causes: The casino story] A roulette wheel has 38 slots, two of which are green. Normally, the ball falls into one of these slots at random. However, the game operator and the casino owner each have buttons they can press which \"rig\" the wheel so that the ball falls into slot 0, which is green, with probability 1/2, while the remaining slots are all equally likely. The game is rigged in the same way no matter which button is pressed, or if both are pressed. In this example, the rigging of the game can be viewed as having two causes. Suppose in this particular game both buttons were pressed. What is the probability of the ball falling into slot 0?"}
{"pdf_id": "0812.0659", "content": "To better understand the intuition behind our definition of probabilistic measure it may be useful to consider an intelligent agent in the process of constructing his possible worlds. Suppose he has already constructed a part V of a (not yet completely constructed) possible world W , and suppose that V satisfies the precondition of some random selection rule r. The agent can continue his construction by considering a random experiment associated with r. If y is a possible outcome of this experiment then the agent may continue his construction by adding the atom a("}
{"pdf_id": "0812.0659", "content": "3 For instance, in the upcoming Example 18, random attributes arsenic and death respectively renect whether or not a given rat eats arsenic, and whether or not it dies. In that example, death and arsenic are clearly dependent. However, we assume that the factors which determine whether a poisoning will lead to death (such as the rat's constitution, and the strength of the poison) are independent of the factors which determine whether poisoning occurred in the first place."}
{"pdf_id": "0812.0659", "content": "The value of P(F) is interpreted as the degree of reasoner's belief in F. A similar idea can be used in our frame work. But since the connectives of Answer Set Prolog are different from those of Propositional Logic the notion of propositional formula will be replaced by that of formula of Answer Set Prolog (ASP formula). In this paper we limit our discussion to relatively simple class of ASP formulas which is sufficient for our purpose."}
{"pdf_id": "0812.0659", "content": "Note that in the above cases the new evidence contained a literal formed by an attribute, q, not explicitly defined as random. Adding a fact a(t) = y to a program for which a(t) is random in some possible world will usually cause the resulting program to be incoherent."}
{"pdf_id": "0812.0659", "content": "The above program tells us that the rat is more likely to die today if it eats arsenic. Not only that, the intuitive semantics of the pr atoms expresses that the rat's consumption of arsenic carries information about the cause of his death (as opposed to, say, the rat's death being informative about the causes of his eating arsenic)."}
{"pdf_id": "0812.0659", "content": "An intuitive consequence of this reading is that seeing the rat die raises our suspicion that it has eaten arsenic, while killing the rat (say, with a pistol) does not affect our degree of belief that arsenic has been consumed. The following computations show that the principle is renected in the probabilities computed under our semantics."}
{"pdf_id": "0812.0659", "content": "Propositions relevant to a cause, on the other hand, give equal evidence for the attendant effects whether they are forced to happen or passively observed. For example, if we feed the rat arsenic, this increases its chance of death, just as if we had observed the rat eating the arsenic on its own. The conditional probabilities computed under our semantics bear this out. Similarly to the above, we can compute"}
{"pdf_id": "0812.0659", "content": "Note that even though the idea of action based updates comes from Pearl, our treatment of actions is technically different from his. In Pearl's approach, the semantics of the do operator are given in terms of operations on graphs (specifically, removing from the graph all directed links leading into the acted-upon variable). In our approach the semantics of do are given by non-monotonic axioms (9) and (10) which are introduced by our semantics as part of the translation of P-log programs into ASP. These axioms are triggered by the addition of do(a("}
{"pdf_id": "0812.0659", "content": "This phenomenon is known as Simpson's Paradox: conditioning on A may increase the probability of B among the general population, while decreasing the probability of B in every subpopulation (or vice-versa). In the current context, the important and perhaps surprising lesson is that classical conditional probabilities do not faithfully formalize what we really want to know: what will happen if we do X? In (Pearl 2000) Pearl suggests a solution to this problem in which the effect of deliberate action A on condition C is represented by P(C|do(A)) — a quantity defined in terms of graphs describing causal relations between variables. Correct reasoning therefore should be based on evaluating the inequality"}
{"pdf_id": "0812.0659", "content": "I.e., if we know the person is male then it is better not to take the drug than to take the drug, the same if we know the person is female, and both agree with the case when we do not know if the person is male or female."}
{"pdf_id": "0812.0659", "content": "There are rooms, say r0, r1, r2 reachable from the current position of a robot. The rooms can be open or closed. The robot cannot open the doors. It is known that the robot navigation is usually successful. However, a malfunction can cause the robot to go off course and enter any one of the open rooms."}
{"pdf_id": "0812.0659", "content": "The first action consists of the robot attempting to enter the room R at time step 0. The second is an exogenous breaking action which may occur at moment 0 and alter the outcome of this attempt. In what follows, (possibly indexed) variables R will be used for rooms."}
{"pdf_id": "0812.0659", "content": "In this section we consider an example from (Hilborn and Mangel 1997) used to illustrate the notion of Bayesianlearning. One common type of learning problem consists of selecting from a set of models for a random phe nomenon by observing repeated occurrences of the phenomenon. The Bayesian approach to this problem is to begin with a \"prior density\" on the set of candidate models and update it in light of our observations."}
{"pdf_id": "0812.0659", "content": "As an example, Hilborn and Mangel describe the Bayesian squirrel. The squirrel has hidden its acorns in one of two patches, say Patch 1 and Patch 2, but can't remember which. The squirrel is 80% certain the food is hidden in Patch 1. Also, it knows there is a 20% chance of finding food per day when it looking in the right patch (and, of course, a 0% probability if it's looking in the wrong patch)."}
{"pdf_id": "0812.0659", "content": "The failure to find food in the first day should decrease the squirrel's degree of belief that the food is hidden in patch one, and consequently decreases her degree of belief that she will find food by looking in the first patch again. This is renected in the following computation:"}
{"pdf_id": "0812.0659", "content": "of possible worlds resulting from each successive experiment is not merely a subset of the possible worlds of the previous model. The program however is changed only by the addition of new actions and observations. Distinctive features of P-log such as the ability to represent observations and actions, as well as conditional randomness, play an important role in allowing the squirrel to learn new probabilistic models from experience."}
{"pdf_id": "0812.0659", "content": "Note that the classical solution of this problem does not contain any formal mention of the action look(2) = p1. We must keep this informal background knowledge in mind when constructing and using the model, but it does not appear explicitly. To consider and compare distinct action sequences, for example, would require the use of several intuitively related but formally unconnected models. In Causal Bayesian nets (or P-log), by contrast, the corresponding programs may be written in terms of one another using the do-operator."}
{"pdf_id": "0812.0659", "content": "In this example we see that the use of the do-operator is not strictly necessary. Even if we were choosing betweensequences of actions, the job could be done by Bayes theorem, combined with our ability to juggle several intu itively related but formally distinct models. In fact, if we are very clever, Bayes Theorem itself is not necessary — for we could use our intuition of the problem to construct a new probability space, implicitly based on the knowledge we want to condition upon."}
{"pdf_id": "0812.0659", "content": "However, though not necessary, Bayes theorem is very useful — because it allows us to formalize subtle reasoning within the model which would otherwise have to be performed in the informal process of creating the model(s).Causal Bayesian nets carry this a step further by allowing us to formalize interventions in addition to observa tions, and P-log yet another step by allowing the formalization of logical knowledge about a problem or family of problems. At each step in this hierarchy, part of the informal process of creating a model is replaced by a formal computation."}
{"pdf_id": "0812.0659", "content": "From the standpoint of P-log things are somewhat different. Here, all probabilities are defined with respect to bodies of knowledge, which include models and evidence in the single vehicle of a P-log program. Within this framework, Bayesian learning problems do not have such a distinctive quality. They are solved by writing down what we know and issuing a query, just like any other problem. Since P-log probabilities satisfy the axioms of probability, Bayes Theorem still applies and could be useful in calculating the P-log probabilities by hand. On the other hand, it is possible and even natural to approach these problems in P-log without mentioning Bayes Theorem. This would be awkward in ordinary mathematical probability, where the derivation of models from knowledge is considerably less systematic."}
{"pdf_id": "0812.0659", "content": "To put this work in the proper perspective we need to brieny describe the history of the project. The RCS actuates the maneuvering of the shuttle. It consists of fuel and oxidizer tanks, valves, and other plumbing needed to provide propellant to the shuttle's maneuvering jets. It also includes electronic circuitry, both to control the valves in the fuel lines, and to prepare the jets to receive firing commands. To perform a maneuver, Shuttle controllers (i.e., astronauts and/or mission controllers) must find a sequence of commands which delivers propellant from tanks to a proper combination of jets."}
{"pdf_id": "0812.0659", "content": "Answer Set Programming (without probabilities) was successfully used to design and implement the decision support system USA-Adviser (Balduccini et al. 2001; Balduccini et al. 2002), which, given information about thedesired maneuver and the current state of the system (including its known faults), finds a plan allowing the con trollers to achieve this task. In addition the USA-Advisor is capable of diagnosing an unexpected behavior of the system. The success of the project hinged on Answer Set Prolog's ability to describe controllers' knowledge about the system, the corresponding operational procedures, and a fair amount of commonsense knowledge. It also depended on the existence of efficient ASP solvers."}
{"pdf_id": "0812.0659", "content": "The USA-Advisor is build on a detailed but straightforward model of the RCS. For instance, the hydraulic part of the RCS can be viewed as a graph whose nodes are labeled by tanks containing propellant, jets, junctions of pipes, etc. Arcs of the graph are labeled by valves which can be opened or closed by a collection of switches. The graph is described by a collection of ASP atoms of the form connected(n1, v, n2) (valve v labels the arc from n1 to n2) and controls(s, v) (switch s controls valve v). The description of the system may also contain a collection of faults, e.g. a valve can be stuck, it can be leaking, or have a bad"}
{"pdf_id": "0812.0659", "content": "describes the relationship between the values of relation pressurized(N ) for neighboring nodes. (Node N is pressurized if it is reached by a sufficient quantity of the propellant). These and other axioms, which are rooted in a substantial body of research on actions and change, describe a comparatively complex effect of a simple nip operation which propagates the pressure through the system."}
{"pdf_id": "0812.0659", "content": "After the development of the original USA-Advisor, we learned that, as could be expected, some faults of the RCS components are more likely than others, and, moreover, reasonable estimates of the probabilities of these faults can be obtained and utilized for finding the most probable diagnosis of unexpected observations. Usually this is done under the assumption that the number of multiple faults of the system is limited by some fixed bound."}
{"pdf_id": "0812.0659", "content": "Intuitively, a program is causally ordered if (1) all nondeterminism in the program results from random selections, and (2) whenever a random selection is active in a given possible world, the possible outcomes of that selection are not constrained in that possible world by logical rules or other random selections. The following is a simple example of a program which is not causally ordered, because it violates the second condition. By comparison with Example 12, it also illustrates the difference between the statements a and pr(a) = 1."}
{"pdf_id": "0812.0659", "content": "If negated literals are treated as new predicate symbols we can view this program as stratified. Hence the program obtained in this way has a unique answer set. This means that the above program has at most one answer set; but it is easy to see it is consistent and so it has exactly one. It now follows that Condition 2 is satisfied for i = 2."}
{"pdf_id": "0812.0659", "content": "\"Causal ordering\" is one of two conditions which together guarantee the coherency of a P-log program. Causal ordering is a condition on the logical part of the program. The other condition — that the program must be \"unitary\" — is a condition on the pr-atoms. It says that, basically, assigned probabilities, if any, must be given in a way that permits the appropriate assigned and default probabilities to sum to 1. In order to define this notion precisely, and state the main theorem of this section, we will need some terminology."}
{"pdf_id": "0812.0659", "content": "Poole presents his rationale behind the above assumptions, which he says makes the language weak. His rationale is based on his goal to develop a simple extension of Pure Prolog (definite logic programs) with Clark's completion based semantics, that allows interpreting the number in the hypotheses as probabilities. Thus he restricts the syntax to disallow any case that might make the above mentioned interpretation difficult."}
{"pdf_id": "0812.0659", "content": "• (Body-not-overlap2) Since Poole's PHA assumes that the definite rules with the same hypothesis in the head have bodies that can not be true at the same time, many rules that can be directly written in our formalism need to be transformed so as to satisfy the above mentioned condition on their bodies"}
{"pdf_id": "0812.0659", "content": "• (Obs-do) Unlike us, Poole does not distinguish between doing and observing. • (Gen-upd) We consider very general updates, beyond an observation of a propositional fact or an action that makes a propositional fact true. • (Prob-def) Not all probability numbers need be explicitly given in P-log. It has a default mechanism to implicitly assume certain probabilities that are not explicitly given. This often makes the representation simpler. • Our probability calculation is based on possible worlds, which is not the case in PHA, although Poole's later formulation of Independent Choice Logic (Poole 1997; Poole 2000) (ICL) uses possible worlds."}
{"pdf_id": "0812.0659", "content": "7 Poole's possible worlds are very similar to ours except that he explicitly assumes that the possible worlds whose core would be obtained by the enumeration, can not be eliminated by the acyclic programs through constraints. We do not make such an assumption, allow elimination of such cores, and if elimination of one or more (but not all) possible worlds happen then we use normalization to redistribute the probabilities."}
{"pdf_id": "0812.0659", "content": "LPAD is richer in syntax than PHA or ICL in that its rules (corresponding to disjoint declarations in PHA and a choice space in ICL) may have conditions. In that sense it is closer to the random declarations in P-log. Thus, unlike PHA and ICLP, and similar to P-log, Bayes networks can be expressed in LPAD fairly directly. Nevertheless LPAD has some significant differences with P-log, including the following:"}
{"pdf_id": "0812.0659", "content": "A ground BLP clause is similar to a ground logic programming rule. It is obtained by substituting variables with ground terms from the Herbrand universe. If the ground version of a BLP program is acyclic, then a BLP can be considered as representing a Bayes network with possibly infinite number of nodes. To deal with the situation when the ground version of a BLP has multiple rules with the same atom in the head, the formalisms allows for specification of combining rules that specify how a set of ground BLP rules (with the same ground atom in the head) and their CPT can be combined to a single BLP rule and a single associated CPT."}
{"pdf_id": "0812.0659", "content": "The aim of BLPs is to enhance Bayes nets so as to overcome some of the limitations of Bayes nets such as difficulties with representing relations. On the other hand like Bayes nets, BLPs are also concerned about statistical relational learning. Hence the BLP research is less concerned with general knowledge representation than P-log is, and this is the source of most of the differences in the two approaches. Among the resulting differences between BLP and P-log are:"}
{"pdf_id": "0812.0659", "content": "In this formalism each predicate represents a set of similar random variables. It is assumed that each predicate has at least one attribute representing the value of random attributes made up of that predicate. For example, the random variable Colour of a car C can be represented by a 2-ary predicate color(C, Col), where the first position takes the id of particular car, and the second indicates the color (say, blue, red, etc.) of the car C."}
{"pdf_id": "0812.0659", "content": "The combining rules serve similar purpose as in Bayesian logic programs. Note that unlike Bayesian logic pro grams that have CPTs for each BLP clause, the probabilistic sentences in PKBs only have a single probability associated with it. Thus the semantic characterization is much more complicated. Nevertheless the differences between P-log and Bayesian logic programs also carry over to PKBs."}
{"pdf_id": "0812.0659", "content": "The goal behind the semantic characterization of an NS-PLP program P is to obtain and express the set of (prob abilistic) p-interpretations (each of which maps possible worlds, which are subsets of the Herbrand Base, to a number in [0,1]), Mod(P), that satisfy all the p-clauses in the program. Although initially it was thought that Mod(P) could be computed through the iteration of a fixpoint operator, recently (Dekhtyar and Dekhtyar 2004) shows that this is not the case and gives a more complicated way to compute Mod(P). In particular, (Dekhtyar and Dekhtyar 2004) shows that for many NS-PLP programs, although its fixpoint, a mapping from the Herbrand base to an interval in [0, 1], is defined, it does not represent the set of satisfying p-interpretations."}
{"pdf_id": "0812.0659", "content": "So far we have discussed logic programming approaches to integrate logical and probabilistic reasoning. Besides them, the paper (De Vos and Vermeir 2000) proposes a notion where the theory has two parts, a logic programming part that can express preferences and a joint probability distribution. The probabilities are then used in determining the priorities of the alternatives."}
{"pdf_id": "0812.0659", "content": "P-log comes with a natural mechanism for belief updating — the ability of the agent to change degrees of belief defined by his current knowledge base. We showed that conditioning of classical probability is a special case of this mechanism. In addition, P-log programs can be updated by actions, defaults and other logic programming rules, and by some forms of probabilistic information. The non-monotonicity of P-log allows us to model situations when new information forces the reasoner to change its collection of possible worlds, i.e. to move to a new probabilistic model of the domain. (This happens for instance when the agent's knowledge is updated by observation of an event deemed to be impossible under the current assumptions.)"}
{"pdf_id": "0812.0659", "content": "The expressive power of P-log and its ability to combine various forms of reasoning was demonstrated on a number of examples from the literature. The presentation of the examples is aimed to give a reader some feeling for the methodology of representing knowledge in P-log. Finally the paper gives sufficiency conditions for coherency of P-log programs and discusses the relationship of P-log with a number of other probabilistic logic programming formalisms."}
{"pdf_id": "0812.0659", "content": "with counterfactuals and probabilistic abductive reasoning capable of discovering most probable explanations of unexpected observations. Finally, we plan to explore how statistical relational learning (SRL) can be done with respect to P-log and how P-log can be used to accommodate different kinds of uncertainties tackled by existing SRL approaches."}
{"pdf_id": "0812.0659", "content": "[Path Value] Let T be a tree in which every arc is labeled with a number in [0,1]. The path value of a node n of T, denoted by pvT(n), is defined as the product of the labels of the arcs in the path to n from the root. (Note that the path value of the root of T is 1.)"}
{"pdf_id": "0812.0659", "content": "Finally, we claim that every node n in A has a unique child in Ay, which we will label ychild(n). The existence and uniqueness follow from (27), along with Condition 3 of Section 3.2, and the fact that every node in A branches on a(t) via [r]. Thus from (30) we obtain"}
{"pdf_id": "0812.0659", "content": "To prove (3) let us first notice that the set of literals S formed by relations do, obs, and intervene form a splitting set of programs PB and Pobs(B). Both programs include the same collection of rules whose heads belong to this splitting set. Let X be the answer set of this collection and let QB and Qobs(B) be partial evaluations of PB and Pobs(B) with respect to X and S. From the splitting set theorem we have that (3) holds iff"}
{"pdf_id": "0812.0659", "content": "We begin with some preliminary definitions. Let V be a finite set of variables, where each v in V takes values from some finite set D(v). By an assignment on V , we mean a function which maps each v in V to some member of D(v). We will let A(V ) denote the set of all assignments on V . Assignments on V may also be called possible worlds of V ."}
{"pdf_id": "0812.0659", "content": "9 This part of the definition captures some intuition about causality. It entails that given complete information about the factors immediately innuencing a variable v (i.e., given the parents of v in G), the only variables relevant to inferences about v are its effects and indirect effects (i.e., descendants of v in G) — and that this property holds regardless of the intervention performed."}
{"pdf_id": "0812.0698", "content": "Information systems on the World Wide Web have been increasing in sizeand complexity to the point that they presently exhibit features typically at tributed to bona fide complex systems. They display rich high-level behaviorsthat are causally connected in non-trivial ways to the dynamics of their inter acting elementary parts. Because of this, concepts and formal tools from the science of complex systems can play an important role in understanding the structure and dynamics of such systems."}
{"pdf_id": "0812.0698", "content": "Our work is based on experimental data from one of the largest and most popular collaborative tagging systems, del.icio.us, currently used by over a million users to manage and share their collections of web bookmarks. The main point of our work is neither to present a new spectral community detection algorithm, nor to report a large data set analysis. Rather, we want to show that, choosing the right projection and the right weighting procedure,we can produce a weighted undirected network of resources from the full tri partite folksonomy network, which embed a meaningful social classification of resources. This is especially surprising, considering that users annotate resources in a very anarchic, uncoordinated and noisy way."}
{"pdf_id": "0812.0698", "content": "In section 2 we describe the experimental data we collected. In Section 3 we introduce a notion of resource distance based on the collective activity of users. Based on that, we set up an experiment using actual data from del.icio.us and we build a weighted network of resources. In section 4 we show that spectral methods from complex networks theory can be used to detect clusters of resources in the above network and we characterize those clusters in terms of user tags, exposing semantics. Finally, section 5 gives an overview of our results and points to directions for future work."}
{"pdf_id": "0812.0698", "content": "In a collaborative tagging system, a set of resources defines a \"semantic space\" that is explored and mapped by a community of users, as they bookmark and tag those resources [6]. We want to investigate whether the tagging activity is actually structuring the space of resources in a semantically meaningful"}
{"pdf_id": "0812.0698", "content": "Fig. 3. Probability distributions of link strengths. The logarithmically-binned his togram of link strengths for all pairs of resources within a given set is displayed for three sets of resources: empty squares correspond to resources tagged with design,filled squares correspond to resources tagged with politics, and blue circles corre spond to the union of the above sets. It is important to observe that strength values span several orders of magnitude, so that a non-linear function of link strengths becomes necessary in order to capture the full dynamic range of strength values."}
{"pdf_id": "0812.0698", "content": "The problem we have to tackle now is finding the sequence of row and column permutations of the similarity matrix that permits to visually identify the presence of communities of resources, if at all possible. The goal is to obtain a matrix with a clear visible block structure on its main diagonal. One possible way to approach this problem is to construct an auxiliary matrix and use information deduced from its spectral properties to rearrange row and columns of the original matrix. The quantity we consider is the matrix"}
{"pdf_id": "0812.0698", "content": "Fig. 5. Eigenvalues of the matrix Q (Eq. 3). Resource communities correspond to non-trivial eigenvalues of the spectrum, such as the ones visible on the leftmost side of the plot and in the inset. The three eigenvalues marked in the inset correspond to the eigenvectors plotted in Fig. 6."}
{"pdf_id": "0812.0698", "content": "Fig. 6. Eigenvectors of the matrix Q (Eq. 3). The scatter plot displays the com ponent values of the first three non-trivial eigenvectors of the matrix (marked with circles in Fig. 5). The scatter plot is parametric in the component index. Five or six clusters are visible, corresponding to the smallest non-trivial eigenvalues of the similarity matrix. Each cluster, marked with a numeric label, defines a community of \"similar\" resources (in terms of tag-clouds). Blue and red points correspond to resources tagged with design and politics, respectively. Notice that our approachclearly recovers the two original sets of resources, and also highlights a few finer grained structures. Tag-clouds for the identified communities are shown in Fig. 8."}
{"pdf_id": "0812.0698", "content": "The increasing impact of web-based social tools for the organization and shar ing of resources is motivating new research at the frontier of complex systemsscience and computer science, with the goal of harvesting the emergent se mantics [11] of these new tools. The increasing interest on such new tools is based on the belief that the anarchic, uncoordinated activity of users can be used to extract meaningful"}
{"pdf_id": "0812.0698", "content": "and useful information. For instance, in social bookmarking systems, people annotate personal list of resources with freely chosen tags. Wheter or not thiscould provide a \"social\" classification of resources, is the point we want to in vestigate with this work. In other words, we investigate whether an emergent community structure exists in folksonomy data. To this aim, we focused on a popular social bookmarking system and introduced a notion of similarity between resources (annotated objects) in terms of social patterns of tagging. We used our notion of similarity to build weighted networks of resources, and showed that spectral community-detection methods can be used to exposethe emergent semantics of social tagging, identifying well-defined communi ties of resources that appear associated with distinct and meaningful tagging"}
{"pdf_id": "0812.0698", "content": "The authors wish to thank Melanie Aurnhammer, Andreas Hotho and GerdStumme for very interesting discussions. This research has been partly supported by the TAGora project funded by the Future and Emerging Tech nologies program (IST-FET) of the European Commission under the contract IST-34721. The information provided is the sole responsibility of the authors"}
{"pdf_id": "0812.0790", "content": "It can be shown that every answer set of the program consisting of the rules repre senting the graph and the above rules corresponds to an Hamiltonian cycle of the graph and vice versa. Furthermore, the program has no answer set if and only if the graph does not have an Hamiltonian cycle."}
{"pdf_id": "0812.0790", "content": "• Trace-based debuggers provide the entire search sequence, including the failed paths, which might be irrelevant in understanding how specific elements are introduced in an answer set. • The process of computing answer sets is bottom-up, and the determination of the truth value of one atom is intermixed with the computation of other atoms; a direct tracing makes it hard to focus on what is relevant to one particular atom. This is illustrated in the following example."}
{"pdf_id": "0812.0790", "content": "A program is definite if it contains only definite rules. The answer set semantics of a program (Subsection 2.2) is highly dependent on the truth value of atoms occurring in the negative literals of the program. For later use, we denote with NANT (P) the atoms which appear in NAF literals in P—i.e.,"}
{"pdf_id": "0812.0790", "content": "We will now review two important semantics of logic programs, the answer set semantics and the well-founded semantics. The former is foundational to ASP and the latter is important for the development of our notion of a justification. We will also brieny discuss the basic components of ASP systems."}
{"pdf_id": "0812.0790", "content": "assumptions A—where an assumption is an atom for which we will not seek any ex planations. The assumptions derive from the inherent \"guessing\" process involved in the definition of answer sets (and in their algorithmic construction), and they will be used to justify atoms that have been \"guessed\" in the construction of the answer set and for which a meaningful explanation cannot be constructed."}
{"pdf_id": "0812.0790", "content": "• The graph (i) describes the true state of p by making it positively dependent on the true state of q and r; in turn, q is simply assumed to be true while r is a fact in the program. • The graph (ii) describes more complex dependencies; in particular, observe that t and u are both false and they are mutually dependent—as in the case of a program containing the rules"}
{"pdf_id": "0812.0790", "content": "We are now ready to instantiate the notion of e-graph by forcing the edges of the e-graph to represent encodings of local consistent explanations of the corresponding atoms. To select an e-graph as an acceptable explanation, we need two additional components: the current interpretation (J) and the collection (U) of elements that have been introduced in the interpretation without any \"supporting evidence\". An e-graph based on (J, U) is defined next."}
{"pdf_id": "0812.0790", "content": "The two additional conditions we impose on the e-graph force the graph to be connected w.r.t. the element b we are justifying, and force the selected nodes and edges to renect local consistent explanations for the various elements. The next condition we impose on the explanation graph is aimed at ensuring that no positive cycles are present. The intuition is that atoms that are true in an answer set should have a non-cyclic support for their truth values. Observe that the same does not happen for elements that are false—as in the case of elements belonging to unfounded sets (Apt and Bol 1994)."}
{"pdf_id": "0812.0790", "content": "We are interested in the subsets of V with the following property: if all the elements in the subset are assumed to be false, then the truth value of all other atoms in A is uniquely determined and leads to the desired answer set. We call these subsets the assumptions of the answer set. Let us characterize this concept more formally."}
{"pdf_id": "0812.0790", "content": "Justifications are built by assembling items from the LCEs of the various atoms and avoiding the creation of positive cycles in the justification of true atoms. Also, the justification is built w.r.t. a chosen set of assumptions (A), whose elements are all assumed false.In general, an atom may admit multiple justifications, even w.r.t. the same as sumptions. The following lemma shows that elements in WFP can be justified without negative cycles and assumptions."}
{"pdf_id": "0812.0790", "content": "Proposition 2 underlines an important property—the fact that all true elements can be justified in a non-cyclic fashion. This makes the justification more natural, renecting the non-cyclic process employed in constructing the minimal answer set(e.g., using the iterations of TP ) and the well-founded model (e.g., using the characterization in (Brass et al. 2001)). This also gracefully extends a similar property sat isfied by the justifications under well-founded semantics used in (Roychoudhury et al. 2000). Note that the only cycles possibly present in the justifications are positive cycles associated to (mutually dependent) false elements—this is an unavoidable situation due the semantic characterization in well-founded and answer set semantics (e.g., unfounded sets). A similar design choice has been made in (Pemmasani et al. 2004; Roychoudhury et al. 2000)."}
{"pdf_id": "0812.0790", "content": "to address this problem is to refine the notion of justification to make possible the \"declarative tracing\" of atoms w.r.t. a partially constructed interpretation. This is similar to debugging of imperative languages, where breakpoints can be set and the state of the execution explored at any point during the computation. In this section, we introduce the concept of on-line justification, which is generated during the computation of an answer set and allows us to justify atoms w.r.t. an incomplete interpretation—that represents an intermediate step in the construction of the answer set."}
{"pdf_id": "0812.0790", "content": "The concept of on-line justification is applicable to computation models that con struct answer sets in an incremental fashion, e.g., Smodels and DLV (Simons et al. 2002;Eiter et al. 1998; Gebser et al. 2007; Anger et al. 2005). We can view the compu tation as a sequence of steps, each associated to a partial interpretation. We will focus, in particular, on computation models where the progress towards the answer set is monotonic."}
{"pdf_id": "0812.0790", "content": "It is worth to point out that an on-line justification can be obtained in answer set solvers employing the computation model described in Definition 13. This will be demonstrated in the next section where we discuss the computation of on-line justifications in the Smodels system. We next illustrate the concept of an on-line justification."}
{"pdf_id": "0812.0790", "content": "Various approaches to logic program understanding and debugging have been in vestigated (and a thorough comparison is beyond the limited space of this paper). Early work in this direction geared towards the understanding of Prolog programs rather than logic programs under the answer set semantics. Only recently, we can find some work on debugging inconsistent programs or providing explanation forthe presence (or absence) of an atom in an answer set. While our notion of justi fication is related to the research aimed at debugging Prolog and XSB programs,its initial implementation is related to the recent attempts in debugging logic pro grams under the answer set semantics. We will discuss each of these issues in each subsection."}
{"pdf_id": "0812.1014", "content": "Abstract. This paper proposes a novel solution to spam detection inspired by a model of the adaptive immune system known as the cross regulation model. We report on the testing of a preliminary algorithm onsix e-mail corpora. We also compare our results statically and dynami cally with those obtained by the Naive Bayes classifier and another binary classification method we developed previously for biomedical text-mining applications. We show that the cross-regulation model is competitive against those and thus promising as a bio-inspired algorithm for spam detection in particular, and binary classification in general."}
{"pdf_id": "0812.1014", "content": "1. If one or two E bind to antigen, they proliferate with a fixed rate. 2. If one or two R bind to the antigen, they remain in the population. 3. if an R binds together with an E to the same antigen, the R proliferates with a certain rate and the E remains in the population but does not proliferate."}
{"pdf_id": "0812.1014", "content": "Finally, the E and R die at a fixed death rate. Carneiro et al. [5] showed that the dynamics of this system leads to a bistable system of two possible stable population concentration attractors: (i) the co-existence of both E and R types identifying harmless self antigens, or (ii) the progressive disappearance of R, identifying harmful antigens."}
{"pdf_id": "0812.1014", "content": "Naive Bayes (NB). We have chosen to compare our results with the multi nomial Naive Bayes with boolean attributes [12] which has shown great success in previous research [15]. In order to fairly compare NB with ICRM, we selected the first and last unique n = 50 features. The Naive Bayes classifies an e-mail as spam in the testing phase if it satisfies the following condition:"}
{"pdf_id": "0812.1014", "content": "Static Evaluation Results. As clearly shown in table 1, ICRM, NB and VTT are very competitive for most enron datasets, indeed the performance of ICRM is statistically indistinguishable from VTT (F-score and Accuracy p-values 0.15and 0.63 for the paired t-test validating the null hypothesis of variation equivalence), though its slightly lower performance against NB is statistically signifi cant (F-score and Accuracy p-values 0.01 and 0.02 for the paired t-test, rejecting the null hypothesis of variation equivalence with 0.05 level of significance). However, the ICRM can be more resilient to ham ratio variations12 as shownin table 2 and figure ??. While the performance of both algorithms was com parable for 50% spam (though significantly better for NB), the performance of"}
{"pdf_id": "0812.1014", "content": "In this paper we have introduced a novel spam detection algorithm inspired by the cross-regulation model of the adaptive immune system. Our model has proved itself competitive with both spam binary classifiers and resilient to spam to ham ratio variations in particular. The overall results, even though not stellar, seem quite promising especially in the areas of spam to ham ratio variation and also of tracking concept drifts in spam detection. This original work should be regarded not only as a promising bio-inspired method that can be further developed and even integrated with other methods but also as a model that could help us better understand the behavior of the T-cell cross-regulation systems in particular, and the vertebrate natural immune system in general."}
{"pdf_id": "0812.1014", "content": "Acknowledgements. We thank Jorge Carneiro for his insights about applying ICRM on spam detection and his generous support and contribution for making this work possible. We also thank Florentino Fdez-Riverola for the very useful indications about spam datasets and work in the area of spam detection. We would also like to thank the FLAD Computational Biology Collaboratorium at the Gulbenkian Institute in Oeiras, Portugal, for hosting and providing facilities used to conduct part of this research."}
{"pdf_id": "0812.1029", "content": "Open Access 2008 Abi-Haidar et al. Volume 9, Suppl 2, Article S11 Research Uncovering protein interaction in abstracts and text using a novel  linear model and word proximity networks Alaa Abi-Haidar1,2, Jasleen Kaur1, Ana Maguitman3, Predrag Radivojac1,  Andreas Rechtsteiner4, Karin Verspoor5, Zhiping Wang6 and  Luis M Rocha1,2"}
{"pdf_id": "0812.1029", "content": "Background: We participated in three of the protein-protein interaction subtasks of the Second BioCreative Challenge: classification of abstracts relevant for protein-protein interaction (interaction article subtask [IAS]), discovery of protein pairs (interaction pair subtask [IPS]), and identification of text passages characterizing protein interaction (interaction sentences subtask [ISS]) in full-text documents. We approached the abstract classification task with a novel, lightweight linear model inspired by spam detection techniques, as well as an uncertainty-based integration scheme. We also used a support vector machine and singular value decomposition on the same features for comparison purposes. Our approach to the full-text subtasks (protein pair and passage identification) includes a feature expansion method based on word proximity networks."}
{"pdf_id": "0812.1029", "content": "Results: Our approach to the abstract classification task (IAS) was among the top submissions for this task in terms of measures of performance used in the challenge evaluation (accuracy, F-score, and area under the receiver operating characteristic curve). We also report on a web tool that we produced using our approach: the Protein Interaction Abstract Relevance Evaluator (PIARE). Our approach to the full-text tasks resulted in one of the highest recall rates as well as mean reciprocal rank of correct passages."}
{"pdf_id": "0812.1029", "content": "Conclusion: Our approach to abstract classification shows that a simple linear model, using relatively few features, can generalize and uncover the conceptual nature of protein-protein interactions from the bibliome. Because the novel approach is based on a rather lightweight linear model, it can easily be ported and applied to similar problems. In full-text problems, the expansion of word features with word proximity networks is shown to be useful, although the need for some improvements is discussed."}
{"pdf_id": "0812.1029", "content": "In most text-mining projects in biomedicine, one must first collect a set of relevant documents, typically from abstract information. Such a binary classification, between relevant and irrelevant documents for PPI, is precisely what the IAS subtask in BioCreative II aimed to evaluate. Naturally, tools developed for IAS have great potential to be applied in many other text-mining projects beyond PPI. For that reason, we opted to produce a very general and lightweight system that can easily be applied to other domains and ported to different computer infrastructure. This design criteria lead us to a novel linear model inspired by spam-detection techniques. For comparison purposes, we also used a support vector machine (SVM) and singular value decomposition (SVD) enhanced with an uncertainty-based integration scheme."}
{"pdf_id": "0812.1029", "content": "As for the IPS and ISS subtasks, our approach is centered on a feature expansion method, using word proximity networks, which we introduced in the first Biocreative challenge [9]. Below, we describe our approach in detail and discuss ourvery positive results. We also report on a web tool we pro duced using our IAS approach: the Protein Interaction Abstract Relevance Evaluator (PIARE)."}
{"pdf_id": "0812.1029", "content": "As can be seen in Table 1, all of our three runs were above the mean and median values of accuracy, F-score, and area under the receiver operating characteristic curve (AUC) measurescomputed from the results of all 51 submissions to the challenge [10]. We can also report that our novel VTT method per formed better than our two other runs: SVM and SVD-UI.Moreover, the corrected VTT run improved from the submit ted version; only 2 out of 51 other submissions (from 1 out of 19 groups) report higher values of all three performance measures above [10]."}
{"pdf_id": "0812.1029", "content": "As we discuss in the Materials and methods section (below), the SVD vector model alone produced the same classification of the test abstracts as SVD-UI, except that different rankings of abstracts were attained. Therefore, the values of accuracy and F-score are identical for the SVD vector model alone and SVD-UI. However, the AUC of the SVD method alone was much lower (0.68) than that of the SVD-UI method (0.75). We can thus say that the integration method improved the"}
{"pdf_id": "0812.1029", "content": "aCalculated from 51 runs submitted by 19 teams. AUC, area under the curve; IAS, interaction article subtask; SVD, singular value decomposition;  SVM, support vector machine; SVD-UI, SVD with uncertainty integration; VTT, variable trigonometric threshold. Bold entries for accuracy, F-Score,  and AUC denote best value obtained for all our submitted runs."}
{"pdf_id": "0812.1029", "content": "AUC of the SVD method alone. On the other hand, its per formance according to accuracy, F-score, and AUC was worsethan the other constituent methods employed in the uncer tainty integration, such as VTT as submitted in run 2. Thus, uncertainty integration did not improve the VTT alone. The fairly lackluster performance of this uncertainty integration method is possibly due to computing Shannon's entropy for the only two classes of this problem: positives and negatives. The method was originally developed [4] to classify more than 1,000 PFAM protein families, which is much moreappropriate for this uncertainty measure. A probability distribution on two elements is not an ideal situation for calculat ing Shannon's entropy."}
{"pdf_id": "0812.1029", "content": "Data issues and trainingOne of the problems encountered by all methods, but partic ularly so for our SVM and SVD methods, was the significantdifference between the training and the test IAS data in Bio Creative II. It is clear that the abstracts in the training data aredistinct from those in the test data. To quantify this distinc tion, after the challenge we trained a SVM model to classify labeled and unlabeled data - that is, between training and testdata, regardless of them being relevant (positive) or irrele vant (negative) for protein interaction. If the two sets of abstracts were sampled from the same coherent semantic"}
{"pdf_id": "0812.1029", "content": "Accuracy versus F-score plane Figure 1Accuracy versus F-score plane. Our methods on the accuracy versus F score plane for IAS. Mean and median are for the set of all submissions  from all groups. Red squares denote our three submissions (SVM, VTT,  and SVD-UI). In this plane, SVD alone occupies the same point as SVD-UI.  The orange oval denotes the results for one of the versions of VTT (with  bigrams+) included in the SVD-UI method. IAS, interaction article subtask;  SVD, singular value decomposition; SVM, support vector machine; SVD-UI,  SVD with uncertainty integration; VTT, variable trigonometric threshold."}
{"pdf_id": "0812.1029", "content": "F-score versus AUC plane Figure 3 F-score versus AUC plane. Our methods on the F-score versus AUC  plane for IAS. Mean and median are for the set of all submissions from all groups. Red squares denote our three submissions (SVM, VTT, and SVD UI). The orange polygon denotes the results for SVD alone, and the  orange oval denotes the results for one of the versions of VTT (with  bigrams+) included in the SVD-UI method. AUC, area under the receiver  operating characteristic curve; IAS, interaction article subtask; SVD,  singular value decomposition; SVM, support vector machine; SVD-UI, SVD  with uncertainty integration; VTT, variable trigonometric threshold."}
{"pdf_id": "0812.1029", "content": "useful in achieving a generalization of the 'concept' of proteininteraction in the bibliome. Figure 4 depicts the decision surface for the VTT on the test data, as well as the decision sur face that would have been submitted if we had trained exclusively on the training data supplied. Figures 5 and 6depict the same surfaces but on one of the training and addi tional data partitions, respectively."}
{"pdf_id": "0812.1029", "content": "VTT decision surface for a training partition Figure 5 VTT decision surface for a training partition. Decision boundary for VTT  on the space of P(a)/N(a) and np(a), for one of the training k-fold  partitions. Red and blue dots represent negative and positive abstracts.  Dotted line represents surface that optimizes training data alone. VTT,  variable trigonometric threshold."}
{"pdf_id": "0812.1029", "content": "standard deviation of the mean. On the other hand, recall was above the mean and median of all submissions; very close tobeing above the mean plus one standard deviation for all articles; and above it for the subset of articles containing exclu sively SwissProt IDs. There were 6 submissions (from 4 groups) out of 45 with higher recall for the set of all articles, and 7 for the case of articles with SwissProt IDs only (see [11] for more details). The F-score was very near to the mean and median of all submissions. Table 2 lists the details."}
{"pdf_id": "0812.1029", "content": "Regarding the IPS task, although obtaining a good recall measure, our system could improve precision by considering additional biologically relevant information. In particular, because our system, for the same protein mention, outputs different Uniprot IDs for each of the organism MeSH terms of the document at stake, it could be improved by identifying organism information in the text. Using a classifier (such as our VTT or an SVM) to preclassify documents and passages according to different organisms could result in increased precision. We should also do more about removing genetic"}
{"pdf_id": "0812.1029", "content": "Because, even so, the expansion of feature words was modestly beneficial,and because it is clear from the manual observation of proximity networks that they do capture the contextual relation ships of individual documents, we plan to use the method tofind additional words related to general features, not just protein names"}
{"pdf_id": "0812.1029", "content": "In general, our participation in three subtasks of the BioCre ative II challenge, with such a large set of members, was very useful in validating our approaches as well as learning from other groups. It also led us to a position where we are more easily able to extend the methods to biomedical applications other than protein interaction."}
{"pdf_id": "0812.1029", "content": "Figure 7 depicts the 1,000 abstract co-occurrence word pairs (the third feature set) with largest Sab(wi, wj) = |pTP(wi, wj) - pTN(wi, wj)|, plotted on a plane where the horizontal axis is the value of pTP(wi, wj) and the vertical axis is the value of pTN(wi, wj); we refer to this as the pTP/pTN plane"}
{"pdf_id": "0812.1029", "content": "One should note that our bigrams+ are built only from the 650 single word features, and therefore they are not necessarily constituted of words immediately adjacent in abstracts. They include traditional bigrams only if both words are in the set of 650 single word features. However, they also include pairs of words that are not necessarily adjacent in an abstract, but are adjacent in the word vectors comprised of only the top 650 single word features produced for each abstract. As for the abstract co-occurrence word pairs, all of these co-occur in the same abstracts, but they are likewise comprised of only the 650 single word features."}
{"pdf_id": "0812.1029", "content": "Training and additional data To train the various classification methods described below, we first performed k-fold tests on the supplied training data. Specifically, we randomly generated eight different partitions of the training set of abstracts, with 75% of the abstracts used to train the classification algorithms employed, and 25% to test them. In addition, we forced the 25% test sets of abstracts in these partitions to have a balanced number of positive (TP) and (TN) negative abstracts. We conducted a second test using additional data not supplied by the BioCreative II organizers. We collected 367 additional positive abstracts from the MIPS (Munich Information Center for Protein Sequences) database [18], and 427 negative proteomics abstracts curated by hand that were graciously donated to our"}
{"pdf_id": "0812.1029", "content": "team by Santiago Schnell. The second test then consisted of training the classification algorithms with all of the supplied positive and negative abstracts (TP and TN), and testing on the additional data that were also balanced with the addition of 60 randomly selected, likely positive abstracts from TP. We produced eight different randomly selected balanced test setswith the additional data. Finally, we used the k-fold and addi tional data tests to select the best parameters for the various classification algorithms employed, as described below."}
{"pdf_id": "0812.1029", "content": "Testing our SVM with this feature selection method on the eight k-fold training data and eight additional data partitions (as well as on the test data itself after the challenge) yielded no gains in performance, suggesting that our selection of the top 650 words with largest S for VTT is sufficient for classification"}
{"pdf_id": "0812.1029", "content": "Singular value decomposition classification To best compare this method with VTT, we started from the same original feature set: the 650 single words with largest S. We represented abstracts as vectors in this feature space. Wethen calculated the inverse document frequency (IDF) meas ure, so the vector coefficients were the TF*IDF [22] for the respective features. The number of protein mentions per abstract, np(a) (see Feature selection subsection), was addedas an additional feature. The abstract vectors were also nor malized to Euclidean length 1. We computed the SVD [20] of the resulting abstract-feature matrix (from the training data).The top 100 components were retained (this number pro vided best results on our tests on training and additional data)."}
{"pdf_id": "0812.1029", "content": "We classified the set of abstracts using a nearest neighbor classifier on the eigenvector space (of dimension 100)obtained via the SVD of the feature/abstract matrix. To classify a test abstract vector a, we project it onto this SVD sub space and calculate the cosine similarity measure of a to every training abstract t:"}
{"pdf_id": "0812.1029", "content": "Where |TP| and |TN| are the number of positive and negative abstracts in the training data, respectively. (Often, the aggregation of vector contributions would be made for the nearest K vectors [or a neighboring hypercone in vector space] rather than summing the contributions of every vectort in the space. Using all training vectors could result in distortions by the existence of large masses of vectors in an oppos"}
{"pdf_id": "0812.1029", "content": "Using this uncertainty measure we integrate the predictions issued by each method by selecting, for each abstract a, the prediction issued by the method M with lowest UM(a); thisvalue of uncertainty is also used to rank the abstracts for relevance. In our original submission to the BioCreative II chal"}
{"pdf_id": "0812.1029", "content": "lenge, we submitted a run (run 3) based on this uncertainty driven integration method with additional characteristics described in detail in [11]. Here, we report on updated results (run 3') after fixing the software error that afflicted the original VTT submission (run 2). Specifically, our SVD-UI scheme integrated three methods."}
{"pdf_id": "0812.1029", "content": "Items 2 and 3 were chosen so that there would be a model from each of the word pair feature sets. It is important to notethat in our tests with training and additional data, the SVD UI improved only very slightly over the SVD vector modelalone. Indeed, for the test set the SVD vector model alone pro duced the same relevant/nonrelevant classification as the integration method; the difference was only in the ranking of abstracts, thus affecting only the AUC performance measure, as discussed in Results (above). This was true for both the run submitted to the challenge (run 3) and the updated version (run 3'), as shown in Table 1."}
{"pdf_id": "0812.1029", "content": "The fact that SVD-UI and SVD alone yielded the same rele vant/nonrelevant classification, indicates that when abstracts are projected onto the compound vector space described above, the classification via SVD is less uncertain (lower Shannon entropy) than the one via VTT. By this we mean that abstracts deemed positive (negative) by SVD tend to have less"}
{"pdf_id": "0812.1029", "content": "negative (positive) abstracts around them in the compound vector space (as measured by cosine similarity) than those classified by VTT. We decided to submit the results of the SVD-UI method other than SVD on its own, because it led to slightly better AUC measure results than the SVD vector model on the learning and additional data (see Results [above]). Thus, although SVD and SVD-UI classified the abstracts in the same manner, they led to different rankings. This indicates that using Shannon's measure of entropy onthe compound vector space yields a better ranking than dis tance from the SVD decision surface alone."}
{"pdf_id": "0812.1029", "content": "Feature selectionFrom the features extracted from abstracts in the IAS sub task, we collected 1,000 abstract co-occurrence word-pairfeatures, (wi, wj), from the third feature set. Because the pur pose of these tasks is to identify portions of text in which PPI information appears, we do not need to worry about features indicative of negative PPI information. Thus, these features were chosen and ranked according to the highest values of the following:"}
{"pdf_id": "0812.1029", "content": "Where pTP and pTN are as defined in the IAS task methods subsection. This measure is a variation of the trigonometric measures we used in the VTT model for the IAS subtask. We multiply the cosine measure by the probability of the feature being associated with a positive abstract, to ensure that the many features which have zero probability of being associated with a negative abstract (PTN = 0) are not equally ranked."}
{"pdf_id": "0812.1029", "content": "We also obtained an additional set of features from PPI-rele vant sentences: the 'sentence feature set'. These sentences were extracted from all PPI evidence sentences provided byBioCreative II for these tasks; these contained the 63 sen tences associated with the set of training articles, as well as the sentences extracted from other resources detailed in [13].From these PPI evidence sentences, we calculated the fre quency of stemmed words: fppi(w). Then, we calculated the frequency of stemmed words of the entire training corpus of 740 full-text articles: fc(w). Finally, similarly to the word pair features above, we selected as sentence features the top 200 stemmed words which maximize the following score (top 10 in Table 5):"}
{"pdf_id": "0812.1029", "content": "Paragraph selection and ranking Our next step was to select paragraphs in each document that are more likely to contain protein interaction information. For this we used our two feature sets defined in the previous subsection, plus protein mention information. Thus, for each full-text document, we ordered paragraphs according to three different preference criteria."}
{"pdf_id": "0812.1029", "content": "Selection and ranking of protein-protein interaction pairs for IPS Finally, for the IPS task we returned all the combinations of protein pairs (UniProt accession numbers) occurring in the same sentence - for sentences included in the paragraphs of ranks 1, 2, and 3 above. For a given document (PMID), the"}
{"pdf_id": "0812.1029", "content": "rank of each PPI pair is the rank of the highest ranked para graph in which the pair occurs in a sentence. We submitted three distinct rankings of PPI pairs according to the three ranks 1, 2, and 3 above. Because only paragraphs with feature matches and protein mentions remain after computing ranks 1, 2, and 3, we return a ranked list of all PPI pairs identified in every paragraph still in these three ranks."}
{"pdf_id": "0812.1029", "content": "such as 'mitochondri', 'mtHSP70', 'kda', 'endonuclease', and so on. This way, the more generic features extracted from the entire training data to detect protein interaction can be expanded with words that are specific to the context of the article, which can in principle improve the detection of the best sentences to describe protein interaction."}
{"pdf_id": "0812.1029", "content": "Next, for every PPI pair (obtained by IPS rank 1) occurring in a given document, we obtain the words closest to the protein labels in the document's proximity network. Notice that these protein labels are words identified by ABNER for the given PPI pair, and they should appear on the proximity network as regular nodes - unless stemming or other processing breaks them. For each protein pair we selected the five stemmed"}
{"pdf_id": "0812.1029", "content": "words (nodes) in the proximity network with largest mini mum proximity to both protein names. These additional stemmed words were then added to the list of general features obtained from the training data, but only for the respective document. Therefore, each document contains general word features extracted from the entire corpus, plus five specific word features near to each PPI pair in the proximity network. The assumption is that these additional word features endow our method with additional context sensitivity."}
{"pdf_id": "0812.1029", "content": "Word proximity network for document 10464305 Figure 10 Word proximity network for document 10464305. Proximity network of 706 stemmed words produced from document 10464305 [24]. Showing only  edges with proximity weights (formula 8) greater than 0.4. Inset detail showing cluster of highly associated words very related to the specific context of the  article, whose title is 'Stable association of 70-kDa heat shock protein induces latent multisite specificity of a unisite-specific endonuclease in yeast  mitochondria'. Plotted using Pajek"}
{"pdf_id": "0812.1029", "content": "Detail of word proximity network for document 10464305 Figure 11 Detail of word proximity network for document 10464305. Proximity subnetwork of cluster of stemmed words produced from document 10464305 [24].  Showing only edges with proximity weights (Equation 8) greater than 0.4. This cluster shows highly associated words very related to the specific context of  the article, whose title is 'Stable association of 70-kDa heat shock protein induces latent multisite specificity of a unisite-specific endonuclease in yeast  mitochondria'. Plotted using Pajek."}
{"pdf_id": "0812.1029", "content": "Abbreviations ABNER, A Biomedical Named Entity Recognizer; AUC, areaunder the receiver operating characteristic curve; IAS, inter action article subtask; IDF, inverse document frequency; IPS, interaction pair subtask; ISS, interaction sentences subtask; MINT, Molecular Interactions Database; PIARE, ProteinInteraction Abstract Relevance Evaluator; PPI, protein-pro tein interaction; SVD, singular value decomposition; SVD-UI, SVD with uncertainty integration; SVM, support vector machine; TN, true negative; TP, true positive; VTT, variable trigonometric threshold."}
{"pdf_id": "0812.1029", "content": "Acknowledgements We would like to thank Santiago Schnell for graciously providing us with additional proteomics-related articles not containing PPI information. We would also like to thank the FLAD Computational Biology Collaboratorium at the Gulbenkian Institute in Oeiras, Portugal, for hosting and providing facilities used to conduct part of this research. It was at the collaboratorium that we interacted with Florentino Riverola, whose SpamHunting systeminspired our approach to the IAS task, and who was most helpful in discuss ing his system with us. We are also grateful to Indiana University's Research and Technical Services for technical support. The AVIDD Linux Clusters used in our analysis are funded in part by NSF Grant CDA-9601632."}
{"pdf_id": "0812.1029", "content": "Kerrien S, Alam-Faruque Y, Aranda B, Bancarz I, Bridge A, Derow C, Dimmer E, Feuermann M, Friedrichsen A, Huntley R, Kohler C, Khadake J, Leroy C, Liban A, Lieftink C, Montecchi-Palazzi L, Orchard S, Risse J, Robbe K, Roechert B, Thorneycroft D, Zhang Y, Apweiler R, Hermjakob H: IntAct: open source resource for molecular interaction data"}
{"pdf_id": "0812.1340", "content": "After iterative application of averaging filtering to error energy for each disparity, we  selected the disparity (d ), which has minimum error energy  ~( , , ) e i j d  as the most  reliable disparity estimation for pixel  ( , ) i j  of disparity map"}
{"pdf_id": "0812.1340", "content": "b)  Step 3: For every  ( , ) i j  pixel, find the minimum error energy  ~( , , ) e i j d , assign its  disparity index (d ) to  ( , ) d i j  which is called disparity map"}
{"pdf_id": "0812.1340", "content": "VLG , associate this point to region. Otherwise, back to step 1 to find a new root point.  Step 3: Proceed the Step 1 and Step 2 row by row until reaching end point of image.  Grown disparity regions compose of the disparity map  ( , ) d i j .  Figure 2. Method using line growing"}
{"pdf_id": "0812.1340", "content": "Depth Map Generation From Disparity Map:  To better understand depth and disparity relation, let see stereo projection  representation illustrated in the Figure 3. By considering the figure, one can derive  relation between dept ( Z ) and disparity (d ) by using basic geometrical calculations as  following,"}
{"pdf_id": "0812.1340", "content": "Figure 3. Representation of the stereo projection  In order to obtain smoother depth map to be used in applications such as robot  navigation,  5x window sized median filtering should be applied to disparity (d )  before computing dept ( Z ).  Filtering Unreliable Disparity Estimation By Average Error  Thresholding Mechanism:  We define reliability ( R ) of the obtained disparity map d by mean value of the"}
{"pdf_id": "0812.1340", "content": "Disparity map contains some unreliable disparity estimations for some points  around the object boundaries mostly as a result of object occultation in images. These  unreliable disparities can be detected by observing high error energy in the  E . In order  to increase reliability of obtained disparity map  ( , ) d i j , simple thresholding mechanism  , described by equation (7), can be applied to filter some unreliable disparity estimations  in the  ( , ) d i j ."}
{"pdf_id": "0812.1340", "content": "~( , ) d i j  will be the more reliable version of  ( , ) d i j  by filtering some unreliable disparity  estimations. Setting disparity to ne in equation (8) refers \"no-estimated\" state and  ( , ) Ed i j  values that have ne state is excluded in calculation of  R .  S parameter in the"}
{"pdf_id": "0812.1462", "content": "Answer set programming (ASP) is a logic programming paradigm that can be used to solve complex combinatorial search problems. Aggregates are an ASP construct that plays an important role in many applications. Defining a satisfactory semantics of aggregates turned out to be a difficult problem, and in this paper we propose a new approach, based on an analogy between aggregates and propositional connectives. First, we extend the definition of an answer set/stable model to cover arbitrary propositional theories; then wedefine aggregates on top of them both as primitive constructs and as abbrevi ations for formulas. Our definition of an aggregate combines expressiveness and simplicity, and it inherits many theorems about programs with nested expressions, such as theorems about strong equivalence and splitting."}
{"pdf_id": "0812.1462", "content": "The paper is divided into three main parts. We start, in the next section, with the new definition of a stable model for propositional theories, their properties and comparisons with previous definitions of stable models and equilibrium logic. In Section 3 we present our aggregates, their properties and the comparisons with other definitions of aggregates. Section 4 contains all proofs for the theorems of this paper. The paper ends with the conclusions in Section 5. Preliminary reports on some results of this paper were published in [Ferraris, 2005]."}
{"pdf_id": "0812.1462", "content": "where each wi is the amount of money (possibly negative) obtained by accepting bid i, and each ci is the money requested by the junkyard to remove item i. Note that (20) is neither monotone nor antimonotone. We define a solution to Joe's problem as a set of accepted bids such that"}
{"pdf_id": "0812.1462", "content": "of a stable model is equivalent to the definition of a stable model in the senseof [Gelfond and Lifschitz, 1991] (and successive definitions) when applied to dis junctive programs. Next proposition shows a relationship between our concept of an aggregate and FLP-aggregates. An FLP-program is positive if, in each formula (31), p = m.Next proposition shows that our semantics of aggregates is essentially an ex tension of the"}
{"pdf_id": "0812.1462", "content": "Proof. Part (a) is easy to verify by structural induction. Computing the reduct essentially consists of checking satisfaction of subexpressions of each formula of the theory. Each check doesn't require too much time by (a). It remains to notice that each formula with aggregates has a linear number of subformulas."}
{"pdf_id": "0812.1462", "content": "Proof. Let G be F with each monotone aggregate replaced by (15) and each antimonotone aggregate replaced by (16). It is easy to verify that G is a nested ex pression. Nested expressions have all negative occurrences of atoms in the scope of negation, so if Y |= GX then Z |= GX by Lemma (9). It remains to notice that F X and GX are satisfied by the same sets of atoms by Propositions 13 and 12."}
{"pdf_id": "0812.1462", "content": "We have proposed a new definition of stable model — for proposition theories — that is simple, very general, and that inherits several properties from logic programswith nested expressions. On top of that, we have defined the concept of an aggre gate, both as an atomic operator and as a propositional formula. We hope that this very general framework may be useful in the heterogeneous world of aggregates in answer set programming."}
{"pdf_id": "0812.1843", "content": "A new classification of emotions by grouping them into pairs based on certain mental processes underlying these emotions has been proposed. This method ignores the external expression of emotions completely. Elements in each pair are symmetrical with respect to each other in the sense that they contain identical sets of parameters that underlie them except that one element is a negative emotion while the other is a positive emotion. This classification uses these underlying parameters of emotions instead of treating emotions as black boxes. It will be particularly useful for those who want to model emotions in the field of artificial intelligence."}
{"pdf_id": "0812.2535", "content": "In this paper, we deal with usage of MNN concept for:  •  Feature extraction of patterns  •  Mapping the extracted features of the patterns  •  Construction of the pattern recognition  architecture  2: PATTERN RECOGNITION AND MEMORY  MAPPING  We construct a software architecture which does  feature extraction coupled with memory mapping for a  \"pattern recognizer\""}
{"pdf_id": "0812.2535", "content": "and classify it. So we see an MNN does the following  tasks: (i) compresses the input data, (ii) extracts a  suitable feature set characterizing the input pattern and  (iii) has the property to reconstruct the original data  given the compressed data. It may be noted, one MNN  can be used to recognize either one pattern or a  particular pattern from a set of patterns. We use a  MNN as a module of our pattern recognition  architecture."}
{"pdf_id": "0812.2535", "content": "To summarize this section we can say that we  implement the MNN's feature extraction (at level I) on  different kinds of patterns i.e., voice samples besides  image patterns. In addition to usual feature extraction,  at the upper level, the MNN concept is to carry out a"}
{"pdf_id": "0812.2535", "content": "The  grayscales (intensity levels of each pixel) whose range  is from 0 to 255 are rescaled [16] so that they all lie  between -1 to +1, these 510 intensity values constitute  the input vectors for each sample image and are given  as input vectors to MNN II"}
{"pdf_id": "0812.2535", "content": "If we  assume that the sensory input I is related to Sensory  input II, this will happen if the word face is presented  simultaneously with the image of a face, then MNN I  and MNN II can then classify their inputs and put  them in the same group (say group 1 for face),  simultaneously MNN I1-II1 in Level II will be trained  such that the reduced input given to MNN I1-II1  (from the MNN I in Level I) is mapped (matched) to  the reduced input of MNN II at level I"}
{"pdf_id": "0812.2535", "content": "Example, if the input  (garden word, garden image) is fed as data to level I  MNNs then the 20 feature vector (of garden word)  from MNN I is given as input to MNN I3-II3 in Level  II so that its output is equal to the 20 dimensional  feature vector of the garden-image, obtained by data  reduction using MNN II in Level I"}
{"pdf_id": "0812.2535", "content": "into their appropriate group is found to be 91.6% and  95.3% respectively (using only the reduced input  vector of 20 dimensions).  The overall efficiency of recognition, that is, the  rate of correct prediction of a voice input to its  appropriate image output is found to be 91.6%.  Table 1: Pattern recognition and memory mapping  using MNN  Input  to the  system"}
{"pdf_id": "0812.2535", "content": "3: CONCLUSIONS AND FUTURE WORK  We have demonstrated the successful functioning  of an unsupervised learning algorithm which has the  following features: (i) It is hierarchical and modular  (ii) each module runs on a common algorithm, (iii)  capable of automatic data reduction and feature  extraction and (iv) provides an efficient associative  memory map"}
{"pdf_id": "0812.2574", "content": "For  feature selection, successful solutions seem to be  appearance-based approaches, (see [3], [2] for a  survey), which directly operate on images or  appearances of face objects and process the images as  two-dimensional (2-D) holistic patterns, to avoid  difficulties associated with Three-dimensional (3-D)  modelling, and shape or landmark detection [2]"}
{"pdf_id": "0812.2574", "content": "It is generally  believed that, LDA based algorithms outperform PCA  based  ones  in  solving  problems  of  pattern classification, since the former optimizes the low dimensional representation of the objects with focus  on the most discriminant feature extraction while the"}
{"pdf_id": "0812.2574", "content": "The  proposed method is compared, in terms of the  classification error rate performance, to KPCA (kernel  based  PCA),  GDA  (Generalized  Discriminant  Analysis)  and  KDDA  algorithm  with  nearest  neighbour classifier on the multi-view UMIST face  database"}
{"pdf_id": "0812.2574", "content": "The maximization process in (3) is not directly  linked to the classification error which is the criterion  of performance used to measure the success of the FR  procedure. Modified versions of the method, such as  the Direct LDA (D-LDA) approach, use a weighting  function in the input space, to penalize those classes  that  are  close  and  can  potentially  lead  to  misclassifications in the output space."}
{"pdf_id": "0812.2574", "content": "KDDA introduces a nonlinear mapping from the  input space to an implicit high dimensional feature  space, where the nonlinear and complex distribution  of patterns in the input space is \"linearized\" and  \"simplified\" so that conventional LDA can be applied  and it effectively solves the small sample size (SSS)  problem in the high-dimensional feature space by  employing an improved D-LDA algorithm."}
{"pdf_id": "0812.2574", "content": "In GDA, to remove the null space of  WTH , it is  required to compute the pseudo inverse of the kernel  matrix K, which could be extremely ill-conditioned  when certain kernels or kernel parameters are used.  Pseudo inversion is based on inversion of the nonzero  eigenvalues."}
{"pdf_id": "0812.2574", "content": "In practice this criterion is softened to the  minimization of a cost factor involving both the  complexity of the classifier and the degree to which  marginal points are misclassified, and the tradeoff  between these factors is managed through a margin of  error parameter (usually designated C) which is tuned  through cross-validation procedures"}
{"pdf_id": "0812.2574", "content": "The  SVM's  non-parametric  mathematical  formulation allows these transformations to be applied  efficiently and implicitly: the SVM's objective is a  function of the dot product between pairs of vectors;  the substitution of the original dot products with those  computed in another space eliminates the need to  transform the original data points explicitly to the  higher space. The computation of dot products  between vectors without explicitly mapping to another  space is performed by a kernel function."}
{"pdf_id": "0812.2574", "content": "The output value of the decision function of an  SVM is not an estimate of the p.d.f. of a class or the  pair wise probability. One way to estimate the required  information from the output of the SVM decision  function is proposed by (Hastie and Tibshirani, 1996)  The Gaussian p.d.f. of a particular class is estimated  from the output values of the decision function,"}
{"pdf_id": "0812.2574", "content": "The UMIST repository is a multi-view database,  consisting of 575 images of 20 people, each covering  a wide range of poses from profile to frontal views.  Figure 1 depicts some samples contained in the two  databases, where each image is scaled into (112 92),  resulting in an input dimensionality of N = 10304."}
{"pdf_id": "0812.2574", "content": "For the face recognition experiments, in UMIST  database is randomly partitioned into a training set and  a test set with no overlap between the two set. We  used ten images per person randomly chosen for  training, and the other ten for testing. Thus, training  set of 200 images and the remaining 375 images are  used to form the test set."}
{"pdf_id": "0812.2574", "content": "A new FR method has been introduced in this  paper. The proposed method combines kernel-based  methodologies with discriminant analysis techniques  and SVM classifier. The kernel function is utilized to  map the original face patterns to a high-dimensional  feature space, where the highly non-convex and  complex distribution of face patterns is simplified, so  that linear discriminant techniques can be used for  feature extraction."}
{"pdf_id": "0812.2574", "content": "Then feature space will be fed to SVM classifier.  Experimental results indicate that the performance of  the KDDA algorithm together with SVM is overall  superior to those obtained by the KPCA or GDA  approaches. In conclusion, the KDDA mapping and  SVM classifier is a general pattern recognition method  for  nonlinearly  feature  extraction  from high dimensional input patterns without suffering from the  SSS problem."}
{"pdf_id": "0812.2575", "content": "Given a set of training samples, AdaBoost  [Schapire and Singer 1999] maintains a probability  distribution, W, over these samples. This distribution  is initially uniform. Then, AdaBoost algorithm calls  Weak Learn algorithm repeatedly in a series of  cycles. At cycle T, AdaBoost provides training"}
{"pdf_id": "0812.2575", "content": "applied efficiently and implicitly: the SVM's  objective is a function of the dot product between  pairs of vectors; the substitution of the original dot  products with those computed in another space  eliminates the need to transform the original data  points  explicitly  to  the  higher  space.  The  computation of dot products between vectors  without explicitly mapping to another space is  performed by a kernel function.  The nonlinear projection of the data is performed  by this kernel functions. There are several common  kernel functions that are used such as the linear,"}
{"pdf_id": "0812.2575", "content": "We tested our system on the MIT+CMU frontal  face test set [Rowley et al. 1994] and own database.  There are more than 2,500 faces in total. To train the  detector, a set of face and nonface training images  were used. The pairwise recognition framework is  evaluated on a compound face database with 2000  face images hand labelled faces scaled and aligned  to a base resolution 32 by 32 pixels by the centre  point of the two eyes and the horizontal distance  between the two eyes. For non-face training set, an  initial 10,000 non-face samples were selected  randomly from 15,000 large images which contain  no face."}
{"pdf_id": "0812.2575", "content": "The SVM-based component classifier and  AdaBoost algorithm are used for the classification of  each pair of individuals. We compare the detection  rates to other commonly used Adaboost methods,  such as Decision Trees and Neural Networks, on  face database.  For showing the performance of our AdaBoosted  svm-based component classifier algorithm, the  results are shown in Table 1.  False detections  Detector"}
{"pdf_id": "0812.2785", "content": "Prediction of Platinum Prices  Using Dynamically Weighted Mixture of Experts  Baruch Lubinsky, Bekir Genc and Tshilidzi Marwala  University of the Witwatersrand  Private Bag x3  Wits, 2050, South Africa  Abstract—Neural  networks  are  powerful  tools  for  classification and regression in static environments"}
{"pdf_id": "0812.2785", "content": "network has a vector of weights corresponding to each  region. These weights are adjusted during training. For each  sample, if the network classifies correctly, the relevant  weight is multiplied by 1.2 otherwise it is multiplied by 0.4.  These values are found to give weights that are constrained  to reasonable values. When the ensemble is tested, the  output is then the weighted average of the output of each  network, according the weights calculated."}
{"pdf_id": "0812.2785", "content": "These results show that the performance of an ensemble  is improved by giving more strength to the output of a  network that has better accuracy. The performance of the  ensemble is improved even further when the input space is  divided and weights are assigned for each region. These  regions need not divide the different classes perfectly to be  effective. The regions in figure 1 are separated along the  median of each feature which proves to be an adequate  method for defining the regions. This test shows that the  divisions in the input space need not represent any complex"}
{"pdf_id": "0812.2785", "content": "make accurate predictions, the weights of the networks  must be adjusted for the current market situation. This  method relies on the assumption that the factors influencing  the price of platinum exist in a bounded space and vary  slowly.  After each sample becomes known the weights are  recalculated for the 10 previous samples. It is not necessary  to retain the past input data, as each network's output will  not change. The most recent sample is given the most  significance as shown in figure 2."}
{"pdf_id": "0812.2785", "content": "The ensembles with constantly updated weights  (Dynamic Weight) clearly outperform the ensembles which  are un-weighted or statically weighted. The statically  weighted ensembles are weighted at the start of the test  period, but those weightings remain fixed. This gives an  advantage in the short term, but over a longer time period  does not improve the performance at all.  The results of table II are achieved by ensembles in  which each network is trained for 20 epochs. Increasing this  period to 40 epochs improves the performance of the  dynamically weighted ensembles to 0.4069 over 11 weeks."}
{"pdf_id": "0812.2892", "content": "methods [7]. In the SCA context, m sparse sources  (which the most of their samples are nearly zero)  and n linear observations of them are available.  The goal is to find these sparse sources from the  observations. The relation between the sources and  the observations are:  x = As (1)"}
{"pdf_id": "0812.2892", "content": "From equations (6), (7), (9) and the preceding  discussion, the matrix H is  1: ,1: where we use MATLAB matrix notation. The  matrix G is obtained simply from equation (11)  and knowing that the DCT transform is separable  of the form ( , , , ) ( , ) ( , ) t x y u v = t x u t y v . So, we have:"}
{"pdf_id": "0812.2892", "content": "4.1  Random-valued impulsive noise  In this experiment, random valued impulsive noise  with different levels is added to the image. The  results of the simulations are shown in Fig. 3. As  we can see the combination of the methods has the  best result in high level of noise (30% to 60%"}
{"pdf_id": "0812.2892", "content": "Fig. 9 The result for the missing sample  experiment  5.  Conclusion  In this paper, a novel method is proposed to  remove impulsive noise from images. This method  is essentially based on the sparsity of the images in  the DCT domain. Using the nearly zeros in the  DCT domain, an exact equation is provided to  recover the impulse noises (or errors). To solve"}
{"pdf_id": "0812.2892", "content": "this equation, the smoothed- 0l method [11] is  utilized. In addition, in the simple case of fixed  gray level salt and pepper noise, we present a new  version of our method. To obtain better results  when high level of noise is present, a combination  of our SCA method with traditional median  filtering is suggested. The simulation results show  the efficiency of our method in the three cases of  impulsive noise (random-value, fixed salt and  pepper and missing sample)."}
{"pdf_id": "0812.3478", "content": "The need for domain ontologies in mission critical applications such as risk management and hazard identification is becoming more and more pressing. Most research on ontology learning conducted in the academia remains unrealistic for real-world applications. One of the main problems is the dependence on non-incremental, rare knowledge and textual resources, and manually-crafted patterns and rules. This paper reports work in progress aiming to address such undesirable dependencies during ontology construction. Initial experiments using a working prototype of the system revealed promising potentials in automatically constructing high-quality domain ontologies using real-world texts."}
{"pdf_id": "0812.3478", "content": "• The term candidates in this evaluation were automatically extracted from real-world texts without human intervention. The text processing phase and specifically, the extraction of term candidates have errors of their own (e.g. incorrect noun phrase chunking). Such errors will inevitably propagate to the next phase of term recognition."}
{"pdf_id": "0812.3478", "content": "• \"hazard indices\" was not extracted as part of any frame due to its absence from the textbook. Possible related terms such as \"chemical exposure index\" and \"instantaneous fractional annual loss\" have less than ten occurrences in the book and were excludedfrom the 4, 000 frames. Other terms such as \"runaway reaction hazard index\" and \"mor tality index\" which could help in discovering a generalised concept do not appear in the textbook. Another useful term \"fire and explosion index\", which was mentioned in the book, was not included for term recognition as a complete term due to an error with noun phrase chunking during text processing. The term was extracted as two separate parts \"fire\" and \"explosion index\" in the 4, 000 frames."}
{"pdf_id": "0812.3563", "content": "Abstract This paper provides an introduction to the Text Encoding Initia tive (TEI), focused at bringing in newcomers who have to deal  with a digital document project and are looking at the capacity that  the TEI environment may have to fulfil his needs. To this end, we  avoid a strictly technical presentation of the TEI and concentrate  on the actual issues that such projects face, with parallel made on  the situation within two institutions. While a quick walkthrough  the TEI technical framework is provided, the papers ends up by  showing the essential role of the community in the actual technical  contributions that are being brought to the TEI."}
{"pdf_id": "0812.3563", "content": "Introduction Most scholars in the humanities who have been in the situation of man aging a textual source in digital format are aware of the existence of the  TEI (Text Encoding Initiative, www.tei-c.org) as a possible background  for its actual computer representation. Still there is quite a proportion of  such scholars who would intuitively consider the TEI as not being fully  appropriate for them, and sometimes even fearing that adopting the TEI may cause more trouble then benefit to their research project. This usu ally stems from a perception of the TEI as being both overly complex  and at the same time under-empowered for dealing with the specificities  of one's precise research."}
{"pdf_id": "0812.3563", "content": "We will thus try to see how the  TEI may provide a valuable context for textual projects, identifying the  first steps to go through to make an easy start with it, together with some  practicalities that may just help any one to edit its first document within a  quarter of an hour"}
{"pdf_id": "0812.3563", "content": "Finally, I would want this paper to be an opportunity to demonstrate that the TEI exists because it has been put together not so much by techies, but by scholars themselves who, over the last twenty years, con stantly tried to find the best compromise between scientific expectations  and technical constraints"}
{"pdf_id": "0812.3563", "content": "1  See  for  instance  projects  like  the  BNC (http://www.natcorp.ox.ac.uk/) or DTA (http://www.deutsches textarchiv.de/).  2 A document formatting system for the TeX typesetting program.  See http://www.latex-project.org/  3 A series of DTDs designed for the National Library of Medicine  for  the  representation  of  journal  article  (see  http://dtd.nlm.nih.gov/)"}
{"pdf_id": "0812.3563", "content": "Editorial workflow  The usual trade-off for such a document type is to be able to provide  coherent editorial guidelines, when, at the same time, the researchers are  producing the content all by themselves and may thus introduce or even  impose their own peculiarities. In particular, since the computer science community has a long-standing relationship with TeX, this rather pre sentational format has been chosen as the \"natural\" source format for  authors'. The chapters, once proofread and finalized are then converted  into an XML structure for archival and dissemination. Besides, some of  the bibliographical information can — and in the long term, must — be"}
{"pdf_id": "0812.3563", "content": "Main characteristics of the documents  ISO standards have a strict document organisation9, which reflects the necessity for clearly identifying components such as scope, terms and definitions, normative documents, etc. They also come with a precise meta-data description stating the document title(s), the technical committee responsi ble for the preparation of the standard, the publication information  (date, copyright, etc.). Besides, the variety of technical fields covered by  ISO imposes that the content itself may contain many different types of  objects such as graphics, formulas, technical drawings or specification code. In a way, ISO documentary base could be seen as the ideal play ground for anyone who is interested in technical documentation."}
{"pdf_id": "0812.3563", "content": "perts in their own technical fields, do not have specific IT background  beyond the basic usage of a word processor. As a result, most standard  editing activities are operated in Microsoft Word with documents being  disseminated as PDF's when ballots are taking place. At the final stage of  the standard production phase the ISO central secretariat is manually  converting the available document to produce an XML document to be  integrated into the main ISO document management system."}
{"pdf_id": "0812.3563", "content": "Overview  The two projects briefly presented here are indeed typical cases where  institutions are faced with the necessity to define a document format,  which will be used for a large number of documents over a rather long  period. This implies that the underlying document format, or schema, has to be reliably defined in such way that it is easy to be used, main tained and that it comes with a clear documentation. In the course of this  paper we will see whether the TEI can offer such a framework and relate  this analysis the actual history of both institutions in their endeavour to  define such a format."}
{"pdf_id": "0812.3563", "content": "History After a period during which INRIA annual reports were completely edited as Tex documents, it became clear that the definition of a production line involving multiple output formats together with web accessibil ity would require the use of a more content oriented format. XML very  soon came up as the unavoidable choice, in particular in the context of  INRIA being one of the three academic pillars of the W3C in the late  1990s. At that time, the importance of fully situating oneself within a  standardised framework was not seen as a deep priority, in particular since the development of the underlying document scheme was itera"}
{"pdf_id": "0812.3563", "content": "Difficulties The constant evolution of the document structure, together with the re sulting lack of maintained documentation, created a situation where, first,  tools had to be systematically updated to cope with the changes, and  second, changes were made as small as possible (in the form of  \"patches\") so that the whole editorial workflow would not break and  prevent a timely production of the annual reports. The situation was  made even worse when it was contemplated to refine the content to be able to produce precise research production indicators needed for insti tutional assessment."}
{"pdf_id": "0812.3563", "content": "Perspectives  Given the context expressed so far and the difficulties that INRIA would  face in changing its editorial workflow in haste, the best strategy that has  been identified is to actually design a target document format, that is, an  ideal document format (thus departing from the patch-syndrome) at  which a corresponding evolution plan could aim. As a matter of fact it  has been identified that the current document structure could be easily  mapped onto a subset of the TEI guidelines and that by doing so, one  could progressively switch older tools into TEI-aware components."}
{"pdf_id": "0812.3563", "content": "History  Because of the need to provide precise access to standard document  content, ISO introduced at a very early stage an SGML11 back-office  document structure. This allowed standards to be precisely checked at  production time and potentially be fully exploited at a very fine-grained level of representation. The underlying document type definition was de fined as a fully proprietary format closely sticking to ISO constraints.  When XML came into play, the format was made compliant to the XML  syntax without any major changes in its element set."}
{"pdf_id": "0812.3563", "content": "Difficulties One constant feeling in ISO is that there has always been a strong discrepancy between the editing process of standards within ISO commit tees and the final production line. In particular, nothing facilitates the conversion of committee-produced documents into the ISO XML structure. Besides, just like for INRIA, the proprietary nature of the ISO for mat induced difficulties both of documentation maintenance and tool  update when new features would come into play (for instance when new  technical domains would be tackled within ISO)."}
{"pdf_id": "0812.4296", "content": "Nowadays, the idea of nonextensivity has been used in many applications. Nonex tensive statistical mechanics has been applied successfully in physics (astrophysics,astronomy, cosmology, nonlinear dynamics) [26,27], biology [41], economics [38], hu man and computer sciences [1,4,2,39] and provide interesting insights into a variety of physical systems (two-dimensional turbulence in pure-electron plasma [10], variety of self-organized critical models [33], long-range interaction conservative systems [3], and among others [42]). Thomson ISI Web of Science [14] is a widely used database source for such works."}
{"pdf_id": "0812.4296", "content": "We remind that extremizing entropy Sq under appropriate constraints we obtain a probability distribution, which is proportional to q-exponential function. In this work, we focus on the analysis of the distribution of citations of scientificpublication, more precisely those that have been catalogued by the Institute for Sci entific Information (ISI). In [2000] Tsallis and Albuquerque [37] suggested that the citation phenomenon appears to be deeply related to thermostatistical nonextensivity."}
{"pdf_id": "0812.4296", "content": "However, they conclude that it is important to understand what physical mechanism of the nonlinear dynamics of this phenomenon is responsible for the specific values of q, which fit the experimental data. In their discussion they tried to understand whya stretched exponential form does not fit the entire experimental range when cita tions per paper were focused, whereas it appears to be satisfactory when citations per scientist were focused instead."}
{"pdf_id": "0812.4360", "content": "I argue that data becomes temporarily interesting by itself to some self-impro ving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and morebeautiful. Curiosity is the desire to create or discover more non-random, non arbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and (since 1990) artificial systems."}
{"pdf_id": "0812.4360", "content": "First version of this preprint published 23 Dec 2008; revised 15 April 2009. Short version: [91]. Long version: [90]. We distill some of the essential ideas in earlier work (1990-2008) on this subject: [57, 58, 61, 59, 60, 108, 68, 72, 76] and especially recent papers [81, 87, 88, 89]."}
{"pdf_id": "0812.4360", "content": "Therefore physicists have traditionally proceeded incrementally, analyzing just a small aspect of the world at any given time, trying to find simple laws that allow for describing their limitedobservations better than the best previously known law, essentially trying to find a pro gram that compresses the observed data better than the best previously known program"}
{"pdf_id": "0812.4360", "content": "Although its predictive power is limited—for example, it does not explain quantum nuctuations of apple atoms—it still allows for greatly reducing the number of bits required to encode the data stream, by assigning short codes to events that are predictable with high probability [28] under the assumption that the law holds"}
{"pdf_id": "0812.4360", "content": "Since short and simple explanations of the past usually renect some repetitive regularity that helps to predict the future as well, every intelligent system interested in achieving future goals should be motivated to compress the history of raw sensory inputs in response to its actions, simply to improve its ability to plan ahead"}
{"pdf_id": "0812.4360", "content": "A long time ago, Piaget [49] already explained the explorative learning behav ior of children through his concepts of assimilation (new inputs are embedded in old schemas—this may be viewed as a type of compression) and accommodation (adaptingan old schema to a new input—this may be viewed as a type of compression improve ment), but his informal ideas did not provide enough formal details to permit computerimplementations of his concepts"}
{"pdf_id": "0812.4360", "content": "(1990-2008) [57, 58, 61, 59, 60, 108, 68, 72, 76, 81, 88, 87, 89] to make the agent dis cover data that allows for additional compression progress and improved predictability.The framework directs the agent towards a better understanding the world through ac tive exploration, even when external reward is rare or absent, through intrinsic rewardor curiosity reward for actions leading to discoveries of previously unknown regulari ties in the action-dependent incoming data stream."}
{"pdf_id": "0812.4360", "content": "2 will informally describe our algorithmic framework based on: (1) a contin ually improving predictor or compressor of the continually growing data history, (2) a computable measure of the compressor's progress (to calculate intrinsic rewards), (3) areward optimizer or reinforcement learner translating rewards into action sequences ex pected to maximize future reward"}
{"pdf_id": "0812.4360", "content": "The basic ideas are embodied by the following set of simple algorithmic principles distilling some of the essential ideas in previous publications on this topic [57, 58, 61, 59, 60, 108, 68, 72, 76, 81, 88, 87, 89]. As mentioned above, formal details are left to the Appendix. As discussed in Section 2, the principles at least qualitatively explain many aspects of intelligent agents such as humans. This encourages us to implement and evaluate them in cognitive robots and other artificial systems."}
{"pdf_id": "0812.4360", "content": "2. Improve subjective compressibility. In principle, any regularity in the data history can be used to compress it. The compressed version of the data can be viewed as its simplifying explanation. Thus, to better explain the world, spend some of the computation time on an adaptive compression algorithm trying to partially compress the data. For example, an adaptive neural network [8] maybe able to learn to predict or postdict some of the historic data from other his toric data, thus incrementally reducing the number of bits required to encode the whole. See Appendix A.3 and A.5."}
{"pdf_id": "0812.4360", "content": "3. Let intrinsic curiosity reward renect compression progress. The agent should monitor the improvements of the adaptive data compressor: whenever it learns toreduce the number of bits required to encode the historic data, generate an intrin sic reward signal or curiosity reward signal in proportion to the learning progress or compression progress, that is, the number of saved bits. See Appendix A.5 and A.6."}
{"pdf_id": "0812.4360", "content": "4. Maximize intrinsic curiosity reward [57, 58, 61, 59, 60, 108, 68, 72, 76, 81, 88, 87]. Let the action selector or controller use a general Reinforcement Learning (RL) algorithm (which should be able to observe the current state of the adaptive compressor) to maximize expected reward, including intrinsic curiosity reward. To optimize the latter, a good RL algorithm will select actions that focus the agent's attention and learning capabilities on those aspects of the world that allow for finding or creating new, previously unknown but learnable regularities. In other words, it will try to maximize the steepness of the compressor's learning curve. This type of active unsupervised learning can help to figure out how the world works. See Appendix A.7, A.8, A.9, A.10."}
{"pdf_id": "0812.4360", "content": "The framework above essentially specifies the objectives of a curious or creative system, not the way of achieving the objectives through the choice of a particularadaptive compressor or predictor and a particular RL algorithm. Some of the possi ble choices leading to special instances of the framework (including previous concrete implementations) will be discussed later."}
{"pdf_id": "0812.4360", "content": "Of course, the real goal of many cognitive systems is not just to satisfy their curiosity, but to solve externally given problems. Any formalizable problem can be phrased as an RL problem for an agent living in a possibly unknown environment, trying to maximize the future external reward expected until the end of its possibly finite lifetime. The new millennium brought a few extremely general, even universal RL algorithms (universal problem solvers or universal artificial intelligences—see Appendix A.8, A.9) that are optimal in various theoretical but not necessarily practical senses, e. g., [29, 79, 82,"}
{"pdf_id": "0812.4360", "content": "They leave open an essential remaining question: If the agent can execute only a fixed number of computational instructions per unit time interval (say, 10 trillion elementary operations per second), what is the best way of using them to get as close as possible to the recent theoretical limits of universal AIs, especially when external rewards are very rare, as is the case in many realistic environments? The premise of this paper is that the curiosity drive is such a general and generally useful concept for limited-resource RL in rare-reward environments that it should be prewired, as opposed to be learnt from scratch, to save on (constant but possibly still huge) computation time"}
{"pdf_id": "0812.4360", "content": "An inherent assumption of this approach is that in realistic worlds a better explanation of the past can only help to better predict the future, and to accelerate the search for solutions to externally given tasks, ignoring the possibility that curiosity may actually be harmful and \"kill the cat"}
{"pdf_id": "0812.4360", "content": "There is one thing that is involved in all actions and sensory inputs of the agent, namely, the agent itself. To efficiently encode the entire data history, it will profit from creating some sort of internal symbol or code (e. g., a neural activity pattern) representing the agent itself. Whenever this representation is actively used, say, by activating the"}
{"pdf_id": "0812.4360", "content": "corresponding neurons through new incoming sensory inputs or otherwise, the agent could be called self-aware or conscious.This straight-forward explanation apparently does not abandon any essential as pects of our intuitive concept of consciousness, yet seems substantially simpler than other recent views [1, 2, 105, 101, 25, 12]. In the rest of this paper we will not have to attach any particular mystic value to the notion of consciousness—in our view, it is justa natural by-product of the agent's ongoing process of problem solving and world mod eling through data compression, and will not play a prominent role in the remainder of this paper."}
{"pdf_id": "0812.4360", "content": "What's beautiful is not necessarily interesting. A beautiful thing is interesting only as long as it is new, that is, as long as the algorithmic regularity that makes it simple has not yet been fully assimilated by the adaptive observer who is still learning to compress the data better. It makes sense to define the time-dependent subjective Interestingness I(D, O(t)) of data D relative to observer O at time t by"}
{"pdf_id": "0812.4360", "content": "the first derivative of subjective beauty: as the learning agent improves its compression algorithm, formerly apparently random data parts become subjectively more regular and beautiful, requiring fewer and fewer bits for their encoding. As long as this process is not over the data remains interesting and rewarding. The Appendix and Section 3 on previous implementations will describe details of discrete time versions of this concept. See also [59, 60, 108, 68, 72, 76, 81, 88, 87]."}
{"pdf_id": "0812.4360", "content": "Note that our above concepts of beauty and interestingness are limited and pristinein the sense that they are not a priori related to pleasure derived from external re wards (compare Section 1.3). For example, some might claim that a hot bath on a cold day triggers \"beautiful\" feelings due to rewards for achieving prewired target values of external temperature sensors (external in the sense of: outside the brain which is controlling the actions of its external body). Or a song may be called \"beautiful\" foremotional (e.g., [13]) reasons by some who associate it with memories of external plea sure through their first kiss. Obviously this is not what we have in mind here—we are focusing solely on rewards of the intrinsic type based on learning progress."}
{"pdf_id": "0812.4360", "content": "Consider two extreme examples of uninteresting, unsurprising, boring data: A vision based agent that always stays in the dark will experience an extremely compressible, soon totally predictable history of unchanging visual inputs. In front of a screen fullof white noise conveying a lot of information and \"novelty\" and \"surprise\" in the tra ditional sense of Boltzmann and Shannon [102], however, it will experience highlyunpredictable and fundamentally incompressible data. In both cases the data is boring [72, 88] as it does not allow for further compression progress. Therefore we re ject the traditional notion of surprise. Neither the arbitrary nor the fully predictable is truly novel or surprising—only data with still unknown algorithmic regularities are [57, 58, 61, 59, 60, 108, 68, 72, 76, 81, 88, 87, 89]!"}
{"pdf_id": "0812.4360", "content": "Generally speaking we may say that a major goal of traditional unsupervised learning is to improve the compressionof the observed data, by discovering a program that computes and thus explains the his tory (and hopefully does so quickly) but is clearly shorter than the shortest previously known program of this kind"}
{"pdf_id": "0812.4360", "content": "We have to extend it along the dimension of active action selection, since our unsupervised learner must also choose the actions that innuence the observed data, just like a scientist chooses his experiments, a baby itstoys, an artist his colors, a dancer his moves, or any attentive system [96] its next sen sory input"}
{"pdf_id": "0812.4360", "content": "Works of art and music may have important purposes beyond their social aspects [3] despite of those who classify art as supernuous [50]. Good observer-dependent artdeepens the observer's insights about this world or possible worlds, unveiling previ ously unknown regularities in compressible data, connecting previously disconnected patterns in an initially surprising way that makes the combination of these patterns subjectively more compressible (art as an eye-opener), and eventually becomes known and less interesting. I postulate that the active creation and attentive perception of all kinds of artwork are just by-products of our principle of interestingness and curiosity yielding reward for compressor improvements."}
{"pdf_id": "0812.4360", "content": "Hence any objective theory of what is good art must take the subjective observer as a parameter, to answer questions such as: Which sequences of actions and resulting shifts of attention should he execute to maximize his pleasure? According to our principle he should select one that maximizes the quickly learnable compressibility that is new, relative to his current knowledge and his (usually limited) way of incorporating / learning / compressing new data"}
{"pdf_id": "0812.4360", "content": "Some of the previous attempts at explaining aesthetic experiences in the context of information theory [7, 41, 6, 44] emphasized the idea of an \"ideal\" ratio between expected and unexpected information conveyed by some aesthetic object (its \"order\" vs its \"complexity\"). Note that our alternative approach does not have to postulate an objective ideal ratio of this kind. Instead our dynamic measure of interestingness renects the change in the number of bits required to encode an object, and explicitly takes into account the subjective observer's prior knowledge as well as the limitations of its compression improvement algorithm."}
{"pdf_id": "0812.4360", "content": "the progress in terms of intrinsic reward without being able to say exactly which of his memories became more subjectively compressible in the process. The framework in the appendix is sufficiently formal to allow for implementation of our principle on computers. The resulting artificial observers will vary in terms of the computational power of their history compressors and learning algorithms. This will innuence what is good art / science to them, and what they find interesting."}
{"pdf_id": "0812.4360", "content": "Just like other entertainers and artists, comedians also tend to combine well-known concepts in a novel way such that the observer's subjective description of the result is shorter than the sum of the lengths of the descriptions of the parts, due to some previously unnoticed regularity shared by the parts"}
{"pdf_id": "0812.4360", "content": "All of this makes perfect sense within our algorithmic framework: such grins presumably are triggered by intrinsic reward for generating a data stream with previously unknown regularities, such as the sensory input sequence corresponding to observing oneself juggling, which may be quite different from the more familiar experience of observing somebody elsejuggling, and therefore truly novel and intrinsically rewarding, until the adaptive pre dictor / compressor gets used to it"}
{"pdf_id": "0812.4360", "content": "As mentioned earlier, predictors and compressors are closely related. Any type of par tial predictability of the incoming sensory data stream can be exploited to improve the compressibility of the whole. Therefore the systems described in the first publicationson artificial curiosity [57, 58, 61] already can be viewed as examples of implementa tions of a compression progress drive."}
{"pdf_id": "0812.4360", "content": "Early work [57, 58, 61] described a predictor based on a recurrent neural network [115, 120, 55, 62, 47, 78] (in principle a rather powerful computational device, even by today's machine learning standards), predicting sensory inputs including reward signals from the entire history of previous inputs and actions. The curiosity rewards were proportional to the predictor errors, that is, it was implicitly and optimistically assumed that the predictor will indeed improve whenever its error is high."}
{"pdf_id": "0812.4360", "content": "Recently several researchers also implemented variants or approximations of the cu riosity framework. Singh and Barto and coworkers focused on implementations withinthe option framework of RL [5, 104], directly using prediction errors as curiosity rewards as in Section 3.1 [57, 58, 61] —they actually were the ones who coined the ex pressions intrinsic reward and intrinsically motivated RL. Additional implementations were presented at the 2005 AAAI Spring Symposium on Developmental Robotics [9]; compare the Connection Science Special Issue [10]."}
{"pdf_id": "0812.4360", "content": "Figure 2 provides another example: a butterny and a vase with a nower. It can be specified by very few bits of information as it can be constructed through a very simple procedure or algorithm based on fractal circle patterns [67]—see Figure 3. People who understand this algorithm tend to appreciate the drawing more than those who do not. They realize how simple it is. This is not an immediate, all-or-nothing, binary process though. Since the typical human visual system has a lot of experience with circles, most people quickly notice that the curves somehow fit together in a regular way. But few are able to immediately state the precise geometric principles underlying the drawing"}
{"pdf_id": "0812.4360", "content": "[81]. This pattern, however, is learnable from Figure 3. The conscious or subconscious discovery process leading from a longer to a shorter description of the data, or from less to more compression, or from less to more subjectively perceived beauty, yields reward depending on the first derivative of subjective beauty, that is, the steepness of the learning curve."}
{"pdf_id": "0812.4360", "content": "The crucial ingredients of the corre sponding formal framework are (1) a continually improving predictor or compressorof the continually growing data history, (2) a computable measure of the compressor's progress (to calculate intrinsic rewards), (3) a reward optimizer or reinforce ment learner translating rewards into action sequences expected to maximize future reward"}
{"pdf_id": "0812.4360", "content": "To improve our previous implementations of these ingredients (Section 3), we will (1) study better adaptive compressors, in particular, recent, novel RNNs [94]and other general but practically feasible methods for making predictions [75]; (2) in vestigate under which conditions learning progress measures can be computed bothaccurately and efficiently, without frequent expensive compressor performance evalu ations on the entire history so far; (3) study the applicability of recent improved RL techniques in the fields of policy gradients [110, 119, 118, 56, 100, 117], artificial evolution [43, 20, 21, 19, 22, 23, 24], and others [71, 75]"}
{"pdf_id": "0812.4360", "content": "So we conceptually separate the goal (explaining / compressing the history) from themeans of achieving the goal. Once the goal is formally specified in terms of an algo rithm for computing curiosity rewards, let the controller's reinforcement learning (RL) mechanism figure out how to translate such rewards into action sequences that allow the given compressor improvement algorithm to find and exploit previously unknown types of compressibility."}
{"pdf_id": "0812.4360", "content": "The previous sections only discussed measures of compressor performance, but not ofperformance improvement, which is the essential issue in our curiosity-oriented con text. To repeat the point made above: The important thing are the improvements ofthe compressor, not its compression performance per se. Our curiosity reward in re sponse to the compressor's progress (due to some application-dependent compressor improvement algorithm) between times t and t + 1 should be"}
{"pdf_id": "0812.4360", "content": "3. Let some (application-dependent) compressor improvement algorithm (such asa learning algorithm for an adaptive neural network predictor) use hold to ob tain a hopefully better compressor pnew (such as a neural net with the same sizebut improved prediction capability and therefore improved compression perfor mance [95]). Although this may take many time steps (and could be partially performed during \"sleep\"), pnew may not be optimal, due to limitations of the learning algorithm, e.g., local maxima."}
{"pdf_id": "0812.4360", "content": "Obviously this asynchronuous scheme may cause long temporal delays between con troller actions and corresponding curiosity rewards. This may impose a heavy burdenon the controller's RL algorithm whose task is to assign credit to past actions (to in form the controller about beginnings of compressor evaluation processes etc., we may augment its input by unique representations of such events). Nevertheless, there are RL algorithms for this purpose which are theoretically optimal in various senses, to be discussed next."}
{"pdf_id": "0812.4360", "content": "[90] J. Schmidhuber. Driven by compression progress: A simple principle explainsessential aspects of subjective beauty, novelty, surprise, interestingness, atten tion, curiosity, creativity, art, science, music, jokes. In G. Pezzulo, M. V. Butz, O. Sigaud, and G. Baldassarre, editors, Anticipatory Behavior in Adaptive Learning Systems, from Sensorimotor to Higher-level Cognitive Capabilities, LNAI. Springer, 2009. In press."}
{"pdf_id": "0812.4460", "content": "(or an N -tier variation of it), where the user profile infor mation and recommendation engine are centralized. However, the Semantic Web vision [4] that we share is more likely to be based on decentralized architectures, like the ones provided by peer-to-peer (P2P) overlay networks, where agents would interact via free information exchangeor trading. We present an alternative to centralized collab orative filtering, exploiting the advantages of peer-to-peer networks."}
{"pdf_id": "0812.4460", "content": "We introduce Swarmix, a distributed architecture (Fig ure 1) whose epidemic-style protocol is responsible for the overlay P2P network construction and maintenance. Theprotocol is able to associate each peer v with a fixed num ber of highly similar neighbors whose similarity with respectto v improves during the perpetual execution of the proto col. Each peer v runs a recommender system locally and is in control of its profile and ratings; v's recommendations are computed using only its peers; which requires no globalknowledge of the network or access to a central server re sponsible for storing or computation. The rest of the paper is organized as follows. In Section"}
{"pdf_id": "0812.4460", "content": "2, we present a general model shared by epidemic-style pro tocols based on a push-pull mechanism. In Section 3, weintroduce the Swarmix protocol at the core of our architec ture. The distributed recommender system implementationis presented in Section 4. In Section 5, we present the ex perimental setup and evaluation metric used. In Section 6, we report our experimental results. In Section 7, we pointto some related work. Finally, Section 8 presents our con clusions and future research."}
{"pdf_id": "0812.4460", "content": "Initially, each peer may have some data, and new data or new versions of old data may enter the system through any peer, at any time. Data is transmitted through the network by exchanging and merging the caches of two neighboring peers v and w with the goal to maximize the utility of eachpeer's cache, conforming to some constraints as, e.g., a max imal cache size. As several copies and versions of the same data may pile up during runtime at each peer, we need some method for duplicate elimination"}
{"pdf_id": "0812.4460", "content": "As selection function as well as for neighborhood selection we opted for retaining a fixed number k of most useful peers as described above already. As the size of the cache and the size of the neighborhood are the same, the neighbors are just the peers specified by the Swarmix items in the updated cache after one round of the protocol."}
{"pdf_id": "0812.4460", "content": "4. RECOMMENDATION ALGORITHM The problem space of automated collaborative filtering can be formulated as a matrix R of users versus items. Each cell of the matrix R represents a user's rating on a specific item, and each row corresponds to a user profile. The task of the recommender, under this formulation, is to predict values for specific, empty cells; i.e., to predict a user's rating for a not-yet-rated item. A neighborhood-based collaborative filtering recommender system comprises the three fundamental steps described by Herlocker et al. [12]:"}
{"pdf_id": "0812.4460", "content": "3. Aggregation and prediction computation. The active user's profiles are aggregated computing the union of consumed items. The system also removes items already consumed by the active user, in order to guarantee that just new items are recommended.A weight is associated to each item based on its im portance in the aggregation; consequently, the best N items, having the highest weights, are reported to the active user as the final recommendations."}
{"pdf_id": "0812.4460", "content": "where by abuse of notation v and w denote the respective rating profiles of peers v and w. Alternatively, any other similarity measure proposed in the literature could be used, e.g., Pearson correlation, Spearman rank, etc. Finally, each peer is able to compute its recommendationlist based on its neighborhood, that is, through its cache entries. For our architecture, we have implemented the most frequent items approach suggested by Sarwar et al. [18]. Their technique can be seen as a majority voting election scheme, were each of the members of peer v's neighborhood casts a vote for each of the items he has consumed. Those N"}
{"pdf_id": "0812.4460", "content": "5. EXPERIMENT OUTLINETo evaluate the result of the top-N (with N =10) rec ommendations provided by our distributed architecture, wesplit the dataset into training and test set by randomly se lecting a single rating (a hidden item) for each user to bepart of the test set, and used the remaining ratings for train ing. Breese et al. [5] called this kind of experimental setup all-but-1 protocol. The nearest neighbors and top-10 recommendations were computed using the training set only. The quality was measured by looking at the number of hits, which corresponds to the number of items in the test set that were also present in the top-N recommended items returned for each peer. More formally, hit-rate, is defined as"}
{"pdf_id": "0812.4460", "content": "6.2 Recommendation Quality Next, we look at the hit-rate score, which help us evaluate whether the system is making recommendations for items that the peers will recognize and value.The hit-rate for the pure-CF recommender implementa tion is presented in Figure 5.In looking at the figure one can observe how the recom mendation quality improves over time, as a consequence of the intra-neighborhood similarity improvement. The seriesshows that for the Swarmix architecture, the hit-rate mea sure is nearly equal to the central server's."}
{"pdf_id": "0812.4460", "content": "Failures. We perform these experiments considering that a peer v, disconnected from the network as a consequence of a failure, is not able to receive recommendations, but still wants to receive them. Therefore, we consider the total number of peers (i.e., 943) when computing the hit-rate.Voluntary leavings. In case of a peer leaving the network voluntarily, we modified the hit-rate to take into con sideration only those peers that remain connected to the overlay. If L represents the set of peers that have left the network, the hit-rate for voluntary leavings is computed as"}
{"pdf_id": "0812.4460", "content": "Therefore, we assumed that peers leaving the network do not want to receive their recommendations anymore. Note that this is a worst case scenario, because they are able to receive recommendations, locally computed from the cache entries, even in the case when no connection to the overlay exists (i.e., using their cache entries). Figure 6 shows the simulation results."}
{"pdf_id": "0812.4460", "content": "7. RELATED WORKIn this section, we present some examples of related research on deploying recommender systems in distributed ar chitectures.PocketLens [16] is a P2P-based collaborative filtering al gorithm that incrementally updates an item-item model [7]for later use to make recommendations. In contrast to Pock etLens, Swarmix builds a user-based matrix [12] for eachpeer v, where the users in the matrix correspond to v's neigh bors only, avoiding scalability problems when the amount of users in the network increases. Haase et al. [11] deploy a CF recommender system over a P2P-based personal bibliography management tool. The recommender system assists users in the management and evolution of their personal ontology by providing detailed"}
{"pdf_id": "0812.4460", "content": "suggestions of ontology changes. These suggestions are based on the usage information of the individual ontologies across the P2P network. Swarmix is domain-independent and could be tuned to deliver recommendations of actions, not only items, only requiring a meaningful way to represent userprofiles in order to compute their similarity for neighbor hood formation. An entirely distributed CF algorithm called PipeCF, basedon a content-addressable distributed hash table (DHT) in frastructure, is presented in [17]. Swarmix depends on a epidemic-style protocol for information dissemination.One area of research that intersects with peer-to-peer rec ommender systems systems is that of mobile and intelligent software agents. Yenta [9], for example, is a decentralized multi-agent system that focuses on the issue of finding other peers with similar interests using referrals from other agents."}
{"pdf_id": "0812.4461", "content": "For Cross System Music Blog Mining, we used two data sets: one data set consisted of personal music blogs from Blogger.com, one of the most popular blogsites, whereas the second data set consisted of tagged tracks from Last.fm,a radio and music community website and one of the largest social music plat forms. The details of each data set are presented in this section."}
{"pdf_id": "0812.4542", "content": "We provide a comprehensive and critical review of the h-index and its  most important modifications proposed in the literature, as well as of  other similar indicators measuring research output and impact.  Extensions of some of these indices are presented and illustrated.  Key words: Citation metrics, Research output, h-index, Hirsch index, h-type"}
{"pdf_id": "0812.4542", "content": "Egghe, L. (2008b). Dynamic h-index: The Hirsch Index in Function of Time. Journal of the  American Society for Information Science and Technology (to appear).  (available at: http://dx.doi.org/10.1002/asi.v58:3)  Egghe, L. (2008c). Mathematical Theory of the h- and g-Index in Case of Fractional  Counting of Authorship. Journal of the American Society for Information  Science  and  Technology,  59(10),  1608-1616  (available  at:"}
{"pdf_id": "0812.4542", "content": "Egghe, L. and Rao, R. (2008). Study of Different h-indices for Groups of Authors. Journal  of the American Society for Information Science and Technology, 59(8), 1276-1281.  (available at: http://dx.doi.org/10.1002/asi.20809)  Egghe, L. and Rousseau, R. (2006). An Informetric Model for the Hirsch Index.  Scientometrics, 69(1), 121-129.  (available at: http://dx.doi.org/10.1007/s11192-006-0143-8)"}
{"pdf_id": "0812.4580", "content": "namely to extract the right state representation (\"fea tures\") out of the bare observations. Even if potentially useful representations have been found, it is usually notclear which one will turn out to be better, except in situ ations where we already know a perfect model. Think of a mobile robot equipped with a camera plunged into anunknown environment. While we can imagine which im age features are potentially useful, we cannot know which ones will actually be useful."}
{"pdf_id": "0812.4580", "content": "(Un)known environments. For known Env(), finding the reward maximizing agent is a well-defined and formallysolvable problem [Hut05, Chp.4], with computational ef ficiency being the \"only\" matter of concern. For most real-world AI problems Env() is at best partially known. Narrow AI considers the case where function Env() is either known (like in blocks world), or essentially known"}
{"pdf_id": "0812.4580", "content": "The log-terms renect the required memory to code (or the time to learn) the MDP structure and probabilities. Since each state has only 2 realized/possible successors, we need n bits to code the state sequence. The reward is a deterministic function of the state, hence needs no memory to code given s."}
{"pdf_id": "0812.4581", "content": "Heuristic structure search. We could also replace the well-founded criterion (3) by some heuristic. One suchheuristic has been developed in [SDL07]. The mutual in formation is another popular criterion for determining the dependency of two random variables, so we could add j as a parent of feature i if the mutual information of xj"}
{"pdf_id": "0812.4581", "content": "ture of the DBN. They are usually complex functions of the (exponentially many) states, which cannot even bestored, not to mention computed [KP99]. It has been sug gested that the value can often be approximated well as a sum of local values similarly to the rewards. Such a value function can at least be stored."}
{"pdf_id": "0812.4581", "content": "Exploration. Optimal actions based on approximaterather than exact values can lead to very poor behav ior due to lack of exploration. There are polynomiallyoptimal algorithms (Rmax,E3,OIM) for the exploration exploitation dilemma. For model-based learning, extending E3 to DBNs is straightforward, but E3 needs an oracle for planning ina given DBN [KK99]. Recently, Strehl et al. [SDL07] ac complished the same for Rmax. They even learn the DBN structure, albeit in a very simplistic way. Algorithm OIM [SL08], which I described in [Hut09] for MDPs, can alsolikely be generalized to DBNs, and I can imagine a model free version."}
{"pdf_id": "0901.0213", "content": "Background Recent studies have demonstrated that the cyclical nature of mouse lactation1 can be  mirrored at the transcriptome2 level of the mammary glands but making sense of  microarray3 results requires analysis of large amounts of biological information which  is increasingly difficult to access as the amount of literature increases"}
{"pdf_id": "0901.0213", "content": "Results Our results demonstrated that a previously reported protein name co-occurrence  method (5-mention PubGene) which was not based on a hypothesis testing framework, is generally more stringent than the 99th percentile of Poisson distribution based method of calculating co-occurrence. It agrees with previous methods using  natural language processing to extract protein-protein interaction from text as more  than 96% of the interactions found by natural language processing methods to  coincide with the results from 5-mention PubGene method. However, less than 2% of"}
{"pdf_id": "0901.0213", "content": "the gene co-expressions analyzed by microarray were found from direct co occurrence or interaction information extraction from the literature. At the same time,  combining microarray and literature analyses, we derive a novel set of 7 potential  functional protein-protein interactions that had not been previously described in the  literature."}
{"pdf_id": "0901.0213", "content": "Mathematically, precision is the number of true positives  divided by the total number of items labeled by the system as positive (number of true  positives divided by the sum of true and false positives), whereas recall is the number  of true positives identified by the system divided the number of actual positives  (number of true positives divided by the sum of true positives and false negatives)"}
{"pdf_id": "0901.0213", "content": "entities are related in some way and the likelihood of such relatedness increases with  higher co-occurrence. In another words, co-occurrence methods tend to view the text  as a bag of un-sequenced words. Hence, depending on the threshold allowed, which  will translate to the precision of the entire system, recall could be total, as implied in  PubGene (Jenssen et al., 2001)."}
{"pdf_id": "0901.0213", "content": ", 2001) defined interactions by co-occurrence to the simplest  and widest possible form by assigning an interaction between 2 proteins if these 2  proteins appear in the same article just once in the entire library of 10 million articles  and found that this criterion has 60% precision (1-Mention PubGene method)"}
{"pdf_id": "0901.0213", "content": "Our results demonstrate that 5-mention PubGene method is generally statistically more significant than 99th percentile of Poisson distribution method of calculating co occurrence. Our results showed that 96% of the interactions extracted by NLP  methods (Ling et al., 2007) overlapped with the results from 5-mention PubGene method. However, less than 2% of the microarray correlations were found in the co occurrence graph extracted by 1-mention PubGene method. Using co-occurrence  results to filter microarray co-expression correlations, we have discovered a  potentially novel set of 7 protein-protein interactions that had not been previously  described in the literature."}
{"pdf_id": "0901.0213", "content": "The 4 microarray datasets are from Master et al. (2002) using Affymetrix Mouse Chip  Mu6500 and FVB mice, Clarkson and Watson (2003) using Affymetrix U74Av2 chip  and C57/BL6 mice, Rudolph et al. (2007) using Affymetrix U74Av2 chip and FVB  mice, and Stein et al. (2004) using Affymetrix U74Av2 chip and Balb/C mice."}
{"pdf_id": "0901.0213", "content": "Using a pre-defined list of 3653 protein names which was derived by Ling et al.  (2007) from Affymetrix Mouse Chip Mu6500 microarray probeset, PubGene  established 2 measures of binary co-occurrence (Jenssen et al., 2001): 1-mention  method and 5 mentions method. In the 1-mention method, the appearance of 2 entity  names in the same abstract will be deemed as a positive outcome whereas the 5  mentions method will require the appearance of 2 entity names in at least 5 abstracts  before considered positive."}
{"pdf_id": "0901.0213", "content": "For co-occurrence modelled on Poisson distribution (Poisson co-occurrence), the  number of abstracts in which both entity names appeared in is assumed to be rare as it  only requires the appearance of 2 entity names within 5 articles in a collection of 10  million articles to give a precision of 0"}
{"pdf_id": "0901.0213", "content": "The product of relative occurrence frequency of  each of the 2 entities can be taken as the mean expected probability of the 2 entities  appearing in the same abstract if they are not related, which when multiplied by the  total number of abstracts, can be taken as the mean number of occurrence (lambda) of  Poisson distribution"}
{"pdf_id": "0901.0213", "content": "Two sets of comparisons were performed: within the different forms of co-occurrence,  and between co-occurrence and text processing methods. The first set of comparison  aims to evaluate the differences between the 3 co-occurrence methods described  above. PubGene's 1-mention and 5-mentions methods were co-related singly and in  combination with Poisson co-occurrence methods."}
{"pdf_id": "0901.0213", "content": "Using 3563 transcript names, there is a total of 6345703 possible pairs of interactions  - 927648 (14.6%) were found using 1-Mention PubGene method and 431173 (6.80%)  were found using 5-Mention PubGene method. The Poisson co-occurrence method  using both 95th or 99th percentile threshold found 927648 co-occurrences, which is the  same set as using 1-Mention PubGene method."}
{"pdf_id": "0901.0213", "content": "Using Pearson's correlation coefficient to signify the presence of a co-expression  between the pair of spots (genes) on the Master et al. (2002) data set, there are 210283  correlations between -1.00 to -0.75 and 0.75 to 1.00, of which 2014 (0.96% of  correlations) are found in 1-PubGene co-occurrence network, 342 (0.16% of  correlations) are found in activation network extracted by natural language processing  means and 407 (0.19% of correlations) are found in binding network extracted by  natural language processing means."}
{"pdf_id": "0901.0213", "content": "Mapping an intersect of co-expression networks of all 4 in vivo data sets (Master et  al., 2002; Clarkson and Watson, 2003; Stein et al., 2004; Rudolph et al., 2007), there  are 1140 correlations, of which 14 (1.23%) are found in 1-PubGene co-occurrence  network, none of which corresponds to the interactions found in activation or binding  networks extracted by natural language processing means (Ling et al., 2007)."}
{"pdf_id": "0901.0213", "content": "Comparing the difference between PubGene (Jenssen et al., 2001) and Poisson  modelling method for co-occurrence calculations, three observations could be made.  Firstly, one of the common criticisms of a simple co-occurrence method as used in  this study (co-occurrence of terms without considering the number of words between"}
{"pdf_id": "0901.0213", "content": "This suggests that as  the size of corpus increases, it is likely that each co-occurrence of terms is more  significant, suggesting that a statistical measure might be more useful in a very large  corpus of more than 10 million as it takes into account both frequencies and corpus  size"}
{"pdf_id": "0901.0213", "content": "Thirdly, the number of co-occurrences found using 5-Mention PubGene method is  substantially lower (less than half) of that by 1-Mention PubGene method which was  also shown in Jenssen et al. (2001). This suggested that 5-Mention PubGene is  appreciably more stringent than using Poisson co-occurrence at 99th percentile; thus,  providing statistical basis for \"5-Mention PubGene\" method."}
{"pdf_id": "0901.0213", "content": "Our results comparing the numbers of co-occurrence demonstrated a 50.79% decrease  in co-occurrence from 1-Mention PubGene network to 5-Mention PubGene network.  However, the 5-Mention PubGene network retained most of the \"activation\" (98.5%)  and \"binding\" (98.0%) interactions found in 1-Mention PubGene network. This might  be the consequence of 30% recall of the NLP methods (Ling et al., 2007) as it would  usually require 3 or more mentions to have a reasonable chance to be identified by  NLP methods. This might also be due to the observation that the 5-Mention PubGene  method is more precise, in terms of accuracy, than the 1-PubGene method as shown  in Jenssen et al. (2001)."}
{"pdf_id": "0901.0213", "content": "The probability of a true interaction (Ling et al., 2007) existing in each of the 9661 NLP-extracted binding interactions that are also found in 1-Mention PubGene co occurrence would be raised. The probability of a true interaction existing in each of  the 9465 NLP-extracted binding interactions that are also found in 5-Mention PubGene co-occurrence would be higher. Hence, combining NLP and statistical co occurrence techniques can improve the overall confidence of finding true interactions.  However, it should be noted that statistical co-occurrence used in this work cannot  raise the confidence of NLP-extracted interactions."}
{"pdf_id": "0901.0213", "content": "Nevertheless, these results also suggest that graphs of statistical co-occurrence could  be annotated with information from NLP methods to indicate the nature of such  interactions. In this study, 2 NLP-extracted interactions from Ling et al. (2007),  \"binding\" and \"activation\", were combined. The combined \"binding\" and \"activation\" network covered 1.96% and 3.85% of 1-Mention and 5-Mention PubGene co occurrence graph respectively. Our results demonstrate that the combined network has  a higher coverage than individual \"binding\" or \"activation\" networks. Thus, it can be  reasonable to expect that with more forms of interactions, such as degradation and  phosphorylation, extracted with the same NLP techniques, the co-occurrence graph  annotation would be more complete."}
{"pdf_id": "0901.0213", "content": "By overlapping the co-expression network analyzed from Master et al. (2002) data set  to 1-Mention PubGene co-occurrence network, our results demonstrated that about  99% of the co-expression was not found in the co-occurrence network. This might  suggest that the choice of Pearson's correlation coefficient threshold of more than 0.75  and less than -0.75 as suggested by Reverter et al. (2005) is likely to be sensitive in  isolating functionally related genes from microarray data at the cost of reduced  specificity."}
{"pdf_id": "0901.0213", "content": "Reverter et al. (2005) had previously analysed 5 microarray data sets by expression  correlation and demonstrated that genes of related functions exhibit similar expression profile across different experimental conditions. Our results suggest 1126 co expressed genes across 4 microarray data sets are not found in the co-occurrence  network. This may be a new set of valuable information in the study of mouse  mammary physiology as these pairs of genes have not been previously mentioned in  the same publication and experimental examination of these potential interactions is  needed to understand the biological significance of these co-expressions."}
{"pdf_id": "0901.0213", "content": "percentile of Poisson distribution method. In this study, we demonstrate the use of a  liberal co-occurrence-based literature analysis (1-Mention PubGene method) to  represent the state of research knowledge in functional protein-protein interactions as  a sieve to isolate potentially novel hypotheses from microarray co-expression analyses  for further research."}
{"pdf_id": "0901.0318", "content": "Artificial Chemistries (ACs) are symbolic chemical metaphors for the explo ration of Artificial Life, with specific focus on the problem of biogenesis or the origin of life. This paper presents authors thoughts towards defining a unified framework to characterize and classify symbolic artificial chemistries by devising appropriate formalism to capture semantic and organizational information. We identify threebasic high level abstractions in initial proposal for this framework viz., informa tion, computation, and communication. We present an analysis of two important notions of information, namely, Shannon's Entropy and Algorithmic Information, and discuss inductive and deductive approaches for defining the framework."}
{"pdf_id": "0901.0318", "content": "Aim of this section is to present a brief introduction to artificial chemistries. We will start with a discussion on the epistemological foundations of the area and will illustrate further details using examples relevant to this proposal. The examples are followed by discussions to motivate the main theme of the proposal which is elaborated in coming sections."}
{"pdf_id": "0901.0318", "content": "Reaction Rules - Function Composition and Normal Form Reduction: The reaction rules in Alchemy consist of application of one lambda term over the other, which is then reduced to a normal form. The choice of lambda calculus allows the abstract formulation of chemical substitution during chemical reactions. Normalization is used to"}
{"pdf_id": "0901.0318", "content": "The Chemical Abstract Machine (CHAM) was proposed in [Berr96] as an abstract formalism for concurrent computation using closely a metaphor of chemical reactions. There are two description levels. On the upper level, CHAM abstractly defines a syntactic framework and a simple set of structural behavior laws. An actual machine is defined by adding a specific syntax for molecule and a set of transformation rules that specify how to produce new molecules from old ones."}
{"pdf_id": "0901.0318", "content": "The qualitative dynamics of ARMS is investigated by generating rewriting rules ran domly. This led them to derive a formal criteria for the emergence of cycles [Sujuki96] in terms of an order parameter, which is roughly the relation of the number of heating rules to the number of cooling rules [Sujuki98]. For small and large values of this order parameter, the dynamics remains simple, i.e., the rewriting system terminates and no cycles appear. For intermediate values, cycles emerge."}
{"pdf_id": "0901.0318", "content": "are examples of those which demonstrate several of high level organizational properties, for example origin of diversity of life, in Tierra [Ray91], but the power comes out of in-built self replicating and self organizing properties in the basic structures (programs). On the other hand we have examples which closely simulate the bio chemical reactions, e.g., self assembly of protocell structures, but these are complex, time consuming, and do not explain the emergence of complex organizational patterns or life-like properties. This motivates for the need of correctly abstracting the most essential and basic properties from real chemical environment and to explore dynamic structures in an unified way."}
{"pdf_id": "0901.0318", "content": "Based upon the analysis of ACs and discussion on the relevance of \"context based func tional information\", we propose here an initial sketch for a new framework to study the emergent phenomenon such as emergence of self replication in molecules, emergence of hypercycles, metabolic networks, self organization and other life-like properties from a basic AC set-up in a unified way. We identify three basic high level abstractions in our framework, viz., information, computation, and communication. These notions need to be further refined and clearly formalizes in the context of ACs and in general AL studies. These are discussed next."}
{"pdf_id": "0901.0318", "content": "were introduced and formally characterized using reactor now equations in [Eigen79]. That characterization is general enough to capture any kind of population dynamics. Though again this is quantitative characterization and cannot be used to explain why hyper cycles actually emerge or whether they will emerge at all in an organization where new species keep emerging. To take this approach further, we identify the following basic elements in emergence of self-replication in an AC set-up."}
{"pdf_id": "0901.0318", "content": "Identity - these are the most elementary entities of replication, that is, which self replicate itself. Examples of individual cells in an multi-cellular organism are such examples. In real chemistry we notice that, though atoms are the basic components (of self-replicating entities), they do not self-replicate. Thus identification of these self-replicating entities is important to understand any level of self-organization. This is not easy always because there is no bound on the \"size\" or \"type\" of these replicating molecules. This might be the case that there are several hierarchies of self- replicating entities, each replicating on its level."}
{"pdf_id": "0901.0318", "content": "Self-preservation - this means structure is robust against perturbations and thus small changes in the structure cannot be taken for dissimilarity. Before talking about replication, the entities need to be able to preserve their own identity. How do we assign an identity to the entities that is preserved over time?"}
{"pdf_id": "0901.0318", "content": "Equivalence Relation - This relation is used to correctly formulate the characteristics, which will be used to determine the presence of replication. To clarify the point, again consider the case of replicating cells, there not everything replicates itself during cell division, therefore similarity in overall chemical composition or equal cell sizes cannot be the basis of characterizing self-replication. In fact it is mainly genetic material which replicates during cell division and we treat is as cell replication."}
{"pdf_id": "0901.0318", "content": "Period of replication - this is measured to find out after how many reaction steps, a self-replicating structure will replicate itself. In most of the simple cases it is just one reaction period, which means structures maintain and replicate themselves for each reaction. It need not to be the case for a larger self-replicating organization, which might involve gradual replication of its components across several reaction cycles."}
{"pdf_id": "0901.0318", "content": "The main concep tual motivation ACs borrow from real chemistries is not the actual chemical structures or reactions but the abstract concept that life originated as a result of complex dynamical interplay between the rule space consisting of reaction rules or semantics and the objectspace consisting of the molecules which react"}
{"pdf_id": "0901.0358", "content": "This formulation shows that the decision function is nothing but a linear combination (whose  coefficients are the weights wni) of the elementary decision rules attached to each node elements in Sd.  Various learning policies or adhoc strategies can be proposed to set up these weight parameters."}
{"pdf_id": "0901.0358", "content": "On these two experiments, we notice that the NBS models perform slightly better than the NB model  as previously shown by other studies [4][23]. This is corroborated by Bratko and Filipic [2][3] that find  similar results when comparing the naive bayes classifier applied on flat text or in conjunction with a  splitting method. On the other hand the SCANB model, with the proposed weightings heuristic shows"}
{"pdf_id": "0901.0786", "content": "The partition function of a graphical model, which plays the role of normalization con stant in a MRF or probability of evidence (likelihood) in a BN is a fundamental quantity which arises in many contexts such as hypothesis testing or parameter estimation. Exactcomputation of this quantity is only feasible when the graph is not too complex, or equiv"}
{"pdf_id": "0901.0786", "content": "Figure 2: Fisher's rules. (Top) A node a of degree two in G is split in two nodes in Gext. (Bottom) A node a of degree three in G is split in three nodes in Gext. The squares on the right indicate all possible matchings in Gext related with node a. Note that the rules preserve planarity."}
{"pdf_id": "0901.0786", "content": "each node neighbors exactly one edge from the subset. The weight of a matching is the product of weights of edges in the matching. The key idea of this mapping is to extend the original Forney graph G into an new graph Gext := (VGext, EGext) in such a way that each perfect matching in Gext corresponds to a 2-regular loop in G. (See Figures 1b and c for an illustration). Under the condition of planarity, the sum of all weighted perfect matchings can be calculated in a polynomial time following Kasteleyn's arguments. Here we reproduce these results with little variations and more emphasis on the algorithmic aspects."}
{"pdf_id": "0901.0786", "content": "Given a Forney graph G and the BP approximation, we simplify G and obtain the 2-core by removing nodes of degree one recursively. After this step, G is either the null graph (and then BP is exact) or it is only composed of vertices of degree two or three."}
{"pdf_id": "0901.0786", "content": "Cluster Variation Method (CVM-Loopk) A double-loop implementation of CVM (Heskes et al., 2003). This algorithm is a special case of generalized belief propagation (Yedidia et al.,2005) with convergence guarantees. We use as outer clusters all (maximal) factors to gether with loops of four (k=4) or six (k=6) variables in the factor graph."}
{"pdf_id": "0901.0786", "content": "Figure 8: Two examples of planar graphs used for comparison between methods. We fix the number of concentric polygons to 9 and change the degree d of the central node within the range [3, ..., 25]. (left) Graph for d = 3. (right) Graph for d = 25. Here nodes represent variables and edges pairwise interactions. We also add external fields which depend on the state of each nodes (not drawn)."}
{"pdf_id": "0901.1152", "content": "2. Turing universality and learning. A person with a good visual memory can be taught to perform, in principle, any mental computation with the use of an imaginarymemory aid. Ignoring some theoretically unimportant limitations on the size of the imag inary memory aid, this observation means that the human brain must be treated by a system theorist as a Turing universal learning system. It is interesting to ask: Q2. What is the simplest architecture of a Turing universal learning system? This question is directly related to Q1. It is easy to prove that a learning system that cannot answer question Q1 cannot be a Turing universal learning system."}
{"pdf_id": "0901.1152", "content": "3. Memorization, recollection, and synthesis. People can memorize and recall long sequences of real sensory and motor events. At the same time, they can synthesize a combinatorial number of imaginary events. It is attractive to think that the same learning algorithm can account for all outlined phenomena. We can ask: Q3. What learning algorithm satisfies the requirements of correct recollection, and combinatorial synthesis? We argue that a learning algorithm that attempts to do a lot of preprocessing of the learner's experience before putting this experience in the learner's LTM cannot answer this question. In contrast, an algorithm that simply memorizes all learner's \"raw\" experience, call it a complete memory algorithm (CMA), does not have this limitation (Section 6)."}
{"pdf_id": "0901.1152", "content": "The general architecture of the cognitive model used in this paper is shown in Figure 1. The model consists of an external world, W (represented by a keyboard and a screen), and a robot, (D,B), consisting of the sensorimotor devices, D, and the brain, B. From the system-theoretical viewpoint, it is convenient to treat system (W,D,B) as a composition of two subsystems: the external system, (W,D) and the brain B. In this representation, both systems can be viewed as abstract machines, the outputs of (W,D) being the inputs of B, and vice versa. Note that the brain does not know about the external world, W, per se. It knows only about the external system (W,D)."}
{"pdf_id": "0901.1152", "content": "4. Motor centers (nuclei), NM=(NM1,NM2,NM3), that work as a multiplexer switching between the output of the teacher, T.y = (T.y1, T.y2, T.y3), and the output of system AM, AM.y = (AM.y1, AM.y2, AM.y3). We assume that each multiplexer has a select input, sel (not shown), that can be set by the experimenter."}
{"pdf_id": "0901.1152", "content": "Both systems, AM and AS, are in the, so-called, supervised learning mode. In the course of training, the teacher can produce any desired output of centers NM. The teacher can also switch the output of centers NS (NS.y) between the output of system AS (AS.y) and the output of the eye, dout. When NS.y = dout, system (W,D) serves as the teacher for system AS. Both systems, AS and AM, have inputs denoted as xy. These inputs deliver the output signals needed for learning. Such inputs are often referred to as desired outputs."}
{"pdf_id": "0901.1152", "content": "Assume that a traditional learning system is used as system AS in Figure 1. It is conve nient to redraw the relevant part of Figure 1 as the experimental setup shown in Figure 4, where the external system, (W,D), is replaced by GRAM. Think of input xy as the desired output of the above learning system. Let NS.sel = 1, so the GRAM serves as the target system (the teacher) for system AS. We claim that, in this experiment of supervised learning, no traditional learning systems, used as system AS, can learn to simulate the target system with the properties of a GRAM. In what follows we prove this claim for two broad classes of learning systems."}
{"pdf_id": "0901.1152", "content": "Theorem 1. Let M be a learning system with some statistical (or any other) learn ing algorithm that learns to predict the output of a target system, T, from the samples of its input/output sequence. Let the maximum length of the samples taken into account not exceed m. System M cannot learn to simulate system T with the properties of a GRAM. Remark. Many learning systems treat a training sequence as a set of input/output pairs. For such systems m = 1."}
{"pdf_id": "0901.1152", "content": "Proof. Let us use the same GRAM as in Theorem 1. Suppose M satisfies the above definition and nevertheless has learnt to simulate the specified GRAM. To produce the contradiction do the following test: Step 1. Send to the input of M a sequence x(1), ...x(m1), ...x(m2), ...x(m + 1), such"}
{"pdf_id": "0901.1152", "content": "• r(:) .= (r(1), ..r(n)) is a retrieval array. In general, r(i) is an element of a real array that represents the level of activation of the i-th location of OLTM. In this model, we use a random winner-take-all choice, so only one component of this array, r(iwin), corresponding to the winner, iwin, is not equal to zero. Formally, in this example we need only the variable iwin. The r-array is introduced for the sake of completeness. It does not appear in the following equations. This array is needed in more complex models of primitive E-machines that employ more complex encoding procedures."}
{"pdf_id": "0901.1152", "content": "2. The concept of E-machine supports the notion that the E-states (the states of dynamic STM and ITM) are associated with the properties of individual neurons and synapses. There is an interesting possibility to formally connect the dynamics of the phenomenological E-states with the statistical conformational dynamics of ensembles of membrane proteins treated as Markov systems [11]."}
{"pdf_id": "0901.1152", "content": "Proof. First of all, we need to specify the parameters of the PEM (5.2) and the con ditions of the experiments. System AS is organized as Model (5.2) with parameters AS.m = 2; AS.p = 1; AS.wx(1) = AS.wx(2) = 1.0. We assume that AS.n is as big as needed to record all training data."}
{"pdf_id": "0901.1152", "content": "Remarks: 1. To transform the system of Figure 1 into a working model one needs to take care of the synchronization of units (W,D),NS,NM,AS, and AM. These technical problems are of no significance for the purpose of this paper. The model was actually implemented as an interactive C++ program for the MS Windows called EROBOT."}
{"pdf_id": "0901.1152", "content": "1. Decoding temporal sequences. Adding lateral pre-tuning to the next E-state procedure addresses this problem. The corresponding PEM can learn to simulate, in principle, any output independent finite memory machine. Introducing a delayed feedback in the above PEM leads to a system capable of learning to simulate any output dependent finite memory machine."}
{"pdf_id": "0901.1289", "content": "and y are normalized, then   is also normalized. Of course, the reader can redefine the  neutrosophic conjunction operator, depending on application, in a different way, for example in a  more optimistic way, i.e. I  or T prevails with respect to I , then we get:  1 2 1 2 2 1 1 2 1 2 1 2 2 1 2 1 ( , ) cITF x y TT T I T I I I F F F I FT F T F I ."}
{"pdf_id": "0901.1289", "content": "1 2 2 3 1 2 1 2 1 2 T I F T F I I F T I T F F I T FT I .  Similarly, the neutrosophic tri-nary disjunction/union of neutrosophic variables x, y, and  z is:  ( , , ) d FIT x y z TT TI TF TIF II IF FF ="}
{"pdf_id": "0901.1289", "content": "(T1T2T3 + T1T2I3 + T1I2T3 + I1T2T3 + T1I2I3 + I1T2I3 + I1I2T3 + T1T2F3 + T1F2T3 + F1T2T3 +  T1F2F3 + F1T2F3 + F1F2T3 + T1I2F3 + T1F2I3 + I1F2T3 + I1T2F3 + F1I2T3 + F1T2I3, I1I2I3 + I1I2F3 +  I1F2I3 + F1I2I3 + I1F2F3 + F1I2F3 + F1F2I3, F1F2F3)  Surely, other neutrosophic orders can be used for tri-nary conjunctions/intersections and  respectively for tri-nary disjunctions/unions among the componenets T, I, F.  5. Neutrosophic Topologies."}
{"pdf_id": "0901.1289", "content": "References:  [1]  F. Smarandache & J. Dezert, Advances and Applications of DSmt for Information  Fusion, Am. Res. Press, 2004.  [2]  F. Smarandache, A unifying field in logics: Neutrosophic Logic, Neutrosophy,  Neutrosophic Set, Neutrosophic Probability and Statistics, 1998, 2001, 2003,  2005.  [3]  H. Wang, F. Smarandache, Y.-Q. Zhang, R. Sunderraman, Interval Neutrosophic  Set and Logic: Theory and Applications in Computing, Hexs, 2005.  [4]  L. Zadeh, Fuzzy Sets, Information and Control, Vol. 8, 338-353, 1965."}
{"pdf_id": "0901.2850", "content": "In order to prove these results we use program splittings (Lifschitz and Turner 1994), but the focus is shifted from splitting sequences (whose elements are sublanguages) to the corresponding sequences of subprograms, that enjoy more invariant properties and may be regarded as a sort of normal form for splitting sequences"}
{"pdf_id": "0901.2850", "content": "sistency checking and skeptical reasoning can be found in Section 5. Then, for a better, goal-directed calculus, the completeness theorem for skeptical resolution is extended to all finitely recursive programs in Section 6. Section 7 relates finitely recursive programs and our iterative approach to previous approaches to decidable reasoning with infinite stable models, and makes a first step towards a unified picture based on our framework. Finally, Section 8 concludes the paper with a summary and a brief discussion of our results, as well as some interesting directions for future research."}
{"pdf_id": "0901.2850", "content": "Disjunctive and normal programs may have one, none, or multiple stable models. We say that a program is consistent if it has at least one stable model; otherwise the program is inconsistent. A skeptical consequence of a program P is any closed first order formula satisfied by all the stable models of P. A credulous consequence of P is any closed first order formula satisfied by at least one stable model of P. The dependency graph of a program P is a labelled directed graph, denoted by DG(P), whose vertices are the ground atoms of P's language. Moreover,"}
{"pdf_id": "0901.2850", "content": "An atom A depends positively (respectively negatively) on B if there is a directed path from A to B in the dependency graph with an even (respectively odd) number of negative edges. Moreover, each atom depends positively on itself. A depends on B if A depends positively or negatively on B. An odd-cycle is a cycle in the dependency graph with an odd number of negative edges. A ground atom is odd-cyclic if it occurs in an odd-cycle. Note that there exists an odd-cycle iff some ground atom A depends negatively on itself. The class of programs on which this paper is focussed can now be defined very concisely."}
{"pdf_id": "0901.2850", "content": "For example, most standard list manipulation programs (member, append, remove etc.) are finitely recursive. The reader can find numerous examples of finitely recursive programsin (Bonatti 2004). In general, checking whether a program is finitely recursive is undecid able (Bonatti 2004). However, in (Bonatti 2001a; Bonatti 2004) a large decidable subclasshas been implicitly characterized via static analysis techniques. Another expressive, decid able class of finitely recursive programs can be found in (Simkus and Eiter 2007). We will also mention frequently an important subclass of finitely recursive programs:"}
{"pdf_id": "0901.2850", "content": "Example 2.3 Typical programs for reasoning about actions and change are finitary. Fig. 4 of (Bonatti 2004) illustrates one of them, modelling a blocks world. That program defines—among others—two predicates holds(nuent, time) and do(action, time). The simplest way to add a constraint that forbids any parallel execution of two incompatible actions a1 and a2 is includ ing a rule"}
{"pdf_id": "0901.2850", "content": "1 This definition differs from the one adopted in (Bonatti 2002) because it is based on a different notion of dependency. Here the dependency graph contains edges between atoms occurring in the same head, while in (Bonatti 2002) such dependencies are dealt with in a third condition in the definition of finitary programs. Further comparison with (Bonatti 2002) can be found in Section 7."}
{"pdf_id": "0901.2850", "content": "In other words, for a given program P, either all module sequences are inconsistent, orthey are all consistent. In particular, if P is consistent, then every member Pi of any mod ule sequence for P must be consistent. The converse property would allow to define a procedure for enumerating the stable models of P (as shown in the following sections). Unfortunately, even if each step in a module sequence is consistent, the entire program P is not necessarily consistent, as shown by the following example."}
{"pdf_id": "0901.2850", "content": "Proof The proof is by reduction of inconsistency checking for normal finitely recursive programs to the problem of skeptical inference of a ground formula from a normal finitely recursive program. Let P be a normal finitely recursive program and q be a new ground atom that doesn't occur in P. Then, P is inconsistent iff q is a skeptical consequence of P. Since q occurs in the head of no rule of P, q cannot occur in a model of P. So, P skeptically entails q iff P has no stable model."}
{"pdf_id": "0901.2850", "content": "We are left to illustrate the last rule of the calculus, that models negation as failure. In order to abstract away the details of the computation of failed facts, the rule is expressed in terms of so-called counter-supports, that in turn are derived from the standard notion of support. Recall that a support for a ground atom A is a set of negative literals obtained by applying SLD resolution to A with respect to the given program P until no positive literal is left in the current goal (the final, negative goal of the SLD derivation is a support for A)."}
{"pdf_id": "0901.2850", "content": "In other words, the first property says that K contradicts all possible ways of proving A, while the second property is a sort of relevance property. Informally speaking, the failure rule of skeptical resolution says that if all atoms in a counter-support are true, then all attempts to prove A fail, and hence notA can be concluded. Of course, in general, counter-supports are not computable and may be infinite (while skeptical derivations and their goals should be finite). In (Bonatti 2001b) the notion of counter-support is generalized to non ground atoms in the following way:"}
{"pdf_id": "0901.2850", "content": "The actual mechanism for computing counter-supports can be abstracted by means of a suitable function CounterSupp, mapping each (possibly nonground) atom A onto a set of finite generalized counter-supports for A. The underlying intuition is that function CounterSupp captures all the negative inferences that can actually be computed by the chosen implementation. Now negation-as-failure can be axiomatized as follows:"}
{"pdf_id": "0901.2850", "content": "exploited in Example 3.10 is not, as well as any normal program whose dependency graph contains some odd-cycle. The above program shows that a program may fail to be order consistent even if the program is acyclic. However, if P is normal and finitely recursive, then it can be shown that P is order consistent iff P is odd-cycle free (Bonatti 2004). This observation justifies the definition of finitary programs (Definition 2.2): By requiring"}
{"pdf_id": "0901.2850", "content": "finitary programs to have finitely many odd-cycles, it is possible to confine all odd-cycles into a single, finite program module Pk and ensure that the \"top\" programs are odd-cycle free and hence consistent. As proved in (Bonatti 2004), the extra condition on odd-cycles suffices to make bothcredulous and skeptical ground queries decidable. However, in (Bonatti 2004) the state ment erroneously fails to include the set of odd-cyclic literals among the inputs of the algorithm. Here is the correct statement and a slightly different proof based on module sequences:"}
{"pdf_id": "0901.2850", "content": "1. First assume that the unlabelled edges of DG(P) are ignored, that is, let A depend on B iff there is a path from A to B in DG(P) with no unlabelled edges. This is equivalent to adopting a dependency graph similar to the traditional graphs for normal programs, with no head-to-head edges. Using the resulting notion of atom dependencies, one can find programs that are order consistent but have no stable"}
{"pdf_id": "0901.3769", "content": "1. INTRODUCTION The Adaptative Landscape metaphor introduced by S. Wright [1] has dominated the view of adaptive evolution: an uphill walk of a population on a mountainous fitness landscape in which it can get stuck on suboptimal peaks. Results from molecular evolution haschanged this picture: Kimura's model [2] assumes that the over whelming majority of mutations are either effectively neutral orlethal and in the latter case purged by negative selection. This as sumption is called the neutral hypothesis. Under this hypothesis,"}
{"pdf_id": "0901.3769", "content": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$5.00."}
{"pdf_id": "0901.3769", "content": "dynamics of populations evolving on such neutral landscapes are different from those on adaptive landscapes: they are characterized by long periods of fitness stasis (population stated on a 'neutral network') punctuated by shorter periods of innovation with rapidfitness increases [3]. In the field of evolutionary computation, neu trality plays an important role in real-world problems: in design of digital circuits [4] [5] [6], in evolutionary robotics [7] [8]. In those problems, neutrality is implicitly embedded in the genotype to phenotype mapping."}
{"pdf_id": "0901.3769", "content": "2.2 A metaheuristic to improve the ND design Using algorithm 1, exhaustive fitness allocation does not create a landscape with a neutral degree distribution close enough to the input distribution. The reason is the fitness function is completelydefined before the neutral degree of every solution has been considered. Hence, we use a simulated annealing metaheuristic to improve the landscape created by algorithm 1. Here, simulated an nealing is not used to find a good solution of a ND-Landscape but to adjust the landscape by modifying the fitness of some solutionssuch as neutral distribution of a ND-Landscape be closer to the in put distribution. The local operator is the changement of fitness value of one solution of the landscape, which can alter at most N+1"}
{"pdf_id": "0901.3769", "content": "2.4 Sizes of the generated Neutral Networks Figure 4 shows the diversity of sizes of neutral networks for 4distributions. For every distribution we created 50 different NDLandscapes. Graphics on the left show the input and the mean re sulting distribution. Graphics on the right show all of the networks of these landscapes sorted by decreasing size with a logarithmic scale. We clearly see that the neutral degree distribution is a really determining parameter for the structure of the generated landscape."}
{"pdf_id": "0901.3769", "content": "where d(x) is the Hamming distance to the global optimum, di vided by N, between x and one particular solution. The problem is most deceptive as r is low and b is high. In our experiment, we will use two kinds of Trap functions with r = 0.9, one with b = 0.25 and another one with b = 0.75 (see figure 5 (a) and (b)). To affect a fitness value to each neutral network, we first choose the optimum neutral network, denoted NNopt, (for example the one containing the solution 0N) and set its fitness to the maximal value 1.0. Then, for each neutral network, we compute the distanced between its centroid1 and the centroid of NNopt ; finally the fit"}
{"pdf_id": "0901.3769", "content": "ness value of the NN is set according to a trap function2 and thedistance d. In order to ensure that all adjacent networks have dif ferent fitness values, it is possible to add a white noise to the fitness values of each NN. In the following experiments, the length of bitstring is N = 16. ND-landscapes are constructed with uniform neutral degree distributions. We use the distributions defined by"}
