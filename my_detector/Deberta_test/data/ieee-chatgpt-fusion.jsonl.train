[{"label": 1, "content": "To solve the problems of the data reliability for NAND flash storages, a variable-node-based belief-propagation with message pre-processing (VNBP-MP) decoding algorithm for binary low-density parity-check (LDPC) codes is proposed. The major feature is that, by making use of the characteristics of the NAND flash channel, the proposed algorithm performs the message pre-processing (MP) scheme to effectively prevent the propagation of unreliable messages and speed up the propagation of reliable messages. Additionally, the VNBP-MP algorithm includes a treatment for oscillating variable nodes (VNs) to further accelerate decoding convergence. Simulation results show that the proposed VNBP-MP algorithm has a noticeable improvement in convergence speed without compromising the error-correction performance, compared with the existing algorithms."}, {"label": 1, "content": "Many techniques have been proposed to solve the simultaneous localization and mapping (SLAM) problem, and among them, the Particle Filter (PF) is considered to be one of the most effective ways. However, the PF algorithm needs a large number of samples to approximate the posterior probability density of the system, which makes the algorithm complex. Furthermore, the judgment of resampling is imperfect. Based on this, an improved PF algorithm which introducing population diversity factor and genetic algorithm into the process of re-sampling is proposed in this paper. The effective sample size and the population diversity factor are used to determine whether to re-sampling. When re-sampling is needed, the genetic algorithm is used to optimize the particle set. The simulation result shows that estimation accuracy of the improved algorithm is better than that of traditional particles filter, not only in accuracy, but also in efficiency."}, {"label": 1, "content": "In the future scenario of multiple wireless network coverage, the choice of vertical handoff decision algorithm will directly affect the continuity of the session, the mobility of the user, and seamless roaming under heterogeneous wireless networks. Therefore, the study of vertical handover related algorithms is the key to the success of various wireless access networks in the future. In this regard, this paper proposes an optimized algorithm that combines two multiple attribute decision making (MADM) techniques, namely the Entropy and the improved Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS). The Entropy method is applied to obtain objective weights and the improved TOPSIS method is used to rank the alternatives. The simulation results show that the proposed technique can make the distribution of weights more reasonable, and effectively reduce the number of handoffs."}, {"label": 1, "content": "Passive sound source localization using time-difference-of-arrival (TDOA) measurements is a challenging non-linear inversion problem. This paper investigates a new data-driven approach to SSL using TDOA measurements. A neural network (NN) is viewed as an architecture constrained non-linear function, with its parameters learnt from the training data. The NN is trained to learn the non-linear mapping between TDOA measurements and source location from the training data. Experimentally, we show that, NN trained even on noise-less TDOA measurements can achieve good performance for noisy TDOA inputs also. These performances are better than the traditional spherical interpolation (SI) method. Furthermore, we demonstrate that the NN model trained offline using simulated TDOA measurements outperforms the SI method for localizing real-life speech signals inside a simulated enclosure."}, {"label": 1, "content": "With the development of the Internet, social bots are increasingly spreading on social platforms. Detecting these accounts that pose a threat to social networks has become an essential task that requires an effective detection algorithm. In this paper, a social bots detection model based on deep learning algorithm (DeBD) is proposed. The model mainly includes three layers. The first layer is the joint content feature extraction layer, which focuses on the extraction of features from tweets' content and their relationships. The second layer is the tweet metadata temporal feature extraction layer, which uses tweet metadata as temporal information to extract user social activity temporal features through LSTM. The third layer is the feature fusion layer, which integrates the extracted joint content features with the temporal features to detect social bots. We evaluated the proposed DeBD model on three different types of social bot data sets from the real world, and the experimental results demonstrate the model's effectiveness."}, {"label": 1, "content": "In recent years, deep learning object detectors including Fast/Faster R-CNN, SSD, R-FCN and Mask R-CNN have shown significant performance for general object detection except for pedestrians. The Region Proposal Network (RPN) in Faster R-CNN works well yet lacks of adaptability. Therefore, we suggest an adaptive real-time pedestrian detection and attribute identification scheme based on Caffe. The first contribution is the Adaptive Threshold Adjustment (ATA) algorithm for intelligent monitoring, which adjusts the threshold using pedestrian movement information. Moreover, to overcome the time-consuming defect, we analyze the influences of the number of layers, the size of convolution kernels and the number of feature maps to reduce redundant computation while maintaining satisfactory performance. By optimizing the neutral network structure, selecting model parameters, and data augmentation, we obtained a stable and well-performed model with fast detection rates and high accuracy. Furthermore, pedestrian information can be identified in our program, providing better service in security monitoring, intelligent robots, and other fields. Extensive experimental results demonstrate that even in complex and athletic scenarios, our method can make quality and speed improvements over current state-of-the-art techniques."}, {"label": 1, "content": "Human heart is a vital organ therefore proper diagnosis of heart activities is essential. To estimate heart parameters, various parameter estimation techniques have been developed. In this work, we use Ensemble Kalman Filter (EnKF) and Particle Filter (PF) for dynamic assimilation of human heart parameters. EnKF and PF are modified filters specifically designed for state prediction of nonlinear systems with large data samples. A third-order mathematical heart model was employed to estimate three heart parameters, including movements of heart muscle fiber, tension in heart muscle, and electrochemical activity of the heart. The EnKF and PF were applied to the heart model, and different case studies were performed to observe the prediction accuracy by comparing the sum squared error values. The case studies were conducted with variable state and measurement noise values. The proposed approach demonstrated promising results in accurately predicting the human heart parameters."}, {"label": 1, "content": "Currently, the field of computer vision widely uses the distance function to learn image pairs, with the Euclidean distance being the most commonly used method. But traditional Euclidean distance has disadvantage of distinguishing ability in the feature similarity measure. To address this issue, we propose a new algorithm called weighted Pairwise Constrained Component Analysis (wPCCA) for person reidentification (Re-ID), which is based on the weighted Euclidean distance. The wPCCA algorithm builds upon the PCCA and improves its measurement of characteristics using the weighted Euclidean distance. The experiments were conducted on two challenging datasets named i-LIDS and CAVIAR, and gained good results."}, {"label": 1, "content": "Jamming identification is the precondition of taking targeted anti-jamming measures, and it is very important to improve the adaptability of electronic information system to electromagnetic environment. While the most commonly used method of jamming recognition relies on expert knowledge-based feature extraction, the varied patterns and parameters of jamming can make it difficult to determine the correct feature set. Therefore, this paper introduces a deep learning approach, which automatically extracts features from the original data to identify the jamming factors of electronic information system. In order to demonstrate the effectiveness and practicability of this approach, the noise jamming factor identification of the superheterodyne receiver is introduced."}, {"label": 1, "content": "This paper presents a novel approach to track a walking person and automatically capture a frontal photo using an unmanned aerial vehicle (UAV). The proposed method consists of three main parts, namely person detection and recognition, face detection and feature points localization, and vision-based UAV control. The person tracking module employs the YOLOv3 deep neural network for detecting the target person, and utilizes the Locality-constrained Linear Coding (LLC) algorithm for matching the target person. In terms of frontal face perception, we use Multi-task Cascaded Convolutional Neural Networks (MTCNN) for face detection. Based on the vision information obtained from the two modules, the UAV can fly around the target person and obtain the target's frontal face image. The outdoor experiments based on a Parrot Bebop2 drone verify the effectiveness and practicability of our method."}, {"label": 1, "content": "Localization is one of the most promising technologies in modern society, and wireless sensor networks (WSNs) are being studied as a popular method to solve the problem. However, one of the main challenges in localization is the non-line of sight (NLOS) propagation. The main challenge in localization problem is the non-line of sight (NLOS) propagation. The residual analysis is a powerful a posteriori algorithm that can be used to improve the reliability of the localization process. On the other hand, the particle filter is a powerful technique that does not make presumptions about the probability density function or linear system models. By combining the two methods, we can improve the accuracy of localization and enhance the robustness of the method. The mix of the residual analysis and particle filter could improve the localization accuracy. At the same time, the randomness of the particle improves the robustness of the method. Overall, the proposed method appears to be effective and robust, providing a promising solution to the challenge of NLOS propagation in localization."}, {"label": 1, "content": "The objective of the present work is to improve the epoch extraction performance from emotive speech by proposing a post processing approach to the conventional zero frequency filtering (ZFF) method using variational mode decomposition (VMD) based spectral smoothing. Identifying the epochs in emotional speech signals is a challenging task due to the fast and uncontrolled variations of pitch. In the proposed method, the spectra of the short frames of zero frequency filtered signal (ZFFS) is subjected variational mode decomposition to get component spectra in five modes. A smoothed short-time spectra is generated by eliminating the spectra from the two higher VMD modes that contain high spectral variations. The modified ZFFS is then reconstructed using the sinusoidal parameters corresponding to single dominant frequency present in the smoothed spectra using VMD by parameter interpolation based sinusoidal synthesis. The resulting re-synthesized ZFFS has reduced spurious zero crossings as compared to that obtained from the conventional ZFF method for emotive speech signals. This is evident from the improved epoch identification accuracy and rate for all the emotive utterances (with 7 emotions) in the German emotion speech database with simultaneous speech and electroglottographic (EGG) signal recordings. The performance of the proposed method is found to be better or comparable with the other existing ZFF based post processing methods proposed for emotive speech signals in terms of the epoch identification accuracy with respect to the corresponding reference epochs estimated from EGG signals."}, {"label": 1, "content": "Lately, the problem of code-switching has gained a lot of attention and has emerged as an active area of research. In bilingual communities, individuals often incorporate words and phrases from a non-native language into their day-to-day communication in their native language. Although code-switching is a global phenomenon among multilingual communities, there is still a lack of adequate acoustic and linguistic resources available. For developing effective speech-based applications, the ability of the existing language technologies to deal with the code-switched data cannot be over emphasized. Code-switching is broadly classified into two categories: intra-sentential and inter-sentential. This study focuses on the intra-sentential problem in the context of code-switching language modeling. The contributions of this research include the creation of a Hindi-English code-switching text corpus by crawling selected blogging sites that educate about Internet usage, and the exploration of the use of parts-of-speech features to improve the modeling of Hindi-English code-switched data using monolingual language models trained on native (Hindi) language data."}, {"label": 1, "content": "At present, the improved social network analysis methods are mainly based on homogeneous networks. However, the actual social networks are heterogeneous in nature. Heterogeneous social networks can provide a better representation of the system's composition and the associated relationships than the homogeneous social network model. The previously proposed ranking clustering provides a new idea, however, the algorithm only completes the clustering results of specific target types and cannot cover the complete heterogeneous network type. By introducing collaborative clustering algorithm and combining it with ranking, we propose a ranking collaborative clustering algorithm. Firstly, based on ranking clustering, the ranking distribution matrix is obtained, and then collaborative clustering is used to complete different types of clustering, and the relationship between different types and the same types is fully utilized. The experimental results on real twitter and foursquare datasets demonstrate that the ranking collaborative clustering algorithm outperforms ranking clustering in terms of modularity."}, {"label": 1, "content": "To address the limitations of conventional speaker segmentation and clustering techniques, this paper presents a novel multilevel speaker re-segmentation and re-clustering algorithm utilizing GMM-UBM. The algorithm is based on the method of statistical modeling in the field of speaker recognition, and makes full use of the speaker information after segmenting and clustering in traditional methods to re-segment and re-cluster speech files, which improves the performance of the system effectively."}, {"label": 1, "content": "This paper proposes a method for channel estimation in Multi-input Multi-output/Orthogonal Frequency Division Multiplexing (MIMO/OFDM) systems operating in fast linear-time-varying (LTV) multi-path channels using a special frequency-division (FD) pilot. Unlike linear-time-invariant (LTI) channels, MIMO/OFDM systems operating in LTV channels may suffer from significant inter-carrier interference (ICI) caused by Doppler frequency shift resulting from the relative movement of transmitters and receivers. To address this problem, this paper presents a redesigned frequency-division pilot for LTV channel, which enables effective estimation of the channel tap of an intermediate instant of each OFDM symbol with relatively low ICI. Finally, we use well-known basis expansion model (BEM) to fit the whole channel. Numerical results indicate that our new frequency-division pilot combined with BEM fitting can obtain high precise channel estimation for fast LTV multi-path channels."}, {"label": 1, "content": "As the high-bandwidth and data-intensive applications evolving rapidly, traditional electrical networks are no longer able to meet the increasing traffic requirements. In order to improve the communication performance of data center network(DCN), the hybrid electrical/optical architecture has become a new research topic. Moreover, traditional architectures are often too complicate to manage. As a new technology, Software Defined network (SDN) addresses this issue effectively. In this paper, we propose a hybrid architecture based on SDN, where the control manager is used to monitor and allocate traffic, and then configure the network using SDN. The control manager is used to monitor and allocate traffic, then configure the network by SDN. Our experiment shows that the proposed scheme reduces the total time required for virtual machine migration compared to running on an electrical architecture. Additionally, our scheme enables flexible network topology configuration and load balancing. Overall, our proposed hybrid SDN-based architecture presents a promising solution for improving DCN communication performance and addressing network complexity management."}, {"label": 1, "content": "Audio forensics has numerous applications that can benefit from the ability to classify audio recordings based on the device they originated from, especially on social media platforms where huge amounts of data are posted daily. This paper utilizes passive signatures associated with the recording devices, as extracted from recorded audio itself, in the absence of any extrinsic security mechanism such as digital watermarking, to identify the source cell-phone of recorded audio. This approach uses device-specific information present in both low and high-frequency regions of the audio recordings, even in the absence of any extrinsic security mechanism like digital watermarking. The proposed system achieves a closed set accuracy of 97.2% on the only publicly available dataset in this field, MOBIPHONE, matching the state-of-the-art accuracy reported for this dataset. Furthermore, the proposed methodology outperforms existing methods by 4% in average accuracy on audio recordings which have undergone double compression, which frequently occurs when a recording is shared on social media."}, {"label": 1, "content": "With Convolutional Neural Networks (CNN) achieving success in computer vision tasks, Steganalysis is moving towards Network Engineering from Feature Engineering. To capture weak embedded signals in low Signal-to-Noise (SNR) scenarios, deep neural networks are being used. In this paper, we propose a novel Convolutional Neural Network based on aggregated residual transformations, which generate stronger image representations helpful for steganalysis. The architecture has very few hyperparameters to set and focus on increasing the classification accuracy while keeping the depth and number of parameters fixed. The residual skip connections further help preserve the weak embedded signals and improve the gradient flow. The proposed CNN was evaluated on BOSSbase against S-UNIWARD and HILL steganographic algorithms with different payloads. Comparing with the state-of-the-art Deep Residual Learning (DRN) based on Residual Learning and the SRM plus Ensemble, our proposed CNN gives a better classification Accuracy."}, {"label": 1, "content": "We focused on a problem where balanced use of sensor nodes' battery power is considered to maximize the overall lifetime of ad-hoc Wireless Sensor Networks. To optimise the network lifetime, it is important to utilise less attended sensor nodes as compared to heavily used ones. To perform this process, we propose a joint optimization problem to select a subset of active sensor nodes and a multi-hop routing structure interconnecting all selected sensor nodes, which helps to route the aggregated information to a querying node. Our optimization problem becomes non-convex over the subset selection and the multi-hop routing paths selection, thus belonging to the class of NP-hard problems. We solve our problem by relaxing one of the variable so that optimization problem becomes convex over this variable, which can be solved efficiently. We also propose an iterative algorithm to solve this problem distributively. We demonstrate by extensive simulation that the above mentioned both the approaches increase the overall network lifetime for a given power budget. Moreover, the distributed approach yields an optimal routing structure, outperforming the well-known shortest path tree based routing structure."}, {"label": 1, "content": "In this paper, a comparative analysis of various performance enhancement techniques in two-dimensional (2-D) atmospheric optical code division multiple access (OCDMA) system is studied in presence of beam divergence, multiple access interference (MAI), noise and atmospheric turbulence. Lognormal and gamma-gamma probability density functions (pdfs) are considered for evaluating fading process due to atmospheric turbulence. The study finds that double hard limiters, spatial diversity and error correcting code (ECC) can improve the performance of the 2-D atmospheric OCDMA system. Double hard limiters and ECC improve performance substantially as compared to spatial diversity. Additionally, the study also finds that double hard limiters are more cost-effective than the other two techniques. Hence, the study concludes that double hard limiters are superior to other performance improvement techniques for 2-D atmospheric OCDMA systems."}, {"label": 1, "content": "For serving traffic in the inter data centers which provide services such as, duplication of data and migration of the virtual machines, it is requisited to transfer voluminous data for which, under guarantee of a finishing time within the stipulated (i.e., pre-set) deadline, specific latency is tolerable. In this study, we propose offline routing and spectrum assignment (RSA) schemes for transferring deadline-compliant, voluminous data demands in elastic optical networks. The proposed schemes, which jointly optimize time and frequency domains, are initially formulated as an integer linear program (ILP). Additionally, we propose practical scheduling techniques, combining three methods of ordering demands and two RSA schemes. To evaluate the proposed ILP model and the scheduling methods, we conduct simulations considering realistic network parameters and topologies. Our results show that scalable methods achieve similar spectrum usage performance as the ILP model within reasonable times. Lastly, we provide a `rule-of-thumb' for selecting appropriate scheduling techniques based on the obtained results."}, {"label": 1, "content": "In this paper, we propose a progressive spectral mapping learning algorithm for throat microphone (TM) speech enhancement. Unlike previous full-band spectra mapping algorithms, this algorithm divides the spectra mapping from TM speech to Air-conducted (AC) speech into two tasks, one is the voice conversion task, and the other is the artificial bandwidth extension task. Long short-term memory recurrent neural network (LSTM-RNN) is further deployed as the mapping model. Objective evaluation results show that the TM speech quality is improved when compared with conventional full-band spectra mapping framework and DNN-based mapping model."}, {"label": 1, "content": "In satellite-borne Terahertz wave ground detection, accurately estimating the atmospheric absorption attenuation loss of Terahertz waves is crucial for various Terahertz communication modes, including wideband and high-speed networks, interstellar communication, satellite-ground station links, stratosphere aerocraft communication, long distance data transfer, and short-range wireless communication. To address this, this paper introduces an atmospheric absorption loss estimation software for satellite global THz wave ground detection. The realized functions of this software including scene establishment, basic functions and calculation methods were explained in detail. Finally, the monthly change calculation results of satellite ground detection with 0.34THz working band in 10\u00b0\u201390\u00b0 down-looking angle are given."}, {"label": 1, "content": "Indoor localization is a notoriously difficult problem, primarily because GPS signals are often unavailable. Recently, various radio frequency fingerprinting techniques have been proposed to identify indoor locations using simply received signal strength (RSS) measurements. In general however, RSS measurements are time-varying and are difficult to model for complex environments. This paper proposes the use of dictionary learning (DL) to generate high quality fingerprints that depend also on the channel characteristics for each location. To do this, we present an enhanced DL algorithm that leverages prior information about the channel distribution and produces fingerprints in real-time. Simulation results demonstrate the efficacy of the proposed approach."}, {"label": 1, "content": "In an Orthogonal Frequency Division Multiple Access (OFDMA) based multi-cellular WiMAX system, a suitable base station is obtained by cell search. In the cell search technique, in addition to timing and frequency synchronization, the detection of the frame start position is another basic task that the mobile terminal must successfully complete before establishing a communication link with the base station. In this work, we propose a dual correlation algorithm to improve the accuracy of frame detection synchronization. Additionally, we propose a joint blind channel estimation and equalization scheme to minimize the impact of channel fading in low SNR conditions. Compared with the existing scheme, the obtained scheme can obtain better frame detection system performance at a low signal to noise ratio."}, {"label": 1, "content": "Collisions between vehicles and pedestrians often result in fatalities for the vulnerable road users (VRUs). Thus, new technologies are needed to be developed for protecting the VRUs. In this paper, we propose using cloud computing technologies to handle the large amounts of safety-critical messages, given the high density of pedestrians and limited computing capabilities of base stations. Moreover, the wireless multi-hop backhaul technology is adopted to overcome the bottlenecks of limited transmission capability and queueing delay of the transmitted safety-critical messages between base stations and clouds. Based on the multi-hop wireless transmission scheme, the signal transmission success probability and delay between pedestrians and clouds are derived for performance analysis. We conduct numerical simulations to demonstrate the relationship between transmission success probability and received signal to interference plus noise ratio (SINR) threshold."}, {"label": 1, "content": "Unmanned aerial vehicle mounted base stations (UAV - BSs) can provide wireless cellular service to ground users in a variety of scenarios. The efficient deployment of such UAV-BSs while optimizing the coverage area is one of the key challenges. We model the placement problem as a multiple concentric circles placement problem to achieve our objective of maximizing the number of covered users. In this paper, we first highlight the properties of the 3D placement problem and we model the problem as a multiple concentric circles placement problem with the objective of maximizing the numbers of covered users. After some mathematical manipulations, we formulated a Mixed Integer Second Order Cone Problem (MISOCP) and proposed an improved Multi-Population Genetic Algorithm (MPGA) for horizontal dimensions placement problem. Our numerical simulations show that improved MPGA outperforms the Standard Genetic Algorithm (SGA) in solving this problem."}, {"label": 1, "content": "In a wireless sensor network (WSN), coverage holes can significantly impact the efficiency of data collection and the quality of service of the network. Detection of coverage holes is foundation of patching the sensor network to guarantee network quality of service. A new paper proposes a distributed coverage hole detection algorithm based on hole boundary nodes (HPNs-CHD). This algorithm uses a sensing disk model to identify the HBN nodes in the WSN and then uses probabilistic messages to detect coverage holes. The simulation results indicate that the proposed algorithm outperforms other two algorithms in terms of average energy consumption and average time of coverage holes detection."}, {"label": 1, "content": "Local matching approaches remain prevalent tools in real-time applications. Mismatch is a common situation in stereo vision, especially in local approaches. In this paper, we propose a truncated majority voting method (TMVM) to discriminate and reduce mismatches for local matching approaches in stereo. Our experiments using the Middlebury benchmark demonstrate that the proposed method can effectively identify and decrease mismatches while maintaining real-time capabilities."}, {"label": 1, "content": "The accurate estimation of power system frequency and amplitude is essential for power system monitoring, stability, control, and protection. Therefore, this study presents a new strategy for estimating the frequency and amplitude of the power system based on the variational mode decomposition (VMD) algorithm and the Cheb-function (Chebfun) approximation system. In this work, the spectral information of power signals is extracted using VMD as sub-signals or modes. Each mode is further interpolated by Chebyshev polynomials in continuous domain using Chebfun system. The instantaneous frequency and amplitude are estimated based on zero crossings and local extrema locations of the continuous function. The robustness of the approach is evaluated on various power system scenarios and the results are compared with other existing methods. The promising results suggest that the proposed approach can be used as an efficient candidate for power system frequency and amplitude estimation."}, {"label": 1, "content": "Opportunistic Networks, which include users as a vital component, represent a natural evolution of the mobile Ad Hoc Networks field and offer an example of how the Internet of Things can work. So, Opportunistic Networks due to their characteristics, need users- centric considerations when it comes to design Opportunistic Networks' routing, privacy, and authentications schemes. However, most mutual authentication schemes proposed for Opportunistic Networks do not consider, on the one hand, the gregarious aspect of users, and on the other hand, the parameter that pre-established contacts could be used in a mutual authentication process. Considering the factors pre-established contacts, Seed OppNet Identity, and traditional cryptography's principles, this paper proposes a realistic multiple levels authentication scheme for short-term and limited-time wireless network environment. This scheme offers anonymity and privacy to users and enhances the security of Opportunistic Networks overall."}, {"label": 1, "content": "A joint communication and state estimation problem in a Gaussian multiple access channel with common additive state is considered. The state process is assumed to be IID Gaussian and is known to both transmitters without causality. The receiver not only has to decode the messages from the transmitters, but also needs to estimate the state process to within some prescribed squared error distortion. We provide a complete characterization of the optimal sum-rate versus distortion performance."}, {"label": 1, "content": "Modeling of spectrum occupancy is important for better channel utilization, accurate spectrum sensing, and enhanced Quality of Service (QoS) to the primary user (PU) in a cognitive radio (CR) system. Traditional models heavily rely on PU activity, which varies spatially and temporally, making it challenging to develop an accurate model. In this work, a generalized Gaussian Mixture model (GMM) has been investigated for characterizing the spectrum occupancy of the PU in three spectrally different CR scenarios, viz. VHF/UHF band, GSM band, and ISM band. Additionally, the robustness of the GMM is validated through learning-based prediction using Recurrent Neural Networks (RNN). These findings suggest an approach that combines statistical and predictive modeling of spectrum occupancy to enhance dynamic spectrum access."}, {"label": 1, "content": "Internet of Things (IoT) is playing an important role in our lives. It connects lots of embedded devices, which can deal with very complicated and difficult tasks to facilitate our work. In this study, we explore two opportunistic relay selection schemes, namely single relay selection (SRS) and multi relay selection (MRS), to improve the physical layer security of IoT. In order to provide a secure environment for IoT, in this paper, we investigate two opportunistic relay selection schemes to further enhance physical layer security, which are single relay selection(SRS) and multi relay selection(MRS) respectively. Our simulation results indicate that MRS outperforms SRS regarding both OP and IP. Additionally, MRS offers a more optimal STP compared to SRS. Besides, the STP for MRS is also more perfect than that of SRS. Thus, our work offers a path towards achieving a secure and reliable IoT system."}, {"label": 1, "content": "The increasingly popularity of vehicles embeded with high computational devices and resources has attracted great interests as a mean for sharing resources in a new or existing cloud infrastructure. Vehicles can be consolidated into clusters to simplify management, improving the effectiveness and overall capability of a vehicular cloud. In pooling their resources, these vehicles can serve the upper layers of the network infrastructure. This paper explores the trends and state-of-the-art developments of vehicular clustering over the past five years, especially for parked vehicles in the fog computing paradigm. It was observed that static and dynamic clustering are the two common methods, albeit dynamic clustering is more common. Challenges and issues surrounding vehicular clustering are also discussed."}, {"label": 1, "content": "In this paper, for the defects which the basic genetic algorithm can get local optimum easily and converge slowly, the genetic algorithm is improved by constructing a suitable fitness function and improving genetic operators. The improved genetic algorithm is then utilized to select the optimal access network for a heterogeneous integrated wireless network, taking into consideration the Quality of Service (QoS) requirements of different types of businesses. According to Quality of Service (QoS) requirements of different business types, compared with combined the analytic hierarchy process (AHP) and the technique for order preference by similarity to an ideal solution (TOPSIS) and the basic genetic algorithm, the simulation results indicate that the improved genetic algorithm can find a network with higher fitness and better meet different QoS."}, {"label": 1, "content": "Visible light communication (VLC) transmits the wireless data through optical scintillation. However, the challenge of balancing illumination and communication remains. To address this, this paper proposes using dimming control to adjust brightness for human eyes and non-orthogonal multiple access (NOMA) to improve system throughput. First, a model combining signal power allocation with dimming control is established. Second, gain ratio power allocation (GRPA) and variable on-off keying dimming control are introduced to improve spectral efficiency. Third, the relationships between luminescent angles, user data rate, and luminous intensity are analyzed for indoor users. Experimental results show that the proposed GRPA scheme outperforms previous strategies in terms of user data rate at the same dimming factor. Additionally, optimal indoor VLC cell deployment of semi-angles and dimming factors is discussed."}, {"label": 1, "content": "Nowadays, audio generation plays an important role in human-computer interactive applications. However, machine-generated audio lacks the expressiveness and complexity of natural sound. Currently, conditional variational Auto-encoder (cVAE) has achieved excellent results in data generation, but original cVAE cannot avoid the defects caused by KL divergence which used in stochastic distribution measurement. This paper proposes the incorporation of Hellinger distance in the cVAE model. First of all, the experiment shows that using Hellinger distance can improve the weakness of KL divergence effectively. And then, the relationship between the latent space parameters and the generated music quality is analyzed by experiments, and we found the best generative parameter is the distribution centroid. Finally, the generated music is subjectively evaluated and the results show that it is significantly better than the original model."}, {"label": 1, "content": "Desirable properties of extensions of non-negative matrix factorization (NMF) include robustness in the presence of noises and outliers, ease of implementation, the guarantee of convergence, operation in an automatic fashion that trades off the balance between data approximation and model simplicity well, and the capability to model the inherently sequential structure of time-series signals. The state-of-the-art methods typically have only a subset of these aforementioned properties and seldom simultaneously possess them all. In this paper, we propose a novel approach that provides all these desirable properties by extending the automatic relevance determination framework in NMF from Tan and F\u00e9votte. Using an objective function derived from maximum a posteriori estimation of a Bayesian model, the authors develop majorization-minimization algorithms that effectively determine the correct model order, despite the impact of noise and outliers. Additionally, the proposed algorithms are rigorously analyzed for convergence. The authors also incorporate convolutive bases in the model to capture the temporal continuity of data. We perform experiments on both synthetic and real-world data sets to show the efficiency and robustness of our approach."}, {"label": 1, "content": "Most existing traffic simulation methods focus on simulating vehicles on freeways or city-scale urban networks, but relatively little research has been done to simulate intersectional traffic. However, relatively little research has been done to simulate intersectional traffic to date despite its broad potential applications. In this paper, we propose a novel deep learning-based framework to simulate and edit intersectional traffic. Specifically, we employ a combination of convolution network (CNN) and recurrent network (RNN) to learn the patterns of vehicle trajectories in intersectional traffic, based on an in-house collected dataset. Our approach not only simulates intersectional traffic but also allows for editing of existing intersectional traffic patterns. Through many experiments as well as comparative user studies, we demonstrate that the results by our method are visually indistinguishable from ground truth, and our method can outperform existing methods."}, {"label": 1, "content": "By conducting sparse coding and classifier training together, supervised sparse coding has proven to be effective in various recognition tasks. However, current methods often only consider linear classification, limiting their ability to handle highly nonlinear data. In this letter, we propose a new supervised sparse coding model by incorporating decision tree classifiers. Decision trees are ideal for dealing with non-linear properties of data, and the combination with sparse coding significantly improves its discrimination capabilities. Additionally, sparse coding can produce sparse de-correlated features that decision trees prefer. To further improve performance, an ensemble framework is used to close the loop of sparse coding and decision tree learning. Alternating between learning a dictionary for sparse coding and a decision tree for classification, a series of decision trees and dictionaries are produced to construct a decision forest for classification. Experiments with face recognition and scene classification demonstrate the superiority of this method compared to recent supervised sparse coding approaches."}, {"label": 1, "content": "In this work, we consider the problem of inferring links in a communication network, using limited, passive observations of network traffic. Our approach leverages transfer entropy (TE) as a metric for quantifying the strength of the automatic repeat request (ARQ) mechanisms present in next-hop routing links. In contrast with existing approaches, TE provides an information-theoretic, model-free approach that operates on externally available packet arrival times. We conduct a discrete event simulation of a wireless sensor network and demonstrate that our TE-based topology inference approach is robust to varying degrees of connection quality in the underlying network. When compared with an existing approach that employs linear regression based formulation of Granger Causality, our method exhibits a better asymptotic time complexity and significantly enhanced network topology reconstruction performance. Despite being suboptimal, our approach also boasts better time complexity, while maintaining reasonable performance, when compared to a causation entropy based optimal algorithm proposed in the literature."}, {"label": 1, "content": "Precision viticulture (PV) aims to improve the grapevine production efficiency, quality, and profitability, while reducing the environmental impact. The promises of PV are realized only if large areas are monitored with high spatial and temporal resolutions. This paper considers the integration of a wireless sensor network and a smart unmanned aerial vehicle platform. To this end, local variations of factors that influence grape yield and quality are measured and site-specific management practices are applied. This approach achieves real-time, uninterrupted monitoring of the vine growth environment, and on-demand imaging and high-resolution data collection from any specific location, thereby optimizing the production efficiencies and the application of inputs in a cost-effective way."}, {"label": 1, "content": "Network modeling of high-dimensional time series data is a crucial task with widespread usage in various applications such as macroeconomics, finance, and neuroscience. While the problem of sparse modeling based on vector autoregressive models (VAR) has been investigated in depth in the literature, more complex network structures that involve low rank and group sparse components have received considerably less attention, despite their presence in data. Failure to account for low-rank structures results in spurious connectivity among the observed time series, which may lead practitioners to draw incorrect conclusions about pertinent scientific or policy questions. To mitigate such challenges and accurately estimate a Granger causal interaction network after accounting for latent effects, we propose a novel approach to estimate low-rank and structured sparse high-dimensional VAR models. We introduce a regularized framework involving a combination of nuclear norm and lasso (or group lasso) penalties. Furthermore, we establish nonasymptotic probabilistic upper bounds on the estimation error rates of the low-rank and structured sparse components. We also introduce a fast estimation algorithm and finally demonstrate the performance of the proposed modeling framework over standard sparse VAR estimates through numerical experiments on synthetic and real data."}, {"label": 1, "content": "In this letter, we propose a sensor localization technique for partially calibrated arrays that are highly deformed and have multiple moving targets. Our method involves dividing the deformed array into several subarrays. The first subarray is composed of the pre-calibrated sensors, whereas the sensors in the other subarrays are uncalibrated. We estimate the positions of the sensors by analyzing the phase differences between the pre-calibrated and uncalibrated sensors. The phase ambiguities caused by the highly deformed sensor positions can be solved using the subspace orthogonality and the movement of multiple targets. Simulation results evaluate the performance of the proposed method, and the Cramer-Rao bounds are compared."}, {"label": 1, "content": "There is a numerous color spaces with different properties in literature. To find the appropriate and relevant color space for the fabric defect classification problem, we propose investigating the performance and robustness of the Local Binary Pattern (LBP) descriptor in a supervised context, using an SVM classifier. The experimental results show that the luminance-chrominance spaces are suitable for coding fabric defect with the classification accuracy obtained is 92.1%."}, {"label": 1, "content": "Audiovisual speech synchrony detection is an important part of talking-face verification systems. Previous research has mainly focused on visual features and joint-space models, while standard mel-frequency cepstral coefficients (MFCCs) have commonly been used to represent speech. We focus more closely on audio by studying the impact of context window length for delta feature computation and comparing MFCCs with simpler energy-based features in lip-sync detection. We select state-of-the-art hand-crafted lip-sync visual features, space-time auto-correlation of gradients (STACOG), and canonical correlation analysis (CCA), for joint-space modeling. To enhance joint space modeling, we adopt deep CCA (DCCA), a nonlinear extension of CCA. Our results on the XM2VTS data indicate substantially enhanced audiovisual speech synchrony detection, with an equal error rate (EER) of 3.68%. Further analysis indicates that failed lip region localization and subjects with beards account for most of the errors. Thus, the description of lip motion is the limiting factor, while the use of novel audio features or joint-modeling techniques is unlikely to boost lip-sync detection accuracy further."}, {"label": 1, "content": "Attitude motion periods of the unstable satellites are important parameters for space target surveillance. Traditional methods of period estimation are only applicable to single periods. A new double-period estimation technique based on variational mode decomposition (VMD) and mutual information is proposed for the rotation and precession of unstable satellites. Firstly, intrinsic mode functions (IMFs) and corresponding center-frequencies of the unstable satellite's radar cross-section (RCS) are determined through VMD. Then, through calculating and comparing the mutual information of IMFs and RCS sequence, the rotation period and precession period of satellite are obtained. The experimental results indicate that both rotation period and precession period can be estimated correctly. Compared with spectrum analysis, autocorrelation function and empirical mode decomposition (EMD), the phenomenon of frequency multiplication and mode mixing can be restrained effectively, and the accuracy of period estimation is improved."}, {"label": 1, "content": "In this paper, the improved face recognition method based on two-directional 2DPCA (two-dimensional principal component analysis) in each block of face images is proposed. Firstly, the face image is divided into several sub-images, and then the sub-image features of each corresponding block are extracted by two-directional 2DPCA according to the number of sub-images. The recognition rate is then improved using the support vector machine. Experimental results on ORL face database and YALE face database show that the proposed method is superior to any other 2DPCA methods in face recognition rate."}, {"label": 1, "content": "Railway detection tasks require a large number of images, but the lack of effective image classification methods makes it challenging to analyze detection images deeply. Using convolutional neural networks (CNN) to realize railway image scene classification is an effective technical means. This study presents a method for reducing database bias using Gradient-weighted Class Activation Mapping (Grad-CAM) to enhance scene classification accuracy, achieving an accuracy rate of 95.3% (top3) on Railway12 database. Our approach combines two key insights: (1) the limited amount of railway scene database makes it difficult for CNN to achieve high performance, so we transfer pre-trained ImageNet-CNN for fine-tuning on railway scene database; (2) we introduce the Grad-CAM visualization method to analyze the model's classification pattern and intuitively display possible database biases, providing a clear strategy for reducing dataset bias."}, {"label": 1, "content": "The variations of photoplethysmography (PPG) morphology for the pregnancies with preeclampsia (PE) were studied in this paper. PPG data from 16 hypertensive pregnancies with PE and 26 normotensive pregnancies were acquired by the standard medical monitor. To segment and quantify the descending domain of a pulse, the study introduced a novel hierarchical area ratio (HAR) parameter. The algorithm and features of HAR are fully explained and discussed in the paper. A rough PE distinction based on the statistics of HAR calculated from the original PPG signals was conducted with the precision of 72.7%, sensitivity of 100%, specificity of 76.9% and accuracy of 85.7%. The proposed HAR has shown potential for the quick and accurate distinction of PE."}, {"label": 1, "content": "Conventionally, the signal component frequencies are estimated by spectral peak search process, and suffered with the common signal mismatch problem (SMP). However, nonparametric methods such as MVDR and CCA have been shown to improve spectral estimation through magnitude squared coherence (MSC). This paper develops a novel scalar cost function based on the CCA MSC spectrum, utilizing local peaks to estimate signal frequency. Furthermore, a gradient-based adaptive-step algorithm has been presented to identify these local peaks. Simulation results demonstrate that the proposed algorithm offers notable improvements in frequency estimation accuracy with avoidance of the SMP."}, {"label": 1, "content": "In this paper, the focus is on designing cubic splines as tone correction functions for the achromatic component of a spherical color model. This model is advantageous as it provides more perceptually smooth color changes along its coordinates compared to commonly used color models HSV and HSL. However, using a tone correction function in this model can lead to gamut issues due to the higher number of color points it contains compared to the RGB color model. The paper demonstrates the gamut issue can be avoided when tone correction functions are well designed and the general tone correction techniques still work well in the spherical color model. A particular type of cubic splines is designed to serve the purpose, and these splines can be adopted by the general tone correction techniques including those for low-key, middle-key and high-key images with correspondingly selected parameters. Experimental results demonstrate that these cubic splines work well for tone correction in the spherical color model, ensuring that the tone correction technique seamlessly integrates with this model."}, {"label": 1, "content": "This paper presents a fast depth selection algorithm for CTU (frame coding units) based on machine learning. The algorithm addresses the lack of depth discrimination in initial division of coding units and the inefficiencies of the classifier's input feature selection. Firstly, the paper designs an initial division depth prediction strategy based on texture complexity and quantization parameters to skip nonessential sizes of coding units. Secondly, the input characteristics of the classifier are determined based on the relationship between bit-rate and distortion, and a selection strategy for the termination depth of coding units is designed. By skipping the calculation process of the time-consuming rate distortion cost, the ending dividing depth of the current coding unit can be judged in advance and accelerate the process of the inter-frame coding. The proposed algorithm decreases frame encoding time by 34.56%, while maintaining accuracy compared with HM-15.0."}, {"label": 1, "content": "Fundamental matrix estimation based on RANSAC will encounter the problems of computational inefficiency and low accuracy when outlier ratio is high. In this paper, an optimized method via modification of the RANSAC algorithm is proposed to solve these problems. The proposed technique makes use of an isolation forest algorithm to detect outliers in putative SIFT correspondences based on the consistency of features in location, scale, and orientation. The detection of these outliers eliminates several obvious correspondences, effectively improving the inlier ratio. Finally, fundamental matrix is estimated with the optimized set. It was observed that this method resulted in a remarkable increase in speed and accuracy compared to the RANSAC algorithm."}, {"label": 1, "content": "To address the challenge of underwater object image classification with limited training data availability, a new classification approach based on Convolutional Neural Network (CNN) is proposed. Firstly, an advanced method of Markov random field-Grabcut algorithm is adopted to segment images into two regions: shadow and sea-bottom. To account for the specific properties of the data, a CNN model is constructed with two parts: a convolutional part and a classification part, following the structure of Alexnet. Finally, the transfer learning technique is employed to train the CNN model to classify three distinct shapes of underwater objects (cylinder, truncated cone, and sphere). The method is validated on synthetic aperture sonar (SAS) datasets, and its accuracy is compared to that of Support Vector Machine (SVM) and CNN models that leverage only trial data. The proposed method achieves superior accuracy compared to the SVM and CNN models with limited training data, demonstrating its effectiveness in addressing the challenge of underwater object image classification."}, {"label": 1, "content": "Motivated by the remarkable performance achieved using deep learning strategies in solving action recognition tasks, an effective, yet simple method is proposed for encoding the spatiotemporal information of skeleton sequences into color texture images, referred to as Skeletal Optical Flows (SOFs). SOFs capture meaningful temporal information by representing the kinetic energy, predefined angles, and pair-wise displacements between joints over consecutive frames of skeleton data as color variations, thus making them highly interpretable. To exploit the discriminative features of SOFs for human action recognition, we employ a novel Convolutional Neural Network with Correctness-Vigilant Regularizer (CVR-CNN). Empirical results demonstrate the superior efficiency of our proposed method in terms of the generalizability of the generated model, training convergence speed, and classification accuracy on commonly used action recognition datasets such as MHAD, HDM05, and NTU RGB+D."}, {"label": 1, "content": "In recent years, there has been significant interest in the rapid estimation of parameters for maneuvering targets. However, many existing algorithms are plagued by accuracy and computational complexity issues. Furthermore, when multiple target parameters are estimated at the same time, traditional time-frequency methods suffer from cross-term interference. To address these problems, we propose a fast estimation algorithm for multi-maneuvering target parameters. The algorithm leverages the Higher-order Adjacent Cross Correlation Function (HACCF) expansion of radar echo signals, in which the auto item is constant and the cross term is a function of the adjacent time delay. The algorithm firstly takes the mean extraction of the signal's HACFF to extract the auto items, and inhibits the cross term. Then we can estimate the frequency of auto items further and get accurate estimation of maneuvering target acceleration. Numerical simulations show that the algorithm performs with high accuracy and requires minimal computations, enabling quick estimation of maneuvering target parameters. This algorithm can estimate the parameters of multiple maneuvering targets simultaneously with high accuracy."}, {"label": 1, "content": "Neuroimaging studies have demonstrated that multiple brain regions activate during cognitive tasks. Meanwhile, Real-time functional magnetic resonance imaging neurofeedback (rtfMRI-NF) can assist subject self-regulation brain activity. However, the neural mechanisms of rtfMRI-NF are unclear. To investigate this problem, we combined graph theory with resting state fMRI to explore the topological properties of functional brain networks. Subjects were provided with ongoing functional connectivity information related to emotion regulation. Our results showed that rtfMRI-NF training could alter the small-world properties and nodal degree in the temporal lobe, frontal lobe, limbic system. Together, our results suggested that rtfMRI-NF training was associated with alters in the topological properties of functional brain networks."}, {"label": 1, "content": "Sparse iterative covariance-based estimation (SPICE) method is a computational efficient sparse method for direction of arrival (DOA) estimation but has a poor performance in resolution and noise immunity. The high-order cumulant can extend the array aperture and reduce the Gaussian noise. Therefore, this paper proposed an improved SPICE based on fourth-order cumulant, which shares the same features of SPICE but has higher resolution and outperforms in low SNR case. Moreover, its computational cost is comparatively low by distilling the un-redundant data of uniform linear array. Simulations were conducted to validate and evaluate the proposed method."}, {"label": 1, "content": "A new algorithm for multi-frame image super resolution (SR) has been proposed, which utilizes Bayesian modeling with natural image prior modeled by fields of experts (FoE). Multi-frame SR can be used to obtain a high resolution (HR) image from a set of degraded low resolution (LR) images without changing any hardware device. However, SR is well known to be an ill-posed problem. So state-of-the-art solutions usually formulate the problem with Bayesian modeling techniques, which infer the HR image based on not only the LR input images but also on prior information about the HR image. Nonetheless, majority of the Bayesian SR approaches utilize simple prior models such as L1 norm, TV prior, and Laplacian prior, which do not exploit the statistics of natural scenes accurately. In this paper, a Bayesian multi-frame image SR approach using a FOE model as the prior for natural images is presented. The Maximum a Posteriori (MAP) framework is employed to estimate the HR image. The proposed method cannot only capture the statistics of natural images well, but also require less computational power than the other Bayesian modelling methods such as Sampling methods and Approximate inference. The proposed method shows superior or comparable results to the state-of-art multi-frame SR methods."}, {"label": 1, "content": "The learning methods have recently achieved great success for single image super-resolution. Convolutional neural networks (CNNs) have proven to be robust, achieving state-of-the-art performance. In this paper, we propose a Dynamic Multi-mapping Convolutional Network (DMCN) that improve the SR performance. Rather than relying on fixed kernels like Bicubic interpolation, we utilize dynamic filters to resize the LR input in our pre-trained module. Based on an end-to-end manner, more accurate and effective features from middle layers can be learned. Additionally, our multi-mapping module provides extra information and high-frequency details, resulting in sharper, high-resolution images. Through extensive quantitative and qualitative evaluations, our algorithm effectively improves image resolution."}, {"label": 1, "content": "When capturing photos in low-light situations with artificial lighting, a common challenge is choosing between a short exposure setting that results in a dim, noise image with sharp outlines, or a longer exposure setting that yields a bright, saturated image yet with blurred areas. Unfortunately, neither option is ideal for most cases. Good brightness and color information are retained in longer-exposure images, whereas sharp outlines are retained in shorter ones. In this study, we introduce a patch-based approach that combines these images to produce a better final image. The method employs a coarse-to-fine approach to identify inconsistent pixels resulting from moving objects, followed by extracting information from both exposures using a novel patch-based technique. Experimental results show that the proposed method effectively preserves sharp edges of the short-exposure image, and maintains the color, brightness, and details of the long-exposure image."}, {"label": 1, "content": "In this paper we propose a novel method for detecting adversarial examples by training a binary classifier with both origin data and saliency data. In the case of image classification model, saliency simply explain how the model make decisions by identifying significant pixels for prediction. When a model produces incorrect classification output, it often learns incorrect features, resulting in inaccurate saliency data. Our approach shows good performance on detecting adversarial perturbations. We also evaluate the generalization ability of the detector, demonstrating that detectors trained using strong adversaries exhibit strong performance against weak adversaries as well."}, {"label": 1, "content": "The output frequency response for most DACs rolls off according to the sin(x)/x frequency-response envelope [1]. This paper outlines a method for deriving a FIR filter that is designed, using the minimax sense, to compensate for the frequency response of DACs by incorporating ripple constraints into its design criteria. A function for estimating the filter order is also presented in detail in this paper, which can effectively reduce design time and provide a precise reference for configuring resources in the top-level system design [2]. The simulation example clearly demonstrates the advantages of using a compensation filter designed in the minimax sense, and proves the accuracy of the filter order estimation function."}, {"label": 1, "content": "Spacecraft detection is a crucial issue in aerospace information processing and control, providing dynamic state information of the target to support decisions with regard to recognition, classification, and cataloging. Although numerous spacecraft detection methods exist, most of them cannot achieve real-time detection, and are still lack of better accuracy and fault-tolerance for different scenes. Recent advances in deep learning algorithms have produced the regression-based convolutional neural network YOLOv2, which has demonstrated excellent detection performance, surpassing other state-of-the-art methods. This paper for the first time applies CNN to the detection of spacecraft and sets up a dataset for target detection in space. Our method starts with image annotation and data augmentation, and then uses our improved regression-based convolutional neural network YOLOv2 to detect spacecraft in an image. This performance shows significantly lower time overhead and better robustness to changes in spacecraft rotation and illumination."}, {"label": 1, "content": "In this article, we presented a highly effective method for recognizing facial expressions that is based on a combination of G-2DPCA feature extraction and Sparse Representation-based Classification (SRC). Gabor filters with five scales and eight directions are first employed for feature extraction. To address the high dimension of Gabor features, we select one out of forty Gabor filters with an optimal parameter pair of scales and directions to filter facial images. Next, we utilized two-dimensional principal component analysis (2DPCA) to represent the images and reduce the dimensionality of the data. It retains the 2D geometric structure of an image, and the image matrix does not need to transform into a vector, which reducing the computation time greatly. Finally, Gabor plus 2DPCA (G-2PCA) features are regarded as the atoms of dictionary in SRC. The experimental results show that our proposed method outperforms existing facial expression recognition algorithms."}, {"label": 1, "content": "Emergency evacuation simulation is an important measure to effectively avoid personnel deaths and injuries. In view of the safe evacuation in underground tunnels, an emergency evacuation simulation system for personnel in three-dimensional tunnel is put forward. The system is designed to consider various scenarios encountered by people in the tunnels and visualize their behavior accordingly. Through the design of three-dimensional model, evacuation path generation, visualization simulation, evaluation and analysis, the experiment of the underground tunnel evacuation is carried out. The experimental results demonstrate that this system is effective in simulating tunnel evacuation and can be successfully used for emergency evacuation guidance during disasters as well as evacuation simulation drills before a disaster."}, {"label": 1, "content": "With the emergence of big data and the development of mobile devices, mobile data mining has received more and more attention. It shows its unique advantages, but it also exposes its inability to handle large datasets efficiently. To address this issue, we enhanced traditional mobile data mining procedures by incorporating cloud computing technology. Our proposed model, called MobileWeka2, demonstrated its ability to process large datasets effectively through experimentation with various data sets. Experimental results show that this model can efficiently process large data sets and solve the problems of traditional mobile data mining."}, {"label": 1, "content": "To effectively distinguish between real targets, clutter, and dense multi-false targets detected by radar, we propose an algorithm based on a factorized convolutional neural network (CNN). Our CNN model uses depthwise separable convolution to factorize the processing of inputs and reduce model parameters. To reduce the parameters of the model, we establish the simplified factorized convolutional neural network by reducing the numbers of both convolutional filters and connection nodes of fully connected layers. Our results with measured data demonstrate that the simplified factorized CNN significantly improves discrimination of real targets, clutter, and dense multi-false targets compared to existing models, and achieves a parameter reduction of more than 90% compared to a recently proposed model."}, {"label": 1, "content": "Lip reading, the ability to recognize text information from the movement of a speaker's mouth, is a difficult and challenging task. Recently, the end-to-end model that maps a variable-length sequence of video frames to text performs poorly in real life situation where people unintentionally move the lips instead of speaking. The goal of this work is to improve the performance of lip reading task in real life. The model proposed in this article consists of two networks that are visual to audio feature network and audio feature to text network. The model's accuracy was tested on a dataset that included unintentional lips movement, and the results showed a 92.76% accuracy in lip reading tasks."}, {"label": 1, "content": "This research proposes a significant reduction in the processing time to solve the concurrent AC multistage transmission network expansion and reactive power planning problem with security constraints, by an innovative search space reduction strategy. The strategy involves modeling the concurrent planning problem as a mixed-integer linear programming (MILP) problem, using an AC branch flow formulation to represent the steady-state operation of the transmission network. The innovative strategy consists of obtaining a stage-by-stage solution pool of the MILP model as a static problem, to identify the significant candidate lines. The insignificant lines are not considered as candidates in the multistage problem, thus reducing the search space. Using the updated database, the multistage MILP problem can be solved efficiently. The evaluation of the proposed methodology is done using the IEEE 24- and 118-bus test systems, showing the performance of the proposed methodology."}, {"label": 1, "content": "Understanding the dynamics of a power system requires that information be presented in a meaningful way. Large-scale modal results are presented for a large interconnected power system using visualization methods to reveal the underlying oscillations in the system. By employing visualization tools, the quality of mode estimation among several bus signals is captured, different modal interactions existing in the system are identified, and modal power flows are visualized to track sources of grid oscillations. Wide-area visualization is used in a synthetic large interconnected power grid to reveal critical information about the dynamic state of the system that would not have been captured from a graphical plot of the time-varying signals."}, {"label": 1, "content": "Forecasting of consumer electricity usages plays an important role to make total smart grid system more reliable. However, due to the multiple variables associated with individual residential consumers, predicting residential load levels remains a challenging task. For planning of the electrical resources and to balance demand and supply, accurate forecasting tasks are critical. This study proposes a Deep Neural Network (DNN) based short-term load forecasting approach for residential consumers. In this work, we compare the Mean Absolute Percentage Error (MAPE) value for residential electricity dataset using different types recurrent neural network (RNN). The preliminary results indicate that Long short-term memory (LSTM) based RNNs perform better than simple RNNs and gated recurrent unit (GRU) RNNs for a single user with 1-minute resolution based on one year of historical data sets."}, {"label": 1, "content": "This paper is aimed at evaluating the reliability indices of a distribution network (DN) considering the rerouting of the interrupted customers during outages. The DN is represented by its circuit graph that is used to analyze the effect of the failure mode (FMEA method) and compute the reliability of the system. To have a more accurate estimation, the capacity of the feeder and the voltage deviation when the rerouting occurs, are also considered. After the occurrence of an outage, the problem is finding the optimum tree by interchanging the status of normally open (NO) and normally closed (NC) switches to restore as many as interrupted customers subject to defined constraints. The proposed algorithm provides a systematic approach to solving the restoration problem, decreasing the search space, and making it suitable for analyzing big DN sizes. Additionally, a ranking method is introduced to measure the impact of each NC switch on reliability improvement."}, {"label": 1, "content": "This study aims to examine the accuracy of short-term forecast for MISO's locational marginal pricing (LMP) data sets. A collection of methods such as rolling average, autoregressive integrated moving average (ARIMA), and long short-term memory (LSTM) were applied to a three-year data sets and compared against MISO\u2019s forecasting approach. The results show that the use of recurrent neural networks (RNN) can provide a 25% improvement in forecasting accuracy compared to MISO's method."}, {"label": 1, "content": "The paper presents the results of the research work funded by Salt River Project Agricultural Improvement and Power District (SRP) on maximizing the economic benefits to customers installing residential rooftop PV systems in SRP territory. The optimized discharge of the battery power which would help in the reduction of Demand Charge paid by the customer was the primary goal. To achieve this goal, the study employed Machine Learning algorithms to improve load forecasting techniques used in the industry. The improved battery discharge algorithm would also reduce the battery charge-discharge cycles (cycling aging) thus, improving the battery life. Tests were conducted in Arizona, focusing on a residential rooftop grid-tied PV with storage system installed at the Tempe campus of Arizona State University."}, {"label": 1, "content": "This paper introduces the design and implementation of a Python-based software package for cyber-physical power system research called Andes. Andes is developed in an attempt to bridge the gap between the traditional power system analysis and the fast-growing needs for cybersecurity studies. The architecture design of Andes is discussed in detail, with a focus on research and development considerations. Various examples are presented to demonstrate the capabilities of Andes in modeling, monitoring, and visualizing cyber-physical power system studies. Examples are shown using Andes for modeling, monitoring, and visualization of cyber-physical power system studies."}, {"label": 1, "content": "This paper presents a new geometric approach for analyzing parameter identifiability in models of power systems dynamics. When a model of a power system is to be compared with measurements taken at discrete times, it can be interpreted as a mapping from parameter space into a data or prediction space. Generically, model mappings can be interpreted as manifolds with dimensionality equal to the number of structurally identifiable parameters. Empirically it is observed that model mappings often correspond to bounded manifolds. We propose a new definition of practical identifiability based the topological definition of a manifold with boundary. In many ways, our proposed definition extends the properties of structural identifiability. We construct numerical approximations to geodesics on the model manifold and use the results, combined with insights derived from the mathematical form of the equations, to identify combinations of practically identifiable and unidentifiable parameters. The approach is applied to several examples of dynamic power systems models."}, {"label": 1, "content": "In this paper, we present a scheduling scheme for household Electric vehicles based on deep neural network based demand forecast. A novel clustering based Short Term Load Forecasting (STLF) using deep neural network (DNN) is presented in this paper to forecast the household and EV demand. The forecasting was conducted on demand profiles for 200 households in the Midwest region of the United States. We utilized a Tensor-flow based deep learning platform to develop the DNN structure. The households are clustered according to demand profiles and the grouped consumers are used as the forecasting parameters. The scheduling model uses the forecasted household and EV demand values to develop a linear programming based optimization model to minimize the electricity cost for consumers. The optimization model takes into account household and cluster constraints to prevent sudden surges in power demand during low-price periods."}, {"label": 1, "content": "Classic DC power flow and Generalized Generation Distribution Factors (GGDF) are used for modeling the transmission network constraints in a DC optimal power flow (OPF). The former is praised for its straightforwardness, precision, and stability, while the latter is celebrated for its capacity to convey transmission power flows as a function of power generation with fewer equality constraints. This research study compares the efficacy and efficacy of both methods by analyzing their performance on a PJM 5-bus system and an IEEE 57-bus system, incorporating transmission losses, using various commercial optimization solvers."}, {"label": 1, "content": "A distributed energy management system has been developed and tested for an interconnected multi-microgrid system. The distributed energy management system is formulated using the alternating direction method of multipliers (ADMM) and is implemented using the CVX platform of the MATLAB environment. In this work, microgrids are interconnected and communicate with each other in order to minimize the operational cost of the system and to derive maximum profits via energy exchanges with the main grid. The simulation results show that the proposed method is effective."}, {"label": 1, "content": "The recent past has seen an influx of new generator interconnection to the power grid. Some of these new generator interconnections occur at places where there isn't enough transmission capability and hence get curtailed or penalized during the real time operations. In this paper a procedure is proposed to calculate the maximum possible MW injections with the help of Power Transfer Distribution Factors at each of the buses in the power grid without violating transmission limits. This in turn identifies areas on the grid with abundant transmission capability. Although the computation involved in this procedure is intensive, GPUs were employed to accelerate it, resulting in a significant speed-up gain of up to a factor of 186 for a 9241 bus system."}, {"label": 1, "content": "Power distribution systems require continuous monitoring due to the increasing integration of renewable energy resources and the growing load demand. To enhance situational awareness and develop new monitoring algorithms, synchrophasors have been implemented in distribution systems. This paper proposes a voltage monitoring algorithm based on the synchrophasor-based linear state estimation method. The algorithm combines a set of early warning indicators and the BDS independence test to detect voltage instability in a timely manner while avoiding false alarms when the system is away from the stability boundary. Numerical study has been conducted in the Quebec test feeder to show the effectiveness of the method."}, {"label": 1, "content": "A statistical model has been developed to predict the output power and energy of Solar Photovoltaics (PV), using a multiple input single output (MISO) system based on Jackknife regression. The model generates PV power in kilowatts, with inputs that include irradiance, precipitation, ozone, ambient temperature, and atmospheric aerosol components. The model is trained and tested on data from National Renewable Energy Laboratory and residual statistical tests are applied to validate the estimation results. An absolute error of less than 1 kW is observed for 90.6 % of the predicted values that corresponds to a percentage error of less than 8.33 % for the 12 kW system under study."}, {"label": 1, "content": "Current IoT services are typically reliant on data communication technologies that do not utilize the public switched telephone network (PSTN). Since the telephone numbers have been assigned to machine-type devices, PSTN switches can play a role in IoT service routing. To facilitate interaction between users and IoT devices utilizing PSTN switches, we have developed a PSTN-based IoT mechanism. To our knowledge, this is the first PSTN-based IoT solution in the world. With this mechanism, all PSTN customer premises equipment (CPE; fixed-line and mobile phones) can access IoT services without installing any software (mobile apps). Through the reuse of existing PSTN infrastructure, PSTN-based IoT provides telecommunication-grade service, security, and network management for IoT, all of which would be prohibitively expensive to develop using non-PSTN-based IoT methods. Our approach makes it convenient for existing CPE to access IoT applications, which will significantly boost the growth of the IoT service industry."}, {"label": 1, "content": "The digital revolution in both academia and industry is being led by IoT, which has brought convenience to people's daily lives. However, security and privacy issues have become challenges. Blockchain, a decentralized database based on cryptographic techniques, is promising for IoT security, which may influence a variety of areas including manufacture, finance, and trading. The blockchain framework in an IoT system is an intriguing alternative to the traditional centralized model, which is struggling to meet some specified demands in IoT. In this article, we investigate typical security and privacy issues in IoT and develop a framework to integrate blockchain with IoT, which can provide great assurance for IoT data and various functionalities and desirable scalability including authentication, decentralized payment, and so on. We also suggest some possible solutions to these security and privacy issues in IoT based on blockchain and Ethereum to show how blockchain contributes to IoT."}, {"label": 1, "content": "An increasing number of wireless intelligent equipment is applied to ICS networks. To address this issue, a hybrid-augmented device fingerprinting approach has been developed to improve traditional intrusion detection mechanisms in ICS networks. To address these concerns, a hybrid-augmented device fingerprinting approach is developed to enhance traditional intrusion detection mechanisms in the ICS network. Taking the advantage of the simplicity of the program process and stability of hardware configurations, we first measure inter-layer data response processing time, and then analyze network traffic to filter abnormal packets to achieve the intrusion classification and detection in ICS networks. The device fingerprinting- based intrusion classification and detection approach is evaluated using the data collected from a lab-level micro-grid, and forgery attacks and intrusions are launched against the proposed method to investigate its robustness and effectiveness."}, {"label": 1, "content": "The ubiquity of 802.11 WiFi and the miniaturization as a result of Moore's law has recently enabled the success of IoT. From smart lightbulbs to smart toasters, many home appliances are now becoming both Internet-enabled and interconnected through WiFi. This allows for the vision of smart homes that run themselves, leaving human operators in full control - or so we think. Despite the progress made since the early days of this technology, the same cannot be said for the vulnerabilities these smart devices pose. We analyze a set of common smart home appliances - a lightbulb, power switch, motion sensor, security camera, and home assistant - putting their vulnerabilities to the test to see what a 21st century home intruder could discover."}, {"label": 1, "content": "Covert timing channels have emerged as a viable alternative for transmitting confidential information in an untrusted IoT environment, owing to various security threats. This article aims to investigate the susceptibility of IoT to covert timing channels over mobile networks. It presents the system model of a covert timing channel for IoT and then analyzes whether the traditional covert timing channels based on inter-packet delays apply to IoT over 4G/5G networks. Given that there are so many covert timing channels proposed for computer networks, we investigate different kinds of construction approaches of covert timing channels to illustrate the feasibility of building covert timing channels for IoT, including packet-reordering-based, rateswitching- based, packet-loss-based, retransmission- based, and scheduling-based covert timing channels. Furthermore, this article also discusses several detection methods of revealing and preventing covert timing channels for IoT."}, {"label": 1, "content": "This paper proposes a unified framework for designing sliding-mode control to stabilize delayed memristive neural networks (DMNNs) with external disturbances. The use of this framework enables the DMNNs to achieve finite-time and fixed-time stabilization through the selection of specific control parameter values. It is proved that the system responses can be made reaching the designed sliding-mode surface in finite and fixed time, and then stay on it. Additionally, the sliding-mode control can reject external disturbances. Numerical simulations and comparisons with related works confirm the effectiveness and superiority of the proposed approach."}, {"label": 1, "content": "A Flexible Machine Vision (FMV) Inspection System has been developed that requires minimal retuning in hardware and software as applications are changed up. To test the system's flexibility, it was utilized to inspect three different kinds of small parts: plastic gears, plastic connectors, and metallic coins. The system was required to distinguish between four different known styles of each part, and also one unknown style, resulting in a total of five classes. In previous work, a hybrid Support Vector Machine (SVM) classifier was utilized for the connector application. When applied to the coin application, the hybrid SVM could not achieve the target performance of 95% accuracy. A new hybrid that method that combines SVM and an Artificial Neural Network (ANN) or ANN-SVM classifier was subsequently developed to overcome this problem and the results are presented in this paper. The results of this novel approach are presented in this paper, and the utilized image library is available at http://my.me.queensu.ca/People/Surgenor/Laboratory/Database.html."}, {"label": 1, "content": "The handling of non-rigid objects, such as cables, with industrial robots is characterized by nonlinear, timedependent and location-dependent equations for the object's behavior. Controlling these objects requires real-time capable simulations that can run in cycle-time and highly precise simulations for path planning. To address these requirements, we present a common simulation environment wrapped in a container that includes all dependencies and is easy to set up and orchestrate. Automated code generation is used to set up a standardized communication and virtualization in a Docker container such that only the simulation itself has to be provided by a user. The advantage of containerization, compared to the simulation running natively, is finally demonstrated with a sample robot simulation under heavy CPU load."}, {"label": 1, "content": "The world has long been concerned about environmental problems and the associated issues. However, the emergence of IoT and the development of smart cities, buildings, and grids have opened up new avenues for addressing these pressing concerns. The success is only possible in the real sense when the problematic issues can be addressed. This paper proposes an Internet of Things Technology based protection and monitoring of environment of a poultry house. The proposed software-based hardware can monitor environmental parameters such as air temperature, humidity, O2, CO2 concentration levels, and NH3 concentration. The wireless sensor is responsible for the effective data collection of the described parameters and also these are source coordination and control. The hardware is implemented successfully at different sites within the poultry shed. By implementing this scheme, the poultry industry can achieve both a safe environment and increased profits. This scheme will earn a safe environment and profit to the poultry industry."}, {"label": 1, "content": "We present NavREn-RL, an approach to NAVigate an unmanned aerial vehicle in an indoor Real ENvironment via end-to-end reinforcement learning (RL). In order to meet the constraints of small-sized, cost-efficient drones with minimal sensing capabilities, we have designed a suitable reward function that takes into account these factors. To aid convergence, we have integrated a collection of expert data and knowledge-based data aggregation into the RL process. We conducted experimentation using a Parrot AR drone in various indoor arenas, and compared the results with other baseline technologies. Our approach demonstrated effective obstacle avoidance and successful navigation across different arenas. A video of the drone navigating using NavREn-RL can be viewed at https://youtu.be/yOTkTHUPNVY."}, {"label": 1, "content": "In the engineering phase of modern manufacturing systems, simulation-based methods and tools have been established to face the increasing demands on time-efficiency and profitability. To apply these solutions, model-based digital twins are created as multi-domain simulation models that describe the behavior of the manufacturing system. During the production process, a data-driven digital twin arises in the context of industry 4.0 based on an increasing networking and new cloud technologies. Recent developments in machine learning offer new possibilities in conjunction with the digital twin. These range from data-based learning of models to learning control logic of complex systems. This paper proposes a combined model-based and data-driven concept of a digital twin. Overall, this approach provides an efficient way to simulate manufacturing processes and improve manufacturing system design in terms of time and profitability."}, {"label": 1, "content": "Today, numerical controls (CNC) are the standard for the control of machine tools and industrial robots in production and enable highly flexible and efficient production, especially for frequently changing production tasks. A numerical control has discrete inputs and outputs. Within the NC channel, however, it is necessary to analytically describe curves for the calculation of the position setpoints and the jerk limitation. To overcome this challenge, we propose a new approach in this paper. This can lead to a drop in production speed and thus to longer production times. The generative models are trained to create curves of certain types such as linear and parabolic curves or splines directly as discrete point sequences. This is based on the use of deep generative models and allows the direct generation of interpolated toolpaths without calculation of continuous curves and subsequent discretization. Our preliminary results with straight lines and parabolic curves demonstrate the feasibility of this new approach for the generation of CNC toolpaths. This approach is very well feasible with regard to its parallelization and reduces the computing effort within the NC channel. First results with straight lines and parabolic curves show the feasibility of this new approach for the generation of CNC toolpaths."}, {"label": 1, "content": "The Internet of Things (IoT) is an essential technological application that has emerged as a result of the Industrial Revolution, also known as Industry 4.0. Today, smartphones are not only used for communication, but also as wireless smart devices to access, process, and send information at lightning-fast speeds through enhanced features such as cameras, GPS, and OTT apps. Various development capabilities on other devices that enable a person to do, social media, video conferencing, video streaming, tracking, navigation, drone, remote, forecast, monitoring, payment and all other things that may be computationally proceed by sensor and actuator devices. With the development of cutting-edge technology, this smart capability can be applied to any device, and devices can interact with each other through the internet network. In Industry 4.0, IoT is a crucial aspect that is broadly embodied in smart city (policy-driven), smart industry (business-driven), and smart life (experience-driven) solutions. Utilizing the capabilities of Industrial IoT rightly can lead Nusantara appropriate development as a large archipelago and agrarian area that rich of natural resources. This research investigates the concept of IoT and its use cases against various socio-economic and specific geographic challenges, and then evaluates them based on PESTLE strategic analysis for external and internal factors. The result are Development Strategy and Technology for Developing Nations."}, {"label": 1, "content": "Internet users in Indonesia have increased in recent years. Many product service providers who provide internet access services in accordance with tariff options and their superiority. In this research, sentiment analysis on social media to some service data service operator to see the level of public satisfaction in using data service of telecommunication operator for internet access in Indonesia. The sentiment data was collected using Twitter's API, and pre-processing was conducted to clean and tag the data. The preprocessing stage is then processed to process raw initial data, then perform POS tagging and weighing the word with TF-IDF calculation and perform classification using the Naive Bayes Classifier (NBC) method. This study yields an average value of 94,5% precision rate, 93,3% Recall and 99,09% Accuracy."}, {"label": 1, "content": "Overall Equipment Efficiency (OEE) is a valuable tool for measuring the true production capacity of equipment, and the Theory of Constraints (TOC) has been utilized to enhance system production efficiency. In order to obtain the OEE improvement method based on TOC, bottleneck identification model and buffer model are established. A multi-attribute bottleneck identification model has been created on the basis of the Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) and Entropy Method. Furthermore, a time buffer model based on the Drum-Buffer-Rope (DBR) theory has been introduced. This OEE improvement method has proven to significantly enhance the OEE of bottleneck equipment. Moreover, due to the optimization of bottlenecks, the system production efficiency is improved. A semiconductor package process was used to validate the efficacy of this method."}, {"label": 1, "content": "There have been many varieties of driving assistance, and one aspect of them is the scope of emergency braking. Researchers have analyzed emergency braking and proposed different approaches to identify them. One important scenario to consider is the mistaken acceleration during emergency braking when the accelerator pedal is wrongly pressed instead of the brake pedal. The objective of this study is to develop a classifier using evolutionary computation to identify mistaken pedal pressing based on pedal behavior. A driving simulator is used to collect the data, and genetic programming was used to perform the evolution."}, {"label": 1, "content": "The paper applies image classification techniques to protect users of the Internet from inappropriate information. It describes a modular hierarchical classification system and proposes an approach to integrate image classification modules without additional retraining. The experiments devoted to the integration of an image classification module into the current infrastructure of the Web classification system are outlined and analyzed."}, {"label": 1, "content": "In this paper, a framework for collaborative localization of heterogenous systems is presented. Our approach builds on the original MSCKF framework and introduces a collaborative MSCKF filter, operating at two levels, that enables decentralized 3D collaborative localization without any external computation systems. To achieve that, based on MSCKF localization, we first propose a range based collaboration that we optimize using the extracted environment constraints, an operation allowed by the use of a truncated unscented kalman filtering updates. The collaborative filtering is managed to not impact the original MSCKF odometry properties. We apply the framework to collaborative localization of aerial and ground robots and demonstrate its effectiveness through a series of experiments."}, {"label": 1, "content": "Autism spectrum disorder (ASD) is a developmental disorder that has received a lot of attention from researchers due to its urgency and pervasiveness. The diagnosis and intervention of ASD is still complicated and hard to handle. Fortunately, the rapid development of technology has led to new methods for the auxiliary diagnosis of ASD, such as face detection, gaze estimation, and action recognition. The paper proposed a preliminary visual system for assistant diagnosis of ASD in a core clinical testing scenario-response to name. The system uses eye center localization and gaze estimation to measure the subject's responses. The purpose of this paper is to analyze the feasibility of the system and optimize the sensing structure and evaluation indicators."}, {"label": 1, "content": "This paper presents a novel approach to multilabel classification of High-Voltage (HV) discharges captured using the Electromagnetic Interference (EMI) method for HV machines. The method involves feature extraction from EMI time signals emitted during the discharge events, utilizing 1D-Local Binary Pattern (LBP) and 1D-Histogram of Oriented Gradients (HOG) techniques, and combining them to provide a feature vector. This feature vector is then implemented in a naive Bayes classifier that is designed to identify the labels of two or more discharge sources present within a single signal. The performance of this novel approach is measured using various metrics including average precision, accuracy, specificity, hamming loss etc. Results demonstrate a successful performance that is in line with similar application to other fields such as biology and image processing. This first attempt at multilabel classification of EMI discharge sources opens up new possibilities for research in the field of HV condition monitoring."}, {"label": 1, "content": "The lie is very detrimental to the fraudulent acts of many people who were cheated. The lies are common in the general population. One of these cues is related to our eyes, which can reveal changes in eye tracking and pupil diameter that occur unconsciously when someone is telling a lie. In the context of this final task, researchers have developed a lie detector method that leverages eye tracking and pupil diameter changes through the use of the Wavelet Transform and Gabor Image Processing techniques. By employing a decision tree algorithm to classify the results, researchers were able to determine with high accuracy whether someone is lying or not. With the final test results are accurate. This research has the precision value of 97%, 94%, and recall accuracy 95% of testing has been done."}, {"label": 1, "content": "In recent years, clustering has emerged as a promising approach for facilitating data routing and data aggregation in Wireless Sensor Networks (WSNs). Although clustering based routing approaches are appropriate for small-scale networks, they do not fit large scale WSNs as it is the case in LEACH [1]. Indeed, clustering suffers from the adverse effects of isolated nodes in the network and some coverage problems. The proposed approach makes the major functions of LEACH applicable to large-scale WSNs whose dimension is much larger than the largest transmission radius of the sensor nodes. This imposes a dynamic decomposable structure on the network topology, resulting in a set of smaller subnetworks. The latter imposes a dynamic decomposable structure on the network topology which results in a set of smaller subnetworks. Such decompositions are implemented through a smart m-level hierarchical clustering process. Moreover, the proposed approach involves a two level data aggregation. Therefore, LEATCH-L is a promising solution for large-scale WSNs that can enhance the efficiency and effectiveness of data routing and data aggregation in these networks."}, {"label": 1, "content": "This paper presents a data assimilation technique for social agent-based simulation to fit real world data automatically by a reinforcement learning method. The approach involves using a hidden Markov model to estimate system states during the reinforcement learning process. The proposed method can improve simulation models of the social agent-based simulation incrementally when new real data are available without total optimization. To demonstrate the feasibility of the proposed technique, it was applied to a housing market problem using real Korean housing market data."}, {"label": 1, "content": "When expressing concerns about the credibility of simulation studies, simulation data have been traditionally in the focus. However, what about another and, some might argue, even more central product of simulation studies, i.e., the simulation model itself? Therefore, it is necessary to assess the credibility of the simulation model. To accomplish this, information about the process of generating the simulation model is required, which is referred to as provenance. Provenance connects entities and activities involved in the generating process. The provenance of a simulation model relates to the refinement, extension, composition, calibration, and validation of simulation models to the diverse sources used in these processes. Unambiguously means for specifying entities are central to exploiting this information. Thus, a formal domain-specific language for modeling facilitates assessing and reusing simulation models. Similarly, a declarative domain-specific language for specifying simulation experiments, helps utilizing simulation experiments done with earlier models for future models. Thus, provenance, information about the past, does not only allow to understand the present, but also to design the future, in opening up new avenues for generating and analyzing simulation models."}, {"label": 1, "content": "In this paper, we derived the analytical expressions of the system performance (in term outage probability and throughput) of the power splitting half-duplex power beacon-assisted energy harvesting relay network in both amplify-and-forward and decode-and-forward modes. Moreover, the analytical results are also demonstrated and convinced by using Monte-Carlo simulation. Our numerical findings indicate that the analytical and simulation results match well together, considering all possible system parameters."}, {"label": 1, "content": "Currently, there is a lack of tools available for validating 5G scenarios. Due to the increasing traffic demand on 5G networks, network operators are in search of cost-effective solutions. One such solution being adopted is the multi-tenancy approach, which requires changes to the network architecture to accommodate user mobility. This approach increases service dynamism making it necessary to have tools that provide these new capabilities to be able to validate each development. This work presents a novel experimentation framework for the emulation of 5G scenarios providing them with real-time user mobility and multi-tenancy. The functionality of this novel framework has been validated through different experiments."}, {"label": 1, "content": "Human activity measurement and classification has been hot research topic for several years. While most solutions revolve around mobile phones, some wearable devices have specific functionalities for tracking activity. The goal of this paper is to propose a solution for recognizing human activity and detecting falls, which will offer more safety for individuals working in demanding environments. The system works in conjunction with a monitoring solution that provides real-time information about all workers and automatically raises an alarm in the event of an accident or unusual conditions."}, {"label": 1, "content": "This paper introduces a novel approach to representing microprocessor instruction set truth tables utilizing High-Level Decision Diagrams (HLDD). A behavior level fault model is defined for the microprocessor control parts on the basis of instruction level truth tables (TT). Two methods are proposed for creating HLDDs from TTs with minimization of the edges on graphs: greedy algorithm, and branch and bound algorithm (B&B). Additionally, the B&B algorithm utilizes a simple and fast computable lower bound to prune the search space. Experimental data of using the fault model for several microprocessors and comparison data are provided to show the efficiency of the proposed high-level fault model over the gate-level Stuck-at-Fault (SAF) model."}, {"label": 1, "content": "This paper aims to compare the performance of two different methods for predicting the future values of a vehicle's slip angle - conventional adaptive finite impulse response filters and feed forward multi-layer neural networks. The obtained results depending on a number of inputs and a prediction horizon are compared in terms of prediction error and a required number of operations for executing the algorithms."}, {"label": 1, "content": "High-speed permanent magnetic synchronous machine (PMSM) drive systems offer several advantages compared to low and medium speed drive systems, such as higher power density, capability of directly driving high speed loads. Due to reliability and rotor dynamic balance problem, sensorless drive is preferred in high-speed drive system. This paper proposed a model reference adaptive system based sensorless control method of a high-speed permanent magnetic machine with 1- F startup strategy. The proposed method can guarantee reliable startup ability and achieve smooth transition between the open-loop and closed-loop control scheme. Also, more accurate rotor position estimation is obtained with the compensation of voltage in high frequency."}, {"label": 1, "content": "Ad-hoc Mobile Cloud (AMC) was developed as a solution to the connectivity challenges faced in Mobile Cloud computing (MCC). Thus, mobile devices teamed up in groups to share their resources among one another which are majorly Web Services, Storage and Computing resources. However, potential participants in AMC often feel a level of insecurity as they envisaged loss of control over various personal and confidential data in their mobile devices which can occur in the course of sharing of resources. To address this issue, a trust management system is needed to safeguard the interests of the participating mobile devices. This study proposes a Multi-Criteria Trust Management system (MCTM) architecture for AMC that identifies trustworthy mobile devices and alerts participants of any malicious acts. This system provides an avenue to identify trustworthy mobile devices to whom they can carry out resource sharing and as well alert or notify them of any malicious act that could have happened. The MCTM system provides mobile users with the confidence to participate in the AMC system, knowing that their personal and confidential data is protected. By implementing this trust management system, AMC can increase its appeal to users who wish to share resources through mobile devices."}, {"label": 1, "content": "Recommendation systems for mobile phones play a crucial role in helping mobile operators achieve their desired profit targets. In a client inferred market, the number of contract users and contract phones is especially significant for mobile service operators. With the increasing number of mobile cellular telephone contracts available, it has become imperative to develop recommender systems that help users find suitable contracts based on their usage patterns. To achieve this, a hybrid of both collaborative and content-based filtering was used in this study. A prototype of a mobile recommender system was developed and evaluated based on precision and recall. The developed recommender system was able to successfully recommend packages to subscribers. A precision-recall curve was produced, and it showed good performance of the system. This study successfully showed that a hybrid system was able to recommend products to the mobile subscribers."}, {"label": 1, "content": "In this paper, we introduce an alternative solution to the many existing IoT data acquisition and storage systems. Our approach involves the design and development of a prototype electronic circuit extension for Raspberry Pi development board that allows for sensor data collection. There is also presented a Pi4Java API based Java application used for sensor data collection and storage. To further support the storage of these large volumes of data, we set up an Apache Cassandra database cluster on low-cost servers that provide high availability. In addition, a web application is also presented, that allows different data visualization operations to be performed on the stored data. With our IoT data acquisition, storage and visualization solution, we present a complete system that is both efficient and cost-effective."}, {"label": 1, "content": "Feature extraction is crucial for monitoring the states and evaluating the performance of mechanical electro-hydraulic systems (MEHS). However, MEHS poses a challenge as it has complex multi-domain energy conversion properties, especially during varying operation conditions, making it difficult to extract desired features effectively. Moreover, conventional signals are challenging to collect and analyze due to the mixed coupling of different kinds of information. Therefore, based on a power distribution analysis of MEHS, it is found that the change rate of the kinetic energy (CRKE) can be considered as a suitable index for evaluating the performance, such as energy saving and output stationarity of the considered MEHS. In order to characterize the magnitude of CRKE, a cooperation analysis method is proposed by using internal and external features. The method selected kinetic energy stiffness (KES) and instantaneous speed fluctuation (ISF) as the internal and external features, respectively. According to a Lissajous figure-based information fusion approach and the order tracking technology, a systematic method is developed to obtain the magnitudes of KES and ISF. By analyzing the complementary advantages and mutual relationship of KES and ISF, the system's performance under varying operation conditions was evaluated. The proposed method was verified through experiments on a real rig. The results showed that changes in KES and ISF effectively reflected the operational changes, and lowering KES loss improved the efficiency of the system while restraining the ISF."}, {"label": 1, "content": "Planetary gear is an important part of the transmission system of large electromechanical equipment. Therefore, it is very important to monitor the degradation of the state of the planetary gear. In this context, a new approach is proposed for recognizing the degradation state of planetary gear through multiple perspective features and the linear local tangent space alignment (LLTSA) algorithm. First, the time domain features of the original vibration signal are extracted, which have the statistical properties and global significance. Moreover, detailed features are extracted using an improved complete ensemble empirical mode decomposition with adaptive noise, to pay closer attention to the finer details of the vibration signal. In order to solve the problems of information redundancy and interference features, the original features are processed by LLTSA, and the extraction of low dimensional sensitive features can be achieved. Finally, the optimized support vector machine is implemented to recognize the low-dimensional sensitive features. The result shows that the proposed method can recognize different degradation states of planetary gear accurately and effectively."}, {"label": 1, "content": "The aim of this research is to develop an innovative low cost and affordable platform for smart home control and energy monitoring interfaced with augmented reality. The goal is to educate people about energy usage, particularly as fuel costs continue to rise, and provide new methods of interaction for those with disabilities. In order to increase the awareness of energy consumption, we have developed an interactive system using Augmented Reality to show live energy usage of electrical components. This system allows the user to view his real time energy consumption and at the same time offers the possibility to interact with the device in Augmented Reality. The energy usage was captured and stored in a database which can be accessed for energy monitoring. We believe that the combinations of both, complex smart home applications and transparent interactive user interface will increase the awareness of energy consumption."}, {"label": 1, "content": "Due to the various advantages that the cloud can offer to robots, there has been the recent emergence of the cloud robotics paradigm. Cloud robotics permits robots to unload computing and storage related tasks into the cloud, and as such, robots can be built with smaller on-board computers. The use of cloud-robotics also allows robots to share knowledge within the community over a dedicated cloud space. In order to build-up robots that benefit from the cloud-robotics paradigm, different cloud-robotics platforms have been released during recent years. This paper critically reviews and compares existing cloud robotic platforms in order to provide recommendations on future use and gaps that still need to be addressed. To achieve this, 8 cloud robotic platforms were investigated. Key findings reveal varying underlying architectures and models adopted by these platforms, in addition to different features offered to end-users."}, {"label": 1, "content": "Internet of Things has gained the attention of almost everybody due to its capability of monitoring and controlling the environment. IoT enables data-driven decision-making through the use of everyday devices that have been equipped with sensing, processing, and communication capabilities. One of the main and important aspects of any IoT device is its communication capability for transferring and sharing data between other devices. IoT devices mainly use wireless communication for communicating with other devices. The industry and the research community have proposed many communication technologies for IoT systems. In this paper, the authors present the results of an in-depth study carried out on the benefits and limitations of these communication technologies."}, {"label": 1, "content": "This prototype system known as the Wearable Instantaneous Ball Speed Estimator (WIBASE) was designed to measure the bowling speed of a cricketer during training. This equipment is vital for coaches as it helps them assess the performance of fast bowlers and their ability to bowl consistently. WIBASE comprises two primary hardware components- a computer and a wrist-worn electronic board with a 3-dimensional (3D) acceleration sensor. The system tracks the three-axis acceleration generated by the arm's movement during the delivery of the ball and stores the data. The system tracks the three-axis acceleration generated by the movement of the arm when delivering the ball and stores these values. A Python script on the computer receives the filtered acceleration, consisting of both static and dynamic acceleration. The computer runs a Python script that receives the filtered acceleration which consists of both static acceleration and dynamic acceleration. The WIBASE has undergone three sets of experiments, and test results indicate that the equipment can effectively track 3D acceleration, derive the bowler's speed, and log all data into a file in real-time. The results obtained from the three sets of experiments that were conducted show that the WIBASE can track the 3D acceleration of the hand when bowling, derive the speed of the bowlers and display the speed on a computer while logging all the data into a file."}, {"label": 1, "content": "In the field of Cyber Security there has been a transition from the stage of Cyber Criminality to the stage of Cyber War over the last few years. According to the new challenges, the expert community has two main approaches: to adopt the philosophy and methods of Military Intelligence, and to use Artificial Intelligence methods for counteraction of Cyber Attacks. This research paper focuses on the results achieved by the Technical University of Sofia in the implementation of a project that utilizes intelligent methods to improve computer network security. The analysis of the feasibility of various Artificial Intelligence methods has shown that a method that is equally effective for all stages of the Cyber Intelligence cannot be identified. For Tactical Cyber Threats Intelligence, the research team experimented with a Multi-Agent System, while Recurrent Neural Networks were suggested for Operational Cyber Threats Intelligence."}, {"label": 1, "content": "Data infrastructure and quality are crucial components that influence the overall health of any organization. Due to this fact, stakeholders expect a flawless experience, real-time solutions and support. Unfortunately, these expectations are often too high for IT departments who are inundated with various users' requests. However, the Big Data era and its analytics have enabled organizations to predict and forecast users' needs to produce an excellent user experience. Achieving this relies on intuitive design, error-free coding, and quality performance. There is increasing demand for better business analytics so as to enable organizations establish solid foundation by building out a data management ecosystem that delivers flexibility and performance required by cognitive solutions. The ultimate aim is to generate the actionable insights needed to achieve academic business goals through leveraging better analytics to process the continually generated data from various sources. This study followed an interpretive paradigm taking a case study of a South African university and employed semi-structured interviews. This study found that data value and the use thereof in the management of academic divisions of the university should be consistent with faculty business plans. By aligning business analytics strategically with faculty plans and overall institutional strategic objectives and targets, it will not be haphazardly employed but will be deliberate in addressing needs and in turn produce the desired results."}, {"label": 1, "content": "The rapid advancement of technology is disrupting traditional job markets, leading job seekers to develop new digital skills suitable for the digital economy. This phenomenon has a significant impact on the economy, particularly in developing countries. Consequently, it is crucial for higher education institutions to align their offerings with industry requirements across all disciplines to sustain and improve the economy. To address this issue, this paper presents a framework designed to determine the alignment of the digital skills acquired by students in higher education to the digital skills required by the industry. This alignment will aid higher education institutions in improving the digital skills preparedness of their graduates, and ultimately sustaining the digital economy. As the required digital skills may vary in each sector, the proposed framework is not discipline-specific and may be employed to establish an alignment between any discipline in higher education and the respective industry that its graduates feed into. To develop this framework, authors reviewed relevant articles to determine the factors influencing digital skills preparedness for graduates entering the industry. The authors have systematically reviewed related articles to determine the factors influencing the digital skills preparedness of graduates for industry. By utilizing this model, higher education institutions can more effectively prepare graduates with the necessary digital skills to succeed in the rapidly transforming job market."}, {"label": 1, "content": "This article is depicting the Strengths and weaknesses of Artificial Intelligence related to the improvement of customer online and offline experience, and the possible methods in order to measure them. The retail industry has increasingly recognized the role of AI in shaping customer experiences. By analyzing various data extraction tools, the article seeks to provide guidance on interpreting information, assisting companies in turning such insights into tangible improvements in the online and in-store retail experience. The research not only presents a snapshot of the current state of customer experiences, but it also provides predictions on how AI will become pivotal in the future. Hence, with this research, we also want to provide an insight on how this experience could be improved in the future, and how it will most likely be inherent to our daily customer experience."}, {"label": 1, "content": "Image retrieval is gaining prominence in the area of medical image processing especially in the domain of fundus images. This work aims to propose a proficient algorithm for features mining in Fundus images and thereby extract the information through Content Based Image Retrieval process. The automatic extraction of significant features, such as exudates, assists medical professionals in efficaciously treating patients with various illnesses. Although multiple methods of extracting these features are available, they lack in retrieval aspect of the information or the accuracy of the feature extraction."}, {"label": 1, "content": "Advancements in robotics and cloud computing have resulted in the emergence of cloud robotics, which enables robots to benefit from remote processing, greater memory and computational power, and massive data storage. The integration of robotics and cloud computing has often been regarded as a complex aspect due to the various components involved in such systems. In order to address this issue, different studies have attempted to create cloud robotic architectures to simplify representation into different blocks or components. However, limited study has been undertaken to critically review and compare these architectures. Therefore, this paper analyzes and compares existing cloud robotic architectures to identify key constraints and recommend future improvements. In this study, seven architectures were analyzed and compared, and the results indicate that there is limited evaluation of existing architectures due to security concerns."}, {"label": 1, "content": "Grinding in ball mills is a crucial technological and industrial process which is used for the reduction of the size of particles with variant physical, chemical and mechanical characteristics. The control performance of the ball mills' grinding process is of outmost importance as this will determine the profit, where the energy consumption, the product quality and time efficiency are commonly concerned. In this paper, nonlinear model predictive control for ball mill grinding process is implemented. To evaluate the performance of the proposed control system, the economic performance, time delays, and energy consumption in the grinding process are analyzed using the Discrete Element Method (DEM) software. The results from experimental tests indicate the proposed method to be effective."}, {"label": 1, "content": "This paper reviews the development and application architecture of an expert system to assist the Mauritian population with queries they may have about labor or employment law. The expert system makes use of Machine Learning, Speech Recognition/Synthesis and Natural Language Processing techniques to converse with users through a web interface. One of the key features of the system is its large knowledge base, which is derived from an accessible and easy-to-understand source, \"Understanding Employment Law and Remuneration Orders in Mauritius,\" written by Ved Prakash Torul. The knowledge base is created from \"Understanding Employment Law and Remuneration Orders in Mauritius\", written by Ved Prakash Torul [1], which is a simplified version of the Employment Relations Act and the Employment Rights Act. The evaluation period of the expert system involved a preliminary testing period, which yielded impressive results. The expert system allows users to communicate and express their employment issues, so that they are aware of their next course of action, either they are an employer, employee, or a union. However, the paper concludes that further evaluation needs to be conducted to explore the system's performance in depth. Through the evaluation, it was concluded that the expert system was able to respond to individual responses with a Precision of 66% and Recall of 85%. While the Expert System is able to converse with users on certain topics on Employment Law, further evaluation would need to be conducted. With continued development and evaluation, it is hoped that such systems will improve access to legal resources and empower individuals to make informed decisions about their employment situations."}, {"label": 1, "content": "Association rule mining is a crucial technique in data mining that enables the discovery of significant attribute relationships, which can be valuable for decision-making. Typically, association rule mining uses an item-set manipulation approach that relies on categorical data types. When a dataset contains numerical attributes, they will need to be discretized before rule mining. At the moment, most unsupervised data discretization methods do not account for data distributions, and users have to try different methods and discretization settings in order to improve rule mining results. In this paper, we propose using TwoStep clustering for data discretization. Unlike traditional discretization methods, TwoStep automatically identifies the discretization intervals by considering the unique data distribution properties of each attribute. In our experiments, we evaluated the performance of Apriori algorithm based on four datasets, whereby each dataset was pre-processed using TwoStep and three other commonly used discretization methods. Our results show that TwoStep produced the greatest number of high-quality rules, as compared to common discretization methods."}, {"label": 1, "content": "In our modern world, almost every aspect of our lives is directly affected by the revolution of digitalization. From our smartphones to our cars, every device we use relies on computer chips and software to function. These technologies are interconnected via the Internet of Things (IoT) and make up what are known as cyber-physical systems. This paper addresses the development of Intrusion Detection Schemes for IoT. Data breaches and manipulation continue to be a significant concern. This paper aims to address the development of Intrusion Detection Schemes for the IoT, examining recent hacker methodologies and tools to identify the most effective ways to protect IoT networks. After discussing the problems posed by these risks, the paper will explore existing intrusion detection schemes and their limitations before proposing a more effective solution."}, {"label": 1, "content": "Recent studies have exposed the vulnerability of deep neural networks (DNNs) to small alterations in the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. To this end, a new method is proposed that generates one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. Using this method, the study found that around 67.97% of natural images from Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images could be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Therefore, this study presents a different perspective on adversarial machine learning by exploring the susceptibility of current DNNs to low dimension attacks. In addition, the use of DE in this context highlights its effectiveness in generating low-cost adversarial attacks against neural networks to assess their robustness."}, {"label": 1, "content": "In this paper, the design of the control chart when the variable of interest follows the gamma distribution under the neutrosophic statistical interval method (NSIM) is proposed. The NSIM is used to derive important metrics such as the average run length, the probability in-control, and the probability of out-of-control. The neutrosophic control chart coefficient will be determined by the algorithm under the NSIM. The neutrosophic average run length is calculated for various shifts and specified parameters which provides insight into the control chart's efficiency. The authors discuss the effectiveness of the proposed control chart through the use of a simulation study and a real-life example."}, {"label": 1, "content": "With the increasing application of Internet of Things (IoT) technology in the medical field, the collection of medical data has grown rapidly. However, labeling these data requires significant costs and specialized domain knowledge. Thus, the demand for building an efficient and high-quality clinical decision support model with small amounts of labeled medical data has become a pressing research topic. To address this issue, we propose a novel semi-supervised learning approach combined with generative adversarial networks (GANs) to support clinical decision making in the IoT-based health service system. Our approach utilizes GANs to increase the number of labeled data and to compensate for imbalanced labeled classes through the generation of additional artificial data, thereby improving the semi-supervised learning performance. Extensive evaluations on a collection of benchmarks and real-world medical datasets show that the proposed technique outperforms the others and provides a potential solution for practical applications."}, {"label": 1, "content": "We propose a top-down approach for formation control of heterogeneous multiagent systems, based on the method of eigenstructure assignment. Given the problem of achieving scalable formations on the plane, our approach globally computes a state feedback control that assigns desired closed-loop eigenvalues/eigenvectors. We characterize the relation between the eigenvalues/eigenvectors and the resulting interagent communication topology, and design special (sparse) topologies such that the synthesized control may be implemented locally by the individual agents. A hierarchical synthesis procedure is developed to enhance computational efficiency. Additionally, we expand the proposed methodology to attain fixed-size formation and circular motion, and simulation examples are provided to demonstrate the efficacy of the approach."}, {"label": 1, "content": "In recent years, significant progress has been made in the field of visual object detection, a fundamental task in industrial intelligence. Most existing methods are designed to work on single, well-captured still images. However, an intelligent robot has the capability to adjust its viewpoint to get better images for detection. Therefore, active object detection becomes a very important perception strategy for intelligent robots. In this paper, by formulating active object detection as a sequential action decision process, a deep reinforcement learning framework is established to resolve it. The paper also proposes a novel deep Q-learning network (DQN) with a unique dueling architecture, featuring two separate output channels: one predicting action type and the other predicting action range. By combining the two output channels, the action space is explored more efficiently. Several methods are extensively validated and the results show that the proposed one obtains the best results and predicts action in real time."}, {"label": 1, "content": "With the increasing adoption of distributed photovoltaic generation and energy storage systems in the power system, there is a need for more comprehensive demand-side model structures to accurately depict the dynamic performance of the system. In this paper, a composite demand side model structure with load, distributed photovoltaic generation, and energy storage system together with a model parameter identification method are proposed to improve the traditional load model identification. The traditional load model identification is improved with the proposed model structure and parameter identification method. The demand-side model structure is first proposed and then simplified for identification at a high voltage level bus. The ambient signal data and disturbance data based model parameter identification method is proposed for the new demand side model structure using the differential evolution optimization method. The case study results for the WSCC 9 bus system show the effectiveness of the proposed model structure. The case study results on the WSCC 9 bus system demonstrate the effectiveness of the proposed model structure, while the results on a simplified 500-kV network of the Guangdong Power Grid verify the effectiveness of the parameter identification method."}, {"label": 1, "content": "We propose a novel approach to estimate the area of green algae in geostationary ocean color imager multispectral images using a cofactor-based endmember extraction strategy. Our approach improves the efficiency of the N-FINDR, which is widely used for the same purpose, from two perspectives. First, our strategy exploits the cofactor matrix for searching the largest simplex volume, which just computes matrix inverse and determinant for a small number of times (or even once). This makes the process significantly more efficient. Secondly, we obtain optimal endmembers empirically through a few recursive iterations of cofactor matrix updates, instead of maximizing volumes with random initializations as in N-FINDR. Experimental evaluation in terms of green algae area estimation validates that our strategy achieves the same accuracy as N-FINDR with much more efficiency."}, {"label": 1, "content": "The complexity of modeling deformable materials and infinite degrees of freedom has led to a lack of transfer of rigid robot control techniques to soft robots. Thus, most model-based control techniques developed for soft robots and soft haptic interfaces are specific to the particular device. In this letter, we propose a general approach for stiffness control of soft robots that is applicable to any robot geometry and various types of actuation. Building on previous work that uses finite element modeling for position control, we establish the relationship between end-effector and actuator compliance, including inherent device compliance, to determine the appropriate controlled actuator stiffness for a desired stiffness of the end-effector. Such stiffness control, as the first component of impedance control, can be used to compensate for the natural stiffness of the deformable device and to control the robot's interaction with the environment or a user. We verify this stiffness projection on a deformable robot and integrate it into a haptic control loop to create a virtual fixture."}, {"label": 1, "content": "In this paper, the parameters of the derivative of blood pressure signals for the waveform classification of the wrist pulse have been analyzed. The method used here shows the relationships between pulse waveforms and the amplitudes of the characteristic points of the first derivative of blood pressure signals. The algorithm considered in the paper can be used for the computerization of pulse diagnostics."}, {"label": 1, "content": "A model of neurons for biometric authentication, capable of efficient processing of highly dependent features, based on the agreement criteria (Gini, Cramer-von-Mises, Kolmogorov-Smirnov, the maximum of intersection areas of probability densities) is proposed. An experiment was performed on comparing the efficiency of neurons based on the proposed model and neurons on the basis of difference and hyperbolic Bayesian functionals capable of processing highly dependent biometric data. In addition, various variants of hybrid neural networks capable of being trained on a small number of examples of biometric patterns (about 20) were proposed. To collect dynamic biometric patterns, ninety individuals entered handwritten and voice patterns over the course of a month in the experiment. Intermediate results on recognition of subjects based on hybrid neural networks were obtained. Number of errors in verification of a signature (handwritten password) was less than 2%, verification of a speaker by a fixed passphrase was less than 6%. Testing was performed on biometric samples obtained at a future time after the formation of the training sample."}, {"label": 1, "content": "The paper considers the method of forming a training sample for intelligent methods of predicting electricity loads based on artificial neural networks. The training dataset is designed to meet the criteria of being informative and compact. The research highlights the effectiveness of this approach, which significantly enhances the accuracy of forecasts compared to other conventional methods."}, {"label": 1, "content": "An effective technique for the loss probability calculation in queueing systems with heavy tails is developed. This technique is particularly useful for analyzing queueing systems with power-law distributions, which are commonly used to model network devices that operate under fractal traffic. The study investigates the form of the loss probability, as it relates to buffer capacity in these systems. The effect of the channel number in queuing systems on the dependence is examined. In practice, the developed rapid technique and obtained results of its applying might be used for solving of engineering problems of analysis and design of modern computer networks."}, {"label": 1, "content": "Cooperative navigation is a promising set of approaches for increasing the accuracy of navigating of vehicles as well as road safety in difficult environment such as urban canyons. One key technology used in cooperative navigation is the Dedicated Short-Range Communication (DSRC) standard, which facilitates the sharing of sensor measurements between nearby vehicles. Usually cooperative navigation is based on sharing GNSS and other primary sensors measurements between nearby vehicles via DSRC (or other telecom systems such as 3G/4G). In addition to communication, the on-board IEEE 802.11p DSRC receiver allows to measure the angle between vehicle's building axes and direction of received signal (from nearby vehicle). DSRC signals could share GNSS measurements and mutual headings of other surrounding vehicles. By fusing the coordinate measurements and corresponding heading angles of surrounding vehicles onboard, cooperative navigation technology can significantly improve the accuracy of vehicle navigation in challenging environments. Overall, the proposed approach has the potential to provide tangible benefits for increasing the accuracy and safety of vehicle navigation in challenging environments."}, {"label": 1, "content": "This work considers the problem of diagnostics and control of technical state (TS) of various industrial equipment. The relevance of the problem is caused by a significant increase in the level of requirements for the reliability of industrial facilities, the aging of existing equipment and technological complexes and thus the complexity of systematizing of various diagnostic information for decision-making. Because of this there is a need to develop a universal automated information system based on the method of diagnostics of the TS of a variety of industrial objects. Numerous factors affect the TS of equipment, prompting the use of a technical state index (TSI) as a complex indicator whose calculation method relies on the mathematical model of fuzzy sets. The diagnostics of TS is carried out based on a system of indicators, the configuration of which is related to the structure of the equipment. This work presents the results of developing a decision support system for managing complex technical and software objects. The work also shows a block diagram of the algorithm of how this system functions, describes the functionality and interface of the developed software."}, {"label": 1, "content": "After reviewing the current methods for addressing risk assessment, it became clear that they suffer from limitations related to computational power and access to essential data. Therefore, in such cases it is advisable to use fuzzy mathematical methods. This study focuses on evaluating different approaches for using fuzzy methods to perform risk assessment with uncertain information."}, {"label": 1, "content": "The reliability and lifetime determine the levelized cost of energy (LOCE) of photovoltaic (PV) modules and effectiveness of PV system. Despite recent attention given to this theme by researchers, there remains a lack of an effective method to model the power degradation of PV modules. Therefore, this paper put forward to adopt gamma process to establish the relationship between PV modules power degradation, and temperature, relative humidity (RH). It then predicts the service lifetime of PV modules under accelerated damp-heat conditions. Firstly, accelerated damp-heat tests are carried out on three different temperature and RH levels. Based on Peck model, a data transformed method is proposed to obtain more power degradation data under other seven damp-heat conditions. Secondly, gamma process with an exponential transformation is applied to model PV modules power degradation under accelerated damp-heat conditions. The relationship between power degradation and temperature and RH is established through theoretical derivation and validated through experimental data. Then Expectation Maximum (EM) algorithm is proposed to estimate model's parameters. Finally, PV modules lifetime under several different damp-heat conditions is predicted. It is found that PV modules lifetime is approximate 20 to 25 years under (50\u00b0/45% RH) condition. But we also conclude that PV modules lifetime sharply decreases as the increment of temperature and RH. Therefore, more factors or other test types should be considered in later accelerated tests."}, {"label": 1, "content": "With the ever-increasing number of diagnosed cases of Parkinson's Disease in the Philippines, there is a need for Ambient Assisted Living systems that will help improve the quality of life and independent living of patients with Parkinson's Disease. Currently, there are a lot of existing Ambient Assisted Living systems, such as the RAReFall Detection system, which incorporates various sensors, such as wearables, external sensors, and smartphone sensors to detect and recognize human activities. However, these existing systems are not easily accessible due to the costly and complex nature of the equipment being used. It will incorporate the use of smartphone accelerometer and gyroscope sensors to detect and categorize daily activities and falls of Parkinson's Disease patients, providing immediate response and appropriate advice to prolong their active participation in their communities."}, {"label": 1, "content": "PV power generation has a number of disadvantages, namely, periodicity, volatility and the like. When it comes to connecting with the power grid, it can have a negative impact on the power system. So it is important to predict the power of PV power generation system accurately. In fact, the more similar the weather conditions, the more similar the photovoltaic power generation system. To address this issue, this research aims to introduce a novel PV Generation Power Prediction model that is based on GA-BP Neural Network with Artificial Classification of History Day. Initially, we classify historical weather data manually. Then, we develop different PV Generation Power Prediction models for each specific weather condition utilizing BP neural network and genetic algorithm (GA-BP neural network). Ultimately, we predict the power of the next day using GA-BP neural network tailored for the weather condition of the following day."}, {"label": 1, "content": "Due to an excessive number of databases, unbalanced development, and lagging sensing infrastructures, distributed network data is plagued with inconsistency, missing data, large measurement errors, and other quality issues that hinder the advancement of smart distribution networks. In order to gain a deeper understanding of the more intricate underlying rules and provide more effective decision-making support for power systems, it's imperative to study data mining and analysis techniques that are appropriate for vast amounts of data under the current situation. This paper studies on the method of identifying bad data for multi-temporal and multi-spatial data in distribution networks and propose a method to identify bad data using likelihood-ratio test for 3D spatio-temporal data. In order to speed up the data processing rate, a 3D-LRT method based on multi-threading and Hadoop parallelization methods is proposed."}, {"label": 1, "content": "At present, the injection of false data (FDI) in power system network brings a direct challenge to state estimation and reduces the reliability of the system. However, high frequency and accurate data collected by the WAMS system can effectively deter FDI attacks. This study aimed to optimize the configuration of PMU to address the problem of FDI. By ensuring the overall observability of the system and taking the zero injection node into account, PMU configuration was optimized to mitigate the effect of FDI attacks, improving state estimation data accuracy to the maximum extent possible. Taking IEEE14 IEEE30 and IEEE57 standard nodes as examples, the data accuracy was improved to the maximum extent with the minimum number of PMU, and the feasibility of the method was verified."}, {"label": 1, "content": "Studying the propagation mechanism of cascading failure and identifying the vulnerable lines in the power grid is of great significance for the prevention control of cascading failure. The existing works pay more attention to vulnerable lines and high-risk path, which are hard to explain the overall propagation characteristic of cascading failure. This paper applies sequential pattern mining technology to cascading failure analysis. We begin by introducing the concept of cascading failure pattern (CFP) and propose a cascading failure pattern mining algorithm (CFPMA) based on PrefixSpan. The relevance of lines is then calculated based on the result of mining algorithm. Test result on the IEEE 39-bus system shows that the proposed method can find out the CFP which can clearly show the overall propagation mechanism of cascading failure. Furthermore, test results under different power flow snapshots indicate that the CFP doesn't change with system status."}, {"label": 1, "content": "The wind power curve is a crucial tool in characterizing wind power output features and is essential for wind power planning and operation research. The wind power curve is a high dimension matrix data with local property. Therefore, an effective technique needs to be found to reduce the dimension of the curve. This paper introduces the latest techniques of artificial intelligence and deep learning to explore a new method for reducing the dimension of the wind power curve. The convolutional autoencoder of typical deep learning framework is redesigned, and it learns feature representation from massive history data. The experiment result shows that the proposed autoencoder is better fit the wind power curve dimensionality reduction study."}, {"label": 1, "content": "Transmission network extension is imperative due to the impact of transmission lines on power market businesses. However, the process is currently restricted due to various factors, including financial constraints, health concerns surrounding electromagnetic fields, and substantial contemplation. The implementation of flexible AC transmission system (FACTS) might be fruitful choice to enhance power flow capacity (PFC) in existing transmission network. One option that may prove fruitful in enhancing power flow capacity (PFC) within the existing transmission network is the implementation of flexible AC transmission systems (FACTS). These devices diversify the tracks of exchanged power over transmission lines by changing the bus voltage angle and the reactance of transmission lines. This study focuses on a modified miniature of the Unified Power Flow Controller (UPFC) with Fuzzy Logic (FL) based shunt and serial controllers to improve power network stability. A critical case of 3 phase fault occurrence in a power system has been studied. The results are compared with the presence and absence of UPFC. Simulations have been accomplished on MATLAB/Simulink software. To present the effectiveness of purposed controller, the obtained outcomes have been also compared with PID controller. It reveals that the Fuzzy Logic based UPFC has the best performance over PID Controller."}, {"label": 1, "content": "Dynamic security assessment (DSA) is widely used in dispatching operation systems, and calculation speed is one of its most important performance indices. This paper proposes a Siamese neural network model to predict the transient stability indicators of power systems, specifically the critical clearing time (CCT), which is much faster and suitable for online analysis than simulation. The proposed method involves constructing a simulation sample database using historical online data and training the Siamese model, which uses static state quantities such as the active power of generators as inputs. Firstly, a simulation sample database is constructed based on historical online data; then a Siamese model is trained, which uses static state quantities as its inputs like active power of generators. The final result is then determined comprehensively by the familiar samples. The proposed method's validity is verified through simulation using online data from the State Grid Corp of China (SGCC) and different key faults. Results demonstrate that the method meets the requirements for speed and accuracy of an online analysis system, especially for small sample sets."}, {"label": 1, "content": "A new concept of approximate power flow (APF) is proposed in this paper, aiming to help deal with the non-convergence problem of power flow calculation. In the approximate power flow model, active and reactive power decoupling strategy is adopted, and a branch model with virtual midpoint is the key foundation of the whole research. Based on the branch model, the approximate power flow equation is constructed and its iterative solving method with good characteristics of convergence is also introduced. The algorithm's robustness is enhanced by implementing automatic adjustment measures for active and reactive power. To validate the effectiveness and feasibility of the proposed model, the error and robustness analyses were conducted on practical examples. The developed APF program based on the presented method can be utilized in actual large-scale power grids."}, {"label": 1, "content": "This paper presents a simplified critical AC-DC system voltage interaction factor (SCADVIF) index and a method to evaluate the commutation failure risk of multi-infeed HVDC systems quickly. The method still does not require modeling power system components in detail for dynamic simulation. By calculating and comparing all the ADVIFs and SCADVIFs of an AC/DC system, the receiving-end AC buses at which faults are applied may cause commutation failure of multi-infeed HVDC system can be found quickly: if ADVIFjm \u2265 SCADVIFjm, a fault occurring at receiving-end system AC bus m would result in commutation failure at HVDC j. In this way, it is possible to quickly identify all AC buses where a three phase fault would cause single or multiple commutation failures. The proposed approach significantly reduces the computing time of researchers and enhances their work efficiency. The validity and accuracy of the proposed method are demonstrated by comparing simulation results using an actual planning large power grid. The index and method proposed can be widely used in the planning, design, and operation of AC/DC power grids."}, {"label": 1, "content": "This paper presents an iterative approach to jointly estimate the states of combined heat and power systems (CHPS). The node method is used to address the temperature quasi-dynamics in the district heating system (DHS), resulting in a dynamic state estimation (DSE) model. An alternating estimation strategy is employed to effectively handle the complicated time-delay constraints of temperature in the computation of DSE. Two case studies are conducted on CHPS test systems to validate the proposed DSE and alternating approach. The simulation results indicate that the DSE in CHPS is more accurate than the static state estimation and separate state estimation in individual energy systems."}, {"label": 1, "content": "A novel methodology has been developed to address voltage constraints in distribution generation using a penetration capability approach. The design goal of the methodology is to decide the maximum DGs' capacity and the best integrated location without voltage violation. Instead of dealing with the combinatorial nature of the proposed problem, the proposed methodology employs a two-stage methodology to decide the solutions. The first stage is a linear method to estimate the capabilities under all the possible DGs' locations. In the second stage, a detailed calculation stage is utilized to determine the exact maximum penetration capability using a nonlinear optimization method, considering the best location identified in stage 1. The maximum penetration capability and the corresponding location can be easily calculated by the proposed methodology. The effectiveness of this methodology has been demonstrated using several examples in single and multiple DGs in IEEE 33-bus power system. The numerical studies demonstrate that this methodology is effective in calculating the maximum penetration capability and determining the best location for DGs in distribution network systems."}, {"label": 1, "content": "We propose a novel hybrid probabilistic interval prediction method for short-term load forecasting. This method integrates K-means clustering based feature selection and online Gaussian processes regression(OGPR) to improve the accuracy of load forecasting results. The K-means clustering algorithm contributes to relevant feature selection during a dynamical process to better capture the load characters along with time. OGPR, which includes both dynamically updating the hyper-parameters and the training sample sets, is used as a forecasting engine to perform load probability interval prediction. The load data from Queensland market, Australia is used to validate the model proposed. The comparative results show that our approach outperforms others in terms of gaining higher quality prediction interval."}, {"label": 1, "content": "This paper proposes a distributed demand response algorithm that considers the uncertainty of resident behavior. This algorithm based on the alternating directions method of multipliers (ADMM) allows for distributing the optimization process across several servers/cores, which conserves users' privacy and reduces the computational complexity of demand response. At the same time, the robust optimization method is applied to deal with the uncertainty of the response process, reduce the impact of resident behavior uncertainties. The simulation results validate the efficacy of the proposed algorithm."}, {"label": 1, "content": "The assessment of power system static stability is crucial for ensuring the security and control of power systems. Most of traditional static stability assessment methods are focused on physical models and high-intensity simulations. These methods have a large amount of calculation and high dimensionality, and its online engineering applicability cannot be guaranteed. In order to better ensure the safe operation of power system, this paper proposes a static stability assessment method based on scale-invariant feature transformation(SIFT). By improving the generalized elasticity index based on operating features, accurate assessments of static stability situations can be achieved. Simulation results using the New England 10-machine 39-bus system reveal that the improved grid generalized elasticity index has a higher slope and better engineering application value. The simulation result of the New England 10-machine 39-bus system indicates that the improved grid generalized elasticity index has a higher slope and better engineering application value."}, {"label": 1, "content": "The WTCM system plays a crucial role in enabling wind farm operators to utilize a condition-based O&M approach, which ultimately minimizes O&M costs while enhancing wind turbine reliability. A WTCM method using only SCADA data based on data mining algorithm is proposed in this paper. Firstly, ARD (Automatic Relevance Determination) algorithm is adopted to determine the effective variables that are relevant to wind turbine condition. Feature vector is then extracted using the effective variables to represent the operation condition of wind turbine. Finally, the condition of a wind turbine is determined using outlier detection algorithm based on the extracted feature vector. Real-world dataset is used to validate the efficiency of the proposed method. Experiment results show that the proposed method can provide advanced failure alarm for wind turbines many days before failure happens. The implementation of the proposed WTCM method in a condition-based O&M strategy is shown to reduce O&M costs."}, {"label": 1, "content": "The experimental research on the security and stability control system (referred to as SSCS) of UHVDC transmission project mainly involves the functional verification on information interaction between SSCS and UHVDC control and protection system, the fault criterion of DC converters, the calculation of DC power loss, and the coordinated control strategies such as generators/loads tripping or DC transmission power modulation after failures in DC system or N-2 faults in AC system, and other aspects. This paper analyzed the existing test methods for SSCS, then proposed a modular modeling method. The proposed modeling method converts the external system of the AC/DC hybrid network into REI nodes and performs coherency-based dynamic equivalence on the sending-end generator groups. This modular design reduces the granularity of simulation calculations, improves the speedup ratio of parallel computing, and improves the efficiency of processors usage to meet demands for large-scale closed-loop testing on UHVDC system-level protection technologies including DC co-control and precisely machines tripping. Based on this modelling design, the function of DC control and protection system was simulated, the interface between RTDS and SSCS was realized, and the simulation and test platform for UHVDC SSCS was built on RTDS. The stability control strategies and the system function verification of Zhalute-Qingzhou \u00b1800 kV UHVDC SSCS were carried out on the platform. The simulation and test results verified the effectiveness of the typical test scheme."}, {"label": 1, "content": "Demand for food is increasing with the growing world population. Cultivable land however is decreasing by the day due to rapid urbanization, and farm-yield needs to increase to meet ever-increasing food-security needs. Rapid strides in Internet of Things (IoT) have made it possible to carry out a comprehensive monitoring of the farm at various levels to help achieve this while optimally utilizing essential resources. To have a highly granular view of the farm, we have deployed an IoT based precision farming system consisting of a cluster of devices measuring over 14 ambient parameters below the soil, at the crop level, and the ambient environment. The system has been integrated with our digital farming platform on the cloud that provides a flexible interface to gather sensor data from disparate sources and issues contextual advisory to the farm supervisor in order to carry out specific actions on the field. The setup has been used so far to monitor two horticultural crops, cabbage and capsicum that were suited to the agro-climatic zone of the deployment region, for the Rabi (Winter) season of 2017. Our experience with the deployment has shown a 20% reduction in agri-input cost and an increase in yields of more than 10%."}, {"label": 1, "content": "In this paper the authors would discuss about the enhancement in the existing work done on the Internet of Things (IoT) with reference to the Indian farmers. Specifically, the authors focus on soil testing and the constituents required for crop growth. The main achievement of this paper is the process that should be followed after soil testing. Soil testing provides data and parameters about soil nutrients such as phosphorous, potassium, and nitrogen. After collecting soil data, the data is entered into a developed software application that determines which crop is best suited for that soil. The software application is now equipped with preloaded data about soil, temperature, and weather conditions, which it continuously monitors through sensors. Based on these parameters, the application recommends the most suitable crop for that specific soil. The developed software is supported through mobile app for easy access. This software is supported through a user-friendly mobile app that is freely accessible to researchers in soil testing and IoT for agricultural applications. The authors aim to use this solution to help Indian farmers industrialize their individual farming practices by connecting them directly with the market through a centralized mobile app."}, {"label": 1, "content": "The incremental distribution network operating income will be the focus of attention of the company that has the power of incremental distribution network operation under the electricity reform. Based on this, this passage establishes the dynamic economic dispatch model of incremental distribution network considering P2G. It also proposes a catastrophe genetic algorithm based on double iterative optimization genetic algorithm to solve the time coupling problem under the constraints of control means, such as energy storage and demand-side response. Taking the improvement IEEE33 node model as an example, the influence of various regulatory methods on the operating income of incremental distribution network is analyzed and discussed. It is verified that the consideration of 2PG and demand-side response is essential to improve the operating income of incremental distribution network."}, {"label": 1, "content": "Text analytics has been widely used in many different domains to discover valuable knowledge hidden inside a specific text. However, when it comes to power dispatching, dispatchers often struggle to remember and comprehend the vast amount of unstructured data contained within manuals. This paper addresses the above problems by adopting text analytics. This includes key techniques such as data structure transformation, word segmentation tools for Chinese, and Word2Vec calculation, which can greatly assist dispatchers in handling the dispatching manual."}, {"label": 1, "content": "With the development of smart substation, information sharing and its redundant transmission of analog quantities in secondary system provide new research ideas for state estimation of power system. This has led to the development of a new method for secondary system state estimation based on information redundancy. Furthermore, a numerical example is designed to simulate and verify the proposed method. To verify the effectiveness of this method, a numerical example has been simulated and the results demonstrate a significant reduction in absolute error of analog quantities, increasing the reliability of uploading these quantities. This research method has significant potential for applications and further research in the field of secondary system state estimation."}, {"label": 1, "content": "China's power grids have accumulated a vast amount of dispatching and operating data at different voltage levels, providing a rich historical resource for system analysis. To ensure online safety and stability, a comprehensive system assessment is conducted every 15 minutes using calculation data and result data of around 1G. Based on the six-month online historical data accumulated by a provincial power grid, this paper studies the characteristics of the actual power grid security and stability, and proposes a system stability analysis method based on the online security analysis of historical resources. An operation rule extracting method is also proposed, which selects corresponding features for each type on the hierarchical grid using correlation analysis methods. A power grid security risk assessment system based on load ratio and line limitation is established to automatically discover the characteristics and rules of grid dispatching operations. This study provides a theoretical basis for operational mode and scheduling decisions, improving the ability of large-scale grid dispatching and online safety analysis, and enhancing the ability of power grids to resist external factors for safe and stable operations. The practical application value of this study is significant."}, {"label": 1, "content": "Power System Stabilizers(PSSs) are well-designed devices to measure and enforce improvements in synchronous generators\u2019 system-stability, which offer overwhelmingly superior cost performance compared to other optimal reconstruction or enhancement of power systems. The techniques used in PSSs have been the focus of the power industry and academic circles for many years. The paper presents a performance comparison of several advanced techniques based on Adaptive Fuzzy Control, Artificial Neural Network (ANN), Genetic Algorithm(GA) and Hybrid Artificial Intelligent(HAI), Fuzzy Logic and Particle Swarm Optimization(FLPSO) techniques. With their merits on dealing with PSSs\u2019 implemental structures, models with unknown or variable parameters, we study the main indices to compare the performance of the referred intelligent techniques including simplicity of prototype, robustness and response speed, complexity of algorithm, flexibility in implementation and applicability to hybrid AVRs so on. The comparison shows that intelligent techniques can improve the performance of PSSs, making them more effective in damping out low-frequency oscillations by overcoming the limitations of conventional control methodologies. Intelligent techniques could be especially considered in application of smart grid with large-scale grid-connected renewable energy power and random high power loads."}, {"label": 1, "content": "Poor efficiency and long running time of existent optimization algorithms in dealing with multi-objective multi-variable community microgrid optimization have always been a concern. To address this issue, a novel layered optimization algorithm based on NSGA-II is proposed. The algorithm adopts the structural feature of community microgrids and a multi-agent system concept in the optimization process to decompose complex microgrid optimization into several household optimizations of a smaller scale and one central microgrid optimization. The household operation is optimized first and the central microgrid optimization is solved subsequently based on the Pareto solution set of household operation problems to obtain the optimal operation mode. Simulation results demonstrate that the proposed strategy is effective in improving optimization efficiency."}, {"label": 1, "content": "Automated system inspection with defect detection is a crucial task in large-scale photovoltaic (PV) farms. In this regard, this paper proposes a transfer learning based solution for visible module defects diagnosis. The solution mainly includes three parts: a normative, time-updated, sundry dataset; a pre-trained deep learning model; a transfer learning strategy. The images in the dataset are enhanced with image augmentation technology. The proposed transfer learning based solution is able to extract the defects' features from local representation to global one and low level to high one with more robustness in comparison with the enhanced CNN based method. The effectiveness of the proposed solution is validated through extensive experiments, and the results demonstrate its superiority."}, {"label": 1, "content": "In recent years, the risk posed by cyber-attacks to the stability of power supply has become a pressing concern. Data-driven approaches based on machine learning techniques are frequently used to detect cyber-attacks. To get satisfactory detection performances, these approaches have to establish an in-depth understanding of power system operating behaviors, not only under normal conditions but also under contingencies like short-circuit faults. Contingency data appears far more rarely than normal data in the historical measurement database. If normal and contingency data are uniformly sampled to generate training datasets, the included contingency examples may be too few for machine learning models to recognize the operating patterns thoroughly. In this paper, a data preparation method combining Random Matrix Theory (RMT) and Adaptive Synthetic Sampling (ADASYN) algorithm is proposed. Case studies show that this method can enhance the learning of the statistical characteristics of power system operating data and improve the detection of cyber-attacks. Case studies show that this method can facilitate the learning of the intrinsic statistical characteristics of power system operating data, and then enhance the detection of cyber-attacks."}, {"label": 1, "content": "The inspection robot is able to accurately locate heating defects of equipment through the use of infrared images, making it an essential tool for equipment fault diagnosis. The infrared thermal image is completed by the robot in the shooting stage, while the processing and analysis of the later period still need artificial progress. This paper proposes an approach for processing infrared thermal images of transformer bushings that utilizes image recognition and pattern recognition techniques, with the aim of reducing manual intervention in the processing and analysis of infrared thermal images. Firstly, the Normalized Cross Correlation (NCC) template matching method and Otsu threshold segmentation method are used to determine the Region of Interests (ROI) of the bushing. Subsequently, characteristics such as maximum temperature rise, temperature mean value, temperature variance and temperature gradient of the ROI are extracted. Finally, support vector machine is used to identify the status of bushing. The results show that the proposed model can reduce the manual interference of infrared thermal image and has high accuracy, which is suitable for engineering application."}, {"label": 1, "content": "With the rapid expansion of ultra-high voltage ac and dc projects (UHVAC/DC), a large-scale UHVAC/DC hybrid network has formed in China, increasing the complexity of power grid operation. To maintain the stability and safety of such a complex power grid, it is highly necessary and significant to perform operation characteristics analysis. However, until now, all the existing power grid simulation tools cannot complete this challenging mission suitably because of the requirements for high accuracy modeling, large-scale electro-magnetic transient simulation, and massive off-line calculation, etc. In order to address these issues, a new generation UHVAC/DC power grid simulation platform (NGSP) architecture has been proposed in this paper, along with detailed design schemes. With the help of the proposed designs, the computing scale of NGSP is larger than 6000 nodes in digital-analog hybrid simulation, and the speed-up ratio is more than 3000 in digital simulation, which represents the most powerful power grid digital simulation capacity all over the world. Comparative results between simulation and engineering recorded data are presented to verify the correctness and effectiveness of NGSP. Additionally, the practicality of NGSP is demonstrated through application effects described in this paper."}, {"label": 1, "content": "An effective Physical Protection System (PPS) requires reliable methodologies and tools for automatic evaluation of its effectiveness, providing redesign suggestions and strategies, and assisting NPPs in training their staffs. A virtual reality platform for evaluation, design and training of PPS (IPAD) was proposed in one of the authors' previous works. This paper supplements a training module using IPAD platform for the virtual training of PPS. The virtual training will help detection staffs and response forces grasp the operational procedure of PPS when intrusion or emergency events occurred."}, {"label": 1, "content": "Electromagnetic transient (EMT) simulation programs are often used to obtain the optimal values of circuit and control parameters. In this paper, an EMT-simulation-aided optimization technique using the Kriging method is proposed. The method is then employed in the design of automatic reactive power regulator (AQR) controllers for static synchronous compensators (STATCOMs). It is shown that the optimal solution of the AQR controller can be obtained efficiently by the proposed technique."}, {"label": 1, "content": "A custom designed distribution network communication systems can maintain the fast and reliable information transmission and lead to improved operation and management of a distribution automation system. In this paper, the distribution automation transformation scheme of a regional power grid is analyzed. Then a simulation model of three types units, including the intelligent distributed protection unit, the three-remote unit and the master station unit, is constructed based on the Optimized network engineering tool (OPNET) Modeler platform, which is the main novelty of this paper. Finally, an intelligent distributed protection system is constructed and simulated using the models. The simulation results show that the models can act an effective tool to analyze the network performance, which can provide a quantitative basis for the design of the distribution network communication systems."}, {"label": 1, "content": "Usually, MMS packets are used to encapsulate the information of protection settings and TCP is the suite of protocols used to transmit the MMS packets in smart substations. However, TCP protocols are known to be vulnerable to HoL blocking attacks, which may result in the failure of the transmission of the protection settings that falls short of high-reliability requirements for vital power information. A new method of SCTP protection setting and transmission based on IEC61850 intelligent substation information structure is proposed in this paper. The optimization of SCTP's multi-host function is carried out to ensure seamless handover of dual MMS network during failure time while multi-stream mechanisms are employed to avoid HoL blocking attacks. To avoid HoL blocking attacks, the multi stream mechanism is used. This paper further studies the realization method and implementation process of SCTP transmission with protection setting in detail."}, {"label": 1, "content": "The distributed generator and non-linear loads will lead to deterioration of power quality at the point of common coupling between microgrid and low voltage distribution network. Tracking and compensating for the power quality problem of the converter is a means to ensure the safe and stable operation of microgrid. However, the current research on power quality problems only focuses on pre-emptive and post-processing strategies, which are not enough to ensure continuous and stable operation. Based on this, an advanced converter with situation awareness and orientation function is proposed. Firstly, situation awareness is achieved by establishing an information management system and optimizing the prediction algorithm to manage power quality-related elements. Secondly, the decision model of the grid-connected converter and optimal control are used to actively make decisions to solve power quality problems. Finally, the situation orientation of power quality problems is accomplished by establishing a smooth switching model of an adaptive control strategy. By considering these three aspects, the proposed advanced converter provides a more effective approach to manage power quality problems in microgrid. With its comprehensive functionalities, it can ensure the continuous and stable operation of the microgrid, thereby contributing to a more sustainable and reliable power supply network."}, {"label": 1, "content": "This paper proposes a novel model parameter identification based bus-bar protection principle. An inductance model can be developed when an internal fault occurs on bus. By taking the inductance and the resistance of the model as the unknown parameters to be identified, the equivalent instantaneous impedance and the dispersion of the parameter can be calculated. Utilizing their difference, the external fault and the internal fault with different current transformer (CT) saturation extent can be distinguished correctly. Based on this, a new criterion with self-adaptive restraint characteristic for bus-bar protection is put forward. Compared to traditional phasor based bus-bar protection principles, this new principle is suitable for non-periodic, fundamental, and harmonic components, making it more efficient. Moreover, the proposed principle is inherently immune to the impact of fault current flowing out when a fault occurs in the protection zone of the breaker-and-a-half bus-bar and is insensitive to fault resistance. Simulation results show that the presented principle has high sensitivity and reliability."}, {"label": 1, "content": "In simulations of voltage source converter (VSC)-based DC grids, the use of fast protection schemes for overhead lines, such as traveling wave protection, requires a small time step when simulating high voltage direct current (HVDC) circuit breakers. This paper presents a new method to address this issue and accurately simulate protection processes and realize hardware-in-the-loop (HIL) simulation. After using the transmission line modeling method(TLM) to solve the arrester and constant impedance model of the switch, the admittance matrix of the HVDC breaker will keep constant, which reduces computing time greatly. This results in the admittance matrix of the HVDC breaker remaining constant, which greatly reduces computing time. And a test circuit is implemented on a field programmable gate array (FPGA) board, on which efficient and accurate simulation results are obtained."}, {"label": 1, "content": "The infrared thermal image is an importance in condition monitoring of electrical equipment. However, due to its coarse and fuzzy edges coupled with serious noise, it poses great challenges in infrared image processing. An adaptive optimal threshold edge extraction algorithm based on improved Sobel operator is proposed in this paper. The algorithm uses eight-direction Sobel operators to extract the infrared image edge in high-temperature areas with noisy backgrounds. Furthermore, the wavelet coefficients in sub-band are described by a general Gaussian distribution and the variance is estimated from the local neighborhood information of sub-band wavelet coefficients, creating an adaptive optimal denoising threshold. Finally, the edge extraction infrared image is denoised by combining it with the improved Sobel operator and the optimal threshold. Matlab simulation results show that the algorithm can effectively detect the edge of infrared image and greatly improve its anti-noise ability."}, {"label": 1, "content": "To address the limited speed and scale of the simulation of modular multilevel converter (MMC) in electromagnetic transient simulation program (EMTP), this paper put forward a MMC fast electromagnetic transient model based on rotation transformation. The crucial advantage of the rotation transformation is to reduce the fundamental frequency of the original signal via the rotation transformation of coordinate system. With proposed method implemented in the EMTP of MMC, the simulation step size can be relatively increased, and the fast electromagnetic transient simulation can be achieved. In this paper, the equivalent model of MMC switch function model expressed by the state equation is derived. Based on this, the novel MMC model based on rotation transformation is further derived combined with the concept of rotation transformation. The state equation of the proposed MMC model is calculated in MATLAB, and simulation results and time-consuming are compared to those of a 51-level topological MMC electromagnetic transient model established in PSCAD/EMTDC. The results of comparative analysis show that the simulation duration of the MMC model based on rotation transformation is far less than the traditional MMC electromagnetic transient model without sacrificing accuracy."}, {"label": 1, "content": "This paper explores the technologies that increase energy efficiency in robotized productions. The focus is on optimizing the trajectories of industrial manipulators to reduce energy costs. The module is integrated into an informative space of robotized section and analyzes operating programs of CAM - system. It defines the volume of associated energy costs of technological process, puts forward corrective actions, which are aimed at increasing energy efficient of technological process while changing the program code without going beyond the limitations of the technology. The computing part of the module employs intelligent algorithms to identify non-linear dependencies and generate output decisions. Results of the research are proved by experiment that was made at robotized machining section with industrial robot KUKA KR - 60."}, {"label": 1, "content": "Mission-critical wireless sensor networks are attractive for information gathering in various complex environments and support many mission-critical applications, such as industrial automation and security surveillance. However, in order to fully exploit these networks for such applications, agile and scalable network management for data transfer and computing task implementation are essential. Thus, in this paper, we propose a software-defined mission-critical wireless sensor network (MC-SDWSN) which can solve the existing challenging issues in tradition WSNs, such as resource utilization, data processing, system compatibility, and strict latency requirement. The architecture of the MC-SDWSN is based on the SDN architecture and incorporates hierarchical cloud and edge computing technologies. Additionally, we propose a centralized computation offload strategy that demonstrates the feasibility of the MC-SDWSN architecture in sensor network applications. The simulation results in confirm the MC-SDWSN architecture, and the edge offloading strategy could support the critical missions effectively."}, {"label": 1, "content": "Most of the available graph-based semisupervised hyperspectral image classification methods adopt the cluster assumption to construct a Laplacian regularizer. However, this approach sometimes fails due to the presence of mixed pixels, which have recorded spectra that are a combination of multiple materials. In this study, we propose a novel semisupervised classifier that employs a geometric low-rank Laplacian regularizer by analyzing both the global spectral geometric structure and the local spatial geometric structure of hyperspectral data. To evaluate the spectral-spatial similarity of mixed pixels, we develop a new geometric regularized Laplacian low-rank representation (GLapLRR)-based graph. By exposing the overall low-rank and local spatial structure of images via GLapLRR, the resulting graph exhibits attributes like spatial-spectral geometry description, robustness, and low sparsity, enabling superior classification accuracy of mixed pixels. We experimentally test our method on three actual hyperspectral datasets, and the findings indicate that our technique outperforms its counterparts with only a few labeled instances."}, {"label": 1, "content": "Recently, hyperspectral images (HSI) have become increasingly important in remote sensing applications. HSI classification, as a fundamental issue, has attracted increasing attention and become a hot topic in the remote sensing community. We implemented a regularized convolutional neural network (CNN), which adopted dropout and regularization strategies to address the overfitting problem of limited training samples. Although many kinds of the literature have confirmed that it is an effective way for HSI classification to integrate spectrum with spatial context, the scaling issue is not fully exploited. In this paper, we propose a high efficient deep feature extraction and the classification method for the spectral-spatial HSI, which can make full use of multiscale spatial feature obtained by guided filter. This method is the first attempt to lean a CNN for spectral and multiscale spatial features. Our experimental results, using various datasets such as Indian Pines, Pavia University, and Salinas, show that this approach achieves a 3% improvement in accuracy compared to its counterparts."}, {"label": 1, "content": "In this paper, the problem of inter cell interference suppression which under the multi cell heterogeneous network system model is studied. Game theory is used to build the model of frequency and timeslot selection. The analysis shows that the proposed resource reuse model is a potential game, which can improve the throughput of the network."}, {"label": 1, "content": "A computational benchmark suite is presented for quantifying the performance of modern RCS simulations. The suite contains a set of scattering problems that are organized along six dimensions and range from basic to challenging. The suite also provides standard reference solutions, performance metrics and recommended studies to help evaluate the strengths and limitations of different simulation methods."}, {"label": 1, "content": "A waveform design procedure for improving the estimation of Doppler frequencies in active remote sensing applications is presented. The bound on frequency estimation is analyzed in terms of a continuous waveform, and the optimal waveform is inferred. Several waveform designs are analyzed, showing that a near-optimal dual-pulse waveform can achieve greater estimation accuracy than a single-pulse waveform using the same signal energy."}, {"label": 1, "content": "In this paper, we propose a new method to mitigate the effect of low signal to noise ratio (SNR) on two-dimensional (2-D) Direction of arrival (DOA) estimation. This method consists to extend the antenna steering vectors before applying the MUltiple SIgnal Classification (MUSIC) algorithm for 2-D DOA estimation. The simulation results show a good location accuracy enhancement."}, {"label": 1, "content": "Functional encryption is a recent generalization of public-key cryptography which aims at enabling secret-key owners to decrypt only functions of the encrypted data. This has numerous applications in various fields. Yet, although general constructions of theoretical interests do exist, practical functional encryption is presently limited to the evaluation of low-degree functions of the encrypted inputs. In this paper, we explore how Inner-Product Functional Encryption (IPFE) may be utilized to develop a tax calculation system that incorporates privacy into its design. The paper is also concluded by performances results demonstrating the practicality of the approach on the concrete issue of carbon tax calculations."}, {"label": 1, "content": "In the modern information warfare, the requirements for the reliability and real-time performance of the communication signal recognition technology are getting more and more strict. Although a great number of studies have been conducted in the reliability of communication signal recognition, few studies have been performed in the speed of communication signal recognition. The purpose of this study is to explore an improved feature extraction methods based on extreme learning machine (ELM) which has the advantage of higher speed in communication signal recognition. Simulations conducted in this study have shown that this approach improves the speed of recognition while maintaining high reliability, achieving ideal recognition accuracy even at low SNRs."}, {"label": 1, "content": "Aim of this study is to develop an accurate and reliable method for estimating propagation characteristics inside and outside aircraft cabin so as to advance wireless link design for Wireless Avionics Intra-Communication (WAIC) system. Specifically, this paper assesses the EMF distributions originating from a 4.4 GHz wireless transmitter inside the cabin, and applies the resulting analysis to the propagation of the WAIC system from within the cabin to its exterior-mounted antenna on a passenger aircraft (Airbus 320-200 model). EMF distributions excited by a 4.4 GHz wireless transmitter inside cabin are analyzed and fundamental characteristics for the WAIC system are derived from the analysis results."}, {"label": 1, "content": "The paper formulates the problem of establishing the initial data for assessing insolation and electric power generation by solar photovoltaic systems. Three sources of information are evaluated: reference books on the climate, satellite meteorological data and weather stations data. A more precise assessment of these factors can be achieved by combining various sources of information. The paper, therefore, puts forth a novel method for estimating electricity generation by utilizing reference data along with readily accessible weather station records. The method accounts for various factors that can impact electric power generation such as beam, sky-diffuse and ground-reflected components of solar radiation. The method incorporates the impact of the total and low-level clouds on solar radiation, as well as the impact of temperature on the efficiency of solar photovoltaic systems. The assessment of solar radiation and electric power generation by solar photovoltaic systems is conducted in the village of Narin-Kunta, located in the Irkutsk region on the banks of Lake Baikal."}, {"label": 1, "content": "Energy efficiency is a critical issue in designing communication protocols for Wireless Sensor Networks (WSNs). WSNs are composed of nodes with limited battery power, and it is not feasible to recharge or replace them, especially in harsh environments like underground mines. Therefore, WSN design should focus on energy efficiency. Clustering techniques have been used effectively to achieve scalability and power savings in WSNs. It enables hierarchical structures to be built on the nodes, facilitating more efficient use of scarce resources. In this study, we have proposed a hybrid clustering scheme that meets the energy constraints of WSNs. It allows data transmission from sensor nodes to the sink with reasonable consumption of energy."}, {"label": 1, "content": "This paper is devoted to the description of the storage model and the search for sound sequences based on the theory of active perception. The theory of active perception is used to form an indicative description of the sound signal. The class of problems solved by the proposed model has a wide scope and includes, among other things, the search for musical plagiarism. Additionally, the model can be used to develop a software system for audio signal identification. The search model was implemented using the programming language R and was verified through computational experiments with a database of 10,000 musical compositions, achieving a search accuracy of 96%. The stability of the proposed model is also analyzed for distortion of the sought signal by noise. A comparison is made with similar existing systems in terms of search accuracy."}, {"label": 1, "content": "This paper introduced a hand gesture recognition method based on convolutional neural networks (CNNs). The recognition scenario consisted in a three dimensional radar array to transmit and receive 24GHz continuous electromagnetic (EM) wave, and convert the scattered EM wave to the intermediate frequency (IF) signals. This paper used the the processed frequency spectrum as the input to the CNN. The feature detection layer of the CNN learns from data training, eliminating the need for supervised feature extraction. It highlighted these features through convolution operating, pooling and a softmax function. The system has demonstrated high accuracy, achieving a recognition rate of over 96%."}, {"label": 1, "content": "A fast Fourier transform (FFT) algorithm is introduced for efficiently refining the integral-equation solution of scattering problems, enabling the evaluation of fields at a multitude of nearby points. The proposed algorithm uses a similar approach to the adaptive integral method (AIM) but contends with the fact that the fields are not Galerkin tested with basis functions but instead point tested. By doing so, the algorithm effectively minimizes computational costs, particularly when the number of observer points is high, in contrast to a brute-force approach."}, {"label": 1, "content": "The article presents the results of the research, as well as a software module that allows you to configure the fuzzy logic controller to three possible ways of including a fuzzy logic controller in the control object. It also analyzes the performance of regulation and offers a comparative overview of the outcomes."}, {"label": 1, "content": "The downscaling of CMOS technology has led to an increase in susceptibility of integrated circuits (ICs) to soft errors, which has become a significant concern. Therefore, the study of the radiation-induced transient faults in combinational logic has become one of the most challenging issues as the absence of appropriate error-protection mechanisms may lead to system malfunctions. This paper proposes an efficient and accurate layout-based soft error rate (SER) estimation analysis for ICs, taking into account both single and multiple transient faults. The proposed SER estimator is based on Monte-Carlo simulations, coupled with a detailed grid analysis of the circuit layout to identify vulnerable areas of a circuit. SER estimator, is based on Monte-Carlo simulations taking into account a detailed grid analysis of the circuit layout for the identification of the vulnerable areas of a circuit and, in addition, temperature as one of the factors that affect the generated pulse width. The widening of the fault pulses due to elevated temperature is reflected in increased SER according to our results. Finally, the comparison between the simulation results for some of the ISCAS'89 benchmark circuits obtained from the proposed framework and the respective ones obtained from SPICE indicates a fairly good correlation."}, {"label": 1, "content": "In article describes an intellectual-information system that will be used to make managerial decisions on the operation of technical personnel of power plants and similar process facilities. In particular, the results of its work will help to make a decision on the appropriateness of upgrading the skills of technical personnel. The intellectualization methods utilized to determine the knowledge level through fuzzy logic allow not only for knowledge assessment but also to automate the process of its enhancement and consolidation. The use of this method allows the training of technical personnel on the job. The method of intellectualization of an estimation of a level of knowledge of the basic theory of calculation of optimum feeding parameters of regulating devices of automatic control systems is described. This approach solves the problem of automation evaluation for parametric synthesis in one-loop, cascade, and combined control systems. Control of the level of knowledge for parametric synthesis determined by the correct choice of a point on a graph of the amplitude-phase characteristics and D-partition graphs. The adequacy of established fuzzy output system tested in a program FuzzyTECH."}, {"label": 1, "content": "Unified Communications (UC) is revolutionizing next generation enterprise networks by allowing human voice and video to travel over existing packet data networks along with UC services such as video teleconferencing (VTC), unified messaging, and chat. UC represents the future of fully-converged, cloud-based architecture, but it also presents new forensic challenges to network investigators. The adoption of cloud-based UC offerings, otherwise known as UC as a Service (UCaaS), is gaining momentum in the Information Technology field. Recently, a cloud service delivery model known as UC as a Service (UCaaS) is gaining momentum in the field of Information Technology. This paper discusses the main challenges associated with forensic investigations in an UC cloud environment. The patterns offer a structured approach to network forensic collection and analysis of digital evidence in UCaaS environments, allowing network investigators to specify, analyze and implement network forensic investigations for UC technologies. These patterns provide a systematic approach to network forensic collection and analysis of digital evidence in UCaaS architectures. As UC continues to evolve, it is imperative that forensic techniques and security measures are in place to ensure the protection of sensitive information."}, {"label": 1, "content": "Computer networks are ubiquitous and growing exponentially, with a predicted 50 billion devices connected by 2050. This tremendous growth dramatically increases the attack surface of both private and public networks. These attacks can have a profound impact on the system's behavior, ultimately leading to the detection of the attack. In this manuscript we model the path of an attack through the network by graphs. The model developed aims to better integer attackers intentions. Using the data produced by 5 honeypots, we apply our model. The results showed that our approach was able to quickly detect anomalies in the experimental dataset, making it a valuable tool for identifying and preventing network attacks."}, {"label": 1, "content": "The paper presents a model for controlling AC drives, analyzing both conventional drive control systems and intelligent approaches for implementing control systems. An analysis of conventional drive control system has been performed, its advantages and disadvantages, and preconditions for use of intelligent approaches for implementation of control systems for this kind of objects have been estimated. The intelligent system is supposed to be modeled with use of fuzzy logic controller and different inference algorithms for implementation of control laws in various control loops. The paper includes an analysis of the system's dynamic characteristics with and without fuzzy set theory controllers. The practicability of using the multi-cascade fuzzy systems while implementation of unified intelligent control module and different combinations of fuzzy inference algorithms has been demonstrated for complex drive systems. By using this technology, the authors show that it is possible to implement a control system for the entire class of drive systems, considering all the special aspects and relationships between the coordinates in the complex control object. Additionally, modeling of spacial membership function using multi-cascade fuzzy controller will give a possibility to avoid a variety of quantitative and qualitative restrictions due to complex relations between the coordinate in such systems."}, {"label": 1, "content": "The statistical delay of a path is typically modeled as a Gaussian random variable, assuming that the path is always sensitized by a test pattern. Its sensitization in various circuit instances varies among its test patterns and the pattern induced delay is non-Gaussian. As a result, probability mass functions are used to model the path's delay. To improve defect coverage, machine learning is applied to select the best test patterns. This approach has been shown to result in higher accuracy in detecting defects when compared to existing methods, as demonstrated by experimental results."}, {"label": 1, "content": "With the growing usage of credit card transactions, financial fraud crimes have also been drastically increased leading to the loss of huge amounts in the finance industry. In order to minimize losses, it is crucial for banks and other institutions to implement effective fraud detection systems. However, traditional classifiers often struggle with the highly imbalanced data sets associated with credit card fraud, in which instances of fraud are significantly outnumbered by legitimate transactions. To address this challenge, this paper proposes a sampling method that leverages K-means clustering and the genetic algorithm to improve classification performance for minority class objects. Specifically, we use K-means to cluster minority samples and then apply the genetic algorithm to generate new samples and refine the fraud detection classifier. We used K-means algorithm to cluster and group the minority kind of sample, and in each cluster we use the genetic algorithm to gain the new samples and construct an accurate fraud detection classifier."}, {"label": 1, "content": "As the scale of high performance computing facilities approaches the exascale era, gaining a detailed understanding of hardware failures becomes important. In particular, the extreme memory capacity of modern supercomputers means that data corruption errors which were statistically negligible at smaller scales will become more prevalent. To mitigate the adverse effects of hardware faults on exascale workloads, we must learn from the behavior of current hardware. In this study, we investigate the predictability of DRAM errors using field data from two recently decommissioned supercomputers. By applying statistical machine learning techniques to predict the probability of DRAM errors at previously un-accessed locations, we found that physically-informed models outperformed purely statistical methods. We compare the predictive performance of six machine learning algorithms, and find that a model incorporating physical knowledge of DRAM spatial structure outperforms purely statistical methods. Our findings both support expected physical behavior of DRAM hardware as well as providing a mechanism for real-time error prediction. We demonstrate real-world feasibility by training an error model on one supercomputer and effectively predicting errors on another. Our methods demonstrate the importance of spatial locality over temporal locality in DRAM errors, and show that relatively simple statistical models are effective at predicting future errors based on historical data, allowing proactive error mitigation."}, {"label": 1, "content": "Cloud computing provides on-demand access to IT resources via the Internet. These resources are controlled by access control policies that allow or deny permissions. This paper presents a formalization of the Amazon Web Services (AWS) policy language and a corresponding analysis tool, called ZELKOVA, for verifying policy properties. ZELKOVA encodes the semantics of policies into SMT, compares behaviors, and verifies properties. By solving a PSPACE-complete problem, ZELKOVA is able to detect misconfigurations of policies and is used millions of times each day. With ZELKOVA, AWS users can rest easy knowing their access control policies are properly enforced."}, {"label": 1, "content": "Formal verification is an essential technique to check if a gate-level circuit correctly implements the given specification model. In the case where the equivalence check fails, bugs or errors in the circuit must be debugged. This paper addresses the problem of post-verification debugging and correction (rectification) of finite field arithmetic circuits. This paper addresses the problem of post-verification debugging and correction of finite field arithmetic circuits, focusing on single-fix rectification. The authors use an equivalence checking setup modeled as a polynomial ideal membership test. we address single-fix rectification.Starting from an equivalence checking setup modeled as a polynomial ideal membership test, we analyze the ideal membership residue to identify potential single-fix rectification locations. Subsequently, we use Nullstellensatz principles to ascertain if indeed a single-fix rectification can be applied at any of these locations. If so, the team derives a rectification function using the synthesis of an unknown component problem. The Gr\u00f6bner basis algorithm is used both as a decision and quantification procedure. The approach is demonstrated over various finite field arithmetic circuits, showing effectiveness, whereas SAT-based approaches are infeasible."}, {"label": 1, "content": "This paper presents a novel approach to imitating neurons cells based on the latest breakthroughs in neuroscience. After reobserving the latest revelations in the field of biological neurons, the conventional artificial models has proven strong potentials in image processing and pattern classification, but remains far from presenting a modern imitation of natural intelligent organisms. A Biomimetic cell design is thus proposed with a combination of registers to hold the inputs, outputs, and weights as information codes. These cells use binary equivalence gates to compare the inputs to the weights and deliver the required outputs. The abstraction model provided renders the training process highly simplistic, which speeds up the design phase and opens the way towards a new dimension in artificial intelligence."}, {"label": 1, "content": "The implementation of large-scale Spiking neural networks (SNN) requires hardware realization of scalable neurons and synapses. In this study, a novel transient Joule heating based the leaky-integrate and fire neuron (LIF) in scalable PrMnO3 (PMO) RRAM device is proposed experimentally. The Joule-heating based thermal runaway is utilized to achieve rectified linear unit (ReLU) voltage dependence of spiking frequency, similar to a typical LIF neuron. Second, the Jouleheating hypothesis in PMO is validated by TCAD DC and transient simulations. PMO is extremely thermally resistive semiconductor (300x cf. Si) and hence enables low energy thermal dynamics. The excellent energy, area performance with a synapse in the same material system and thermal engineering makes PMO neuron attractive. These findings suggest that PMO RRAM device-based spiking neuron and synapse can play a pivotal role in the hardware implementation of large-scale SNNs."}, {"label": 1, "content": "Analyzing the sources of performance anomalies in cloud-based applications accurately is a challenging task due to both the multi-tenant nature of cloud deployment and changing application workloads. To that end many different resource instrumentation and application performance modeling frameworks have been developed in recent years to help in the effective deployment and resource management decisions. However, the significant differences among these frameworks in terms of their APIs, ability to instrument resources, and interpretation of the collected information make it difficult to use these frameworks effectively. Not addressing these complexities can result in operators providing incompatible and incorrect configurations leading to inaccurate diagnosis of performance issues and hence incorrect resource management. In order to tackle these challenges, UPSARA is introduced as a model-driven generative framework that provides a lightweight, scalable, and extensible performance monitoring, analysis, and testing framework for cloud-hosted applications. UPSARA helps alleviate the accidental complexities in configuring the right resource monitoring and performance testing strategies for the underlying instrumentation frameworks used. We evaluate the effectiveness of UPSARA in the context of representative use cases highlighting its features and benefits."}, {"label": 1, "content": "Server consolidation and resource elasticity are among two of the most important resource management features in cloud and edge computing. One of two forms of elasticity is often adopted. While horizontal elasticity allows for the acquisition and release of computational nodes based on demand, vertical elasticity involves adjusting the capacity of the resource types allocated to each individual virtual machine (VM) in accordance with its respective application's requirements. However, in the case of vertical elasticity, when there are insufficient resources available to allocate to a VM, its application's performance may suffer degradation. For online applications, the only alternative is to live-migrate the VM to another server. On the other hand, when running batch jobs, the resource-constrained VM could also be suspended or saved to disk and revived elsewhere or on the same host, when resources become available. Given that memory availability has a significant influence on performance and system throughput, this paper investigates the viability of integrating VM migration, pausing and suspension schemes as part of a VM scheduling strategy to support the execution of both online and batch applications in a virtualized infrastructure employing memory elasticity. The results show that combining such schemes can provide utilization benefits for cloud service providers when memory resources are scarce."}, {"label": 1, "content": "By renting pay-as-you-go cloud resources (e.g., virtual machines) to do science, the data transfers required during the execution of data-intensive scientific workflows may be remarkably costly not only regarding the workflow execution time (makespan) but also regarding money. Delays in these transfers can severely compromise workflow execution time, leading to prolonged resource rentals and ultimately exceeding budgets. To mitigate this issue, a new approach is proposed whereby some communication may be traded for computation during the scheduling production of workflows. This paper explores this premise by enhancing the Heterogeneous Earliest Finish Time (HEFT) algorithm and the Lookahead variant of HEFT. The Heterogeneous Earliest Finish Time (HEFT) algorithm and the Lookahead variant of HEFT are leveraged for this purpose, and the approach is evaluated using simulations and synthetic data from four real-world scientific workflow applications. The results suggest that task duplication can effectively reduce data transfer size, leading to shorter rental durations of resources and minimising network traffic in the cloud."}, {"label": 1, "content": "Workflow scheduling and resource provisioning algorithms heavily rely on accurate performance estimation of tasks to produce an effective scheduling plan. A profiler that is capable of modeling the execution of tasks and predicting their runtime accurately, therefore, becomes an essential part of any Workflow Management System (WMS). With the emergence of multi-tenant Workflow as a Service (WaaS) platforms that use clouds for deploying scientific workflows, task runtime prediction becomes more challenging because it requires the processing of a significant amount of data in a near real-time scenario while dealing with the performance variability of cloud resources. Therefore, traditional methods like basic statistical descriptions or offline regression techniques are no longer suitable for such environments. In this paper, we propose an online incremental learning approach to predict the runtime of tasks in scientific workflows in clouds. To improve the performance of the predictions, we harness fine-grained resources monitoring data in the form of time-series records of CPU utilization, memory usage, and I/O activities that are reflecting the unique characteristics of a task's execution. We compare our approach to a state-of-the-art technique that leverages resource monitoring data based on regression machine learning. Our results show that the proposed strategy outperforms the state-of-the-art solutions by up to 29.89% in terms of error."}, {"label": 1, "content": "We consider networks in which each individual link is characterized by two delay parameters: a (usually very conservative) guaranteed upper bound on the worst-case delay, and an estimate of the delay that is typically encountered, across the link. Given a source and destination node on such a network and an upper bound on the end-to-end delay that can be tolerated, the objective is to determine routes they typically experience a small delay, while guaranteeing to respect the specified end-to-end upper bound under all circumstances. We formalize this problem as a shortest-paths problem on graphs and provide optimal algorithms for solving it."}, {"label": 1, "content": "Machine learning (ML) on edge computing devices is becoming popular in the industry as a means to make control systems more intelligent and autonomous. The latest trend involves utilizing embedded edge devices with higher computational power and larger memories over cloud-hosted deployments, making traditional ML tasks now possible for edge-based devices. This work aims to assess real-time predictability and data privacy concerns between traditional cloud services and edge-based ones for some data analytics tasks. By investigating a subset of ML problems appropriate for edge devices, we identify whether they can result in real-time predictable services through widely used ML libraries. We enhance Caffe library's capabilities, making it more suitable for real-time predictability. Then we deploy high-accuracy ML models on an embedded system, exposing it to industry sensor data from the field, demonstrating its efficacy and suitability for real-time processing."}, {"label": 1, "content": "Advertisements are an integral part of internet economics and culture, and video ads are the most popular and arguably the most entertaining form of advertisements. With the recent growth in digital marketing, video ads have seen unprecedented growth and are growing in importance as an advertising means. This poses a problem for creators, advertisers, and ad platforms, who would like to know the effectiveness of a video ad before deployment. To address this problem, we propose a novel algorithm that provides feedback before an ad is placed based on historical data about the effectiveness of other video ads. Our approach leverages a multi-modal mixture based algorithm to predict the effectiveness of the video automatically. We propose a multi-modal mixture based algorithm to predict the effectiveness automatically. In our experiments, we tested our approach on a publicly available dataset and found that our algorithm outperformed other baseline approaches. Our experiments on a publicly available dataset show that our approach can outperform other baseline approaches."}, {"label": 1, "content": "High Efficiency Video Coding (HEVC) is widely used in modern video compression systems, thanks to its ability to provide better compression performance than its predecessors. One of the modules that contributes to higher compression rates is the Motion Estimation module, which consists of Integer and Fractional pixel motion estimation. The Fractional Motion Estimation (FME) process performs interpolations to find sample values at fractional-pixel locations, which can be computationally demanding. In this paper, we propose an interpolation-free method for FME based on Artificial Neural Networks (ANNs). Their proposed method was implemented in the HEVC reference software, HM-16.9, and their findings showed that ANNs can achieve the FME task with an average increase of 2.6% in BDRate and an average reduction of 0.09 dB in BD-PSNR. According to our results, ANNs can accomplish FME task with an average increase of 2.6% in BDRate and an average reduction of 0.09 dB in BD-PSNR."}, {"label": 1, "content": "Virtual reality (VR) applications make use of 360\u00b0 omnidirectional video content for creating immersive experience to the user. However, such content must be projected onto a 2D image plane in order to use existing 2D video compression standards. However, the projection from spherical to 2D domain introduces deformations in the projected content due to the different sampling characteristics of the 2D plane. Such deformations are not favorable for the motion models of the current video coding standards. Consequently, omnidirectional video is not efficiently compressible with current codecs. In this work, a geometry-based motion vector scaling method is proposed in order to compress the motion information of omnidirectional content efficiently. This uniform motion behavior provides optimal candidates for efficiently predicting motion vectors in the current block. The uniform motion behavior provides optimal candidates for efficiently predicting the motion vectors of the current block. The conducted experiments illustrated that the proposed method provides up to 2.2% bitrate reduction and on average around 1% bitrate reduction for the content with high motion characteristics in the VTM test model of Versatile Video Coding (H.266/VVC) standard."}, {"label": 1, "content": "We present novel low-level audio features that are based on correlations between sub-band audio signals decomposed by undecimated wavelet transform. Under the assumption that SVM is used for classifier learning, the experimental results on GTZAN dataset showed that the proposed method demonstrated the best accuracy of 81.5%, outperforming the conventional methods."}, {"label": 1, "content": "In this paper, we introduce our recent studies on human perception in audio event classification. In particular, the pre-trained model VGGish is used as feature extractor to process audio data, and DenseNet is trained by and used as feature extractor for our electroencephalography (EEG) data. By learning the connection between audio stimuli and EEG in a shared space, we can provide accurate classifications. In the experiments, we record brain activities (EEG signals) of several subjects while they are listening to music events of 8 audio categories selected from Google AudioSet. Our experimental results demonstrate that i) audio event classification can be improved by exploiting the power of human perception, and ii) the correlation between audio stimuli and EEG can be learned to complement audio event understanding."}, {"label": 1, "content": "As a multimedia security mechanism, CAPTCHAs are completely automated public turing test to tell computers and humans apart. Although cracking CAPTCHA has been explored for many years, it is still a challenging problem for real practice. In this demo, we present a text based CAPTCHA cracking system by using convolutional neural networks(CNN). To solve small sample problem, we propose to combine conditional deep convolutional generative adversarial networks(cDCGAN) and CNN, which makes a tremendous progress in accuracy. Additionally, multiple models with low Pearson correlation coefficients are selected for majority voting ensemble, which further enhances the system's accuracy. The experimental results show that the system has great advantages and provides a new mean for cracking CAPTCHAs."}, {"label": 1, "content": "Internet has brought about a tremendous increase in content of all forms and, in that, video content constitutes the major backbone of the total content being published as well as watched. This means that video recommendation engines need to find new and innovative ways to suggest newly added videos to their users. However, new videos often lack metadata and user interaction that could be used to rate them, making it difficult to recommend them effectively. To this effect, this paper introduces the several techniques we develop for the Content Based Video Relevance Prediction (CBVRP). We employ different architectures on the CBVRP dataset to make use of the provided frame and video level features and generate predictions of videos that are similar to the other videos. We have also implemented ensemble strategies that explore the complementary nature of these two types of features. The obtained results are encouraging and will impel the boundaries of research for multimedia based video recommendation systems."}, {"label": 1, "content": "This paper aims to enhance the precision of Human Action Recognition (HAR) by fusing depth and inertial sensor data. Firstly, we transform the depth data into Sequential Front view Images(SFI) and fine-tune the pre-trained AlexNet on these images. Then, inertial data is converted into Signal Images (SI) and another convolutional neural network (CNN) is trained on these images. Finally, the learned features from both CNNs are fused to create a shared feature layer, which is fed to the classifier. We experiment with two classifiers, namely Support Vector Machines (SVM) and softmax classifier and compare their performances. The recognition accuracies of each modality, depth data alone and sensor data alone are also calculated and compared with fusion based accuracies to highlight the fact that fusion of modalities yields better results than individual modalities. The experimental results on UTD-MHAD and Kinect 2D datasets demonstrate that the proposed method outperforms other recently proposed visual-inertial action recognition methods and yields state-of-the-art results."}, {"label": 1, "content": "In order to diagnose intermittent faults, it is crucial to extract fault features. An intermittent fault feature extraction method based on wavelet transform is proposed. Firstly, the stress-intensity model is used to describe the fault process of intermittent faults, analysing the main part of intermittent fault signal characteristics. Considering the randomness and suddenness of intermittent faults, wavelet transform is used to extract intermittent fault features based on the advantages of wavelet transform in signal mutation and singularity detection. By extracting signal energy through the wavelet coefficients, intermittent fault features are obtained, which can be used to identify and isolate the intermittent faults. Finally, a simulated circuit is utilised to obtain transient intermittent fault response characteristics, and wavelet transform is used to extract features for intermittent fault location. Our results illustrate that the wavelet transform can successfully extract intermittent fault features, which are applicable for intermittent fault diagnosis."}, {"label": 1, "content": "Traditional intelligent diagnosis methods and current popular deep learning based diagnosis methods basically adopt the approach of batch learning, which may waste time and computing resources since they need to discard the previous learned model and retrain a new model based on the newly added data and prior data. Furthermore, manual feature extraction is often required for intelligent diagnosis, which heavily relies on prior knowledge. To solve the above mentioned problems, this paper proposes a fault diagnosis method based on class incremental learning without manual feature extraction. The proposed method is based on denoising autoencoder and obtains the autoencoders using the raw data acquired for each health state. In the class incremental learning process, only the autoencoder of new health state need to be trained while the former trained autoencoders are retained. Test data is classified according to the minimal reconstruction error calculated through the autoencoders. At the end of this paper, the proposed method is validated through vibration data of rolling bearings for rail vehicle. The experimental results show that the presented method is effective."}, {"label": 1, "content": "The presence of periodic impulses in vibration signals often indicates the occurrence of faults in roller bearings. However, detecting these faults in complex working conditions with heavy noises can be difficult. To solve this problem, a hybrid method of ensemble empirical mode decomposition (EEMD) and L-Kurtosis clustering-based segmentation is proposed. EEMD can express the intrinsic essence using a simple and understandable algorithm to solve the mode mixing phenomenon. L-Kurtosis is an improved version of kurtosis that recognizes the impulses without the influence of outliers. Furthermore, the L-Kurtosis value is employed as an indicator in the clustering-based segmentation method to extract the fault features from the background noises. To illustrate the feasibility of utilizing the EEMD and L-Kurtosis based clustering segmentation method, benchmark data simulations and experimental investigations are performed to detect faults in bearings. The results show that the proposed method enables the efficient recognition of faults in bearings."}, {"label": 1, "content": "The electrostatic sensing technique has been verified to be a viable method for tribo-contact monitoring under laboratory conditions in previous investigations. The monitoring of wind turbine gearbox is a possible approach for electrostatic application. It usually under variable operating conditions while working. This paper introduced a new method called moving window local outlier factor (MWLOF) to process electrostatic monitoring signals with multi-sensors. Consequently, it holds great potential for facilitating the widespread adoption of electrostatic-based monitoring in relevant industrial contexts in the future."}, {"label": 1, "content": "The goal of this paper is to identify any abnormal or unexpected running behaviors in a given machine by detecting structural changes in the machine's status during its successive operations. In order to achieve this objective, we present a new method that integrates the autoregressive integrated moving average model (ARIMA) with one-class support vector machines (SVMs). The ARIMA model has been introduced and adopted for this problem in a previous work, where parameters in the model are somewhat difficult to estimate. In this paper, we introduce the use of one-class SVM for ARIMA model identification. This approach enables us to take advantage of machine learning techniques to create a more reliable and accurate model. We conducted experiments based on an experimental setup. Results along with comparisons with representative methods demonstrated the effectiveness and propriety of the proposed method in real engineering applications."}, {"label": 1, "content": "Tool condition monitoring (TCM) plays an important role in milling process. In order for TCM to be effective, it is essential to distinguish tool wear conditions accurately. Distinguishing the condition of tool wear effectively is a central part in the TCM. The first step of this method involves using the Bandpass Filter to improve the noise-signal ratio of the original signals detected by cutting force and acoustic emission sensors. Next, statistical features of time and frequency domains are calculated for the preprocessed data. Then several statistical features of time and frequency domain for the preprocessed data are calculated. Experimental results confirm that this proposed classification technique outperforms the KELM-based method for different types of signals. Experiment shows that the proposed method has outperformed the KELM-based method with different type's signals."}, {"label": 1, "content": "Traditional methods of analog circuit fault diagnosis are complex and not universal, requiring significant manual effort for feature extraction and selection. However, fault feature extraction and selection take much manual effort, which make the design of the fault diagnosis system complicated and not universal. The method uses a single pulse stimulus signal to the circuit-under-test (CUT), and then directly samples the raw time domain response of CUT to construct fault samples. The proposed method uses the single pulse as the stimulus signal to circuit-under-test (CUT), and then the raw time domain response of CUT is directly sampled to construct fault samples, which are inputted into the ELM network to obtain fault diagnosis results. Experimental results show that the proposed method achieves average diagnostic accuracies of 100% and 99.5% on the Sallen-key band-pass filter circuit and the four opamp biquad high-pass filter circuit, respectively. The algorithm achieves good diagnostic accuracy and efficiency in the absence of feature extraction and selection."}, {"label": 1, "content": "Residual life estimation is significant in reliability engineering. In this paper, a Bayes model is proposed to estimate the residual life of components by fusing expert knowledge, lifetime and degradation data, which provides a new method for residual lifetime estimation of components characterized as small sample, high reliability and long life. The linear Wiener process is used to model the degradation data and the lifetime data is described by the inverse Gauss distribution. The joint maximum likelihood function can be obtained by integrating the lifetime data and degradation data. With the Maximum Entropy Method (MEM), the prior distribution can be determined by the expert knowledge, which is different from the non-informative prior in existing study. Due to the complexity of computation, Monte Carlo Markov Chain (MCMC) is applied to estimate the parameters. Therefore, the probability density function (PDF) of residual life can be determined. This allows for the real-time updating of distribution parameters, and the determination of the probability density function (PDF) of residual life. An illustrative example is presented to validate the proposed method, and the results demonstrate its effectiveness and accuracy. The results prove the effectiveness and accuracy."}, {"label": 1, "content": "This paper proposes a new three-stage method for rotating machines health condition monitoring. To obtain compressively sampled signals from raw vibration signals, the first stage of the proposed method utilizes Multiple Measurement Vectors Compressive Sampling (MMV-CS). In the second stage, a process combining Geodesic Minimal Spanning Tree (GMST), Stochastic Proximity Embedding (SPE), and Neighbourhood Component Analysis (NCA) is used to estimate and further reduce the dimensionality of the compressively-sampled signals. The third stage involves the use of the reduced features to classify machine health conditions employing a multi-class Support Vector Machine (SVM) classifier. To verify the effectiveness of the proposed technique, experiments were conducted on a roller element bearing fault detection and classification task based on vibration signals. The outcomes indicate that the proposed method with fewer features attained a high classification accuracy of bearings health conditions, outperforming recently published findings."}, {"label": 1, "content": "In this paper, the degenerate state data of the Complex Programmable Logic Device (CPLD) functional circuit module of Integrated Modular Avionics (IMA) system is obtained by the method of fault injection. By extracting the characteristics of the time-domain data of the degraded state and using the minimum redundancy maximum relevance (mRMR) method, the related sensitive degradation characteristics are obtained. We then conduct health assessments of the CPLD functional modules using both single parameter and multi-parameter evaluation methods that calculate confidence values and Mahalanobis distance. By providing this practical and engineering approach, our results offer a means of IMA system health evaluation at the module level based on health management technology."}, {"label": 1, "content": "This paper presents a method for fault injection and diagnosis of the cabin pressure control system used in civil aircraft. The method utilizes fault analysis to identify typical faults in the system, and then utilizes Simulink for fault injection, and BP neural networks for fault diagnosis. The proposed fault injection and diagnosis methods are implemented in Rhapsody with the cabin pressure control system model in design phase, and simulations are given to verify the effectiveness of the proposed method."}, {"label": 1, "content": "This paper presents a new approach to fault reconstruction in a three-motor web-winding system with multiple disturbances and actuator faults. The first part of the disturbances is the uncertain external disturbances brought about by the rotating motors, and the second one represents the uncertain variable caused by the parameter uncertainties and unmodeled dynamics, which is bounded by an unknown constant. The latter is bounded by an unknown constant. Meanwhile, H\u221e technique is utilized to attenuate the norm bounded uncertainties. Sufficient condition of asymptotic stability of the estimation error system is derived based on the Lyapunov stability theory. Based on the Lyapunov stability theory, sufficient conditions for asymptotic stability of the estimation error system are derived, and observer gain matrices are obtained using a linear matrix inequality approach. Simulation and analysis demonstrate the effectiveness of the proposed observer."}, {"label": 1, "content": "Motors are crucial components of industrial processes because of their reliability, affordability, and strong performance. Motor failure will lead to the shutdown of a whole production line and cause great loss. Therefore, accurate, reliable, and effective motor fault diagnosis is essential. Currently, motors fault diagnosis has gained much attention to guarantee safe motor operations. In this paper, a novel fault diagnosis method is proposed for the three-phase asynchronous motor using Long Short-Term Memory (LSTM) neural network, which possesses the capacity to learn meaningful representations from raw signal without any feature engineering. This method has the ability to learn significant representations from raw signals without requiring any feature engineering. Then, the raw data were directly fed into LSTM neural network to establish the relationship between the raw vibration signals and fault states. The whole proposition was experimentally demonstrated and discussed by carrying out the tests of six three-phase asynchronous motors under different fault conditions in the drivetrain diagnostics simulator system. Performances of other classification methods such as LR, SVM, MLP, and basic RNN, are tested and contrasted. The results show that the proposed approach achieves the highest accuracy in motor fault diagnosis."}, {"label": 1, "content": "Fault detection and diagnosis are crucial to ensure industrial safety and profitability, and the traditional Bayesian network has been widely used. The traditional Bayesian network as a typical graphical Model has many applications in this area, but it has great limitations in the processing of continuous variables. Based on the model of system causal structure, this paper proposes the kernel density estimation(KDE) method to estimate the probability density function instead of the parameter learning of Bayesian network. This method does not require discretization or making Gaussian assumptions, making it more convincing. The root cause of fault can be positioned through causal inference. Our industrial simulation experiments have shown that this method accurately detects system faults and traces them back to their source. In summary, our proposed KDE method provides a viable alternative to the traditional Bayesian network, offering more accurate and reliable fault detection and diagnosis in industrial settings."}, {"label": 1, "content": "With the continuous progress of the space station project, the rotary mechanism is widely used in the space station. Serious failure of rotating mechanism can cause motor burnout and then safety problems. Therefore, it is necessary to monitor and diagnose rotating mechanism. The vibration signal of the rotating mechanism is nonlinear and non-stationary. To overcome this issue, the acceleration sensor vibration signal is decomposed using the CEEMD method to achieve intrinsic mode function. Next, Fourier transform is used to obtain time-frequency information based on fault characteristics. The degree of dimensionality reduction about fault feature is determined by selecting the number of principal components through adaptive cumulative contribution. Finally, the fault feature is trained by support vector machine and tested with data. The results show that this optimized method is less computational and effectively extracts fault features for optimal functioning of the rotating mechanism."}, {"label": 1, "content": "Currently, the planetary gear box health monitoring system has collected a huge amount of data, and the data needs to be quickly learned and real-time monitoring diagnostic requirements. Traditional fault diagnosis methods involve complex signal processing procedures, which are not always effective in extracting and classifying features. In order to diagnose the planetary gearbox effectively, this paper presents a fault diagnosis method for planetary gearbox based on hierarchical extreme learning machine (H-ELM). The method eliminates the need for complex signal processing while adaptively mining fault characteristics and automatically identifying the health conditions of machinery. The effectiveness of the H-ELM method is tested using Stacked Denoising Auto-encoders (SDAE) and Deep Belief Network (DBN). The experimental results show that the method has good effect and application value in the fault diagnosis of planetary gearbox."}, {"label": 1, "content": "In order to address the challenges of testing ship electrical equipment, including large samples, difficult fault injection, high risk, long periods, and low conclusion credibility, this paper proposes an integrated experiment technology that combines virtual simulation, semi-physical testing, and physical prototype testing. A multi-source data fusion-based integrated testability evaluation model is also presented. Finally, the feasibility and rationality of proposed method are verified by taking typical ship electrical equipment as example. This approach provides technical support for more efficient and effective testing of ship electrical equipment."}, {"label": 1, "content": "Service composition is the foundational operation mode for cloud manufacturing, however, it is not invulnerable to exceptions. To maintain the resilience of service composition during processing, this study examined the entire execution process of service composition, and developed a dynamic adjustment framework for it. This framework includes three components: anomaly detection, anomaly diagnosis, and anomaly handling. This framework is driven by data, which are collected from the weakest links of the service composition through Internet of things (IoT) technology and monitoring technology. Besides, all data processing and the adjustment strategies are operated by the cloud manufacturing platform."}, {"label": 1, "content": "Prognostics and health management (PHM) of engineered systems involve the challenging task of estimating the remaining useful life (RUL). For data-driven prognostics, machine learning algorithms are nowadays attracting the attentions of researchers. This paper introduces relevance vector regression (RVR) algorithm into RUL prediction, as it models the nonlinearity and uncertainty of the degradation process very well. However, conventional RVR models cannot recognize the overall degradation pattern, leading to inaccurate long-term RUL predictions. When applying it for long-term prediction to estimate RUL, the result might deviate from the real situation greatly. This new design matrix offers accurate long-term RUL predictions, as demonstrated through a case study of turbofan engine RUL estimation. Notably, both the kernel width and the normalization of the input vector significantly impact the RVR-NDM learning results. Therefore, the authors propose a strategy for optimizing the model. Overall, the proposed RVR-NDM model is effective and outperforms the basic RVR and generalized linear regression methods. This research has significant implications for improving PHM and enhancing the reliability and safety of engineered systems."}, {"label": 1, "content": "During the detection and diagnosis of bearing faults, one of the most important steps is to extract fault features accurately. This paper proposes a new redundant fault feature extraction technique based on tunable Q-factor wavelet transform (TQWT), which can separates complex non-stationary signals due to its oscillatory behavior rather than the frequency band. By implementing different couples of Q-factor and redundancy, we collect energies of multi-scale sub-band signals to characterize the failure symptoms. We conducted experiments using two cases of bearing datasets, which demonstrated that our proposed method is much more robust compared to the traditional single-scale method in terms of bearing fault classification and performance degradation assessment."}, {"label": 1, "content": "A degradation model with random parameters can improve the accuracy of reliability assessment compared with that with fixed parameters. However, the application of such a model to analyze accelerated degradation data is challenging. To address this issue, a technique that utilizes the inverse Gaussian degradation model with random parameters has been developed in this study. Acceleration factor constant principle was used to deduce the relationships that the parameters of Inverse Gaussian degradation model should satisfy under different stresses. Then, the expression of acceleration factor for an inverse Gaussian degradation model was obtained. Accelerated degradation data was converted to equivalent data under normal stress levels using acceleration factors. The conjugate prior distributions of random parameters were used, and the Expectation Maximization algorithm was employed to estimate hyper parameters. Simulation tests validated the feasibility and effectiveness of proposed method, and a case study demonstrated the proposed method has a good engineering application value."}, {"label": 1, "content": "Due to low training complexity, high stability, quick convergence and simple construction, the probabilistic neural network (PNN) has got extensive application in many fields. However, this study has found that by introducing weight coefficients and compensating factors, the PNN model structure can be improved. The weight coefficients and compensating factors are introduced into the network and put between pattern layer and summation layer to create the weighted probabilistic neural network (WPNN). The weights are derived using the sensitivity analysis procedure when the radial kernels are used as the output of the pattern layer. Simultaneously, compensating factors compensate for the impact of the sensitivity analysis among the patterns. The performance of the WPNN is examined in various experiments, including fault diagnosis of aircraft wing skin. The results indicate that the WPNN is feasible and has excellent prediction accuracy, highlighting its broad application in various fields. Overall, this study introduces a novel approach that addresses the limitations of the PNN model by introducing weights and compensating factors, leading to the creation of the WPNN that significantly improves prediction accuracy."}, {"label": 1, "content": "In the modern industry, the prediction of fatigue remaining useful life of materials is important for safety improvement and cost reduction. In the era of Internet of Things, large amount of data can be easily collected and analyzed using deep learning based approach for decision making. Deep learning represents a new opportunity for effective prediction of fatigue remaining useful life prediction in facing the challenge of big data. This paper presents a deep learning based approach for material fatigue remaining useful life prediction. First, a relationship between acoustic emission signals and fatigue life is established using a long short-term memory (LSTM) model. Then, convolutional neural networks (CNN) are combined with LSTM to extract features. Finally, based on the carbon steel samples, the model is tested with 1193 groups of carbon steel fatigue test data. As results shown, the prediction results are promising."}, {"label": 1, "content": "With the extensive application of phased mission system (PMS), the study of phased mission characteristics becomes more and more necessary and urgent. Due to the existence of multi-mode equipment failures, this paper puts forward a reliability modeling and prediction approach based on micro-replacement fault tree analysis to tackle the challenges associated with analyzing the reliability of phased mission systems. We take a typical phased mission system as an example to verify the feasibility of the method. The method offers a more straightforward and efficient approach to evaluate the reliability of complex systems."}, {"label": 1, "content": "The application of machine learning techniques in fault diagnosis of induction motors has become increasingly popular in recent years. However, one of the major challenges is the selection of handcrafted statistical features, which greatly limits the performance of the classifiers. Deep learning, a feature representation based method opens up a new horizon, where feature descriptors are extracted from the raw signals. The paper reports preliminary findings in motor fault detection using novel semi 2D Convolution Neural Networks. The experimental results of the proposed approach show 3-10% enhanced performance compared to the conventional feature engineering based methods. The computation is relatively faster than 2D Convolution Neural Networks and the generalization of the results is promising for real life applications."}, {"label": 1, "content": "To diagnose faults of bearings accurately, a fault diagnosis method based on Empirical Mode Decomposition (EMD), Singular Value Decomposition (SVD) and adaptive Multiclass-Mahalanobis-Taguchi System (aMMTS) is proposed in this paper. The condition of the bearing is monitored in real time by sensors. Then, the vibration signal is decomposed by EMD and the features are extracted by using SVD. This hybrid method overcomes the disadvantages of the Mahalanobis-Taguchi system in terms of over-fitting and non-adaptive feature selection for fault diagnosis, and has advantages over the traditional auxiliary noise fault analysis method when dealing with nonlinear signals. The hybrid method based on EMD-SVD and adaptive Multiclass-Mahalanobis-Taguchi system overcomes the shortcomings in the Mahalanobis-Taguchi system in terms of over-fitting and non-adaptive feature selection for fault diagnosis and has some advantages over the traditional auxiliary noise fault analysis method when dealing with nonlinear signal, and can diagnose the bearing fault without manual intervention. The effectivity and feasibility of the proposed method is validated by an experiment."}, {"label": 1, "content": "Recently, deep learning technology has gained significant breakthroughs and rapid development, offering a new direction for research in Prognostics Health and Management (PHM). In this paper, we propose two deep learning models to solve feature extraction problem and faults diagnosis problem. First model is based on Auto-Encoder (AE) and Support Vector Machines (SVM). AE is used to efficiently extract features and reduce the original signal dimensions. Then, the extracted features are classified as SVM input. Second model is based on Convolution Neural Network (CNN), we propose a 1D-CNN model to process the original bearing vibration signal and directly output the type of fault. These models have yielded good results on the milling datasets and CWRU bearing dataset respectively. This paper verified the feasibility of these methods, summarized the application experiences and obtained their performance indicators as a benchmark for research."}, {"label": 1, "content": "As the motor drive system is a complex and non-linear system, traditional diagnosis methods have proven to be ineffective in accurately identifying faults. So a multi-feature information fusion intelligent monitoring and fault diagnosis algorithm is proposed in this paper which combing the time-domain and frequency-domain information for feature fusion, multi neural network (MNN) for diagnosis fusion. The signal conditioning and fault diagnosis boards were developed and then integrated to the diagnostic system on the train. The results of the diagnosis showed that the intelligent fusion method was not only more accurate, but also saved significant amounts of time compared to traditional methods. The diagnosis results showed that the intelligent fusion method was much more accurate and time-saving compared with traditional methods."}, {"label": 1, "content": "This paper presents a novel technique for estimating differential detection to minimize laser noise in an electro-optic sensor system. This new estimation method evaluates the differential detection effect on noise reduction by using a phasor diagram and fast Fourier transform. The estimation results obtained from this method agree with the DC level dependence of the signal-to-noise ratio, thus confirming that the proposed estimation method is suitable for evaluating the effect of differential detection on noise reduction."}, {"label": 1, "content": "In geological studies, one vital aspect is to identify the sedimentary rock type or its grain size. The accuracy of this analysis is crucial, and it can be done in the field or in a laboratory. As the size of the study area grows, this activity can be time consuming and error prone because the number of specialists working under rigid criteria also increases. This paper proposes a novel methodology to classify grain size using unique wavelength reflectance data and artificial neural networks. The results indicate that the proposed method can be reliably used in the field."}, {"label": 1, "content": "This paper presents a novel obstacle detection algorithm in the indoor environment. The algorithm utilizes the YOLO object detection algorithm and a light field camera, which is simpler than a normal RGB-D sensor but can acquire depth images and high-resolution images simultaneously. The RGB image captured by the light field camera is used as input to the trained YOLO model, which can recognize nearly 100 categories of common objects. According to the object information and the depth map, the obstacle was accurately calculated including its size and position. Experimental results demonstrate that the proposed method can provide higher detection accuracy under indoor environment."}, {"label": 1, "content": "The fast growth of the Internet has resulted in IPv4 addresses depletion. Internet Service Providers (ISPs) are trying to replace their IPv4 networks with IPv6 gradually. IPv6 was launched with new features like simpler header format, larger address space, efficient routing and built-in security. The co-existence of IPv4 and IPv6 presents several challenges, as the two protocols are not compatible, and packet traversing and routing can be difficult. One solution to this problem is tunneling, which provides a temporary solution to packet traversing. Tunneling is a temporary solution which is used to resolve packet traversing. This research presents the behavioral analysis of Open Shortest Path First (OSPFv3) through several IPv6 tunneling protocols (6in4, 6to4, ISATAP & GRE) over large scale IPv4 network. The study aims to analyze the performance of OSPFv3 through route summarization over hybrid IPv4-IPv6 networks."}, {"label": 1, "content": "The prevalence of diabetes is on the rise across the globe. To prevent this condition, increasing calorie expenditure is among the recommended tactics. Measuring the current calorie expenditure can improve awareness in daily life and help increase calorie expenditure. Conventional research has estimated calorie expenditure using an acceleration sensor. However, the accuracy of these estimates is dependent on the positioning of the sensor, raising concerns about reliability. Consequently, the aim of this study was to develop a non-contact sensor that could effectively estimate calorie expenditure. Using a 3D range image sensor, we conducted experiments to evaluate the feasibility of this approach. The results of the evaluation demonstrated that the proposed method indeed proved effective in estimating calorie expenditure."}, {"label": 1, "content": "Finding the rising stars is an interesting problem and has been recently studied in various domains including academic networks. This study formulates the problem of rising stars prediction as a machine learning task. This approach involves applying classification models to categorize features as co-authors, authors, and venues. The effectiveness of this categorization strategy is empirically analyzed. To test this approach, data from Pakistani researchers retrieved from Web of Sciences between 2008 to 2014 was used. Feature sets were calculated and classification techniques applied to predict rising star researchers. The researchers predicted by the proposed method are compared with top researchers of Pakistan in 2016 and 2017. The proposed technique successfully solves the problem of finding rising star researchers in computer sciences based on the authors' contribution, mutual influence, and venue citations scores."}, {"label": 1, "content": "The internet of Things (IoT) architecture was originally envisaged as a two-layer technical platform, with sensors collecting data at the edge with minimal compute requirements, solely to prepare and transporting the data to a centralized or cloud based infrastructure for processing. While this model may work in some scenarios, such as where data is being stored for historical or regulatory purposes, it may not be suitable for health monitoring or autonomous vehicle computer vision applications where latency in data processing can result in poor performance of these applications. There are various definitions of IoT topologies being discussed within the industry, and this paper reviews the original topology of an IoT solution. This paper reviews this original topology of an IoT solution, and different techniques and layers available to alleviate the issues inherent of the original paradigm, and how a new method of defining at these topologies is gaining speed."}, {"label": 1, "content": "The goal of this paper is to present a literature study on the use of Wireless Sensor Networks (WSNs) in Preventative Maintenance applications for Industry 4.0. The paper delves into discussing the fundamental requirements for employing WSNs in industrial applications and draws a comparative analysis between the features of current and emerging WSN technological enablers. Moreover, the paper highlights the design considerations that are intrinsic to efficient utilization of WSNs as a tool for driving maintenance efficiencies in a practical context, as exemplified by the successful implementations described in research literature and commercial solutions available on the market."}, {"label": 1, "content": "Rotameter is a type of flow meter that operates on the variable area principle. In this paper, an intelligent inductive pickup type flow transducer using rotameter is developed with temperature compensation. The rotameter's float is connected to a ferromagnetic wire that changes the self-inductance of the inductive pickup according to the float's movement. The self-inductance is changed into a voltage with the modified Maxwell bridge network. The density of the fluid moving through the rotameter is affected by variations in temperature, which disturb the calibration of the rotameter. This paper proposes using different ANN (Artificial Neural Network) algorithm schemes to compensate for the temperature influence in the modified rotameter."}, {"label": 1, "content": "The tracking of 3D human motion from monocular video sequences has become increasingly popular in recent years. Among these human motion tracking methods, the particle filter is considered as an effective approach. However, current particle filter-based methods have limitations, such as producing many particles that are inconsistent with the observed image due to their lack of dependence on image information. In this paper, we present an image-constrained particle filter approach to track 3D human motion from monocular video clips with the assistance of a pre-captured motion library. We introduce two new particle filtering criteria and a hierarchical likelihood function. The top layer of the function consists of the particle filtering criteria, and the bottom layer consists of the likelihood functions based on image contours and edge features. We eliminate particles that do not match the image significantly at the top level, and the remaining particles are evaluated using the underlying likelihood function. The results of our experimental research demonstrate that our method can improve the accuracy of motion tracking and constrain the estimation of human body position efficiently."}, {"label": 1, "content": "Extracting agarwood consumes higher energy expenditure due to time consuming. However, a new controller scheme has been developed to overcome this challenge. With implementation of control strategy, the energy usage can be reduced and at the same time can improve the transient response performance. This paper evaluates the comparison between self-tuning FuzzyPID with 49 rules and PID controller (Ziegler-Nichols and AMIGO) tuning rules on both transient response and energy consumption through simulation. For transient analyses, three criterias have been analysed which are settling time, rise time, and percentage of overshoot. The energy consumption was evaluated using Integral Absolute Control Signal (IACS). The simulation results showed that the self-tuning FuzzyPID outperformed the PID controller in transient response and consumed less energy."}, {"label": 1, "content": "As part of ongoing research on agarwood oil quality classification, this paper presents a non-linear SVM model using polynomial as the kernel parameter. The work involves of 96 agarwood oil collection, from different high qualities. The input for SVM modeling is the abundances (%) of volatile and the output is agarwood oil qualities either low or high. The experimental works are carried out automatically via MATLAB software version R2016a. The results indicate that the polynomial tuning kernel parameter is effective in classifying agarwood oil volatile to high and low qualities. The study achieved 100% accuracy, as supported by the confusion matrix, sensitivity, precision, and specificity. The finding in this study is important and benefits other future work focusing on agarwood oil research area."}, {"label": 1, "content": "Cellular Automata is a mathematical model that provides a solution to complex computational systems by utilizing a fixed set of predefined rules. It is a cell based system where cells are arranged in a regular grid. The simple modular and regular structure of cellular automata attracts scientists, researchers and practitioners to continue their research in different fields of science and technology. As a result, there is a growing number of innovative applications being developed in this field. In particular, researchers are showing more interest in developing efficient hardware and software applications based on Cellular Automata. This paper aims to draw attention to several of these useful applications. It provides an overview of the available literature and methodology of those innovations, and highlights some of the more interesting applications. Finally it explains how CA can be applied in different areas of science and technology. Overall, this paper intends to provide enough information that will be helpful to researchers working in this field."}, {"label": 1, "content": "This paper introduces a novel approach to distinguish between a Wiener structure and a Hammerstein structure for systems with rate saturation nonlinearity using the coherence function. The coherence function is known to be used for static nonlinearity, but this work extends its application to cover dynamic nonlinearity. The test capitalizes on the same set of data generated when estimating the best linear approximation of the system. The identification of the system parameters through the best linear approximation is also analyzed in terms of the sensitivity and convergence properties. The effectiveness of the approach is illustrated through simulations. The results show that the proposed approach combining the structure identification and parameter estimation outperforms an existing method."}, {"label": 1, "content": "The detection of insulators with cluttered backgrounds in aerial images is a challenging task for an automatic transmission line inspection system. However, in this paper, we propose an efficient and reliable insulator detection method that utilizes a deep learning technique for aerial images. In the proposed deep detection approach, the single shot multibox detector (SSD), a powerful deep meta-architecture, is incorporated with a strategy of two-stage fine-tuning. This SSD-based model can extract automatic multi-level features from aerial images instead of relying on manual feature extraction. Additionally, we implement a two-stage fine-tuning strategy that is inspired by transfer learning, using separate training sets. In the first stage, the basic insulator model is obtained by fine-tuning the COCO model with aerial images, including different types of insulators and various backgrounds. In the second stage, the basic model is fine-tuned by the training sets of the specific insulator types and specific situations to be detected. After the two-stage fine-tuning process, the well-trained SSD model can accurately identify insulators in aerial images by simply feeding them into the model. The results show that both the porcelain insulator and composite insulator can be quickly and accurately identified in the aerial images with complex background. Our proposed approach significantly enhances the accuracy, efficiency, and robustness of insulator detection."}, {"label": 1, "content": "Fog computing has become the primary infrastructure on the Internet for improving the quality of service. We consider a fog queueing system with limited infrastructure resources to accommodate real-time tasks with heterogeneities in task types and execution deadlines. Due to the uncertain execution duration, fog resource allocation and task offloading should be jointly considered to satisfy the deadline requirements. In this study, we apply a parallel virtual queue model to buffer each type of task in a separate queue and propose a framework with three parallel algorithms, namely, offloading, buffering, and resource allocation, to improve resource allocation balance, throughput, and task completion ratio. Task offloading is decided based on the task urgencies in terms of the laxity times, which considers the deadline, estimated execution time, and transmission delay to the cloud. The task offloading is decided according to the task urgencies in terms of the laxity times, which accounts for the deadline, estimated execution time, and transmission delay to the cloud. The resource allocation uses an adaptive queue weight based on the Lyapunov drift to avoid task starvation, which may lead to a long queueing delay for tasks with long execution time. The resource allocation uses an adaptive queue weight based on the Lyapunov drift to avoid task starvation that may lead to a long queueing delay for tasks with long execution time. Therefore, the use of fog computing with efficient resource allocation and task offloading strategies is a promising solution for real-time applications with different requirements on the Internet."}, {"label": 1, "content": "In this article, we introduce an innovative and durable approach to quantized matrix completion. First, we propose a rank minimization problem with constraints induced by quantization bounds. We then form an unconstrained optimization problem by regularizing the rank function with Huber loss. Huber loss is leveraged to control the violation from quantization bounds due to two properties: first, it is differentiable; and second, it is less sensitive to outliers than the quadratic loss. A smooth rank approximation is utilized to endorse lower rank on the genuine data matrix. Thus, an unconstrained optimization problem with differentiable objective function is obtained allowing us to advantage from gradient descent technique. Novel and firm theoretical analysis of the problem model and convergence of our algorithm to the global solution are provided. Secondly, our method does not require projections or initial rank estimation, unlike the state-of-the-art. In the Numerical Experiments section, the noticeable outperformance of our proposed method in learning accuracy and computational complexity compared to those of the state-of-the-art literature methods is illustrated as the main contribution."}, {"label": 1, "content": "Scenario-based approaches have been widely employed in stochastic economic dispatch with wind power integration. However, due to computational limitations, the number of wind power scenarios usually need to be reduced to a small subset of all available scenarios. The limited number of scenarios could affect the objective performance related to the correlations and uncertainties of wind power, especially when the influence of extreme scenarios needs to be accounted for. To increase the computational efficiency of economic dispatch problem with a large number of scenarios, we propose a Lagrangian relaxation with incremental proximal method to solve the dispatch problem. This approach combines the benefit of Lagrangian relaxation, which allows the problem to be decomposed into a large number of smaller problems and the proximal method, which lead to much faster convergence. Further, we propose a multipliers and primal variables initialization method for reducing the convergence time. Results of case studies show that this framework greatly reduces the computation time and the system cost of economic dispatch compared with the traditional approaches."}, {"label": 1, "content": "This letter unfolds a digital predistortion (DPD) technique that improves the linearity of limited range mobile front haul links for the contemporary long-term evolution (LTE) and future (5G) networks. In particular, the proposed technique is applied to radio-over-fiber links based on single-mode (SM) vertical cavity surface emitting lasers emitting at 850 nm and standard SM fibers. To identify the predistorter, both memory and generalized memory polynomial models are utilized via indirect learning architecture. The impact of the DPD technique is observed by the link performance improvement in terms of normalized mean square error and adjacent channel power ratio, referring to complete LTE frames of 10 ms occupying 5-MHz bandwidth and having 64-quadrature amplitude modulation format. Furthermore, the effectiveness of the DPD approach, when varying input power levels, is investigated. The experimental results demonstrate the capability of the proposed DPD technique to achieve promising linearization performance."}, {"label": 1, "content": "Considering an ultra-reliable low latency communication scenario, we assess the trade-off in terms of energy consumption between achieving time diversity through retransmissions and having to communicate at a higher rate due to latency constraints. We consider Nakagami-m block-fading channels with Chase combining hybrid automatic repeat request and derive a fixed-point equation to determine the best number of allowed transmission attempts. Our analysis provides insights into the system behavior and compares the energy consumption of the proposed approach against direct transmission with frequency diversity. Furthermore, we compare the energy consumption of the proposed approach against direct transmission with frequency diversity. Results show substantial energy savings using retransmissions when selecting the maximum number of transmission attempts according to our approach. For instance, considering a Rayleigh channel and smart grid teleprotection applications, our approach uses around 8 times less energy per bit compared with a direct transmission with frequency diversity."}, {"label": 1, "content": "The prediction of daily stable warfarin dosage for a specific patient is difficult. To improve the accuracy of predictions and develop a highly accurate model, we developed a method called evolutionary fuzzy c-mean (EFCM) clustering algorithm with support vector regression (SVR) for ensemble learning. Our dataset comprised of 517 Han Chinese patients, which included data from The First Affiliated Hospital of Soochow University and the International Warfarin Pharmacogenetics Consortium for training and testing purposes. In EFCM+SVR, we adopted SVR to build a generalized base model (SVR model). The SVR and clustering models were integrated into an ensemble model using stepwise functions. Our experiment included three artificial neural networks, SVR, two ensemble models, and three regression models as comparators to the EFCM+SVR model. EFCM+SVR obtained the smallest mean absolute error (0.67 mg/d) in warfarin dosage prediction and the largest R-squared (43.9%). The model achieved satisfactory predictions, with 36% of patients having predicted doses within 15% and 46.6% of patients having predicted doses within 20% of the actual stable therapeutic dose."}, {"label": 1, "content": "This paper introduces a novel modeling approach that allows to obtain fast simulations of pulsewidth-modulated dc-dc switched-mode power supplies (SMPS). The proposed method employs precise switched models during transient phases and averaged models during steady-state or slowly varying conditions. This mixed modeling strategy enables us to obtain detailed switching behavior of SMPS in long-term simulations. The switching between models occurs automatically through an algorithm that identifies transient or slowly varying conditions based on specific model variables. When using the precise switched model, the mentioned algorithm adjusts the averaged model parameters to ensure accurate results regardless of the operating point. Additionally, this paper illustrates the implementation of the methodology in the Modelica language and presents simulation experiments that demonstrate the results are as accurate as those obtained using precise switched models, yet several times faster."}, {"label": 1, "content": "A co-occurrence pattern is an interesting pattern in human mobility, which has essential values in business intelligence, social activities, and urban planning. However, due to the deluge and complexity of mobile big data, as well as the complicated intrinsic features of the co-occurrence pattern, mining and analyzing the co-occurrence pattern are computationally highly expensive. Therefore, in this paper, we propose a framework to mine co-occurrence event data from mobile data and to explore the urban co-occurrence pattern visually. Our framework contains two modules: data modeling, to obtain the co-occurrence event data effectively utilizing frequent itemsets mining algorithm based on traffic GPS records, and visualization, to explore the co-occurrence pattern in urban scenarios from global, regional, statistical, and location perspectives. Our visualization system has been demonstrated using case studies with a real-world data set."}, {"label": 1, "content": "The deployment of smart hybrid heat pumps (SHHPs) can introduce considerable benefits to electricity systems via smart switching between electricity and gas while minimizing the total heating cost for each individual customer. Optimal control technology can provide flexible heat distribution to better utilize low-carbon energy, improving overall energy efficiency for heating systems. In order to achieve this, accurate preheating quantification is necessary to understand the flexible heat capabilities of the system. This paper proposes a novel data-driven preheating quantification method to estimate the capability of the heat pump demand shifting and isolate the effect of interventions. Using fine-grained data from a real-world trial and Bayesian deep learning techniques, the method estimates baseline heat demand while considering uncertainties. Several case studies show the efficacy of the quantification method, and the estimated demand shift is utilized in the whole-system model to study the implications and benefits of SHHPs for future GB electricity systems, especially those developed by PassivSystems."}, {"label": 1, "content": "This paper proposes a new and efficient technique to regularize the neural network in the context of deep learning using correlations among features. Existing research has demonstrated that excessively large neural networks tend to generate redundant features that are either shifted versions of one another or exhibit very little variation, leading to redundant filtering. We propose a way to address this problem and show that such redundancy can be avoided using regularization and adaptive feature dropout mechanism. We show that regularizing both negative and positive correlated features according to their differentiation and based on their relative cosine distances yields network extracting dissimilar features with less overfitting and better generalization. We validate this concept using various deep neural network architectures, including multilayer perceptron, convolutional neural network, sparse autoencoder, gated recurrent unit, and long short-term memory on several widely used datasets such as MNIST digits recognition, CIFAR-10, ImageNet, and Stanford Natural Language Inference."}, {"label": 1, "content": "In this paper, we investigate the reconstruction of sparse signals using nonconvex regularization methods, with a particular emphasis on two popular techniques: minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). An approximate message passing (AMP) algorithm is an effective method for signal reconstruction. Based on the AMP algorithm, we propose an improved MCP iterative thresholding algorithm and an improved SCAD iterative thresholding algorithm. The convergence of the new algorithms is analyzed, and we provide a series of experiments to assess their performance. Our results show that the new algorithms demonstrate stronger reconstruction capabilities, higher phase transition for sparse signal reconstruction, and better variable selection ability than the original MCP iterative thresholding algorithm and the original SCAD iterative thresholding algorithm."}, {"label": 1, "content": "Deep convolutional neural networks (CNNs) have been successfully applied on no-reference image quality assessment (NR-IQA) with respect to human perception. However, most existing methods rely on processing small image patches and calculating the average score of the patches to predict overall image quality. Our team has discovered that homogeneous image patches are unreliable both for neural network training and for estimating final image quality score. Conversely, image patches with complex structures have higher chances of yielding better image quality prediction. Building on these findings, we have improved the conventional CNN-based NR-IQA algorithm by eliminating homogeneous patches from network training and quality score estimation. Furthermore, we have introduced a variance-based weighting average to bias the final image quality score toward patches with complex structures. Our experiments demonstrate that this simple approach outperforms well-established NR-IQA algorithms and achieves state-of-the-art performance."}, {"label": 1, "content": "This paper proposes an adaptive neural control method for a class of nonlinear time-varying delayed systems with time-varying full-state constraints. The proposed method is the first to investigate how to tackle time-varying constraints and delays using a unified framework. Time delays and constraints severely limit the performance of the system, and in some instances, lead to system instability. The method leverages appropriate Lyapunov-Krasovskii functionals to eliminate the effect of unknown delays. The approach addresses constant constraints, which are the only special case of time-varying constraints, leading to more complexities. A time-varying asymmetric barrier Lyapunov function is employed to ensure that the full state of the system is always within the time-varying constrained interval. The effectiveness of the method is confirmed with the help of two simulation examples."}, {"label": 1, "content": "With abundant spectrum resources, the millimeter-wave (mmWave) band has emerged as a leading candidate for fulfilling the requirements of faster data rates and lower latency for future automotive networks. However, the incorporation of mmWave directional transmission and diversely vehicular mobility triggers frequent beam realignment, thus largely increasing the beamforming overhead and leaving less time for data transmission. In this paper, an energy-angle domain access and transmission frame structure is investigated for mmWave vehicle-to-everything communications, which consists of two phases, initial access and data transmission. Considering the transmission interruption issue caused by blockage, we propose an energy-angle domain initial access scheme, by which the signals are labeled by different directions with multi-power level. Several performance metrics are subsequently obtained to evaluate the proposed scheme. Subsequently, the access time-throughput tradeoff problem is mathematically formulated, and it is demonstrated that optimal access time exists, yielding the highest throughput for data transmission. Moreover, a binary-decision beam tracking scheme is designed to maintain the directional link connection in the data transmission phase. To verify our stance's accuracy and our proposed strategies' superiority, numerical evaluations and simulations were conducted."}, {"label": 1, "content": "Modeling a cloud computing center is crucial to evaluate and predict its inner connectivity reliability and availability. Several previous studies have focused on evaluating the system availability/reliability of virtualized systems comprising singular servers in cloud data centers. In this paper, we propose a hierarchical modeling framework for the reliability and availability evaluation of tree-based data center networks. The hierarchical model comprises three layers, with reliability graphs at the top layer, a fault-tree to model subsystem architecture, and stochastic reward nets to capture the detailed behaviors and components\u2019 dependency in the subsystems. Two representative data center networks based on three-tier and fat-tree topologies are modeled and analyzed in a comprehensive manner. We specifically consider a number of case-studies to investigate the impact of networking and management on cloud computing centers. Furthermore, we conducted various detailed analyses to determine reliability and availability measures for the system models. The results showed that appropriate networking to optimize node distribution in data center networks could enhance reliability/availability. This paper's conclusion can be useful for practical management and the construction of cloud computing centers."}, {"label": 1, "content": "This paper presents a novel approach for detecting selective harmonics from a single-phase power system that operates in a changeable frequency environment. The proposed method utilizes an open-loop configuration, which consists of a frequency tracking algorithm using a fixed tuned demodulation approach and a selective harmonic detection scheme based on an adaptively tuned cascaded delayed signal cancellation (CDSC) strategy. Unlike other techniques, this method can easily detect one or several specific harmonics from the input signal contaminated by various harmonics. The proposed approach is robust, easy to tune, and unconditionally stable without any interdependent loops. The real-time utilization of trigonometric and inverse trigonometric functions is also avoided in the technique. When compared with the discrete Fourier transform and CDSC-based technique reported in the technical literature, it demands less computational effort and can create faster dynamics and better steady-state accuracy in the estimated frequency. The usefulness of the technique is confirmed by simulation and real-time experimental results."}, {"label": 1, "content": "This paper addresses the event-triggered tracking for a class of uncertain nonlinear systems. Unlike previous related works, this paper considers systems with serious uncertainties and limited a priori information on the reference signal. As a result, event-triggered control becomes more challenging due to the difficulty in precisely bounding sampling errors. As main contributions of this paper, new adaptive event-triggered tracking schemes are proposed for the systems in two event-triggering architectures with different roles of the event-triggering mechanism on the information transmitting and control computing/updating. The dynamic gain is incorporated in both schemes to counteract serious uncertainties and overcome the negative impact of sampling errors. Based on the dynamic gain, two adaptive event-triggered controllers are designed with distinct event-triggering mechanisms, respectively. Particularly, to ensure application flexibility, one of the triggering mechanisms is rendered relatively independent of the controller signal. The proposed event-triggered controllers can achieve practical tracking for the systems, ensuring prespecified arbitrary tracking accuracy while avoiding infinitely fast sampling/execution."}, {"label": 1, "content": "In this paper, we investigate cooperative spectrum sensing (CSS) in a cognitive radio network (CRN) where multiple secondary users (SUs) cooperate in order to detect a primary user, which possibly occupies multiple bands simultaneously. To address the issue, we introduce the concept of deep cooperative sensing (DCS) - the first CSS framework based on a convolutional neural network (CNN). DCS is designed to autonomously learn the strategy for combining individual sensing results of SUs through training sensing samples, regardless of sensor quantization. DCS leverages both spectral and spatial correlations of individual sensing outcomes to enable an environment-specific CSS. As demonstrated through simulations, the performance of CSS can be significantly improved using DCS."}, {"label": 1, "content": "An increasing number of user applications are utilizing edge cloud data centers (ECDCs) for computing resources. Effective management of these resources has a positive impact on cloud service providers' profitability and meets the stringent quality-of-service (QoS) requirements of user applications. This research proposes a software-defined networking (SDN) based resource allocation framework for interconnected ECDCs. SDN technology ensures QoS and efficiently embeds user applications within the ECDCs. The proposed framework, called infrastructure as a service provisioning using SDN (IaaSP-SDN), includes the two-phase coordinated IaaS requests. The performance of IaaSP-SDN is evaluated in two steps: first, a prototype is compared with generalized multi-protocol label switching (GMPLS), and second, the impact of SDN controller physical location on the framework's performance is evaluated. The performance of IaaSP-SDN is evaluated in two steps: 1) a prototype is compared with generalized multi-protocol label switching (GMPLS) and 2) the impact of SDN controller physical location on the performance of the IaaSP-SDN framework is evaluated. Furthermore, the research shows that the SDN controller location attributes have a significant effect on the acceptance ratio of IaaS requests. We also show that SDN controller location attributes have a significant effect on the acceptance ratio of IaaS requests."}, {"label": 1, "content": "Social media marketing is expanding rapidly with the advancements in information and communication technology. Investigating how companies are exploiting social media for marketing, advertisement, and consumer's engagement is gaining more and more interest. In this paper, brands/companies data on Twitter is collected and analyzed to compute the overall company response on Twitter. Responsiveness of a company is inferred from three features: company popularity, average company replies, and average followers' replies. Additionally, Twitter network features are used to calculate the posting frequency for companies and their followers. It is shown that the proposed approach can be used in finding the responsiveness of companies and their followers. The study also extracts useful links for brand consumers and analyzes the posting behavior of brands and their followers through Twitter network features, such as retweet count and geolocation. This paper contributes to the literature on how Twitter data and its network structure features can be exploited in finding the responsiveness and posting behavior of companies and their followers. This approach can be effectively employed in developing prediction and information-filtering systems, such as personalized recommendation systems."}, {"label": 1, "content": "Recent advances in the film industry have given rise to exponential growth in movie/drama production and adaptation of the Big Data concept. Automatic identification and classification of movie characters have received tremendous attention from researchers due to its applications in video semantic analysis, video summarization, and personalized video retrieval for which several methods have been recently presented. However, these methods cannot detect main characters properly due to their variation in pose and style in different scenes of a movie. To address this problem we present DeepStar, a novel framework for starring character identification based on deep high-level robust features. The proposed framework is threefold: the extraction of shots with clear faces from the input video; face clustering using discriminative deep features; and the occurrence matrix generation, helping in the selection of starring characters. The promising results obtained using representative Hollywood movies demonstrate the effectiveness of our method in detecting starring characters over the state-of-the-art methods."}, {"label": 1, "content": "The development of the vocational education system is closely tied to the demand for professional services in the economy and labor market. The regions, society, and institutions of higher education hold the responsibility of improving the quality of vocational training, qualifications, and motivation for personal and professional growth in the citizens of the Russian Federation. Nowadays, the labor market of the Republic of North Ossetia-Alania is characterized by imbalance and contradictory tendencies both in quantitative and qualitative composition of employees. To eliminate the disproportion in the development of the labor market and educational services, the vocational education system's primary objective is to produce highly qualified specialists for the Republic's economy. To achieve this objective and address the challenges faced in education development, information and communication technologies, including distance education, must be used efficiently. To address the Republic's education development problems comprehensively, an integrated program of joint actions of responsible and interested individuals and organizations is necessary. This cooperation will provide a foundation of organizational sustainability, leading to the growth of educational services and the development of the vocational education system in the Republic."}, {"label": 1, "content": "In article problems of an innovation project management for a formation of digital engineer and conditions of digital transformation of the Russian enterprises are presented. The digital enterprise is viewed as a cyber-physical system, with both pre- and post-measurement data available in both electric and economic forms. Approaches to formalization of metrics of assessment of a maturity of innovative infrastructure of the hi-tech enterprise are given. Special attention in article is paid to procedures of digitalization of production and application of tools of project management."}, {"label": 1, "content": "The article considers modern trends that characterize changes in the corporate culture of international companies. The paper also reveals the impact of corporate culture on the company growth. Several illustrations are provided to demonstrate how a corporate culture can be successfully cultivated, how human resources can be utilized effectively, and how the internal projects of a company's staff can be supported. In this regard, the authors describe the vision of corporate culture development as foreign and Russian corporate leaders see it."}, {"label": 1, "content": "A concept for formation of a single cyber environment of higher education has been represented. The principles necessary for building this environment have been carefully reviewed, and the potential impacts on both the functioning of higher educational establishments and the quality of staff training have been thoroughly analyzed. Moreover, a proposed model for a basic educational program, which serves as the foundation for perpetual information support for training highly qualified specialists in science-intensive industries, has been introduced. This model will ensure minimization of routines, ensure coordinated self-documented learning process within which all reporting documentation is formed automatically on information contained in the educational program model. The results of developing an e-learning system based on interactive teaching materials, integrated in cyber environment of virtual enterprises have been represented."}, {"label": 1, "content": "This article delves into the features of the scientific and technical cooperation between ETU \"LETI\" and LLC \"C.Nord\" in joint projects. Effect of the cooperation on the learning process in ETU \"LETI\" is shown. Additionally, the article analyzes the lack of expertise in developing real estate protection systems and proposes solutions to this problem through personnel training."}, {"label": 1, "content": "The paper analyzes the state program of competitiveness improvement of Russian universities in the international arena. The objective of the program is to ensure that the top five Russian universities make it to the top 100 educational institutions in the world by 2020. The analysis of achievements of the Russian universities in the international ratings Quacquarelli Symonds, Times Higher Education and Academic Ranking of World Universities is carried out. Additionally, the study sheds light on the financial aspects of the program, specifically examining the financial indicators of each university from an investment perspective as well as its ranking position. Based on the findings of the research, the effectiveness of the state program is assessed, and the potential for achieving the set goals is evaluated."}, {"label": 1, "content": "In this article, we delve into the crucial elements of building a national innovation system, centred on the generation and transfer of new technologies to industry. Universities possess significant scientific knowledge that can foster innovation, thus enterprises can partner with them to access novel technologies. After examining various forms of university-enterprise collaboration, we identified the most effective and promising options for their partnership. Particular attention was paid to cooperation in the development and promotion of innovation, because the technological development of the industry based on the development and implementation of new technologies. Based on the analysis, it was possible to make recommendations on creating an integrated system of interaction between basic science and industrial production, as a result, the demand for innovation will be satisfied."}, {"label": 1, "content": "In the realm of information and communication technology education and training, notable changes are underway to adjust to new technological demands. Many researches have noted growing imbalances in the labor market and a significant reduction in the employment of people with low qualifications and skills. There is a growing need for higher-level qualifications, mainly based on digital skills, machine learning, big data analytics, the Internet of things, advanced robotics, etc. The report focuses on exploring the management of the education process and the training of IT professionals while accounting for the evolving needs of the labor market."}, {"label": 1, "content": "The article explores the challenges involved in strategically managing the development of the Russian economy with regards to advanced personnel training. It considers several socio-economic and political factors that could potentially impede progress in this area. The article also proposes a model for recognizing problems related to advanced engineering personnel training, based on a linguistic-combinatorial approach, and offers alternative network solutions to address these issues. The network-centered management and the nature of the digital economy set new requirements for training systems, starting at school level and that of the small innovative enterprises up to the global socio cultural cycle. The article reveals the problems of strategic management of the development of the Russian economy and advanced training of personnel. The main socio-economic and political factors that threatening the country in this sphere are considered. An approach to the recognition of problem situations based on linguistic combinatorial modeling, which can be used in the planning of advanced engineering staff training in universities, is proposed. The network-centered management and the limits of the digital economy set new requirements for training systems, starting at school and small innovative enterprises in the global sociocultural cycle. Constraint of the digital economy is proposed by an analogy with the application of Network-centric warfare in military affairs."}, {"label": 1, "content": "In this article the author analyzes the structure of engineering activity, and reveals problems of its formation in higher education institutions. A significant aspect highlighted by the author is the necessity for educational departments in leading higher education institutions located in St. Petersburg to adopt efficient practices that will assist in training graduates for independent work. The article studies issue of the professional orientation of schoolchildren to choosing engineering professions through organization of extracurricular activities in the general secondary school."}, {"label": 1, "content": "To enhance the economic growth in the region, it is imperative to offer top-notch professionals to the businesses in the industrial and economic sector. Higher education institutions are the main executor of the order for quality specialists, therefore, the learning process should take into account the scientific and technical aspects of improving educational activities. By implementing the proposed framework for the construction of the higher education system, the fundamental pillars of the educational activity modeling were formed, and a comprehensive system model was synthesized. Application of the results will allow to manage the system of higher education in the interests of industrial and economic complex of the region."}, {"label": 1, "content": "The article deals with the problems of interaction management of remote working groups involved in the implementation of a large IT project. Given that these groups were granted a significant degree of autonomy, adopting network-centric principles in management emerged as a solution and subsequently formed the foundation for the project's structure."}, {"label": 1, "content": "In conditions of high competition, it is impossible to provide quality education without partnership both with enterprises and with other educational institutions. Cooperation with the first will allow to produce more qualified specialists who will be easily adapted at enterprises. Working with other institutions allows students to access improved information support in various areas."}, {"label": 1, "content": "This piece of work delves into the issue of knowledge formation and competence, with a focus on the goals set by employers and the general trends in economic development. The article notes the problems and considers an approach for modeling and evaluating of the effectiveness of targeted training."}, {"label": 1, "content": "Currently our life has an extremely high speed of development of science and technology, as well as a high coherence of the branches of knowledge. The emergence of each new technology affects not only the development of one narrow direction, but also has an impact on a wide sector of research and production. As a result, education must prepare students with the relevant competencies to meet the challenges of our time. Thus, the so-called the breakthrough technologies become the \u201ccornerstone\u201d of science, technology and education. This article focuses on the key features of the educational process using such technologies."}, {"label": 1, "content": "This paper proposes the multilevel model \u201cSchool - College - University - Enterprise\u201d. The model allows to determine the need for specialists and to plan the vocational guidance work with educational institutions to provide personnel in the industry of the region in the context of digital technologies."}, {"label": 1, "content": "In this article, we present a methodology for creating virtual simulators to simulate potentially hazardous chemical production processes. We describe a conceptual definition of the process of synthesizing such a virtual simulator according to a flexible unified schema using a human-oriented visual assembler. To better illustrate this approach, we provide an example using a carbon nanocluster structure synthesis process simulator that has successfully been integrated into an educational institute's learning process."}, {"label": 1, "content": "The paper is addressed to the problem of compensation for the energy losses caused by the second-order phase frequency dispersion in a transionospheric radio channel. To minimize losses, it is crucial to measure the characteristics of wideband channels and correct them for dispersion with the use of data gathered by GNSS/GPS navigation systems. Correction is effective when the processing gain from said correction surpasses a certain required threshold. The method developed in this paper allows one to determine the potential gain in processing from correcting dispersion distortions in transionospheric radio channels."}, {"label": 1, "content": "This paper proposes a genetic algorithm for solving the problem of placing sensor devices in the monitoring area with obstacles in the form of buildings, trees and other objects. The correspondence of terms borrowed from evolutionary theory to terms of the genetic algorithm is given, which allows the algorithm to be adapted for solving the placement problem. Methods for coding parameters of the algorithm and solution of the placement problem are described. The paper demonstrates the results of the program implementation of the genetic algorithm in which the solution is visualized as a map of the monitoring territory with recognized obstacles and placed sensor devices with their coverage areas."}, {"label": 1, "content": "This paper presents a new processing architecture for automatic speech recognition that focuses on the efficient extraction of Mel-Frequency Cepstrum Coefficients (MFCC). Inspired by the human ear, the energy-efficient analog-domain information processing is adopted to replace the energy-intensive Fourier Transform in conventional digital-domain. In addition, this proposed architecture extracts acoustic features in mixed-signal domains, which helps to reduce the cost of Analog-to-Digital Converters (ADCs) and computational complexity. Circuit-level simulation based on 180nm CMOS technology shows that the proposed architecture can achieve an energy consumption of 2.4 nJ/frame and a processing speed of 45.79 \u03bcs/frame. The proposed architecture achieves 97.2% energy saving and about 6.4\u00d7 speedup than state of the art. Speech recognition simulation reaches the classification accuracy of 99% using the proposed MFCC features."}, {"label": 1, "content": "Recently, there has been a significant increase in the number of accidents and incidents involving remotely piloted aircraft (RPA). Therefore, the development of a complete software and hardware system based on antenna arrays and direction of arrival (DoA) estimation for the localization of intruding RPA has attracted the attention of researchers and engineers. DoA estimation can be achieved via relative phase between antennas in an antenna array or via received signal strength (RSS) estimation of an array of directional antennas. Previous works on DoA estimation via received signal strength (RSS) used platonic solid geometry associated with the multiple signal classification (MUSIC) algorithm. In this paper, we propose a novel DoA estimation algorithm for the localization of drones exploiting their incoming NTSC video signal. Next we validate our algorithm by using an AD-FMCOMMS5-EBZ software defined radio (SDR) platform with a Yagi-Uda antenna array in a measurement campaign. The results from the measurement campaign show that from the 19 different measured DoAs from -90\u00b0 to 90\u00b0 with steps of 10\u00b0, we obtained a good measurement for 14 estimates, including the end-fire positions, which are critical in traditional omnidirectional antenna array based solutions."}, {"label": 1, "content": "Data converters are widely used in data-rich systems and are distributed across the analog-digital interface. However, conventional data converters have limitations such as sacrificing speed, power, and accuracy. Furthermore, intrinsic real-time and post-silicon variations dramatically degrade their performance. In this paper, we employ novel neuro-inspired approaches to design smart data converters that could be trained in real-time for general purpose applications, using machine learning algorithms and artificial neural network architectures. The approach involves the integration of memristor technology with CMOS, which can adapt to the continuously changing conditions of data-driven applications. This concept will pave the way towards adaptive interfaces with the continuous varying conditions of data driven applications."}, {"label": 1, "content": "In this paper the procedure of collision resolution based on successive interference cancellation (SIC) is observed. We evaluate the effectiveness of this procedure in random multiple access (RMA) systems, taking into account the signal subtraction imperfections in real-world conditions. We developed a model of the system and conducted experiments to estimate system parameters and throughput."}, {"label": 1, "content": "This paper presents the performance evaluation of a two-terminal fault location method based on aerial and ground mode traveling waves applied to the Madeira River Bipole 1. The method was tested for its accuracy through pole-to-ground fault scenarios for different fault positions and resistances. The method accuracy was evaluated through pole-to-ground fault scenarios for different fault positions and fault resistances. Moreover, the method demonstrated its robustness in the face of uncertainties in line parameters, keeping the same level of performance independent of propagation velocities and line length. The system was modeled and simulated using Alternative Transients Program (ATP)."}, {"label": 1, "content": "This paper proposes a new approach for accelerating the convergence runtime of a modified Holomorphic Embedding Load-Flow Method (HELM). Instead of using a flat start or an aleatory Initial Guess, an adaptive HELM method is proposed that utilizes a previous starting solution provided by an iterative method based on Newton Krylov subspace. To further enhance this approach, the use of the BiCGStab method, preconditioning, incomplete LU factorization, and reordering strategy are suggested. The goal is to optimize this previous starting solution to assist Adaptive HELM methods for getting the final solution in the fastest way. The results obtained are also compared with the traditional NR-method with flat start, an assisted NR-method, and the original HELM."}, {"label": 1, "content": "In this article, a modification of the traditional half-cycle discrete Fourier transform (HCDFT) algorithm is proposed. The main objective is to combine the speed of a half-cycle data window with an efficient decaying DC component elimination method. The proposed algorithm was compared to other half-cycle ones, revealing the smoothest transient response and lowest overshoot."}, {"label": 1, "content": "The shrinking of device dimensions has undoubtedly enabled the very large scale integration of transistors onto electronic chips. However, this progress has also brought to light the emergence of time-zero and time-dependent variation phenomena that can degrade a system's performance and threaten its functional operation. As such, it is crucial to capture and describe these mechanisms, as well as model their impact effectively. To this extent, we follow existing models and propose a complete framework that evaluates failure probability of electronic components. To demonstrate our framework's effectiveness, we present a case study of packet-switched Network on Chip (NoC) routers, analyzing the failure probability of their SRAM buffers."}, {"label": 1, "content": "The article presents a study in the field of development of next generation industrial blockchain-based wireless sensor networks. These networks offer the ability to transfer and store data according to blockchain technology, guaranteeing the immutability of data and use of smart contracts. A model of blockchain capable of working in the networks of the Internet of things, part of which are wireless sensor networks, has been developed."}, {"label": 1, "content": "This paper offers an in-depth analysis of simulation tools currently available for Space Wire networks. We overview the main abilities of the existing software and then propose the computer-aided design (CAD) system for SpaceWire onboard networks design and simulation - SANDS. SANDS system will support the full on-board network design and simulation flow, which begins from the network topology automated generation and finishes with getting the network structure, configuration and parameters setting, simulation results and statistics. The paper also provides use cases for SANDS application."}, {"label": 1, "content": "In this paper, we propose a method to improve the efficiency of Zero-Force (ZF) and Minimum Mean-Squared Error (MMSE) decision making algorithms in Multiple-Input Multiple-Output (MIMO) data transmission systems. A gain provided by the proposed approach is estimated for a few scenarios via simulations."}, {"label": 1, "content": "This paper proposes a simulation model of clusterization of the sensory field created by a wireless sensor network. Effectiveness of clustered wireless sensor network was comparatively evaluated to non-clustered on parameters of residual energy and duration of the network life cycle. The clustering approach utilized an equally probable rotation of head nodes, taking into consideration the levels of residual energy and the distance between the sensory devices and head node."}, {"label": 1, "content": "With CMOS feature size heading towards atomic dimensions, unjustifiable static power, reliability, and economic implications are exacerbating, prompting for research on new materials, devices, and/or computation paradigms. Thus, research into new materials, devices, and computation paradigms is needed to address these issues. In this paper we build upon the fact that GNR behaviour can be controlled according to some desired functionality via top/back gate contacts and propose to combine GNRs with complementary functionalities to construct Boolean gates. To this end, we introduce a generic GNR-based Boolean gate structure, composed of two GNRs, i.e., a pull-up GNR performing the gate Boolean function and a pull-down GNR performing the inverted Boolean function. Subsequently, by properly adjusting GNRs' dimensions and topology, we design 2-input AND, NAND, and XOR graphene-based Boolean gates, as well as 1-input gates, i.e., inverter and buffer. Our SPICE simulations show that these GNR-based gates have 2 orders of magnitude lower power consumption and require a 1 to 2 orders of magnitude smaller active area footprint than traditional 7 nm CMOS-based counterparts. These results clearly indicate that GNR-based gates have great potential as basic building blocks for future beyond CMOS energy effective nanoscale circuits."}, {"label": 1, "content": "To increase noise immunity of the MFCC (mel-frequency cepstral coefficients) widely used for voice signal parametrisation it is suggested to use a psychoacoustic model of frequency masking. Furthermore, by considering the formation mechanism of formant regions in the voice signal spectrum, the spectral samples that correspond to multiple harmonics of the fundamental tone could also be modified. The modified algorithm is investigated on the basis of the single word recognition system adapted for MFCC voice signal parametrisation only. The positive effect of using proposed additional voice signal transformation in the parametrisation algorithm is shown."}, {"label": 1, "content": "The single multiservice network concept, involving the integration of voice, data and multimedia communication has prompted interest in studying the network traffic nature. Studies of traffic traces recorded on a large scale show the presence of a self-similar structure in it, which requires a revision of modeling infocommunication networks results under the assumption of a Poisson data flow."}, {"label": 1, "content": "A hydrodynamic analysis was carried out on a glider designed by the Center for Engineering and Industrial Development (CIDESI). The CIDESI glider configuration is proposed as an in-house design pretending to be a low cost submarine vehicle for monitoring and collecting data in remote locations of the Mexican seabed. The main goal of the present study is to compute the hydrodynamic forces and moments exerted on the glider at different motion conditions. The analysis is carried out through the implementation of computational fluid dynamics simulations. Steady state simulations were conducted using a RANS turbulence model with wall functions in order to capture the boundary layer effects with a coarse grid. The analysis led to the computation of hydrodynamic curves for force and moment components, with the corresponding coefficients calculated. As a part of the integral project for the results obtained for this study will be incorporated into a dynamic model of the glider in order to simulate its motion and eventually design the required control strategies for the different operating scenarios during its mission."}, {"label": 1, "content": "In this paper, we present a new methodology to generate synthetic data for training a deep neural network (DNN) to estimate depth maps directly from stereo images of underwater scenes. The proposed method projects real underwater images onto landscapes of randomized heights in a 3D rendering framework. This produces a synthetic stereo image pair with its corresponding depth map, which is then utilized to train a disparity estimation DNN. Through this process, we learn to match the underwater feature space using supervised learning without the need to capture extensive real underwater depth maps for ground truth. In our results, we demonstrate improved accuracy of reconstruction compared to traditional computer vision feature matching methods and state-of-the-art DNNs trained on synthetic terrestrial data."}, {"label": 1, "content": "We present a novel method for predicting the steady-state dive trajectory of an autonomous underwater vehicle, the Trawl Resistant Self-Mooring AUV (TRSMAUV). Our approach combines RANS CFD simulation data of flows over the hull of the vehicle with force equilibrium equations to estimate the velocity and pitch angle during an unpowered negatively buoyant dive. Our steady-state model demonstrates good agreement with experimental results from TRSMAUV field trials, as well as with time-resolved motion coupled CFD simulations, but at a much lower computational cost. The steady-state model estimates the trajectory of the TRSMAUV with good agreement with experimental results at minimal computational cost compared to the motior coupled simulations."}, {"label": 1, "content": "In the reliable acoustic path environment, a direct arrival and a surface bounce arrival dominate the received signal of a hydrophone placed below the critical depth, which yields strong sparsity in the time domain. As a result, the cross-correlation function between signals received at different depths in the deep ocean exhibits a distinctive pattern. This paper aims to reduce the ambiguity of the cross-correlation function estimation and thus improve the Iocalization performance through utilizing the pattern and reconstructing the cross-correlation function."}, {"label": 1, "content": "This paper proposes a optimal method for designing frequency-invariant beamformers for circular arrays. First, the desired beampattern is expressed as a general complex-weighted form. It is found that all weighting vectors are close-form functions of the desired complex weighting vectors. A multiple optimization problem is then formulated, and the solution yields the optimal desired weighting vector subject to different constraints. Furthermore, the weighting vectors at other frequencies can be computed using previously derived analytical functions. The proposed approach offers flexible options for designing optimal frequency-invariant beamformers for circular sensor arrays. Simulation and experimental results demonstrate the effectiveness of the proposed method."}, {"label": 1, "content": "Guidance law of the autonomous underwater vehicle (AUV) against maneuvering target with a predefined impact angle is proposed in this paper. The desired impact angle is converted to the desired line of sight (LOS) angle for the target, and the guidance law is designed to converge both the LOS angle and its rate to the desired values using nosingularity sliding mode control (NSMC) and feedback linearization control (FLC). To make the guidance system linearizable, FLC is first utilized, followed by the design of the NSMC controller to ensure the system converges within a finite time. To handle unknown disturbances, an extended state observer (ESO) is used for estimation. To solve the switch gain problem associated with NSMC, an RBF neural network is employed for adaptive adjustment. Due to the switch gain of the NSMC is difficult to choose, an RBF neural network is utilized to adaptive adjust it. Overall, this research presents an effective guidance law for AUVs to accurately track and approach maneuvering targets with a predefined impact angle."}, {"label": 1, "content": "With its long operating range and ability to be used in a turbid environment, sonar sensor is mainly used to explore underwater environment. As a result, many algorithms based on the sonar have been developed. However, in the case of a multi-beam sonar, its mechanism causes crosstalk noise around the underwater object, which degrades the accuracy of these algorithms. To tackle this issue, we present a novel approach for eliminating crosstalk noise from a sonar image by detecting the region where the crosstalk noise occurred using a convolutional neural network. The proposed method could accurately and effectively detect and remove crosstalk noise in a given sonar image. Therefore, the accuracy of the sonar-based algorithms such as a 3-D reconstruction of underwater terrain can be improved."}, {"label": 1, "content": "This work presents a system to perform autonomous landing of a small size fixed-wing Unmanned Aerial Vehicle (UAV) on a Fast Patrol Boat (FPB). We propose a ground-based vision system with the camera, image capture and processing equipment installed in the ship, thus reducing the UAV size, weight and power requirements. The system observes the UAV and computes the control commands to send to the UAV via radio. This approach makes it also possible to use standard UAVs equipped with commercial autopilots. The developed system uses the captured image as input and a Particle Filter (PF) structure to estimate the UAV trajectory. It is also used an Unscented Kalman Filter (UKF) for the translational motion filtering and an Unscented Bingham Filter (UBiF) for the rotational motion filtering. This filtering structure is similar to the Unscented Particle Filter (UPF). The obtained tracking error is compatible with automatic landing requirements."}, {"label": 1, "content": "Vortex-Induced Vibration (VIV) of three rigidly coupled cylinders arranged in an equilateral-triangular pattern is numerically investigated. The spacing ratio is set as L/D=1.5. Simulations are carried out for flow incidence angles with a=0\u00b0, \u03b1=30\u00b0, \u03b1=60\u00b0 and the reduced velocity is set from 3 to 16 with a step increment of 1. Two-dimensional Reynolds-Averaged NavierStokes equations were used to simulate turbulent flow, with the SST k-w turbulence model. Two-dimensional Reynolds-Averaged NavierStokes equations in conjunction with the SST k-w turbulence model are used to simulate the turbulent flow. The numerical results revealed that the response of the three cylinders, including vibration amplitudes, lock-in ranges, frequency response and vortex shedding mode, was strongly influenced by the flow incidence angles and the reduced velocity. Numerical results show that the flow incidence angles and the reduced velocity strongly influence the response of the three cylinders including vibration amplitudes, lock-in range, frequency response and vortex shedding mode."}, {"label": 1, "content": "This work proposes a nonlinear inversion technique for extracting the directional ocean wave spectrum from bistatic High-Frequency Surface Wave Radar (HFSWR) Doppler data. The method utilizes empirical expressions that rely solely on Doppler data to retrieve wind speed and direction. Once the initialization parameters have been defined using these empirical expressions, a blind iterative algorithm based on Tikhonov regularization in Hilbert Scales is used to extract the nondirectional spectrum. The extracted spectrum determines the directional factor, which is modeled using a cosine-power model. The proposed method yields good results with synthetic noise-contaminated HFSWR data with a priori regularization parameters."}, {"label": 1, "content": "This paper introduces the modeling and simulation of a spherical autonomous underwater vehicle that was developed for the exploration of underground flooded mines as part of the European Union H2020 innovation action UNEXMIN. The robot was developed under the European Union H2020 innovation action UNEXMIN for the exploration of underground flooded mines, and is a small spherical robot with thrusters and an internal pendulum for pitch control. Initially, a model of the vehicle is presented without the pendulum, followed by an extended formulation that accounts for a multibody dynamic description of the system. Additionally, experimental identification results for the determination of drag parameters are presented. To enable dynamic simulation of the vehicle, a Modelica based simulator is developed and integrated with the Matlab/Simulink environment. The simulator is then validated based on preliminary experimental results."}, {"label": 1, "content": "Approximation techniques for the coefficients that parameterize the hydrodynamic forces acting on a maneuvering underwater vehicle are presented and discussed. While captive tests yield the highest fidelity coefficients, they also incur significant costs that are prohibitive in early vehicle design phases. An alternative method, which only requires the vehicle's geometric profile, is introduced for predicting hydrodynamic coefficients. The method is applied to the SATLP vehicle. Simulations were used to evaluate the uncertainties with the approximate method, and the findings indicate that the technique produces a satisfactory prediction of a vehicle's dynamic behavior."}, {"label": 1, "content": "We have developed a method that employs visual means of detecting and tracking the 3D pose of self-governing underwater vehicles to facilitate strong, multi-robot convoying. To track by detection, we combine the reliable and mistake-free feature of object detection with tracking algorithms' temporal consistency. Our method relies heavily on a multi-output convolutional network that, along with camera intrinsic parameters and prior knowledge of the vehicle's scale, jointly predicts whether the object is present in the image, the 2D bounding box surrounding the object in the image plane, and the 3D orientation of the object. We can then recover the entire six degrees of freedom pose (translation and orientation) of the target underwater robot. To train our network, we use only synthetic images rendered using the Unreal game engine, which is a cost-effective way to produce a large training set without the need for laborious manual annotations. Our evaluation analyzes the impact of orientation offset on 3D detection accuracy, and demonstrates successful generalization of the learned model to real underwater photographs of the target robot."}, {"label": 1, "content": "As a crucial component of an aircraft, the protective shell plays a key role in safeguarding the aircraft and its payloads. However, when abandoned, it can become an irregular object that falls into water. The hydrodynamic forces acting on an irregular body are complex and difficult to define. In this paper, a semi-cylindrical shell is taken as an irregular body example to study the hydrodynamic characteristics and its expression. Through this approach, the hydrodynamic forces acting on the irregular body were obtained, and their features were studied in detail. It can provide a basis for further research of the irregular body."}, {"label": 1, "content": "This article describes an investigation into the viability of using a genetic algorithm (GA) to localize magnetic anomalies with an autonomous underwater vehicle (AUV). The method, originally proposed by Sheinker et al. [1], has been optimized for use on a moving platform. [1], is optimized for the case of a moving platform. Extensive magnetic field modeling and algorithm simulation have been conducted and suggest that the approach is promising. Field testing of the method was conducted with the use of a self-compensating magnetometer (SCM). Extensive out-of-water field testing was carried out to validate the ability to measure a target signal in a uniform NED frame as well as to validate the effectiveness of the GA. Furthermore, the SCM has been integrated onto a REMUS 100 AUV, and preliminary in-water testing of the system has been undertaken. Overall, this investigation shows that magnetic anomaly localization with an AUV using a GA is a promising approach, offering potential applications in underwater exploration and mapping."}, {"label": 1, "content": "The interest in hydrokinetic conversion systems has significantly grown over the last decade with a special focus on cross-flow systems, generally known as Vertical Axis Water Turbines (VAWTs). However, analyzing the regions of interest for tidal energy extraction and outlining the optimal rotor geometry is currently a computationally expensive process using conventional 3D Computational Fluid Dynamics (CFD) methods. In this work, a VAWT load prediction routine developed at University of Pisa based upon the Blade Element-Momentum (BEM) theory is presented and validated against high-resolution 2D CFD simulations. Our model is able to work in two configurations, i.e. As a practical application, our routine is employed for a site assessment analysis of the Cape Cod area, which quickly highlights oceanic regions with high hydrokinetic potential. As a practical application, our routine is employed for a site assessment analysis of the Cape Cod area to quickly highlight oceanic regions with high hydrokinetic potential, where further higher-order and more computationally expensive CFD analyses can be performed. Ocean data are obtained from data-assimilative ocean simulations predicted by the 4D regional ocean modeling system of the Multidisciplinary Simulation, Estimation, and Assimilation Systems (MSEAS) group of the Massachusetts Institute of Technology."}, {"label": 1, "content": "Software systems that can adapt to new requirements not only survive longer but also save overhead otherwise incurred from manually re-writing the software. Most software systems do not have this capability and older legacy software systems are frequently replaced as they become obsolete. In this paper, we propose a two-part approach to imbuing adaptive features into legacy systems. The first part utilizes program analysis, wherein we automatically transform the source code to create adaptive parameters. Next, we optimize these parameters across a wide range of operating contexts, expanding the program's capabilities beyond its original design. We demonstrate the effectiveness of our approach using a non-adaptive case study, where we transform and optimize the code without relying on specific semantics of the original program. With this generality in hand, this approach may enable the expansion and optimization of a diverse variety of software systems."}, {"label": 1, "content": "In this paper, we revisit the problem of classifying ships (maritime vessels) detected from overhead imagery. Despite the tremendous research efforts invested in the task over the past decade, it remains a stubbornly unsolved problem. One of the major issues with the detection and classification of ships and other objects in the maritime domain is the lack of substantial ground truth data needed to train state-of-the-art machine learning algorithms. We tackled this pressing issue by creating a large synthetic dataset comprising 200,000 images using the state-of-the-art Unity gaming engine combined with high-quality 3D models of ships. We demonstrate that with the use of synthetic data, classification performance increases dramatically, particularly when there are very few annotated images used in training."}, {"label": 1, "content": "Nonhydrostatic, multiscale ocean dynamics is a crucial aspect of understanding ocean behavior. However, traditional computational techniques can be expensive and inefficient in resolving these complex dynamics. We apply the hybridizable discontinuous Galerkin (HDG) finite element methodology to perform computationally efficient, high-order, nonhydrostatic ocean modeling by solving the Navier-Stokes equations with the Boussinesq approximation. In this study, we propose a distributed implementation of our HDG projection method algorithm and perform numerical experiments to verify our methodology through the method of manufactured solutions. We provide numerical experiments to verify our methodology using the method of manufactured solutions and provide preliminary benchmarking for our distributed implementation that highlight the advantages of the HDG methodology in the context of distributed computing. Lastly, we present simulations that capture nonhydrostatic internal waves resulting from tidal interactions with ocean topography. First, we consider the case of tidally-driven oscillatory flow over an abrupt, shallow seamount, and next, the case of strongly-stratified, oscillatory flow over a tall seamount. Our analysis and comparison of our simulations with other literature results suggest the effectiveness of HDG methodology in efficiently resolving complex multiscale ocean dynamics."}, {"label": 1, "content": "In this paper, we propose an efficient near-field source DOA estimation method based on the Fractional Fourier transform. In this method, the received data of the sensor array for the near-field source is modeled as a linear-frequency modulated (LFM) signal in spatial domain after the Fresnel approximation. We then utilize the Fractional Fourier transform to estimate the modulated frequency rate, which represents the effect of the quadratic phase of the near-field source. After compensating this quadratic phase term, the received signal can subsequently be converted into a far-field expression. Finally, classical DOA estimation methods for far-field can be applied to find the direction of the source. For multiple sources, we propose a CLEAN technique to extract the sources iteratively. Simulations are finally carried out to demonstrate its validity."}, {"label": 1, "content": "Assume a narrowband signal propagate through the ocean waveguide. This results in a multi-rank signal covariance with the signal energy dispersed within a small angular bandwidth, whose spatial signatures are known but orientation is unknown and random. Mathematically, this kind of signals received on the array are named as multi-rank signals, whose spatial signatures lie in a known subspace, but the orientation in that space is unknown and random. Conventional direction-of-arrival (DOA) estimation methods, such as delay and sum (DAS) beamforming and minimum variance distortionless response (MVDR) beamforming, show poor ability to resolve this kind of signals. The MR-SpSF method is an extension of the sparse spectrum fitting (SpSF) method. Performance of MR-SpSF is compared with DAS, MVDR, SpSF and eigenvalue beamforming (EB) by simulation experiments. Simulation results suggest that both EB and MR-SpSF can provide high resolution in resolving multi-rank signals, but MR-SpSF outperforms EB with more accurate signal power estimation without compensation and more reliable DOA estimation results in snapshots limited and signal subspace mismatch scenarios."}, {"label": 1, "content": "In this paper, we address the signal processing of a cylindrical object, specifically a pipe (in air), which is exposed to random excitations caused by a hammer-like surface from a cam-driven motor. The overall objective of this effort is to develop real-time hardware capable of monitoring mechanical systems (structures) for potential anomalies. The development process consists of seven stages: (1) theoretical modeling; (2) complex simulations; (3) analysis; (4) controlled experiments; (5) signal processing; (6) modal frequency estimation; and (7) modal tracking. The primary emphasis is on the signal processing, modal extraction and tracking for single and multi-channel data."}, {"label": 1, "content": "In this work we compare the performance of seven popular feature detection algorithms on a synthetic sonar image dataset. The dataset comprises a single mine-like object (MLO) overlaid on three distinct backgrounds, namely grass, sand ripple, and sand. We explore the performance of Harris, Shi-Tomasi, SIFT, SURF, STAR, FAST, and ORB on each of these backgrounds, and all the backgrounds at once by training an SVM classifier. The evaluation of performance is carried out using ROC curves, by measuring the number of accurately detected features that belong to objects (True Positives) against the number of inaccurately identified features that belong to the background noise (False Positives)."}, {"label": 1, "content": "In this article we consider the navigation problem for an autonomous underwater vehicle (AUV) for reaching a desired way-point. Due to the highly coupled dynamics of the vehicles and unknown parameters of dynamic model, traditional control architectures face significant challenges in solving the navigation problem. However, advancements in reinforcement learning have shown promising results for robotics applications, especially in the case of underwater autonomous vehicles. In this proposal, we solve the navigation problem using a deep reinforcement learning approach \u2013 specifically, the deep deterministic policy gradient. In contrast to model-based approaches, a model-free approach is used, where raw sensor information is utilized as inputs to a policy network, and the outputs of this network are directly mapped to the thrusters. In this proposal a model free approach is used, where the raw sensor information is used as inputs to a policy network, and the outputs of this network are directly mapped to the thrusters. Simulated results demonstrate that the proposed approach is capable of successfully solving AUV navigation problems. The obtained simulated results show its capacity for successfully solving AUV navigation problems."}, {"label": 1, "content": "Autonomous Underwater Vehicles (AUVs) are becoming increasingly important for their many underwater applications, thanks to their unique characteristics and functionalities. Making use of a imaging sonar, it is possible to acquire the AUV's distance to existing obstacles. Then, through an implementation of a feature detection algorithm and an estimator, it is possible to interpolate the vehicle's relative position. To further enhance the localization accuracy of AUVs, this paper proposes a new method for structured environments utilizing a mechanical scanning sonar feeding an extended Kalman filter. Some tests were then run in two different water tanks in order to verify the effectiveness of the solutions. In the first phase, all readings were taken with the AUV steady and immobile. In the second phase, the AUV was put into motion to test the method's performance under variable conditions. The second phase was executed with the vehicle in motion. The results are presented and compared against ground-truth measurements."}, {"label": 1, "content": "VM Placement algorithms play a crucial role in determining data center utilisation and power consumption. The problem of VM Placement is to obtain an optimal packing of VMs on hosts such that the number of hosts required is minimum. The problem being NP-Hard, it becomes practically infeasible to get an optimal placement within the time constraints for making scheduling decisions. VM Placement can be modeled as a Multi-dimensional Vector Packing Problem(MDVPP). VPSolver, using arc-flow formulation with graph compression, gives an optimal solution for Bin-Packing and related problems. This paper proposes heuristics for large instances of MDVPP, based on the Divide-and-Conquer paradigm, using VPSolver. An extensive evaluation of 3260 instances was conducted, comparing the proposed heuristics with existing popular heuristics in 2D and 3D vector spaces, which also included VM sizes obtained from utilization traces of a private cloud. The study's observations revealed that for most large instances, the proposed heuristics offer better solutions compared to existing methods, although sometimes at the cost of higher computation time required. The proposed heuristic gives solutions where the number of bins required is reduced up to 7.34% and 8.15% for 2D and 3D vector spaces respectively."}, {"label": 1, "content": "Although there are many provisioning tools available for the deployment and management of composite cloud services, manual efforts are often tedious and error-prone, requiring users to specify low-level scripting solutions for Infrastructure-as-Code (IAC). Provisioning services across heterogeneous cloud platforms using IAC demands domain knowledge and a steep learning curve. To overcome these challenges, a technology- and platform-agnostic self-service framework called CloudCAMP has been presented. It incorporates domain-specific modeling so that the specifications and dependencies imposed by the cloud platform and application architecture can be specified at an intuitive, higher level of abstraction without the need for domain expertise. CloudCAMP transforms the partial specifications into deployable IAC by leveraging an extensible and reusable knowledge base, using the Transformational-Generative paradigm. The auto-generated IAC can be handled by existing tools to provision the services components automatically. The approach has been validated quantitatively, demonstrating a comparative study of savings in manual and scripting efforts when using CloudCAMP."}, {"label": 1, "content": "Edge computing will likely be of significant import to the Smart City context, providing a means to deploy and manage large distributed applications [5] [3]. While there has been much attention given to this emerging field, many aspects of edge computing are still not well understood. This paper focuses on the specifics of extending data centre networking technologies to edge devices. To that end, we present a solution we have developed that enables Open Virtual Networking (OVN) to be extended to the edge, allowing for integration with a docker engine located at the edge of the network. The mechanisms consist of a process by which a docker engine can request to join the network, an approval process which leverages Openstack authentication and access control mechanisms, a mechanism by which the remote system can connect to the OVN and finally containers can be initialized on the remote docker engine and interact with entities operating in the Enterprise Data Centre."}, {"label": 1, "content": "Every year, traffic collisions have increased rapidly in proportion to the increase in the number of vehicles, especially at intersections. The primary reason is human error in recognition and decision-making. The advent of Autonomous Vehicles (AVs) and Autonomous Intersection Management (AIM) systems pose significant challenges. AVs can take a great deal of different actions when approaching an intersection. Various research centers are developing algorithms to tackle intersection management, aiming to prevent collisions and reduce traffic congestion. In this context, security is the main concern, due to the high exposure to data and information between Vehicle-to-Vehicle (V2V) and Vehicle-to-Intersection (V2I) communications. Blockchain and smart contracts are disruptive technologies that have emerged in recent years, offering a possible solution to security issues. Smart contracts are protocols that facilitate, verify and enforce the agreement between the consenting parties participating in the Blockchain network. In this article, we propose a Multi-Agent AIM (MA-AIM) system based on V2I/I2V communication, leveraging Blockchain facilities to securely manage vehicles crossing through intersections. At each intersection, a central Intersection Manager Agent (IMA) is implemented, while each vehicle is controlled by a Driver Agent (DA)."}, {"label": 1, "content": "Cloud computing is relatively a new technique to host and use the services and applications from the internet. Although it offers a multitude of advantages like scalability, low operating cost, accessibility and maintainability, etc., they are often not utilized to the fullest due to the lack of timeliness property associated with the cloud. Cloud services are mainly designed to maximize throughput and utilization of resources and hence incorporating predictable execution time properties in to the cloud is arduous. Nevertheless, cloud computing's elasticity, multi-tenancy, ability to handle hardware failures, and virtualization and abstraction layer support make it an extremely attractive platform for hosting real-time applications and services. However, to exploit the potential of cloud computing for real-time safety-critical applications, it is essential to ensure the predictable real-time behavior of cloud services. In this paper, we conduct a systematic mapping study on real-time cloud services to identify current research directions and research gaps. Our study focuses on analyzing the current architectures and software techniques that are available at present to incorporate real-time property of the cloud services. Furthermore, we investigate current challenges associated with achieving predictable real-time behavior in cloud services."}, {"label": 1, "content": "Traffic congestion and accidents cause cities to be the principal source of pollutant emissions. However, the TIMON project initiative seeks to provide real-time information and cloud-based services to drivers, vulnerable road users, and businesses through an open web-based platform and mobile application. TIMON establishes a cooperative ecosystem to connect people, vehicles, infrastructure and business and contributes to intelligent transport, IoT and Cloud computing. The first part of this paper provides an overview of TIMON and how it increases safety, reduces congestion and emissions. The TIMON ecosystem represents the perfect use case of distributed technologies, as it collects data from IoT sensors, open and closed data sources and user engagement data, processes it and provides useful information not only for road users, but also for scientists and technicians who need real systems to study the data, infrastructure and IT safety management. In the second part, the Cloud deployment of the TIMON system is described in detail and a new, more distributed design is proposed to exploit the potential of current emerging technologies of Fog and Edge computing."}, {"label": 1, "content": "We present the complexity of marine-based IoT solutions, i.e. devices with sensors deployed in boats and vessels managed by the Fog and Cloud. The paper describes issues and challenges that arise in this specific environment and presents approaches taken to tackle them. The approach is demonstrated with the implementation of a solution for an open-source fog to cloud platform, mF2C, which results in a working prototype showing promising advantages of exploiting the platform over independently developing a proprietary solution."}, {"label": 1, "content": "It is sometimes useful to be able to prototype Internet of Things protocols and technologies. Our aim for this project was to have edge (sensor) devices being measured by Arduinos and relaying data through a \"fog computing\" layer implemented with Raspberry Pis to aggregating services run in the cloud. The implementation was done as a part of a Horizon2020-funded project called mF2C, and demonstrated the use of MQTT relaying JSON messages over BlueTooth and WiFi and over more traditional Internet connections to the cloud services. One of the most significant benefits of this project was the ability to quickly prototype the technologies using affordable devices, and the attention it generated both within the organization and publicly through our outreach activities."}, {"label": 1, "content": "Live TV production can be a costly process due to the need for equipment and human resources to be deployed in several different places. This increases production costs. The traditional method through outside broadcasting vans is expensive. Migrating this type of application onto clouds is a promising method to reduce the cost. However, the Quality of Experience (QoE) can hardly be assured because of the cloud performance uncertainty. The feasibility of using this framework is demonstrated through live events broadcasting, providing a significant attempt to fill the DevOps gap when migrating applications from legacy systems to the cloud. One example of the use case for this framework is a live streaming application. It makes a significant attempt to fill the DevOps gap when migrating applications from legacy systems onto clouds. Overall, the use of cloud-based technologies for live TV production offers a promising solution for reducing costs and improving the broadcasting experience for viewers."}, {"label": 1, "content": "Numerous studies have attempted to establish a relationship between the geometry of carbon-nanomaterials and the intensity profile of Raman spectra. Researchers have observed changes in the intensity profiles based on properties such as the length of carbon nanotubes and the thickness of graphene. the length of carbon nanotubes and the thickness of graphene. Specifically, we focus on identifying the number of graphene layers through Raman peak shift recognition using principal component analysis (PCA). In this work, we focus on the Raman peak shift recognition using principal component analysis (PCA) to identify the number of graphene layers and this framework can accelerate processes in both measurement and geometric property analysis."}, {"label": 1, "content": "In recent years, the Information centric network (ICN) based Mobile Edge Computing (MEC) network has gained significant attention. The distributed network architecture brings new security problems, especially the identity security problem. Because of the cloud platform deployed on the edge of the MEC network, multiple channel attributes can be easily obtained and processed. Thus this paper proposes a multiple channel attributes based spoofing detection mechanism. To further reduce the complexity, we also propose an improved clustering algorithm. The simulation results indicate that the proposed spoofing detection method can provide near-optimal performance with extremely low complexity."}, {"label": 1, "content": "This work is targeting a solid security improvement in the wireless communication between existing and possibly future implantable medical devices (IMDs) and Programmer Monitor Device (PMD). To achieve this, a public medical server is introduced, acting as a trustworthy authority. Additionally, a Pacemaker Proxy Device (PPD) will act as a mediator between PMD and IMD, ensuring that security, liability, and responsibility issues are fully addressed. The system will embed low-complexity and resilient digital physical identities, based on a new concept, to prevent physical substitution/cloning attacks. A biometric identity extracted from the patient's ECG (electrocardiogram) is supporting the security system by adding rather hard-to-clone patient personal health profile. This identity will serve as a hard-to-clone patient personal health profile, supporting the security system. The initial results of the proposed approach showed practical accuracy in extracting the biometric identity approaching 95%. Overall, the proposed system ensures robust, resilient, and high-level protection for the future smart medical environment."}, {"label": 1, "content": "In the area of IoT (Internet of Things), more and more intelligent devices are being connected to the Internet. These intelligent devices have been producing a huge amout of useful data over time, however there is still a lack of a platform which can efficiently transfer and utilize the value of the massive IoT data. Blockchain is able to transfer value with a relative low cost, which makes it possible for the data from smart devices to create economic value. The aim of this paper is to develop a high-performance blockchain platform that incorporates distributed network architecture, intelligent device node mapping, and PBFT-DPOC consensus algorithms to enable decentralized autonomy of intelligent devices."}, {"label": 1, "content": "Named Data Networking (NDN) is an emerging communication paradigm that aims to resolve the problem of traffic explosion caused by repeated and duplicated delivery of large multimedia content. To make NDN being useful more widely, however, it should support various types of traffic and their Quality of Service (QoS) requirements. In this paper, a queue scheduling algorithm called Name Weighted Round Robin (NWRR) is proposed to work on top of the diffserv model in NDN. The proposed algorithm performs combined scheduling of Interest packets and Data packets and adjusts the weight of each queue according to the average packet length of different queues. Results in ndnSIM simulator demonstrate that NWRR can provide different levels of service for different priority applications as well as can cope with the issue of lack of fairness caused by different average packet length in each queue."}, {"label": 1, "content": "Scientists and researchers often need to run midscale to large-scale scientific computations on powerful computing machines or platforms. However, even with the availability of strong computing platforms, many computations may still take a significant amount of time to execute. With the rise of cloud computing technology, researchers can now significantly reduce computational time by outsourcing computation functions to cloud systems. We show in this paper how AWS (Amazon Web Services) cloud computing platform can be automated in executing large-scale computationally expensive scientific experiments. Specifically, we show how quantum chemistry simulations can be executed in parallel and in a cluster-based fashion using the publicly available and popular Amazon cloud platform. With Amazon cloud, we were able to reduce the computation time by almost five orders of magnitude. In addition, the paper offers many important useful guidelines, scripts, and commands for scientists and researchers on how to automate and execute parallel and cluster-based scientific jobs on any cloud platform."}, {"label": 1, "content": "Vehicular environment is exposed to many security threats such as illegal copying of software IP, counterfeiting of electronic and mechatronic components, and illegal tampering of digital data inside electronic control units (ECUs). This is largely due to the ease with which ECUs can be cloned. Physical Unclonable Functions (PUFs) were proposed to be used in many applications such as secure memory-less key storage and devices identification. However, their usage for automotive security is still very limited due to their inconsistency and high implementation cost and complexity. This paper presents a novel VANET security architecture embedding a new consistent digital clone-resistant technology called Secret Unknown Cipher SUC as a physical security anchor. The paper addresses two sample use cases in software update and V2X link protocols fulfilling VANET security architecture and requirements. Because the SUC concept is based on pure consistent digital structures, it is well-suited for future vehicular IoT environments, as it is both low-cost and robust, and can be used throughout the lifetime of vehicular entities. Since SUC concept is using pure consistent digital structures, it fits perfectly as a low-cost and robust technology in future vehicular IoT environment over the long lifetime of vehicular entities."}, {"label": 1, "content": "Named Data Networking (NDN) architecture inherits the hourglass shape of IP, whereas the narrow waist part is changed from IP addresses to content names. In NDN architecture, consumers can retrieve data from multiple content chunks and through multiple paths by sending Interest packets carrying a given name rather than data objects' location. However, these features make end-to-end congestion control mechanisms ineffective. In this paper, we propose a novel window based congestion control mechanism (WinCM) to support high-throughput applications in NDN. WinCM contains three modules: Active Queue Management (AQM) module, Consumer Window Adjustment module and Forwarding Strategy module. AQM module detects congestion by monitoring packet-sojourn time and notices consumers and downstream nodes along delivery path. AQM keeps marking each Data packets that are dequeued continuously for the duration of a queue delay, so that consumers can decrease their windows every time they receive a congestion tag. Simultaneously, each downstream node's Forwarding Strategy module can accurately and instantly adjust the per-interface Unsatisfied Interest Window, which is one of parameters recorded in Measurements Table to decide how to forward Interest packets and avoid congestion. Simulations based on ndnSIM show that WinCM can exploit available bandwidth faster and maximize bandwidth utilization while maintaining lower queue delay."}, {"label": 1, "content": "Quantile summarization is a useful tool in data streams management and mining that can efficiently capture the distribution of the data. A quantile of a sequence of points is the point with a given rank in the sequence. Given a sequence of uncertain points S on the real line, each represented by one-dimensional probability density function (pdf), we study the problem of incrementally maintaining quantile summaries on S such that for any query with a given rank, the summaries can provide a point as the quantile within a given error. We define quantile on uncertain data with discrete or continuous pdf in terms of two error metrics under possible worlds semantics. A method to calculate the error value for a quantile query on uncertain data is detailed, along with high-level features of the summaries that can answer approximate quantile queries under the two error metrics. We propose an online, space efficient algorithm to compute such summary data on uncertain data streams. The experimental results show that our algorithm substantially outperforms other techniques, such as Monte Carlo and averaging methods, in terms of query error and space for storing the summary data."}, {"label": 1, "content": "Cybercriminals are taking advantage of the digital age and social media to carry out illegal activities such as cyberbullying, hacking, online fraudulence, blackmailing and drug smuggling. To combat the increasing number of criminal activities, structure and content analysis of criminal communities can provide insight and facilitate cybercrime forensics. In this study, a framework is proposed to analyze chat logs through the use of data mining and natural language processing techniques. The framework extracts the social network from chat logs and summarizes conversations into topics. The crime investigator can use information visualizer to see the crime-related results. To validate the framework, experiments were conducted in collaboration with the cybercrime unit of a Canadian law enforcement agency. The experimental outcomes on real-life data and feedback from the law enforcement officers suggest that the proposed chat log mining framework meets the need for law enforcement agencies and is very effective for crime investigation."}, {"label": 1, "content": "Frequency channel allocation is a vital technique used to improve the performance of cellular networks. In this paper, we address the channel allocation problem for a 5G multi-cell system. We consider a heterogeneous network in which cellular users, micro-cell users, and device-to-device (D2D) communications coexist within the radio footprint of the macro cell. We maximize the aggregate transmission rate, exploiting channel diversity and managing both the inter-cell interference, typical of cellular networks and the intra-cell interference generated by the nonorthogonal transmissions of the small-cell and D2D users. We model the allocation problem as a potential game, and we propose a novel decentralized solution. The Nash equilibria of the potential game correspond to local optima of the objective function, which helps us in developing a better response dynamic based on a message passing approach, ensuring that our scheme's convergence is enforced. The simulation results assess the validity of the proposed scheme in terms of convergence time and achievable rate under different settings."}, {"label": 1, "content": "To evaluate a machine's ability to comprehend high-level scientific articles, the authors have developed an intricate scientific inquiry task titled PaperQA. The PaperQA task is based on more than 80 000 \u201cfill-in-the-blank\u201d type questions on articles from reputed scientific journals such as Nature and Science. A comprehensive linguistic analysis and appraisal is conducted to compare PaperQA with conventional question and answering tasks on general literature (e.g., books, news articles, and Wikipedia texts). The results indicate that the PaperQA task is the most difficult QA task for both humans (lay people) and machines (deep-learning models). Additionally, it is observed that humans typically outshine machines in general QA tasks, but advanced deep-learning models surpass humans by an average of 3%-13% in the PaperQA task. The PaperQA dataset presented in this study is available for public use at http://dmis.korea.ac.kr/downloads?id=PaperQA."}, {"label": 1, "content": "One of the most widely used frameworks for image-based sequence recognition is the convolutional recurrent neural network, which uses a convolutional neural network (CNN) for feature extraction and a recurrent neural network (RNN) for sequence modeling. The CRNN uses a convolutional neural network (CNN) for feature extraction and a recurrent neural network (RNN) for sequence modeling. Some models replace the RNN with an attention mechanism for sequence modeling but still, require expensive iterative computations. Some models replace the RNN with an attention mechanism, but require expensive iterative computations. In this paper, the authors propose a fully convolutional sequence recognition network (FCSRN) that eliminates the need for recurrent operations and can thus be fully parallelized. The paper focuses on the problem of water meter number reading (WNR), a task that has rarely been investigated. The authors demonstrate that the FCSRN is able to capture contextual information, requires fewer parameters and less computation, and outperforms both RNN-based and attention-based models. The experimental results demonstrate that the FCSRN has the ability to capture contextual information and eliminate the need for recurrent layers, and simultaneously requires fewer parameters and less computation. The FCSRN with AugLoss outperforms RNN-based and attention-based models. In addition, AugLoss can effectively improve the performance for RNN-based and attention-based models. Moreover, we constructed and released a dataset that contains 6000 water meter images with labels, which is available at https://github.com/HCIILAB/Water-Meter-Number-DataSet."}, {"label": 1, "content": "With the rapid popularization of mobile intelligent terminals, mobile video and cloud services applications are widely used in people's lives. However, the resource-constrained characteristic of the terminals and the enormous amount of video information make the efficient terminal-to-cloud data upload a challenge. To address this challenge, this paper presents an efficient compressed sensing-based high-efficiency video upload system that simplifies the terminal-to-cloud upload network. The system contains two main new components. The first component involves developing an encoder sampling scheme using skip block-based residual compressed sensing technology for efficiently removing inter-frame redundant information. For the time-varying channel state, the encoder can adaptively allocate the sampling rate for different video frames by the proposed adaptive sampling scheme. The second component is a local secondary reconstruction-based multi-reference frame cross-recovery algorithm developed at the decoder. It further improves the reconstruction quality and reduces the quality fluctuation of the recovered video frames to improve the user experience. Compared with the state-of-the-art reference systems reported in the literature, the proposed system achieves the high-efficiency and high-quality terminal-to-cloud transmission."}, {"label": 1, "content": "To promote efficient and flexible learning, a conceptual learning model can be developed using the differences between the stable features of various concepts, thereby allowing scalable and continuous learning concepts to be obtained from a small number of labeled samples. By representing each concept's stable feature as a vector, the conceptual learning model can identify the representative feature of each concept, thereby augmenting the model's interpretability. In addition to adjusting the mapping relationship between concept instances and concept stability features, constraints on the differences between stable features of different concepts are also introduced into the model. This constraint can improve the sensitivity of the instance features to each concept's stable features, and hence, the proposed model has the potential to learn from a small number of samples. In this paper, sub-networks of the same structure were applied to learn each concept, and unified learning methods were used to achieve scalability and the ability to easily manage new concepts. By training the proposed model using the MNIST dataset, the model demonstrates that an accuracy rate of 96% can be achieved with the use of 100 labeled training samples for each concept. Moreover, the proposed model not only presents an augmented learning ability for small samples but also reduces the training time significantly, with each concept requiring only 18 seconds on average. Thus, the small-sample and continuous learning capabilities of the proposed model enable the construction of a conceptual space that promotes improved knowledge representation and complex scene understanding."}, {"label": 1, "content": "The accumulating evidences regarding circular RNAs (circRNAs) indicate that they play crucial roles in a wide range of biological processes and participate in tumorigenesis and progression. The number of newly discovered circRNAs have increased dramatically in recent years, but the functions of vast majority of circRNAs remain unknown, and little effort has been devoted to discover disease-associated circRNAs on a large scale until now. With the advancement of high-throughput technology, the increasing availability of omics data has provided an unprecedented opportunity for prioritizing candidate circRNAs for diseases by computational models, which will contribute to exploring the pathogenesis of complex diseases at the circRNA level and provide promising applications in disease diagnosis and treatment. Here we propose the assumption that circRNAs with similar functions are normally associated with similar diseases and vice versa, and develop an integrated computational framework called MRLDC to identify disease-associated circRNAs. To our knowledge, little efforts have been developed for uncovering circRNAdisease associations on a large scale. We then utilize a weighted low-rank approximation optimization algorithm with dual-manifold regularizations to predict disease-associated circRNAs. Our experimental results have shown that MRLDC can effectively identify disease circRNA candidates with high accuracy. Additionally, case studies have demonstrated the ability of our method to discover potential circRNA-disease associations. In addition, case studies further demonstrate the ability of our method in discovering potential circRNA-disease associations."}, {"label": 1, "content": "In this letter, we discuss the concept of age of information (AoI) in a sensor network that utilizes wireless power transfer (WPT). Specifically, we study a simple network topology, where a sensor node harvests energy from radio frequency signals (transmitted by a dedicated energy source) to transmit real-time status updates. The sensor node generates an update once its capacitor or battery is fully charged and transmits using all the available energy without any further energy management. The average AoI performance of the considered greedy policy is derived in closed form and is a function of the capacitor's size. Moreover, we determine that the optimal value of the capacitor, which maximizes the freshness of the information, can be obtained through a simple one-dimensional search optimization problem. These theoretical findings can provide useful performance bounds for practical WPT networks."}, {"label": 1, "content": "We present convergence guarantees for the estimation of symmetric mixture coefficients for two linear regressions using the Expectation Maximization (EM) algorithm. Our analysis demonstrates that the empirical EM iterates converge to the target parameter vector at the parametric rate if initialized in an unbounded cone. Specifically, we show that the EM algorithm's sample-splitting version converges to the true coefficient vector with high probability if the initial guess has a sufficiently large cosine angle with the target parameter vector. Interestingly, our analysis borrows from tools used in the problem of estimating the centers of a symmetric mixture of two Gaussians by EM. Furthermore, we establish the necessity of the conic condition by showing that the population of EM operator for mixtures of two regressions is anti-contractive from the target parameter vector if the cosine angle between the input vector and the target parameter vector is too small. Finally, we give empirical evidence supporting this theoretical observation, which suggests that the sample-based EM algorithm may not converge to the target vector when initial guesses are drawn accordingly. However, our simulation study indicates that the EM algorithm performs well even when there is model misspecification in the covariate and error distributions."}, {"label": 1, "content": "Securing Internet of Things (IoT) devices and their applications from privacy leaks is a significant challenge, given their low computational and storage capabilities and the presence of sensitive data. Traditional security methods are often too complicated or unfeasible due to the resource constraints, long lifespan and intermittent connections of IoT devices. To address this problem, we propose a novel approach called address shuffling algorithm with HMAC (AShA) that performs a network-wide (Internet protocol and medium access control) address shuffling procedure. This technique lowers the attack surface of malicious users by constantly modifying device footprint. In this paper, we propose a novel method to perform a network-wide (Internet protocol and medium access control) address shuffling procedure, called address shuffling algorithm with HMAC (AShA), which is simple to implement, and whose network overhead is minimal. To demonstrate its effectiveness, we analyze our approach via theoretical analysis and simulations. Our analysis shows how AShA parameters can be adapted to various network sizes while our simulations results show how AShA can be used to successfully perform a global collision-free address renewal on networks of more than 2000 nodes using 16-bit addresses."}, {"label": 1, "content": "From a single RGB image of an unknown face, taken under unknown conditions, we estimate a physically plausible lighting model. Firstly, we estimate the 3D geometry and texture of the face by fitting a 3D Morphable Model to the 2D input. With this estimated 3D model and a Virtual Light Stage (VLS), we generate a gallery of images of the face with all the same conditions, but different lighting. We consider non-lambertian reflectance and non-convex geometry to account for complex lighting effects. Our hierarchical Bayesian approach automatically suppresses inconsistencies between the model and the input. It estimates the RGB values for the light sources of a VLS to reconstruct the input face with the estimated 3D face model. We discuss the relevance of the hierarchical approach to this minimally constrained inverse rendering problem and show how the hyperparameters can be controlled to improve the results of the algorithm for complex effects, such as cast shadows. Our algorithm is a contribution to single image face modeling and analysis, provides information about the imaging condition and facilitates realistic reconstruction of the input image, relighting, lighting transfer and lighting design."}, {"label": 1, "content": "The explosive growth of digital images in video surveillance and social media has led to the significant need for efficient search of persons of interest in law enforcement and forensic applications. Although significant progress has been made in primary biometric traits such as face and fingerprint recognition, these traits alone may not achieve the desired level of recognition accuracy in forensic scenarios. Tattoos, as one of the important soft biometric traits, have been found to be valuable for assisting in person identification. However, tattoo search in a large collection of unconstrained images remains a difficult problem, and existing tattoo search methods mainly focus on matching cropped tattoos, which is different from real application scenarios. To address this gap, we propose an efficient tattoo search approach that simultaneously learns tattoo detection and compact representation in a single convolutional neural network (CNN) via multi-task learning. By sharing features in the backbone network between tattoo detection and compact representation learning, individual latent layers of each sub-network optimize the shared features towards the respective learning tasks. We also solve the small batch size issue within the joint network using random image stitching and preceding feature buffering. We evaluate the proposed tattoo search system using multiple public-domain tattoo benchmarks and a gallery set of over 300,000 distracter tattoo images compiled from these datasets and from the internet. In addition, we also introduce a tattoo sketch dataset containing 300 tattoos for sketch-based tattoo search. Experimental results show that the proposed approach has superior performance in tattoo detection and tattoo search at scale compared to several state-of-the-art tattoo retrieval algorithms."}, {"label": 1, "content": "Phase imbalance in the U.K. and European low-voltage (415 V, LV) distribution networks causes additional energy losses. The estimation of imbalance-induced energy losses in LV networks with only the yearly average currents of the three phases remains a significant obstacle. To address the data insufficiency issue, this paper presents a new customized statistical approach known as the clustering, classification, and range estimation (CCRE) approach. The proposed approach matches the data-scarce network with only the yearly average phase currents with a cluster of networks with time series of phase current data, which have more data-rich networks. It finds a match between the network with only the yearly average phase currents (the data-scarce network) and a cluster of networks with time series of phase current data (data-rich networks). The range is narrowed down by implementing Chebyshev's inequality, which represents the confidence interval of the imbalance-induced energy loss for the data-scarce network. Chebyshev's inequality is applied to narrow down this range, which represents the confidence interval of the imbalance-induced energy loss for the data-scarce network. The CCRE approach shows promising results and could prove to be effective in estimating the imbalance-induced energy losses in LV networks with limited data."}, {"label": 1, "content": "This paper addresses the problem of detecting anomalous activity in traffic networks where the network is not directly observed. Given knowledge of what the node-to-node traffic in a network should be, any activity that differs significantly from this baseline would be considered anomalous. The probabilistic nature of the model enables the authors to perform statistical goodness-of-fit tests to identify significant deviations from a baseline network. The hierarchical structure of the model allows for reliable statistical tests, even when the empirical models estimated by the EM algorithm are not well specified. The authors demonstrate the effectiveness of the model by applying it to both simulated and real datasets. We apply our model to both simulated and real datasets to demonstrate its superior performance over existing alternatives."}, {"label": 1, "content": "This article presents the Multiscale Quaternion Weber Local Descriptor Histogram (MQWLDH), a novel feature extraction method for hyperspectral images (HSIs). The proposed method utilizes spectral information to model spatial information by transforming it into an orthogonal space using principal component analysis. The first three principal components with the highest variance are extracted and used to construct the MQWLDH for spatial feature extraction. By using the algebraic structure of quaternions, the method unifies the processing of the first three principal components, reducing both computational cost and dimensionality of the extracted spatial feature vector. The constructed quaternion Weber local descriptor is effective in characterizing each pixel neighborhood's variations and detecting the edges of HSIs. To capture more intrinsic spatial information contained in homogeneous regions of different sizes and shapes, multiscale feature histograms are constructed. Finally, a feature fusion framework is proposed to fuse spectral and spatial features so that spectral information can be fully utilized. Experimental results on three HSI datasets demonstrate the proposed method's effectiveness in providing accurate and efficient features for different classification tasks."}, {"label": 1, "content": "Symbolic model checkers can construct proofs of properties over highly complex models. However, the reported results often lack sufficient information to help users gain insight into the proof construction process. It is often useful for users to have traceability information related to the proof: which portions of the model were necessary to construct it. This traceability information can be used to diagnose a variety of modeling problems such as overconstrained axioms and underconstrained properties, measure completeness of a set of requirements over a model, and assist with design optimization given a set of requirements for an existing or synthesized implementation. In this paper, we present a comprehensive treatment of a suite of algorithms to compute inductive validity cores (IVCs), minimal sets of model elements necessary to construct inductive proofs of safety properties for sequential systems. The algorithms are based on the UNSAT core support built into current SMT solvers and novel encodings of the inductive problem to generate approximate and guaranteed minimal inductive validity cores as well as all inductive validity cores. We demonstrate that our algorithms are correct, describe their implementation in the JKind model checker for Lustre models, and present several use cases for the algorithms. Several use cases for the algorithms are presented, followed by a substantial experiment testing the efficiency and efficacy of the algorithms."}, {"label": 1, "content": "This paper presents an analytical approach to evaluate the performance of the device-to-device (D2D) communication underlaid with cellular networks, which is an enabling technology for the Internet-of-Things (IoTs). In such an architecture, a group of IoT devices (IoTDs) communicate with an IoT gateway by reusing the resources of cellular users (CUEs) to enhance the spectral efficiency of the fifth-generation networks. Two interference management schemes are widely used in the literature for the sharing of D2D spectrum, namely the fixed-power margin (FPM) and the cooperative pairing (CooP) schemes. We investigate and compare the performance of the two schemes from the perspective of outage probability (OP). While satisfying the minimal performance of the system, the OP of an arbitrary pair (i.e., one IoTD and one CUE) under both schemes are derived in closed form in terms of hyper-geometric functions via the Mellin transform technique. Moreover, an iterative alternating Dinkelbach algorithm is proposed for the CooP scheme as an outage-optimal power allocation scheme. Analytical and simulation results reveal that the CooP scheme is the outage optimum for the high SNR regime while the FPM scheme is the optimal one for the low SNR regime. The simulation results also suggest that the suitable power margin for the FPM scheme lies between 2 and 3 dB. Under these two interference management schemes, the accuracy of the analytical results is verified through numerical simulation and it turns out that these are well matched."}, {"label": 1, "content": "This brief introduces a novel real-time photoplethysmogram (PPG) signal quality assessment system that effectively reduces overall energy consumption and decreases frequent false alarms. The algorithm employs hierarchical decision rules and incorporates simple features, including absolute amplitude, threshold crossing rate, and autocorrelation function features. The validity of the algorithm is confirmed using both MATLAB software and Arduino Due with 32-bit ARM core microcontroller by utilizing the standard PPG datasets along with the real-time PPG datasets. The algorithm achieved a sensitivity of 99.29%, specificity of 95.31% and accuracy of 97.76% with 754 false positive segments and 182 false negative segments of a total of 16 068 unacceptable and 25 772 acceptable segments, respectively. The algorithm provides a promising false alarm reduction rate of 95.31% on different kinds of artifacts and noise. The proposed quality-aware monitoring system is capable of reducing transmission energy consumption from 3.43% to 95.1% for transmission of noisy 5-60 s PPG signal."}, {"label": 1, "content": "I regret to inform you that the book's completion is nearing its end. I know this is a little bit sad\u2014for you, not for me. It took a lot of effort to write this book, and I'm happy that I'm basically done. It all started with my cats being rehomed, and ten chapters later, we have explored game design, definitions of intelligence, narrow and general artificial intelligence, automatic creativity, and games that learn who you are and what you want, among other things. In order to structure the closing comments, we will here revisit the three broad claims that I made at the beginning of the book and outline how the discussion in the book supports these claims."}, {"label": 1, "content": "In Chapter 4, we explored the question of why artificial intelligence methods are not more widely used in games. I outlined a couple of potential reasons for this. In the subsequent chapters, we delved into various AI methods that can be used for playing games, modeling players, and generating content. In this chapter, we will revisit the question with a focus on the relationship between game design and AI. We will examine how game design can enable AI and, conversely, how AI can enable game design."}, {"label": 1, "content": "I hope that this book has inspired you to read more about its topics and maybe even dive into this research field yourself. In order to facilitate your research journey, I have embedded notes throughout the book that provide references to papers which provide a more detailed exploration of each topic. The papers may be more or less accessible given your technical background, so you might want a more coherent introduction to some topics. Therefore, I have compiled a list of book recommendations that provide a more systematic introduction to these topics."}, {"label": 1, "content": "Mapping and monitoring trees in tropical forests is crucial for forest personnel, but acquiring tree parameters for mapping purposes can be tedious, time-consuming, and costly. Therefore, the demand for remote sensing technology that provides tree parameters in a cost and time-effective manner over large forest areas is increasing. Thus, the advancement of remote sensing technology which provides tree parameters economically in term of cost and time saving over large forest area is in demand. The pan-sharpened Worldview-2 imagery was utilized to classify tropical trees using the support vector machine (SVM) image classification method. The SVM method demonstrated an overall classification accuracy of 90.28%, with individual accuracies for Shorea and mixed tree species ranging from 68.25% to 82.86%. The classified result was then overlaid with tree height information extracted from LiDAR data, forming the 3D distribution of Shorea tree species in the dense tropical forest area. This approach provides an effective means of monitoring and mapping tree distribution, which can assist in forest management and conservation efforts."}, {"label": 1, "content": "This paper presents the estimated communication link budget for the developed Ground Sensor Terminal (GST) to support the BIRDS-2 cubesat's Store-and-Forward (S&F) demonstration. The aim of the communication link is to establish a connection between the nanosatellites situated in Low Earth Orbit (LEO) and a ground terminal. The proposed link budget includes calculations for attenuation parameters such as Free Space Path Loss (FSPL), satellite slant distance, and antenna mismatch to ensure the proper establishment of the communication link. As to conclude the estimation on communication link, the Uplink margin and Downlink margin per two elevation angle values are presented."}, {"label": 1, "content": "This paper presents observations on the possibility of Geomagnetic Induced Current (GIC) activities in the Malaysia region during the quiet period of solar cycle 24 in 2017. In fact, the damage of power system operation might lead to the malfunction of the transformer, which is the main component of the power system. Previous investigations have identified space weather effects, particularly GIC, as the cause of this damage. In this work, the quiet period has been selected by respecting the value of K-Planetary index (Kp) must be less than 3 extracted from OMNIWeb data source. The objective of this study was to investigate GIC behavior in the Malaysia region, as well as identify parameters - ionospheric current and magnetospheric current - that significantly contribute to GIC activity. The type of analysis was carried out using the data of geomagnetic field by extracting the time derivative of the horizontal component (dH/dt) as the indicator for GIC activity. The geomagnetic data was obtained from the MAGDAS observatories at Langkawi (LKW), Perak (PER), and Johor (JOH) stations. The results showed that when the dH/dt value was less than 30 nT/min, no GIC activity occurred on that particular date for all three stations. However, the ionospheric current, DP2, was found to correspond well to the dH/dt amplitude. Plus, the latitudinal correlation gave the best representation of the control factor of GIC activity."}, {"label": 1, "content": "The use of wearable sensors and ambient monitoring for collecting personal data has raised concerns regarding user privacy. Applying cryptography solutions to resource constraint wireless sensors as one of the privacy-preserving solutions demand addressing limited memory and energy resources. In this paper, we set up testbed experiments to evaluate the existing cryptographic algorithms for sensors, such as Skipjack and RC5, which are less secure compared to block cipher based on chaotic (BCC) on existing IEEE802.15.4 based SunSPOT sensors. We have proposed modified BCC (MBCC) algorithm, which uses chaos theory characteristics to achieve higher resistance against statistical and differential attacks while maintaining resource consumption. Our comparison observations show that MBCC outperforms BCC in both energy consumption and RAM usage and that both MBCC and BCC outperform RC5 and Skipjack in terms of security measures, such as entropy and characters frequency. The comparison analysis between MBCC and BCC suggests a 13.44% lower RAM usage for encryption and decryption, as well as 6.4 and 6.6 times reduced consumed time and energy for encrypting 32-bit data, respectively. Further analysis is reported on increasing the length of the MBCC key, periodical generation of master key on the base station, and periodical generation of round key on the sensors to prevent brute-force attacks. An overall comparison of cipher techniques with respect to energy, time, memory and security concludes the suitability of MBCC algorithm for resource constraint wireless sensors with security requirements."}, {"label": 1, "content": "Key nodes refer to nodes within a network that are connected to a specific number of external controllers resulting in minimal control costs. However, identifying such nodes proves to be a challenging task, particularly for larger networks. In this paper, we approximately solve this problem by proposing three algorithms step by step. Firstly, the Boolean constraints in the original optimization model are relaxed, resulting in a convex problem. Secondly, an inexact alternating direction method of multipliers (IADMMs) is introduced and its convergence property is established. Based on the degree distribution, an extension method named degree-based IADMM (D-IADMM) is proposed such that key nodes are pinpointed. Lastly, further improvements are made through the usage of local optimization techniques in LD-IADMM; thereby greatly enhancing performance. The effectiveness of the proposed algorithms is validated on different networks ranging from Erd\u00f6s-R\u00e9nyi networks and scale-free networks to some real-life networks."}, {"label": 1, "content": "The number of component classifiers chosen for an ensemble greatly impacts the prediction ability. In this paper, we propose a geometric framework that can be used to determine the ensemble size in both batch and online environments. While there are some studies examining the impact of ensemble size on majority voting and weighted majority voting, most of them are designed for batch-mode and fail to take into account the constraints of online environments. Almost all of them are designed for batch-mode, hardly addressing online environments. Big data dimensions and resource limitations, in terms of time and memory, make the determination of ensemble size crucial, especially for online environments. When it comes to the WMV aggregation rule, our framework proves that an ideal number of components exists, which is equal to the number of class labels, assuming that the components are completely independent and strong enough. Defining a strong and independent classifier within an ensemble is a challenging task. While giving the exact definition for a strong and independent classifier in the context of an ensemble is a challenging task, our proposed geometric framework provides a theoretical explanation of diversity and its impact on the accuracy of predictions. By conducting a series of experimental evaluations, we have demonstrated the practical value of our theorems and highlighted the existing challenges in this field."}, {"label": 1, "content": "The availability of real-time information about the biological activity is a valuable matter in wastewater treatment employing activated sludge processes, and this is represented by the oxygen uptake rate (OUR). In these processes, measuring OUR has proven to be a quick and reliable way of tracking changes in the waste load or influx of toxic organic compounds, as well as for monitoring process efficiency. To improve OUR estimation, we propose a novel method that utilizes Kalman filters and Pulse Width Modulation (PWM) control of the dissolved oxygen (DO) concentration in the activated sludge reactor. The proposed method presents a shorter response time than the standard online procedure and a lower measurement uncertainty compared to a previous feedback control method, which estimates the OUR from values of the applied control signal and the DO concentration measurements. We present simulations to demonstrate the effectiveness of our method, followed by experiments on a bench-scale activated sludge reactor that validates our estimator's applicability in a real system. The results confirm the fast response of the proposed method and its consistency with respect to the standard procedure for OUR computation."}, {"label": 1, "content": "Due to densification of wireless networks, there exist abundance of idling computation resources at (network) edge helpers (e.g., base stations and handheld computers). These resources can be scavenged by offloading heavy computation tasks from small Internet-of-Things (IoT) devices (e.g., sensors and wearable computing devices) in proximity, thereby overcoming their limitations and lengthening their battery lives. However, these spare resources offered by edge helpers are random and intermittent, unlike dedicated servers. Thus, it is essential to intelligently control a user (IoT device) the amounts of data for offloading and local computing so as to ensure that a computation task can be finished in time-consuming minimum energy. In this paper, energy-efficient control policies are designed for a computation offloading system with a random channel and a helper with a dynamically loaded CPU due to the primary service. Specifically, the policy adopted by the helper aims at determining the sizes of offloaded and locally computed data for a given task in different slots such that the total energy consumption for transmission and local CPU is minimized under a task-deadline constraint. These policies provide an offloading user with robustness against channel-and-helper randomness while balancing offloading and local computing. By modeling the channel and helper CPU as Markov chains, the problem of offloading control is converted into a Markov decision process. Though dynamic programming (DP) for numerically solving the problem does not yield the optimal policies in closed form, we leverage the procedure to quantify the optimal policy structure and apply the result to design optimal or sub-optimal policies. For three cases ranging from zero, small to large helper buffers, the low complexity of the policies overcomes the \"curse of dimensionality\" in DP arising from the joint consideration of channel, helper CPU, and buffer states."}, {"label": 1, "content": "In the field of hyperspectral image classification, deep learning has been gaining much attention recently. Although many effective frameworks exist in the literature, the generally limited availability of training samples poses great challenges in applying DL to HSI classification. This paper presents a novel deep learning framework called semisupervised stacked autoencoders (Semi-SAEs) with cotraining, which addresses this issue. First, two SAEs are pretrained based on the hyperspectral features and the spatial features, respectively. The initial training set is enlarged using an effective region growing method. Finally, classification probabilities obtained by the two SAEs are fused using a Markov random field model solved by iterated conditional modes. Experimental results based on three popular hyperspectral data sets demonstrate that the proposed method outperforms other state-of-the-art DL methods."}, {"label": 1, "content": "This paper explores the relay selection problem in multi-hop full-duplex relay networks where multiple source-destination pairs compete for relays, under the attack of multiple eavesdroppers. The focus is on enhancing physical-layer security by jointly assigning available relays at each hop to different SD pairs to maximize the minimum secrecy rate. Two RS schemes, optimal RS and suboptimal RS (SRS), are proposed for two-hop networks based on global channel state information (CSI) and only SD pairs CSI, respectively. Since all users can communicate within the same coherence time, our joint RS schemes are important for the user-fairness and ultra-reliable low-latency communications. To evaluate the performance, the exact secrecy outage probability of the SRS scheme is derived under two residual self-interference models. The asymptotic analysis shows that the SRS scheme achieves full diversity. A relay-based jamming scheme is proposed, utilizing unassigned relays for user communications. Finally, the two-hop RS schemes and the analysis are extended to the general multi-hop network with multiple eavesdroppers. The numerical results demonstrate that the proposed schemes can significantly enhance secrecy performance."}, {"label": 1, "content": "The frequency-weighted model reduction problem is of great importance in control system design due to its applications in obtaining a lower order controller for significantly high order plant. This paper introduces two algorithms for achieving frequency-weighted model reduction through the application of Krylov subspace-based interpolation frameworks. Numerical examples are presented to signify the efficacy of the proposed algorithms."}, {"label": 1, "content": "In this work, a robust and accurate neural predictive model based on a randomized neural learning scheme is developed for foreign exchange market modelling and forecasting purpose. Our predictive model employs a dynamic single-hidden layer feedforward neural network (SLFN) integrated with tapped-delay-memories to enhance its input layer. To ensure financial input patterns are represented in a clear and strong manner in the hidden feature space, a modified sigmoid function is designed while input weights and hidden biases are randomly allocated. Additionally, a greater number of hidden nodes is utilized to enhance input patterns' representation in the hidden feature space. To improve the model's robustness against internal and external disturbances, output weights of the network are optimized utilizing regularized batch-learning type of least square method. Simulation results show excellent performance of the developed model in both target deviation and directional performance measurements."}, {"label": 1, "content": "This paper explores the different facets of using optical flow (OF) methodology in unmanned aerial vehicle (UAV) navigation. OF only provides estimates of UAV velocities, which can lead to errors in position estimation. This drawback is due to the presence of bias in the speed estimates. However, OF is likely the only viable navigation tool in GPS denied environments when other estimates of UAV position are unavailable. Therefore it is necessary to approach very thoroughly to the OF and other navigation system elements data fusion. This paper specifically looks at the problem of UAV landing using a combination of OF and navigation system sensors."}, {"label": 1, "content": "This paper discusses the analysis of memristive neural networks with time-varying delays from the perspective of extended dissipativity. A model of a memristive neural network is obtained using the characteristic function technique, which is similar to a neural network with polytopic uncertain synaptic weights. This enables the construction of a parameter-dependent Lyapunov functional, which is combined with some integral inequalities to derive a new extended dissipativity criterion in terms of linear-matrix-inequalities. By combining this functional and some integral inequalities, a novel extended dissipativity criterion is obtained in terms of linear-matrix-inequalities, where different Lyapunov matrices are used for each form of the memristive neural network. A numerical example is used to demonstrate that this criterion is less conservative than the one based on a common Lyapunov functional."}, {"label": 1, "content": "In order to reduce the complexity for extracting artificial features from the face image in facial expression recognition (FER), a novel method is proposed based on convolutional neural network (CNN) in this paper. In the proposed method, facial expression images are preprocessed and trainable convolution kernels are employed to extract facial expression features. The largest pooling layer is then used to reduce the dimensions, and the Softmax classifier is used to recognize the seven types of facial expressions. The experimental results show that the method has good recognition performance and generalization ability."}, {"label": 1, "content": "This study presents a novel control design approach that integrates analytical and randomized model predictive control methods. By using the analytical approach, one can consider the optimization in the infinite time horizon and propose several nonlinear optimal control input candidates. Then, by using the randomized model predictive control method, one can determine the \"probably true\" optimal one. Simulation results demonstrate the effectiveness and robustness of this approach."}, {"label": 1, "content": "This paper is concerned with finite-time containment control problem for second-order nonlinear multi-agent systems with multiple dynamic leaders. The authors propose two new containment control protocols that ensure all followers converge to the dynamic convex hull spanned by the dynamic leaders in a finite time. Moreover, the non-singular terminal sliding mode method is used to eliminate the singular problem associated with the terminal sliding mode control. Criteria on the existence of desired containment control protocols are also derived. Furthermore, the settling time is estimated under the proposed protocols. Simulation results illustrate the effectiveness of the developed theory in the last."}, {"label": 1, "content": "With datasets from the domestic medium blast furnace (BF) as a sample place, the contributions of multivariable features of BF system to temperature tendency prediction are analyzed based on the support vector machine and recursive feature elimination (SVM-RFE), and then prediction model of BF temperature is built. First, the initial feature sets are trained to obtain the optimal feature nested subset based on SVM-RFE. Then, the optimal feature nested subset and the current BF temperature tendency are taken as input and output respectively to build support vector machine (SVM) model, which is applied to the independent test set. Third, the optimal feature set and tendency prediction rate are obtained. The simulation results show that the complexity of high dimension data is reduced. In addition, the model can provide an accuracy of 86% in temperature tendency prediction in BF and have some practical use in online monitoring the BF temperature, and thus it has remarkable advantages in feature selection and BF temperature tendency prediction in hot metal."}, {"label": 1, "content": "Deep learning provides the ability to train algorithms (models) that can tackle the problems of data classification and prediction based on deriving (learning) knowledge from raw data. Among its most popular approaches is the use of Convolutional Neural Networks (CNNs), particularly in image classification and detection. In this work we describe a CNN-based method for detecting dogs in potentially complex images and subsequently consider the identification of the type/breed of dogs. The results demonstrated an accuracy rate of almost 85% for breed classification among 50 dog classes, and 64% for 120 other less common breeds. An iOS application and associated big data processing infrastructure utilizing a variety of GPUs was used to support the image classification algorithms."}, {"label": 1, "content": "Previous studies revealed that the ordering of Magnetic resonance imaging (MRI) brain scans following American College of Radiology (ACR) guidelines showed a higher percentage of brain abnormalities compared to scans that do not. As the process of manually labelling patient orders obtained from a local tertiary hospital in accordance to ACR guidelines is intensive and time consuming, this study aims to develop predictive machine learning models; Logistic Regression (LR), Support Vector Machine (SVM), Random Forest (RF) and XGBoost (XGB), to automate the classification process through text mining methods and derive insights that are useful for future clinical decision-making and resource optimization. The goal of this approach was to derive insights from the resulting data that could be used for future clinical decision-making and resource optimization. Using 1,924 labeled observations for training, RF and XGB were found to be the most successful models, with ROC values of 0.9459 and 0.9508 respectively on the validation dataset (comprised of 481 observations). Further analysis, utilizing the model agnostic LIME framework to obtain insights for individual patient level MRI ordering decisions, sheds light on the potential for developing a comprehensive decision support system for MRI scan ordering."}, {"label": 1, "content": "Community detection methods are meaningful and challenging especially in the era of social big data. Typically, the data can be represented as a streaming graph, on which two machine learning techniques can be adopted. The one is the time slice based method which is time consuming, because the algorithms need repeatedly run in each time slice. On the other hand, the incremental method may yield lower community detection accuracy since the algorithm frequently needs to update results for new data points. In this paper, an incremental community detection method is proposed to balance the above two limitations. The experimental results demonstrate that the proposed method is much more efficient compared with the time slices based method, and it is much more effective compared with other incremental community detection methods."}, {"label": 1, "content": "The efficient management of a water supply system requires precise water demand forecasts as inputs. This paper compares existing prediction methods and improves their performance by integrating human-related factors with water consumption in an urban area. Furthermore, a framework for processing and transforming mobility data into time-series is presented. The results demonstrate a notable enhancement in predictive accuracy with the inclusion of human mobility data, achieving a success rate of 87.6%."}, {"label": 1, "content": "This paper mainly focuses on the passivity of memristor-based inertial neural networks (MINNs) with multi-proportional delays. Some novel criteria are proposed to guarantee the delay-independent and delay-dependent passivity of MINNs based on linear matrix inequalities and Lyapunov-Krasovskii functional approach. Furthermore, an example is introduced to demonstrate the effectiveness of the presented results."}, {"label": 1, "content": "Mixture of Matrix Normal Distributions (MMND) is the two dimensional extension of Gaussian Mixture Model, which has been widely applied for clustering three-way data. It plays a crucial role in building an image prior model for solving image denoising problems. In this paper, the Expected Patch Log Likelihood (EPLL) with a prior of MMND is proposed for image denoising. Expectation Maximization algorithm and flip-flop algorithm are adopted to estimate the parameters in MMND. The regularization parameter of the covariance matrix is selected by minimizing the Kullback-Leibler information measure (KLIM) using a heuristic approximation. Under the framework of the EPLL, the approximate MAP estimation for the unknown image x is developed. Experimental results show that the MMND-based patch prior performs exceptionally well in solving image denoising problems."}, {"label": 1, "content": "A high precision network times synchronization algorithm is proposed in this paper based on the theory of machine self-learning. The proposed algorithm aims to address the issue of low time synchronization accuracy by taking the network port delay into consideration. To achieve this, the machine self-learning theory is utilized to study the network port delay under varying environmental conditions. Through experimental verification, it has been demonstrated that this approach greatly improves the accuracy of network time synchronization."}, {"label": 1, "content": "The effectiveness of artificial neural networks, particularly those trained with back-propagation algorithms, is limited by their inability to determine optimal network structures and weights. To address these limitations, the present study proposes a novel Two-Input Single-Output (TISO) power-activation feed-forward neural network model based on multivariate approximation theory and power series expansion. Then, the weights-direct-determination (WDD) method for our feed-forward network is studied and proposed. Further, in order to optimize the structure of network, particle swarm optimization (PSO) is applied to search for global optimal solution to the use of neurons in hidden layer. Then, these lead us finally to propose a weights-structure-determination (WSD) methodology by combining WDD and PSO novelly. Numerical experiments using various objective functions demonstrate that the TISO network model outperforms other techniques in terms of approximation and denoising capabilities. Comparing results with those obtained using stochastic gradient-descent (SGD) and prior studies validates the efficacy of the WSD methodology."}, {"label": 1, "content": "This paper focuses on exploring fixed deviation synchronization for generalized-type neural networks with piecewise constant argument. Based on fixed deviation stability theory, by utilizing discontinuous controller, a sufficient condition is derived to ensure the fixed deviation synchronization between two identical non-directed neural networks. At last, a simulation example is offered to verify the validity of this fixed deviation synchronization criterion."}, {"label": 1, "content": "At present, many deep neural networks are applied to image recognition. But most of them are based on real-valued operations and represents. A complex operation algorithm has been proposed, and in this paper, we applied the VGG model to the complex domain. We provide the advantages which the complex-valued network possesses in terms of the depth and width of networks by calculating. Specifically, the complex-valued network is deeper and wider than the real-valued network when possessing the same parameters. We conducted experiments to test the complex-valued VGG network and real-valued network on image recognition. Experiments show that the complex-valued VGG network has better performance comparing with the traditional real-valued VGG network in terms of stability and convergence speed."}, {"label": 1, "content": "In real-world scenarios, accurate input of impulsive control signals is often hindered by input errors during the stabilization process of memristor-based oscillator systems. Therefore, it is crucial to consider implementing a small time window for impulsive control. That is the occurrence of impulse in each control cycle is limited by small impulse time window. On the other hand, It is necessary to do some research on the chaotic memristor oscillator systems due to the potential applications of memristor. Based on linear matrix inequality, Lyapunov stability theory, and impulsive control with time window, the exponential stability conditions for one type of hyperchaotic smooth memristor oscillator system are displayed. Finally, simulation results show the feasibility and effectiveness of this impulsive stabilization method."}, {"label": 1, "content": "Virtual reality is widely applied in rehabilitation robot to help post-stoke patients complete rehabilitation training for the body function recovery. However, most virtual rehabilitation training systems lack scientific assessment standards, and doctors often employ qualitative observation and conversations with patients to evaluate their limb motor function, rather than quantitative examinations. Based on this situation, a virtual rehabilitation training and assessment system is designed, which contains two rehabilitation training games and one assessment system. Through this virtual system, patients are able to maintain their attention and decrease boredom during their rehabilitation training and assessment. Compared to existing rehabilitation assessment methods, this proposed virtual assessment system can yield results similar to the Fugl-Meyer Assessment, with the added benefits of being more quantitative, interesting, and convenient. Five volunteers participate in the study of assessment system and the experimental results confirm the effectiveness of assessment system."}, {"label": 1, "content": "This paper is committed to investigating the stability of a type of complex-valued neural networks with generalized piecewise constant argument. The network includes a deviating argument that is both advanced and retarded. In order to study the system, an appropriate Lyapunov function is chosen, and inequality techniques are applied to determine the existence and uniqueness of the solution. Moreover, the global exponential stability of the system is analyzed in detail. Finally, numerical simulations are carried out to demonstrate the theoretical analysis and its effectiveness in practical applications."}, {"label": 1, "content": "This paper presents a dynamic model for a quadrotor UAV that includes the wind speed factor, and sliding mode controllers are designed to stabilize the attitude. To improve the performance of the anti-strong wind controller for UAV, a combination of neural network and fuzzy control is proposed. Uncertainty of dynamic model can be approximated by radical basis function (RBF) neural network, and fuzzy control can dynamically correct the coefficients of the symbol function during the state convergence process. The simulation results demonstrated that the use of neural network can effectively track the model uncertainty, and by integrating sliding mode with fuzzy control, the controller displays fast response and robustness. The chattering problem is also significantly reduced, and the anti-strong wind requirements are met, thereby improving the overall stability of the controllers."}, {"label": 1, "content": "Traffic flow prediction is a crucial part of traffic control and induction in the modern world. Short-term traffic flow prediction plays an important role in urban traffic navigation planning and traffic optimization control. Due to the advantage in processing of time series data, LSTM is very suitable for predicting short-term traffic flow. This paper builds a deep learning model based on LSTM to predict the traffic flow volume. In experimental settings, different models were built while varying the hyper parameters for comparison and analysis of each model's performance."}, {"label": 1, "content": "In order to solve the problem of trajectory tracking control of unmanned surface vehicles (USV) with unknown speed information, an adaptive control algorithm based on Radial Basis Function (RBF) neural network and back-stepping method is proposed. This advanced algorithm utilizes the back-stepping method in order to establish an efficient and practical control input alongside model parameters. Then the control law and the weight update law of RBF neural network are designed. Finally, the systemic stability is proved by Lyapunov function. Simulational experiments and physical experiments verify the feasibility and effectiveness of this algorithm."}, {"label": 1, "content": "In this paper, we explore the integration of a two-wheeled robot and Kinect sensor for obstacle detection. When the two-wheeled robot autonomously balances or moves, the ranging value will be unstable due to the shaking of the two-wheeled robots. To address this issue, we propose a smart prediction method to correct distance errors, thereby improving obstacle avoidance decision-making. Our experiments demonstrate the effectiveness of this method and highlight its potential for real-world applications."}, {"label": 1, "content": "This study designs a two-wheeled mobile platform with autonomous movement and face following functionality. The autonomous movement mechanism uses the change of the center of gravity to make the two-wheeled mobile platform move forward or backward, and utilizes simultaneously the PID controller to keep its balance. The face-following function utilizes a tablet equipped with an image sensor and an image processing algorithm, allowing for efficient face recognition and tracking. Experimental results demonstrate that the two-wheeled mobile platform is capable of performing autonomous balance, movement, and face tracking."}, {"label": 1, "content": "This paper introduces a cutting-edge convolutional neural network (CNN) based high-level control structure that employs deep learning techniques to enable autonomous picking control of a six-degree-of-freedom (6-DoF) manipulator using only visual input. The proposed manipulator control system utilizes a stereo camera for measurement purposes and captures a stereo image of the scene in front of the robot. Then, the CNN-based picking controller uses this stereo image as input to predict the optimal picking control command for the manipulator. In the collection of the training dataset, we controlled the manipulator to pick up the object-of-interest (OOI) manually and recorded the stereo images and the corresponding control commands. In the CNN training phase, the supervised end-to-end learning technique is used to learn the mapping between the stereo image observation and the picking control commands of the 6-DoF manipulator. The experimental results demonstrated that the proposed end-to-end visual picking control system achieved an average success rate of 70% and 60% in the random single-object and multi-object picking tasks, respectively."}, {"label": 1, "content": "The accuracy and reliability of the vehicle positioning system are important performance indices of advanced driver assisted systems and the autonomous driving systems. This paper highlights the development and verification of vehicle positioning techniques using Lidar and GPS/IMU sensors. To this end, the techniques for feature extraction, map building, and point cloud matching are investigated. We then integrate and implement these techniques in a robotic operating system (ROS) platform. Experimental results verify the feasibility of the proposed sensor fusion technique with roadside feature extraction characteristics in rendering high accuracy and reliability vehicle positioning."}, {"label": 1, "content": "Along with the rapid development of wireless sensor networks, various applications are getting widespread. Since some of the applications involve with vibration signal, image or video transmissions, sensor nodes with different load burdens and the resulted energy holes should be taken into account. To ensure node energy uniformity in the deployment field and prolong network lifetime, researchers have investigated an optimal mobile sink path planning problem for wireless sensor networks with heterogeneous nodes. By using the load balance and relay node techniques, a path for mobile sink moving to efficiently collect the measured data, including temperature, humidity (sensed by light-load nodes) and vibration (sensed by heavy-load nodes). Simulation results indicate that the proposed method is able to extend the network lifetime by increasing the energy uniformity, in the presence of light-load nodes and heavy-load nodes."}, {"label": 1, "content": "Aspect-based sentiment analysis currently attracts much attention from researchers in sentiment analysis and opinion mining fields. This involves detecting both the aspect and the sentiment associated with a particular text. This paper proposes a Convolutional Neural Network based model in which we integrate extended rich information features into the basic CNN model. Our experiment is conducted on the aspect-based sentiment analysis task of Semeval 2016 and achieves the best results in comparison with previous studies."}, {"label": 1, "content": "In this paper, we present a comparison of the positioning performance of Wi-Fi and Bluetooth low-energy (BLE) beacons in indoor environments. We analyze the accuracy of estimated position using the received signal strength (RSS) of the wireless device and a weighted centroid localization technique. For a comparison trough experiments, we developed an Android-based smartphone application to collect both Wi-Fi and BLE RSS simultaneously at the same smart-phone. Experiments were conducted on the 6th floor of the general office building and both schemes were compared at 8 points in 16m \u00d7 7m space. According to the experimental results, it is shown that the performance between Wi-Fi and BLE can be different with respect to the density as well as the deployment of Wi-Fi APs and BLE beacons."}, {"label": 1, "content": "Fast feature matching is of crucial importance for time-critical applications in computer vision. This article aims to present a comprehensive review of the current state-of-the-art approaches for feature indexing. The article groups indexing methods into four primary categories, including space partitioning, clustering, hashing, and product quantization. The methods are deeply presented, discussed, and linked to each other. Additionally, an empirical report that analyzes the performance of different methods is provided to characterize the studied approaches. Lastly, we give comments on possible room of improvements for some indexing schemes."}, {"label": 1, "content": "Aspect-based Sentiment Analysis (ABSA) is a widely researched topic in several languages due to its capability of identifying sentiment for each aspect of the text. The ABSA problem can be divided into three subtasks as follow: the aspect detection, Opinion Target Expression (OTE) and Sentiment Polarity. In this paper, we present a Convolutional Neural Network architecture for the aspect detection for Vietnamese. The aspect detection is to aim to identify of the entity E and attribute A pairs expressed in the text (Pontiki et al., 2016). The experimental results show the superiority of our model over the winning systems on datasets of the VLSP 2018 challenge for aspect detection task. Our method achieves the F1 score of 80.40% for the restaurant domain and 69.25% for the hotel domain."}, {"label": 1, "content": "In recent times, automated chatbots have become a growing trend in the real estate industry. While they may not entirely replace the traditional interactions between real estate agents and home buyers, chatbots can be instrumental in engaging potential clients in meaningful conversations, thus proving to be highly effective for lead capture. The paper aims to introduce an intelligent chatbot built specifically for this purpose. Various machine learning techniques, including multi-task deep learning technique for intent identification and frequent itemsets for conversation elaboration, have been employed in our system. Our chatbot has been deployed by CEO K35 GROUP JSC with daily updated data of real estate information at Hanoi and Ho Chi Minh cities, Vietnam."}, {"label": 1, "content": "Diabetes mellitus has become an increasingly significant public health problem in recent years and now ranks among the top 10 leading causes of death in lower-middle and upper-middle-income countries around the world. In this study, we sought to address this complex issue by using a hybrid feature selection approach aimed at optimizing feature subsets for use in classifying and predicting diabetes mellitus in patients based on data from the Korea National Health and Nutrient Examination Survey (KNHANES). We used the information gain feature selection approach as the filter phase and used the support vector machine with sequential search method as the wrapper phase. To validate the efficiency of the proposed approach, we also compared our proposed approach with several popular feature selection approaches. Our findings showed that our hybrid approach significantly enhanced classification accuracy and outperformed other feature selection approaches."}, {"label": 1, "content": "Nowadays, the multiple choices test is a popular method to evaluate the knowledge of learners, especially in the national graduation test at mathematics of high-school pupils in Vietnam. However, current programs for multiple choice training tests in mathematics can only automatically generate tests based on set parameters such as the number of questions and time. They cannot evaluate the user's knowledge level or diagnose areas where improvement is needed. In this paper, an intelligent supporting system for multiple choices training test at high-school mathematics is constructed. This system supports pupils training for the national graduation test. It can generate the test automatically with requirements from the user. Additionally, it tracks the user's progress through their tests and diagnoses areas where improvement is needed based on test results. It can follow the studying of users through their tests and diagnosis the content which they need to be improved through the result of a test. This system has been tested on pupils studying at the high-school."}, {"label": 1, "content": "The problem of routing in wireless sensor networks with holes has been extensively researched, resulting in several sophisticated proposals dealing with packet delivery and load balancing. However, existing approaches fail to address the issue of sustaining sensor nodes close to the holes. These nodes experience energy depletion due to traffic concentration on the hole's border, leading to network instability and shortened lifetime. Moreover, sending or receiving data from near-hole nodes with complex shapes remains a challenge. In this paper, our STABLE routing scheme is proposed to address the problem of routing with the existence of a large, complicated-shape hole in every aspect, regardless of where the senders and recipients are located. We propose to use the ball-rolling technique to compute an inner-margined area of a cavern, which helps to distribute traffic away from the hole boundary. Our experimental results show that STABLE outperforms other routing algorithms, providing the longest network lifetime and optimal load balancing."}, {"label": 1, "content": "Assisting systems for detecting obstacles and objects for Visually Impaired People (VIPs) have been studied for quite some time. However, implementing practical systems in real-life environments still poses challenges due to generic object classes in complicated or cluttered backgrounds. In this paper, we propose a unique framework designed to detect and estimate the complete models of common objects that VIPs encounter in their daily lives. The proposed system not only answers questions about where the queried object is located but also provides relevant information about its size and safety directions for grasping on a flat surface. The pipelines combine a series of the point cloud representation, table plane detection, objects detection and the full model estimation via a robust estimator. We leverage recent deep learning advancements such as RCNN and YOLO to efficiently perform detection tasks, while our geometry-based approach estimates complete 3-D models. This scheme does not require separating (or segmenting) the interested objects from the background of the surrounding scenes. The proposed system is compared with other approaches as well as is evaluated on the real datasets collected in common scenes such as Kitchen or cafeteria room. In these evaluations, the proposed frame-work meets requirements of high accuracy, processing time, and suitability for VIPs. We've also made our evaluation datasets publicly available."}, {"label": 1, "content": "A novel radar target recognition method based on the deep one-dimensional residual-inception network is proposed for a high-resolution range profile (HRRP). The traditional methods that rely on shallow models cannot extract the complete information from the target's HRRP at different angles, thereby hindering their recognition accuracy. In an effort to address this, various deep models, such as sparse autoencoder, have been employed, but they pose a challenge of requiring a large number of training samples to enhance their generalization performance due to their huge amount of parameters. However, these deep models with a huge amount of parameters require more training samples to guarantee the generalization performance. This technique helps to resolve translation sensitivity and target aspect sensitivity of the HRRP and significantly improves the generalization performance of the model by reducing the parameters. It is worth noting that the one-dimensional convolutional kernel and pooling operation have the potential to overcome the translation sensitivity and target aspect sensitivity of the HRRP, and both of them can greatly reduce the parameters and improve the generalization performance of the model. Experimental results have shown that when compared with four other deep models, the proposed model achieves better performance in terms of recognition accuracy and robustness. The experimental results show that compared with other four deep models, the proposed model can achieve a good performance in recognition accuracy and robustness."}, {"label": 1, "content": "Apps have concentrated sale platforms, in which there often exist some products similar to the new App to be developed. The main features of these products are given in their introductions, providing an important resource for developers to improve the quality of the requirements of their own App. In this paper, we propose an approach to gain and recommend requirements related information from App descriptions to help developers use the resource efficiently. After mining domain knowledge from App descriptions, we construct a model that retrieves relevant information from the initial requirements. Then, we analyze the information and recommend them from three aspects: static information of the existing Apps for identifying the priorities of requirements; functional features and non-functional properties of features for giving the detailed design of the Apps in requirements; and the combinations of features for enriching the requirements. To validate the proposed approach, we conducted experiments and a survey based on the data in Google Play. The results show that our approach can identify the existing products with initial requirements reasonably, and also indicate that the developers confirm the usefulness of the recommended information in practice."}, {"label": 1, "content": "Video summarization (VS) is to identify important content from a given video, which can help users quickly comprehend video content. Sparse dictionary selection (SDS) has been shown to be an effective solution for VS, assuming a linear relationship between keyframes and non-keyframes. However, this assumption is not always true for video frames which possess intrinsic nonlinear structures and properties. To address this, we propose a nonlinear SDS model for VS, utilizing a kernel function to project a video onto a high-dimensional feature space that transforms the nonlinearity to linearity. Two greedy optimization algorithms are proposed, the standard kernel SDS (KSDS) algorithm and a robust KSDS algorithm with a backtracking strategy. To enable the VS process to be adjustable and adaptable, we have introduced an adaptive criterion, called the energy ratio, to produce video summaries of different lengths for distinct video contents. Experimental results conducted on two benchmark datasets demonstrate the superiority of the proposed algorithm over several state-of-the-art VS algorithms."}, {"label": 1, "content": "The Internet of Things (IoT) is a ubiquitous system connecting many different devices - the things - which can be accessed from the distance. The cyber-physical systems (CPSs) monitor and control the things from the distance. As a result, the concepts of dependability and security get deeply intertwined. The IoT system's vulnerability is amplified by the increasing levels of complexity and heterogeneity, making it a challenge to react to faults. In this article, we review existing methodologies that focus on anomaly detection, fault-tolerance, and self-healing, while also introducing additional methods to enhance the IoT's resilience. We particularly focus on non-intrusive methods ensuring data integrity in the network. Furthermore, this paper presents the main challenges in building a resilient IoT for the CPS, which is crucial in the era of smart CPS with enhanced connectivity (an excellent example of such a system is connected autonomous vehicles). We summarize our research in this field, highlighting our existing solutions, ongoing work, and future work to create a \"Trustworthy IoT for CPS.\" Finally, this framework is illustrated on a selected use case: a smart sensor infrastructure in the transport domain."}, {"label": 1, "content": "As a novel signal control method, signal Cooperative Control with traffic Supply and Demand (CCSD) is superior to the traditional control methods and could satisfy the control requirements under all traffic conditions. However, the optimization solution of CCSD cannot meet the real-time control requirement for its exhaustive search. To address this issue, this paper proposes a method to reconstruct the optimization problem of CCSD by taking into account the time-varying traffic supply and demand. The problem is then reduced to a space search problem, which is solved by the krill herd (KH) algorithm, leading to a fast solution of CCSD (KH-CCSD). During the process of optimization, the search space representation and fitness function for the KH algorithm are constructed to satisfy the solution of CCSD. The optimal signal timing plan is obtained by an iterated search of krill swarm in a multi-dimensional time-varying space cooperatively constrained by traffic supplies and demands. The convergence and effectiveness of KH-CCSD are validated by comparing experiments, in which the convergence of KH-CCSD is tested by different initializations and KH-CCSD is compared with the Webster method and capacity-aware back-pressure (CABP) control under unsaturated, saturated, and oversaturated conditions. The experiments results show that KH-CCSD performs a fast convergence and KH-CCSD is superior to the Webster method and CABP. Thus, KH-CCSD can effectively apply CCSD in all traffic conditions."}, {"label": 1, "content": "This paper presents a proposed chattering-free, adaptive, and robust tracking control scheme that is designed to be applied to a certain class of second-order nonlinear systems with uncertain dynamics. The proposed scheme is composed of two phases. Second, to obtain the desired control target without chattering behavior, the proposed controller with a continuous approach has been applied. In detail, the proposed controller uses an integral of a switching term and an adaptive updating law to compensate the lumped system uncertainty (e.g., disturbances, unmodeled dynamics, nonlinearities, or unmeasurable noise). Our proposed controller does not require knowledge about bound values of those anonymous components. The robust behavior and the defined time convergence have been demonstrated rigorously by the Lyapunov principle. The effectiveness and practicality of the proposed controller are demonstrated through position tracking computer simulations."}, {"label": 1, "content": "Objective: The automatic enhancement of vascular structures in X-ray cineangiography is critical for improved visualization and quantification of coronary arteries in diagnostic and interventional procedures. Methods: A new approach, called Patch-Based Adaptive Background Subtraction Method (PABSM), is proposed for the automatic enhancement of vessels in coronary X-ray cineangiography. First, the pixels in the cineangiogram are described using vesselness and Gabor features. Second, a classifier is used to separate the rough vascular and non-vascular regions. Dilation is then applied to the classified binary image to include more vascular region. Finally, a patch-based background synthesis is utilized to fill in the vascular regions that were removed. Results: We collected a database of 320 cineangiograms from 175 patients, which were annotated by an interventional cardiologist. The performance of PABSM was compared to that of six state-of-the-art vascular enhancement methods using precision-recall curves and C-values. The PABSM achieved an area under the precision-recall curve of 0.7133 and a C-value of 0.9659. Conclusion: PABSM can automatically enhance the coronary arteries in cineangiograms while preserving the integrity of vascular topological structures, particularly in complex vascular regions. It also removes noise caused by the non-uniform gray-level distribution in cineangiograms, which can improve subsequent vascular segmentation. Significance: PABSM can avoid the motion artifacts and it eases the subsequent vascular segmentation, which is crucial for the diagnosis and interventional procedures of coronary artery diseases."}, {"label": 1, "content": "In this paper, we study the nonoverlapping Schwarz waveform relaxation algorithm with Robin transmission conditions (TCs) and numerical recovery for non-Fickian diffusion equations with time-delay. We derive an ideal Robin parameter by technically solving a special min\u2013max problem. We emphasize that the free parameter in TCs plays a crucial role in determining the convergence rate of the algorithm. Finally, we present several numerical results to confirm the effectiveness of the proposed algorithm."}, {"label": 1, "content": "This paper presents a novel autoregressive method for multi-speaker monaural speech separation utilizing deep learning. It exploits a causal temporal context in both mixture and past estimated separated signals and performs online separation that is compatible with real-time applications. The method utilizes a learned listening and grouping architecture inspired by computational auditory scene analysis, incorporating a grouping stage that effectively addresses the label permutation problem at both frame and segment levels. Experimental results on the WSJ0-2mix benchmark show that the new approach can achieve better signal-to-distortion ratio and perceptual evaluation of speech quality scores than most of the state-of-the-art methods for both closed-set and open-set evaluations, even methods that exploit whole-utterance statistics for separation. Combining these results with the fact that the approach requires fewer model parameters highlights the effectiveness of this method."}, {"label": 1, "content": "The thumb is a vital component in the activities of grasping and manipulation for both natural and prosthetic hands. In order to mimic the capabilities of a natural hand with a minimum number of actuators, the mechanical design of a prosthetic thumb should allow for both abduction/adduction and flexion/extension. In this letter, we propose a novel monolithic soft robotic thumb for an anthropomorphic and transradial prosthetic hand. The thumb and the whole prosthetic hand were fabricated using a low-cost three-dimensional printing technology, with sizes comparable to those of real human ones but with much lighter weights. The innovative and compact mechanism of the soft thumb allows for two modes of operation, with only one actuator providing grasping versatility in both abduction/adduction and flexion/extension. The reduced number of actuators required by the novel thumb greatly benefits the development of low-cost, low-power and low-weight prosthetic hands with intrinsic compliance. This novel thumb minimizes the number of actuators and reduces the corresponding requirement on space consumption (for housing its actuators) and power consumption, which are favorable features to develop low-cost, low-power and low-weight prosthetic hands with intrinsic compliance."}, {"label": 1, "content": "There are a lot of non-blind image deblurring methods, especially with the total variation (TV) model-based method. However, how to choose the parameters adaptively for regularization is a major open problem. We proposed a very novel method that is based on the TV deep network to learn the best parameters adaptively for regularization. We used deep learning and prior knowledge to set up a TV-based deep network and calculate the parameters of regularization, such as biases and weights. This eliminates the need for complex calculations by automatically updating these parameters using the concept of a deep network. Our experimental results by our proposed network are significantly better than several other methods, in respect of detail retention and anti-noise performance. At the same time, we can achieve the same effect with a minimum number of training sets, thus speeding up the calculation."}, {"label": 1, "content": "In this letter, we discuss how classical tactical formation patterns and flanking strategies can be viewed as maximizing a natural formation utility. While automatic formation keeping is a well-studied problem in the fields of control and robotics, less attention has been given to why certain formation shapes and positions are chosen. By analyzing a situation with two adversarial teams of agents facing each other, we show that natural assumptions regarding the target selection of the agents and decreasing weapon efficiency over distance, can be used to optimize a measure of utility over agent positions. This optimization results in formations and positions similar to those observed in practice. We present both analytical results for simple examples as well as numerical results for more complex situations."}, {"label": 1, "content": "Keyword spotting (KWS) plays a crucial role in human-computer interaction for smart on-device terminals and service robots. In this paper, based on the powerful ability of DenseNet on extracting local feature-maps, we propose a new network architecture (DenseNet-BiLSTM) for KWS. In our DenseNet-BiLSTM, the DenseNet is primarily applied to obtain local features, while the BiLSTM is used to grab time series features. Our approach employs the DenseNet for extracting local features and incorporates BiLSTM to capture temporal features. However, the standard DenseNet may overlook the contextual information for speech audio, which can disrupt its effectiveness. Therefore, we introduce a new variant called DenseNet-Speech, which retains the temporal details by eliminating the pooling on the time dimension in transition layers. The experimental results show that feature-maps from DenseNet-Speech maintain time series information well. The experimental results demonstrate that our approach surpasses the state-of-the-art approaches in terms of accuracy, achieving a recognition accuracy of 96.6% for the 20-commands recognition task with just 223K trainable parameters. In summary, our DenseNet-BiLSTM architecture with a speech-oriented DenseNet helps in the accurate identification of keywords while maintaining efficiency."}, {"label": 1, "content": "Background: Skill assessment in surgery traditionally has relied on the expert observation and qualitative scoring. However, in our novel study, we examined how the analysis of performance in sensorimotor tasks and bench-top surgical simulators can provide insights into the technical proficiency and training history of surgeons. Methods: We examined metrics for basic sensorimotor tasks in a virtual reality interface as well as motion metrics in clinical scenario simulations. As indicators of the training level, we considered survey responses from surgery residents, including the number of postgraduation years (PGY, four levels), research years (RY, three levels), and clinical years (CY, three levels). Next, we performed a linear discriminant analysis with cross-validation (90% training, 10% testing) to relate the training levels to the selected metrics. Results: Our findings showed that using combined metrics from all stations resulted in more accurate predictions of the training level than using only sensorimotor or clinical scenario tasks. We found greater than chance predictions for each survey category, with an overall accuracy of 43.4 \u00b1 2.9% for PGY, 79.1 \u00b1 1.0% for RY, and 64.2 \u00b1 1.0% for CY. In addition, we found that metrics related to the ability to cope with changes in the task environment were the most important predictors of training level. Conclusions: This research suggests that each simulator-type provided crucial information for evaluating surgical proficiency. These methods could improve evaluations of a surgeon's clinical proficiency as well as training potential in terms of basic sensorimotor ability."}, {"label": 1, "content": "In this paper, we introduce SCENT, a neural network designed for acoustic modeling in voice conversion. At training stage, a SCENT model is estimated by aligning the feature sequences of source and target speakers implicitly using attention mechanism. At the conversion stage, acoustic features and durations of source utterances are converted simultaneously using the unified acoustic model. Our model uses Mel-scale spectrograms as acoustic features, which capture both excitation and vocal tract descriptions of speech signals. The bottleneck features extracted from source speech using an automatic speech recognition model are appended as an auxiliary input. Finally, we use a WaveNet vocoder conditioned on Mel-spectrograms to reconstruct waveform outputs. Notably, our method achieves better duration conversion compared to conventional methods. Experimental results show that our proposed method obtained better objective and subjective performance than the baseline methods using Gaussian mixture models and deep neural networks as acoustic models. This proposed method also outperformed our previous work, which achieved the top rank in Voice Conversion Challenge 2018. Ablation tests further confirmed the effectiveness of several components in our proposed method."}, {"label": 1, "content": "Text-based retrieval systems have long been popular, but in recent years, content-based retrieval systems have gained widespread acceptance. These systems allow for direct retrieval of diverse media based on their visual content, such as color, texture, and shape. Sketch-based retrieval requires the user to draw a freehand sketch query, but freehand drawing can be challenging for those with limited drawing skills. However, freehand drawing can be difficult for those with limited drawing skills, which can degrade retrieval performance as successful retrieval depends on the quality of the sketch query image drawn by the user. To address this issue, we propose a real-time stroke guidance system for freehand sketch retrieval. This system continuously displays next-stroke shadow sketches on the canvas based on the user's step-by-step partial strokes. The proposed stroke guidance for freehand sketch retrieval system runs on a five step next-stroke prediction model that identifies candidate next-stroke sketches from a database of millions of sketches. The system retrieves variable number of sketch object classes at different drawing stages. Deep binary hashing is employed for efficient similarity matching of relevant next-stroke sketches. Deep binary hashing is employed for efficient similarity matching of relevant next-stroke sketches. It can be utilized for drawing guidance, tracing, and sketch retrieval. Tracing refers to the act of copying the shadowed line of a guiding image by drawing over its lines. Overall, the proposed stroke guidance system offers a promising approach to improving the performance of sketch-based media retrieval systems by reducing the difficulty of freehand drawing and enhancing the accuracy and efficiency of sketch retrieval."}, {"label": 1, "content": "Registering models is an essential building block of many robotic applications. This is especially true when dealing with three-dimensional data, where the models to be aligned are typically composed of point clouds. In this paper, we put forward a framework for representing scenes containing high-level geometric primitives, such as lines and planes, in a uniform manner. Additionally, we derive both an iterative and a direct method to determine the transformation between heterogeneous scenes (solver). We analyzed the convergence properties of these methods using synthetic data. Moreover, we performed comparative experiments on a full registration pipeline utilizing raw data, built on top of our solver. To this extent we used public benchmark datasets and we compared against state-of-the-art approaches. Finally, we provide an implementation of our solver along with scripts to facilitate the reproduction of the results obtained in this paper."}, {"label": 1, "content": "The deployment of cloud and edge computing creates a three-tier mobile computing network that allows local processing, edge node processing, and remote cloud server processing. This paper aims to optimize the cooperative three-tier computing network by vertical device, edge node, and cloud server cooperation, as well as horizontal edge node collaboration. The objective is to minimize the average task duration while considering limited battery capacity of devices. However, the formulated problem is a large-scale mixed integer non-linear optimization problem with the growing number of base stations and devices, which is NP-hard in general. To address this issue, we propose an offloading scheme that leverages reformulation linearization technology and a parallel optimization framework built with alternating direction method of multipliers (ADMM) method and difference of convex functions (D.C.) programming. The proposed scheme decomposes the large-scale problem into smaller sub-problems that can be computed across multiple computation units in parallel. Simulation results indicate that the proposed scheme has lower complexity and near-optimal performance, reducing the task duration by up to 24% compared to other schemes. Simulation also shows how much the vertical and horizontal computation cooperations affect the system performance under different network parameters."}, {"label": 1, "content": "This paper proposes a practical methodology to improve the accuracy of fault location on homogeneous transmission lines using traveling wave (TW) analysis. The methodology eliminates uncertainties in line parameters and data time synchronization errors, which are often not available in current fault location devices. A commercially available software for TW analysis is proposed to be used as a support tool to compute auxiliary variables, from which the accurate fault distance is estimated. Both real and simulated TW fault records were tested to evaluate the proposed fault location procedure, highlighting its step-by-step approach. The results demonstrate that the methodology is reliable and straightforward, making it useful for field applications."}, {"label": 1, "content": "We propose the computation framework that facilitates the inference of the distributed deep learning model to be performed collaboratively by the devices in a distributed computing hierarchy. For example, in Internet-of-Things (IoT) applications, the three-tier computing hierarchy consists of end devices, gateways, and server(s), and the model inference could be done adaptively by one or more computing tiers from the bottom to the top of the hierarchy. Unlike previous work, our proposed framework allows trained models to run on the distributed system, thereby enabling co-design of the model and the system. In particular, in addition to the model accuracy, which is the major concern for the model designers, we found that as various types of computing platforms are present in IoT applications fields, measuring the delivered performance of the developed models on the actual systems is also critical to making sure that the model inference does not cost too much time on the end devices. This measured performance can inform the design of the model and system in subsequent design cycles. On top of the framework, we have built the surveillance system for detecting objects as a case study. In our experiments, we evaluate the delivered performance of model designs on the two-tier computing hierarchy, show the advantages of the adaptive inference computation, analyze the system capacity under the given workloads, and discuss the impact of the model parameter setting on the system capacity. We believe that the enablement of the performance evaluation expedites the design process of the distributed deep learning models/systems."}, {"label": 1, "content": "Recently, unmanned aerial vehicles (UAVs) have shown themselves promising in various applications at disaster sites because they can move through the sites easily and are not hindered by topography. The purpose of our study is to develop a path-planning algorithm for the position-estimation systems of UAVs to search for survivors. In order to achieve this, we first identified the key characteristics required for a UAV's path-planning algorithm and proposed an algorithm based on probe request information. Through evaluation in a real environment under various conditions, we demonstrated the algorithm's effectiveness in approaching target objects and its potential for searching for survivors in disaster areas. Our findings suggest that this algorithm may prove invaluable in future UAV-based search and rescue operations."}, {"label": 1, "content": "Artificial Intelligence (AI) has become increasingly prevalent across various industries today. Among the different areas of AI, machine learning is proving to be a highly promising field, as it can help address complex problems through the use of a range of methods and algorithms. In this regard, the current article looks at the application of machine learning in EBM, an additive manufacturing technology. Several algorithms are tested onto the manufacturing of a part in order to check which results can extrapolate at the best the deformation risks vs the quality of the part to build. For this test case, a few parameters related to support structures were identified and varied to determine the effectiveness of the different algorithms."}, {"label": 1, "content": "Due to the steadily increasing global competition, manufacturing companies are forced to constantly improve their products and processes. To this end, the utilization of real-time process adaptation based on inline quality monitoring by leveraging predictive data mining techniques is a promising approach for improving manufacturing process efficiency sustainably and elevating product quality. This paper proposes an approach that uses quality prediction models and similarity search algorithms to improve process and product quality in manufacturing by refining the process parameters. The approach enables a data-driven decision support for process control in interlinked manufacturing processes."}, {"label": 1, "content": "The best subset selection problem in linear regression consists of selecting a small subset with a given maximum cardinality of a set of features, i.e explanatory variables, to build a linear regression model that is able to explain a given set of observations of a response variable as exactly as possible. The reason for choosing models with fewer features is to make the models simpler and more understandable. This article presents a heuristic approach based on the idea of local branching. The heuristic involves performing repeated local-search iterations using mixed-integer programming. In each local-search iteration, we consider a different randomly selected subset of the features to reduce the required computational time. The results of our computational tests demonstrate that the proposed local-branching heuristic delivers better linear regression models than a pure mixed-integer programming approach within a limited amount of computational time."}, {"label": 1, "content": "We hereby set out to improve the efficiency of charging stations with quantitative connections of electric vehicles to the charger and power limitations at the point of common coupling. To achieve this, we have developed a fuzzy logic-based algorithm for electric charging management that allows for maximum fulfilment of electricity demands within the designated power limits. Our algorithm has been tested through a model created in MATLAB Simulink, which verified its effectiveness in enhancing charging station efficiency."}, {"label": 1, "content": "The study of lithium ion batteries plays a paramount role in electric power systems, including aerospace, electric vehicles, and electrical propulsion systems. For good performance and long battery life, it is crucial to achieve state of charge estimation accuracy and robustness. To improve the accuracy of state of charge estimation, an adaptive proportional integral observer has been designed. Real time update of parameters of the battery are acquired according to state of charge and error feedback. This paper deliberates the battery model and details the establishment of the improved adaptive proportional integral observer. In Matlab/Simulink, a first-order resistance capacitance model is designed to verify the algorithm's efficacy. The error is minimized by reducing the operations carried out after data acquisition to convert data into information. The estimation process is highly accurate, stable and reliable as compared with conventional adaptive PI observer with estimation accuracy of 99.4%. Overall, this algorithm proves to be a great aid for power supply applications of lithium ion batteries."}, {"label": 1, "content": "With the increasing adoption of technology by State-owned Enterprises (SoE) in Bhutan, there has been a significant growth in the amount of data they collect. This data has become increasingly important in various aspects of business operations such as human resource management, finance, inventory management, production, process automation, and others. State owned enterprises now needs to prepare towards exploring opportunities to capitalize on the Data Assets. The primary endeavor of this paper is to propose a Data Analytics framework that can be adopted by any organization, independent of the technology being implemented or the type of business operated. With time and complexity of the business, the analysis requirement will correspondingly increase demanding organizations to enhance the analytical capability. Companies, therefore, need to evaluate their current capabilities and assess gaps in comparison to the proposed framework to enhance their ability to perform analytics efficiently."}, {"label": 1, "content": "Tacit knowledge transfer is vital as it is viewed as a fundamental source of sustainable competitive advantage within organizations. According to the previous study, source, recipient, knowledge characteristic and transfer mechanism play important role in determining the effectiveness of tacit knowledge transfer. Additionally, the relationship between tacit knowledge transfer effectiveness and individual performance is also crucial. An agent-based modeling approach was employed in this study since the approach is considered perfectly qualified in dealing with a complex system like knowledge transfer. This study aimed to investigate the change of individual work performance with the consideration of the feedback loop mechanism in the model using the agent-based approach. Data used in this study were collected from 15 university laboratories in a university in Indonesia. Three scenarios with various conditions are generated in this study. The result of all scenarios indicates that several strategies can be implemented in real condition to enhance organization member work performance based on the simulation output. In addition, there is a finding that recipient characteristic likeabsorptive capacity is a key driver to enhance knowledge transfer effectiveness and individual performance."}, {"label": 1, "content": "Intent recognition is a crucial component of computerized control of prosthetic knees. Using an Artificial Neural Network (ANN), many researchers have achieved promising results in developing computerized prostheses. Determining an appropriate activation function in artificial neural networks is an essential issue. This paper aims to investigate the most suitable ANN activation function for intent recognition based on accelerometer and gyroscope sensor data, toward the development of a computerized prosthesis. The study employed a Feed-Forward Artificial Neural Network (FFANN) with back-propagation learning method to identify activity patterns. Efficiency of two activation functions were compared to choose an appropriate ANN activation function. Results indicate that log sigmoid function (LOGSIG) performs better than a tangent sigmoid function (TANSIG)."}, {"label": 1, "content": "This paper proposes a sensorless control strategy using sine-wave high-frequency (HF) voltage injection for three-phase four-switch (TPFS) inverter fed interior permanent magnet synchronous motors (IPMSMs). The paper examines the principle of the TPFS inverter fed IPMSM drive system and proposes a nonorthogonal-nonlinear k-l axis coordinate system for sector identification and voltage projection. Besides, the TPFS inverter fed sensorless control strategy is proposed, where the position estimation scheme is deduced, and the equation of the injected HF voltage in the proposed k-l axis coordinate system is given. The effectiveness of the proposed scheme is verified by simulation results in Matlab/Simulink and experimental results on a 5kW IPMSM motor prototype, which shows that estimated values track the real ones well in different working conditions."}, {"label": 1, "content": "Large multinational companies rely on complex Business Processes (BP) to execute their business functions. Business Units (BU) have implemented various measures to improve the optimization of these complex BPs. Intricate relationships exist in presenting the information flow of complex BP. To address this challenge, BU use a range of techniques to optimize these complex relationships. The Design Structure Matrix (DSM) techniques and Artificial Intelligence (AI) system present an efficient methodology for defining and optimizing collaborative BP. This research explores the application of both approaches for complex BP modeling and management. The research results demonstrate the effectiveness of integrating AI and DSM as an enhanced decision support for optimizing complex BP."}, {"label": 1, "content": "This paper focuses on the sensitivity analysis of extended and dual Kalman filters used for state estimation (i.e., state of charge (SoC) and inner resistance) of lithium-ion batteries mainly for traction applications. Kalman filters in various configurations are widely used in modern battery management systems. The accuracy of the estimation largely depends on the quality of the model used. In this case a compromise between model's complexity and quality of estimation is one of main critical factors while developing a Kalman filter based estimation solution. This is especially true when estimating the state of lithium-ion batteries due to several reasons: the chemical effects transforming into an electrical model result in the loss of model accuracy; the model parameters are not constant due to aging and temperature fluctuation; and the derived discrete time models are susceptible to measurement noise. To enhance the accuracy of SoC estimation, the authors analyze the impact of varying the inner parameters of a lithium-ion battery and the choice of time discrete models on the estimation accuracy. The paper investigates the performance of an extended and dual discrete Kalman filter using the zero-order hold method and Tustin transform in the context of the new European driving cycle."}, {"label": 1, "content": "Multinational enterprises are winding their path by R&D embedding into National Innovation Systems of China. Government subsidy has been proven an effective policy instrument to encourage location of foreign capital R&D. However, few literatures take into account how government subsidy impacts innovation outputs and performance of multinational enterprises R&D in China. In this study, the impact of government subsidies on foreign capital R&D in China was analyzed by constructing a lag-distribute model and a simultaneous-equation model using both series data and panel data."}, {"label": 1, "content": "The two new state observers are proposed in this paper to improve the performance and robustness of the flatness-based control against the changeable parameter. The hardware system of the PMSM control is implemented by using a small-scale PMSM of 6-pole, 1-kW, and 3000 rpm in a laboratory, to validate the proposed methodology. Simulation and experimental validation reflect that the two new state observers are better than the linear observer such as extended Luenberger observer (ELO) method regarding convergence for nonlinear systems and convergence rapidity."}, {"label": 1, "content": "In this paper a method is introduced that supports reuse and maintenance of design information. The method allows sharing design information in different levels of details tailored for the stakeholders according to their needs. In addition, it is possible to share the information in multiple formats to suite different purposes. The effectiveness of this method was demonstrated in an industrial setting, where it was employed by a tooling supplier for the manufacturing industry."}, {"label": 1, "content": "The purpose of this study is to propose a new benefit segmentation method based on customer reviews existing on the web. With the diversification in customer needs, it is difficult to accurately identify the needs of customers with market segmentation using demographic information. Therefore, it is of utmost importance to segment the customer base according to the benefits they derive from a product or service. Utilizing the random forest algorithm for this purpose is advantageous as it accurately identifies training data despite the existence of noise and outliers, and is widely utilized for analysis of text data. Our experiment focuses on the segmentation of hotels based on customer reviews, with the reason for hotel usage considered as the benefit derived from the product/service. We utilized the frequency of words in textual data as explanatory variables to identify the topics discussed by customers. We extracted factors that influenced each benefit to determine customer needs."}, {"label": 1, "content": "A novel framework for designing accelerated degradation tests (ADTs) has been developed in this paper under a Wiener process model. The framework aims to strike a balance between prediction accuracy and mechanism equivalence of a specific ADT. To achieve this, a new optimization criterion, called the MV-optimization criterion, has been proposed using maximum likelihood theory. A nonlinear optimization problem has been constructed under the cost constraint. Finally, a comparison is carried out between our multi-objective optimal plan and other traditional single-objective plans through the case of electrical connector. Results show that the MV-optimization plan has better properties from the perspective of improving the rationality of prediction and the robustness of test plans."}, {"label": 1, "content": "The inventory routing problem (IRP) deals with the transportation of one product from a producer to multiple consumers, which have given demands and inventory capacities, over a discrete time horizon. The customers have set demands and inventory capacities, and the primary goal of IRP is to minimize the combination of inventory and transportation costs and prevent stockouts at customers. This paper proposes two variations of the IRP with profit maximization. First, when the market situation allows prices to be adjusted, the problem involves finding an optimal balance of volume and margin according to a demand function. Second, when prices are fixed, unit production costs depend on the production volume, which can be adjusted to maximize the profit. Both variations create non-linear models that are first linearized to be tested on standard benchmark instances. Computational results show that considering profit maximization instead of cost minimization leads to different decisions, generating a larger revenue and profit."}, {"label": 1, "content": "The Internet of Things offers the ability to interact with and share data, expanding the physical world's capabilities in computation, communication, and key control with humans through a network of connected devices. While such systems are more readily available and cost-effective, the utilization of the technology becomes more complex with progress in the field. The traditional ways like lecture-based and role-playing learning has developed one-sided learning and also expensive for the low-income people to acquire such knowledge. On the other hand, serious gaming has helped the users in acquiring new experiences and complex knowledge which are acquired through solving presented challenges whereby the user applies competency to solve these problems. This paper proposes serious gaming as a learning environment for gaining competence, knowledge, and experiences in IoT and knowledge sharing for the users. Moreover, the design of a serious game, effectiveness of ATMSG framework and evaluation results are also discussed."}, {"label": 1, "content": "In this paper, we study a kind of accelerated degradation model, and put forward a statistic to test the homogeneous of the variance based on Wiener process. Firstly, Wiener process is applied to model the degradation process of the deteriorating system, and the analytical expressions of probability density function and reliability function for such system are derived. The maximum likelihood estimation algorithm is presented to estimate the model parameters. Then, by means of homogeneity test of variance, we can judge whether the failure mechanism of the products is changed or not, which is the basis for the extrapolation of the characteristic quantity. The effectiveness of the proposed method is illustrated through a simulation study. Finally, we use this method to analyze the degradation data of the MOSFET (Metal-Oxide-Semiconductor Field-Effect Transistor) and get its reliability under normal working condition."}, {"label": 1, "content": "The proportion of the stock range that is devoted to spare parts is often considerable in industrial context. As such, even minor enhancements in spare parts demand forecasting can result in significant cost reductions. Time series analysis has been the most popularly applied method in the prior spare part demand forecasting models. However, these approaches need to be improved in terms of prediction accuracy. In this study, we gathered component consumption data including structured and unstructured data from a spare part management information system in military logistics. Our approach yielded superior prediction performance compared to traditional approaches. Overall, our results suggest that this method has great potential for improving spare parts demand forecasting in industrial settings."}, {"label": 1, "content": "This paper explores the utility of supervised machine learning algorithms in predicting the tensile strength of high density polyethylene film produced by extrusion-blown molding process. Three algorithms were used: Artificial Neural Networks, Decision Tree, and k-Nearest Neighbors. Specifically, these algorithms were modeled using five materials-related input parameters and six process-related input parameters. The application of algorithms demonstrated their capability in predicting the intended property of the extrusion-blown process products."}, {"label": 1, "content": "In multi\u2010path transmission control protocol (MPTCP)\u2010based aggregation solutions, aggregation occurs at the TCP layer. The goal of MPTCP aggregation is to transmit data using multiple paths without modifying existing infrastructures such as the 4G mobile network or Wi-Fi access. The MPTCP connection consists of a combination of several TCP connections, with each TCP connection transmitted on a path to appear as a normal connection for the different devices crossed. However, congestion control introduces a limitation provided by the congestion window (cwnd) during a normal TCP connection. If an address becomes invalid during an MPTCP connection, the affected host must advertise it in the REMOVE_ADDR option so that the other end can delete the TCP connections bound to that address. At the start of an MPTCP connection, it is essential to ensure that the path is fully compatible with the MPTCP options."}, {"label": 1, "content": "This chapter delves into the role of the body in the context of the Internet of Things (IoT). Initially, it investigates the representation of the body within the areas of sports and e-health. The initial intention of the IoT was to analyze, comprehend, and implement the appropriate response to the organic, physical, or digital world. Beyond the strictly industrial world, the IoT applies to the domain of wearables, home automation tools or hand\u2010held objects, in the field of the individual and his/her environment, but also the social organization of a country. An information\u2010analysis\u2010reaction loop, classic in the industrial world and that constitutes a technological evolution. As long as everything is going well, \u201cwellness\u201d and m\u2010health encourage the individual to engage with himself in a narcissistic way. In the field of sports, the IoT measures performance and encourages the individual to surpass him/herself."}, {"label": 1, "content": "This chapter outlines the complete ecosystem involved in a collective intelligence platform project for the analysis of knowledge. However, meeting the information modeling needs and design requirements for human-machine interfaces for information retrieval, collation, and evaluation has been challenging, particularly with the concept of hypertextual gardening. Since the semantic garden project did not find partners that were important enough to support its development in the context of a start\u2010up or a company, the authors continued to develop it as part of a university research program to refine theoretical bases and ergonomic perspectives. This resulted in an information and communication sciences thesis and several scientific articles that delved into the fundamental principles for modeling an informational existence within a knowledge ecosystem."}, {"label": 1, "content": "Interpretive semantics establishes a clear distinction between aspects of interpretation that are founded on linguistic knowledge and aspects of interpretation that are derived from knowledge about the world. This difference is essential to determining the boundary between semantics and pragmatics. Developed in the mid\u20101960s in response to the interpretive semantics of Fodor and Katz, generative semantics stipulates that the semantic component is generative while the syntactic component is interpretive. Rastier's semantic approach is presented in this chapter, which includes the levels of linguistic description he has identified to situate his work within the general context. First\u2010order logic makes it possible to represent the semantics of natural languages in a more flexible and compact way than propositional logic. On the other hand, first-order logic takes into account terms, predicates, and quantifiers within the world, making it a valuable tool for semantic representation."}, {"label": 1, "content": "As the number of connected objects continues to grow, the scope of the Internet of Things (IoT) and Big Data is expanding to cover a diverse range of applications. This chapter reviews the objects of the IoT as the agents in multi\u2010agent systems, with the goal of modeling and implementing a multi\u2010agent architecture in the field of the IoT. It also reviews the different paradigms of the IoT and the links that have been established in the literature between the IoT and multi\u2010agent systems. To test the multi-agent architecture in a real-world scenario, the chapter then introduces an IoT-a application utilizing a set of connected \u201cbrick-screens\u201d that allow developers to create an interactive and reconfigurable screen wall. It illustrates this application by visiting the eco\u2010distributed resolution of the N\u2010Puzzle algorithm and applying it to the resolution of an N\u2010puzzle video."}, {"label": 1, "content": "A wireless ingestible capsule operating at the ISM (2.4-2.48 GHz) band requires a reliable and efficient antenna for data transmission with a smartphone. To address this requirement, an antenna-in-package has been proposed and experimentally tested at the system level. A modified planar inverted-F structure is proposed for the antenna design, whose end section is bent to be vertical with respect to the substrate, mainly for the considerations of frequency tuning, size reduction, and polarization diversity. First, a one-layer cube muscle phantom model is used for initial design and optimization. A comprehensive evaluation method was then proposed, considering polarization mismatch loss, to analyze the radiation performance using a CST human body model. The evaluation results revealed that the proposed antenna design provided sufficient margin for the link budget, regardless of the relative positions of the capsule and the smartphone. Moreover, the proposed antenna demonstrated insensitivity to various digestive organs, and reflection measurement was conducted using a muscle-mimicking phantom. The radiation characteristics were studied using a simple human torso model filled with pork, and the received power in the system was measured. Radiation characteristics are presented using a simple human torso model filled with pork by measuring the received power in a system. Successful data transmission, between a transmitter using the proposed antenna and a receiver using a smart phone, is performed in a temperature monitoring system."}, {"label": 1, "content": "Arrhythmias are cardiac electrical abnormalities that can cause severe damage to the heart. An electrocardiogram (ECG) is a useful tool to manifest arrhythmias. In this paper, we present an automatic system using a convolutional neural network and active learning to classify ECG signals. To improve the model performance, breaking-ties (BT) and modified BT algorithms are utilized in the active learning. The system is designed to classify ECG signals in five heartbeat types: normal (N), ventricular (V), supraventricular (S), fusion of normal and ventricular (F), and unknown heartbeats (Q), according to the Association for the Advancement of Medical Instrumentation standard. The experiments are carried out on the MIT-BIH arrhythmia database. To further verify the generalization capability of the system, the ECG data that acquired from our wearable device are also used to conduct in the experiments. Compared with most of the state-of-the-art methods, the obtained results demonstrate that the presented method promotes the classification performance remarkably."}, {"label": 1, "content": "This paper addresses the issue of direction-of-arrival (DOA) estimation in the presence of gain-phase errors for uniform-circular-array (UCA). By leveraging the uniformity and cycle property of UCA, the authors demonstrate that a long data vector constructed from the proper entries of the Hadamard product of autocorrelation matrix and its conjugate can be treated as the received data from virtual two-dimensional sources impinging on virtual multiple UCAs. This finding indicates that azimuths and elevations of virtual sources related to DOAs of original sources can be decoupled and estimated via multiple-UCAs-estimating signal parameter via rotational invariance techniques proposed in this paper. Adding to this contributions made in the prior literature, the authors introduce two solutions - the spatial-filtering-method and parameter-estimation-method - to address issues caused by one-component affecting the DOA estimation of adjacent sources. These novel methods have closed-form solutions and do not rely on iteration, leading to more efficient and effective DOA estimation. Compared with existing methods, the proposed approach requires neither calibration sources nor multidimensional parameter search, making it more versatile and applicable to a wider set of circumstances. The proposed method is also capable of handling more sources and various apertures of UCA. Simulations conducted demonstrate the effectiveness of the proposed method."}, {"label": 1, "content": "In this paper, we concentrate on the problem of cross-domain aerial scene classification. The main assumption behind our proposed cross-domain distance metric learning (CDDML) framework is that while training data is sufficient in the source domain, it's limited in the target domain. Data distribution bias is a significant challenge in cross-domain scene classification due to differences in dates, sensor positions, lighting conditions, and sensor types. To address this, the CDDML framework replaces the existing color space with hybrid color features derived from all candidate color components, reducing the spectral shift between domains. Then, hybrid color features and bag of convolution features (BOCFs) are put into a discriminating DML (DDML) method to reduce the data distribution bias in the feature space. Finally, the image-to-subcategory distance measure is proposed to decrease the effect of intraclass variability on the nearest neighbor classifier by fusing hybrid color features and BOCF in the distance space. The experiments on three aerial target images or data sets confirm that the CDDML framework can obtain better results than most of the previous methods in the case of inadequate samples. Experimental results also demonstrate that DDML, hybrid color features, and the image-to-subcategory distance measure can increase the classification performance."}, {"label": 1, "content": "The recognition of spontaneous facial micro-expressions has become increasingly important as it reveals the genuine emotions of humans. However, handcrafted features employed for recognizing micro-expressions are designed for general applications and thus cannot well capture the subtle facial deformations of micro-expressions. To address this issue, we propose an end-to-end deep learning framework which is tailored to the specific needs of micro-expression recognition (MER). In the deep model, re- current convolutional networks are utilized to learn the representation of subtle changes from image sequences. To ensure the effectiveness of the deep model, we also present a temporal jittering procedure which greatly enhances the training samples. Through experiments on three spontaneous micro-expression datasets - SMIC, CASME, and CASME2, we have verified the effectiveness of our proposed MER approach."}, {"label": 1, "content": "Compared to the visible spectrum image, the infrared image provides much greater clarity in poor lighting conditions. Infrared imaging devices are capable to operate even without the availability of visible light, acquires clear images of objects which are helpful in efficient classification and detection. Successful image object classification and detection have been achieved through the use of CNN, which is a class of feed-forward ANN. Fast RCNN combines advantages of modern CNN detectors i.e. In order to further improve the detection rate and speed of Fast RCNN, this paper proposes two modifications. To further improve the detection rate and speed of Fast RCNN, two modifications are proposed in this paper. One for accuracy in which an extra convolutional layer is added to the network and named it as Fast RCNN type 2, the other for speed in which the input channel is reduced from three channel input to one and named as Fast RCNN type 3.Fast RCNN type 1 has better detection rate than RCNN and compare to Fast RCNN, Fast RCNN type 2 has better detection rate while Fast RCNN type 3 is faster."}, {"label": 1, "content": "Improving the effectiveness of fatigue detection in images and videos is an ongoing challenge. This paper introduces a face image descriptor that can be used for discriminating driver fatigue in static frames. The proposed method works by initially representing each facial image in the sequence with a pyramid divided into non-overlapping blocks of equal sizes. Then the obtained descriptor is filtered out using feature selection. Finally, non-linear SVM is applied to predict the drowsiness state of the subject in the image. Finally, non-linear SVM is then applied to predict the drowsiness state of the subject in the image. This dataset includes a wide range of human subjects of different genders, poses, and illuminations in real-life fatigue conditions. Experimental results show the effectiveness of the proposed method. These results show that the proposed hand-crafted feature compare favorably with several approaches based on the use of deep Convolutional Neural Nets."}, {"label": 1, "content": "We present a real-time vehicle queue length detection method for intersections using image processing technology. We start by capturing an image of the vehicle queue at the intersection and apply automatic brightness adjustment and lane line detection algorithms to account for variations in light intensity and camera shake respectively. Subsequently, we subtract the preprocessed image from the background image to obtain the foreground image of the queued vehicles. Finally, we detect the vehicle queue length using the middle line and measure the actual length through camera calibration. The experimental results show that the proposed method has high accuracy rate and is fast enough for practical application."}, {"label": 1, "content": "We are proud to present our latest invention - an effective low-resolution pedestrian detection system that utilizes targeted pooling and Region Proposal Network (RPN) in the Faster R-CNN framework. Our method is unique in that it rearranges the anchor from the RPN utilizing an optimal hyper-parameter setting known as the \"Elaborate Setup\". Additionally, it refines granularity in the pooling operation from the ROI pooling layer for improved accuracy. Our experimental results demonstrate that our approach, coined LRPD-R-CNN, achieves high average precision and robust performance on the VOC 2007 dataset. This method has great potential in commercial values and wide application prospect in the field of computer vision, security and intelligent city."}, {"label": 1, "content": "In current image registration technology, accuracy in feature points detection and matching feature points has been found to be relatively low. Based on the analysis of SURF feature point detection and information entropy for image registration, an image registration algorithm based on SURF feature points is proposed. Firstly, the image is divided into super-pixels, and the information entropy of each image area is calculated. This enables the elimination of redundant points in feature points, with help from the value of information quantity. The problem that the SURF operator distributes densely is improved and the number of feature points is reduced. Experimental results show that the improved algorithm can improve the accuracy of image feature point pairs, and effectively improve the quality of registration."}, {"label": 1, "content": "Pedestrian detection is a crucial research field in computer vision with broad application prospects, including video security, robotics, and self-driving vehicles. Recently, deep learning methods, e.g., Region Proposal Network (RPN), have achieved major performance improvements in pedestrian detection. To further exploit the deep pedestrian features of RPN, this paper introduces a novel RPN model, RPN_FeaFus, that utilizes feature fusion to enhance detection accuracy. RPN_FeaFus adopts an asymmetric dual-path deep model, constructed by VGGNet and ZFNet, to extract pedestrian features in different levels, which are further combined through PCA dimension reduction and feature stacking to provide more discriminant representation. The low-dimensional fusion features are used to detect the region proposals and train the classifier. Experiments conducted on three commonly used pedestrian detection databases (Caltech, Daimler, and TUD) demonstrate significant performance improvements of RPN_FeaFus over its baseline RPN_BF, making it competitive with state-of-the-art methods in pedestrian detection."}, {"label": 1, "content": "We conducted a study to determine the spatially averaged 1- and 10-gram specific absorption rate (SAR) for PIFA (Planar Inverted- F Antenna)-type mobile phone antennas by utilizing three high-resolution surface-based human models. The use of these human models for SAR estimation had a significant impact on the results, as compared to the previously used SAM (Specific Anthropomorphic Mannequin) phantom. The obtained 1g and 10g peak SAR levels for the human models exceeded those for the SAM phantom by factors of three to four."}, {"label": 1, "content": "In the simulation of a transient electromagnetic problem using the discontinuous Galerkin time-domain method (DGTD), adjusting the polynomial orders in the solution domain locally and dynamically during simulation can achieve similar spatial resolution with a reduced computational cost compared to uniformly high-order polynomials. The resulting dynamic p-adaptation technique has been shown to be very flexible and efficient. However, the explicit time integration method has a limitation on time step size due to the element with the smallest size and highest polynomial order, leading to unnecessarily small time steps for larger elements or lower orders. To overcome this global constraint, a multi-rate time integration technique allowing different time step sizes in different elements is adopted in this work to enhance the efficiency of the p-adaptive DGTD method. An example of an EM scattering problem is provided to demonstrate the effectiveness of the proposed method."}, {"label": 1, "content": "A surface integral equation-based solver for large-scale electromagnetic problems is proposed, with particular attention to accuracy and robustness over a wide range of frequencies and structure sizes. With a focus on achieving high accuracy and robustness over a range of frequencies and structure sizes, the solver employs a differential surface admittance operator to account for the skin effect. This operator is then combined with the augmented electric field integral equation, which is accelerated using the adaptive integral method. The solver was tested on two structures with vastly different electrical sizes over a frequency range of 1 kHz-40 GHz, with results being validated using both direct LU factorization and the finite element method."}, {"label": 1, "content": "In this paper, we present a near-optimal phasing method, referred to as Constructive Analytical Phasing (CAP) to perform beamforming for arbitrary oriented antenna arrays of linearly polarized elements. CAP provides near-optimal phasing to enable directing the beam to any desired point in space swiftly. This simple but efficient method enables us to direct the beam to any desired point in space. In addition, it can be utilized to speed up any further optimization goals for a variety of antenna performance parameters such as pattern synthesis and side lobe level (SLL) reduction."}, {"label": 1, "content": "In this paper, we will analyze the impact of introducing certain hypotheses in optimizing a thinned array. The results are analysed using three different evolutionary optimization algorithms: the Stud-Genetic Algorithm, a very effective implementation of Genetic Algorithm, the binary Particle Swarm Optimization and the binary Social Network Optimization."}, {"label": 1, "content": "This paper discusses a simulation model for a unique indoor localization approach that relies on the phase difference between components in an antenna array. The aim of this localization system is to pinpoint the location of a semi-passive RFID tag. For proof of concept, a set of experiments for phase measurements are designed using a commercially available transceiver. The findings demonstrate that this simulation model provides an accurate estimation and evaluation tool for phase measurements in indoor localization applications."}, {"label": 1, "content": "A novel method for designing thinned arrays with controlled pattern features has been introduced. The optimization of the binary weighting coefficients of the array elements is carried out through a multi-step procedure. Firstly, a Genetic Algorithm (GA) is utilized to identify the binary sequence that has the closest autocorrelation function to the target one. Subsequently, the thinned array is constructed from one of its cyclic sequences, which optimizes a designated figure of merit. To demonstrate the potential of this approach, a preliminary numerical example is provided."}, {"label": 1, "content": "In this paper, we introduce a novel approach for transient electromagnetic problems that harnesses parallelism in both temporal and spatial domains. Our method utilizes space-time domain decomposition formulation and rational approximation of time-domain Green's function to achieve the desired objective. Comparing to traditional space-only parallel algorithms, the method provides high parallelism and significant speedup on high performance computers with a large number of processors. Moreover, it opens up new means of addressing the temporal multi-scale challenge in time-dependent multiphysics problems. Our experiments corroborate our hypothesis and demonstrate the immense potential of our method."}, {"label": 1, "content": "The boundary element method provides a computationally efficient solution to the Electroencephalography (EEG) forward problem on piece-wise homogeneous head models by using surface integral equations. However, accurately modeling the electric nature of the human skull is essential for realistic simulations, which is not feasible with standard surface integral equations. In this work, we present a new formulation that incorporates volume elements within the skull to perturb the standard Lippmann-Schwinger approach. This new surface/volume integral equation can handle computations of fully realistic modeling for both the skull and white matter. Numerical results will confirm the validity of the approach as well as its applicability to real case scenarios."}, {"label": 1, "content": "The authors have recently demonstrated a method for directly measuring the angular velocity of moving objects using distributed radar. The theoretical accuracy of the angular velocity estimation depends on the angular distribution of the electric field intensity, and asymmetry in the distribution will cause the estimate to degrade. In this work, we analyze the effect of asymmetries in the electric field for linear arrays generating a near-optimal dual-beam electric field pattern. We show that for errors of up to 3 dB between the two beam intensities, the estimation accuracy degrades by less than a factor of two."}, {"label": 1, "content": "This paper presents a novel macromodeling approach that allows efficient simulation of large arrays of complex scatterers. In the proposed method, each array element is modeled by a macromodel that is made up of an equivalent electric current density introduced on a fictitious surface surrounding the element and a set of linear discrete operators. The use of this approach leads to a better conditioned linear system, lower memory consumption and faster computations when compared to the standard surface integral equation formulations."}, {"label": 1, "content": "The present article is centered on the development of a convolutional neural network model for detecting the human concealment in images captured from millimeter wave (MMW) scans. The convolutional neural network is applied to the image data set for detection training, pictures are randomly selected for identification, and the target location is marked. This paper proves that convolutional neural network can be applied as a feasible general image detection technology to different detection problems."}, {"label": 1, "content": "We introduce a comprehensive simulation platform that provides an efficient and reliable approach for the design and analysis of computational microwave imaging systems. This platform is based on frequency-diverse metasurface antennas (FDMAs), which are comprised of complementary metamaterial elements with resonant frequencies chosen randomly from a given band of operation. By accurately modeling the fields produced by the FDMA using a dipolar model, it is possible to predict the capabilities of the imaging system in a fast and reliable manner. In contrast to prior work, our approach includes the mutual interactions between metamaterial elements, which are essential for a better understanding of the FDMAs' capabilities such as effective aperture area and radiation pattern correlation. The simplicity and accuracy of the proposed model permits the simulation of different metasurfaces for computational microwave imaging, where traditional antenna design- and metamaterial modeling-are prohibitively costly."}, {"label": 1, "content": "In this contribution, an original approach for invivo estimation of electrical properties of biological tissues is presented. This estimation is a crucial step in hyperthermia treatment planning, where magnetic resonance or computerized tomography images are used to convert the electric parameters based on ex-vivo measured properties to predict the treatment's effects. Since parameters vary from patient to patient, and ex-vivo measurements can be distinct, the proposed approach solves the inverse scattering problem by processing backscattered data from the patient and conveniently uses morphological information on the tissues available from medical images to overcome the issues arising in inverse problems."}, {"label": 1, "content": "In this paper, a novel artificial neural network (ANN) model with three parallel and independent branches is proposed. We also propose a data-classification technique that enables us to categorize antenna geometrical variables properly. Once the geometrical variables are input, the ANN model can simultaneously obtain the antenna VSWR, gain and radiation pattern from each independent branch. The validity and efficiency of this proposed model are confirmed with an optimization design of an ultrawide band (UWB) antenna examnle."}, {"label": 1, "content": "Earlier work devised a shape synthesis approach for closely-spaced electrically-small antennas operating at different frequencies. The method uses the substructure characteristic mode concept. Definition of the objective function of this shape optimization process in terms of such modes allows the feedpoint to only be specified after shaping is complete. In this paper, some of the problems mentioned in the earlier work have been resolved, and an example of the shape optimized antennas is presented."}, {"label": 1, "content": "This study is concerned with meta-materials made of thin wire inclusions arranged in an infinite periodic structure. The polarizability coefficients and the constitutive parameters are extracted by using Lorentz or Maxwell-Garnett homogenization techniques. The validation is performed by calculating the transmission and reflection coefficients for incident fields illuminating the structure from three directions. The results are compared to simulation results from the EM commercial software HFSS. The study highlights the importance of considering the interaction fields within the periodic structure when extracting its polarizability coefficients."}, {"label": 1, "content": "This paper presents an improved adaptive beamforming solution utilizing the Global Sidelobe Canceler (GSC) and Least Mean Square (LMS). By analyzing the impact of step size on LMS performance, a simple yet effective new Cyclic Variable Step Size (CVSS) algorithm is proposed to enhance the convergence speed and excess MSE performance of LMS. By adding just a few shift operations to adapt the step size cyclically, the CVSS LMS is efficient for hardware implementation. The initial study shows that it's possible to double the convergence speed with minimum overhead. The numerical simulation confirmed the effectiveness and performance of the algorithm."}, {"label": 1, "content": "An inhomogeneous absorbing boundary condition (IABC) is proposed to efficiently perform scattering analysis with the presence of a stratified medium using the finite element method. The analytical expressions for the incident fields in the right-hand side of the IABC are derived. The parallel dual-primal finite-element tearing and interconnecting (FETI-DP) algorithm is adopted to enable large-scale electromagnetic simulations. A multilevel fast multipole algorithm (MLFMA)-based far-field calculation is developed to reduce the computational complexity. The proposed method is demonstrated through various examples, which show high solution accuracy and large-scale modeling capabilities."}, {"label": 1, "content": "Indoor positioning systems often experience errors caused by multipath signal reception which can lead to significant distortion of the correlation function used for time-delay estimation. When a band-limited signal is used, the range resolution is often insufficient to decompose the overlapped received signals. This paper presents a multipath mitigation algorithm for ATSC DTV signal-based positioning systems. The algorithm combines sample interpolation with a multipath signal delay estimator based on an equalizer which periodically suppresses a specified number of the lowest-magnitude equalizer weights. The primary objective of the new equalizer is to more accurately predict the channel response. The proposed method was validated through MATLAB simulations and field tests. The proposed approach can reduce the range estimation resolution by 60% compared to the Fourier limit."}, {"label": 1, "content": "This paper proposes novel reflectarray resonant elements designed by genetic algorithm(GA), which have arbitrarily phase difference from -180\u00b0 to 180\u00b0 between the TE (V-pol.) and the TM (H-pol.) polarizations from 13 GHz to 17 GHz. Moreover, these elements have two axially symmetric structure in the unit cell so that the cross polarization level is suppressed less than about -60 dB for the wide frequency range. A reflectarray antenna with two-separated offset feeds has been designed by utilizing these elements in the Ku-band, and its effectiveness has been further verified by evaluating radiation patterns numerically."}, {"label": 1, "content": "The total delivered power of an MRI radio frequency (RF) coil is an important parameter for MRI safety control. For a 2-channel MRI coil, this parameter is not only dependent on the excitation strength on each channel, but also dependent on the object loading inside the coil. This study aimed to propose a rapid method for assessing the total delivered power of such coils. The proposed method was validated via 490 simulations of a high-resolution anatomical model. This method was tested using 10 random excitation-pairs of the 2-channel coil and 49 patient landmarks ranging between -200 to 1000 mm. The results showed that this fast estimation method can calculate the total delivered power of a 2-channel RF coil for any excitation pairs using only two simulations, with a relative error of less than 1.5%."}, {"label": 1, "content": "In this paper, two novel computational processes are proposed to solve Finite-Difference Time-Domain (FDTD) based on machine learning deep neural networks. The field and boundary conditions are employed to establish recurrent neural network FDTD (RNN-FDTD) model and convolution neural network FDTD (CNN-FDTD) model respectively. Numerical examples from scalar wave equations are provided to benchmark the performance of the proposed methods. The results demonstrate that the newly proposed methods could solve FDTD steps with satisfactory accuracy. According to our knowledge, these are unreported new approaches for machine learning based FDTD solving methods."}, {"label": 1, "content": "The Equivalence principle algorithm is proposed to integrated with hierarchical matrix based fast direct solver in order to solve deep multi-scale problem accurately and efficiently. Additionally, model order reduction techniques are applied to the equivalence principle algorithm to further reduce the computational complexities."}, {"label": 1, "content": "In this two-part paper we present a local inverse scattering approach using pulsed-beam frame (PBF) processing. In Part I, we analyzed the scattered fields in the beam-domain and showed that, under the Born approximation, the beam-domain data can be linked to the local Radon transform of the medium. Building upon this finding, in Part II, we present a local reconstruction algorithm employing pulsed beam back-propagators."}, {"label": 1, "content": "A bi-linear approach is proposed to efficiently design planar sparse antenna arrays with rectangular boundary. On account of practical manufacturing and cost limitation, it deals with x-y de-coupled planar sparse arrays whose array factor could be decomposed into two linear array factors. Computational cost for both radiation pattern and array synthesis accordingly is significantly reduced. It is successfully applied to design an array with low sidelobe level."}, {"label": 1, "content": "A 3D-printed air-filled substrate integrated waveguide topology has been proposed to address the issues that have been hampering the performance of dielectric-filled substrate integrated waveguide based components, while still retaining their most appealing features. The proposed topology is showcased through the design of a 3D-printed coaxial-to-AFSIW transition and a power divider/combiner. Based on the measurement results, it has been confirmed that the proposed topology offers exceptional performance in terms of impedance matching and bandwidth."}, {"label": 1, "content": "We aim to create intelligent models that can identify the path loss characteristics of an environment solely through analyzing the geometry. These machine intelligence models can be trained by physics-based propagation analysis techniques such as ray-tracing or the parabolic equation method, to eventually develop an \u201celectromagnetic vision\u201d: the ability to provide the propagation properties of a channel based on a formally defined input file (or a sequence of images) that contains the channel's geometric specification. In our initial efforts to achieve this goal, we have successfully trained a model to recognize the path loss exponent in rectangular tunnels, demonstrating a strong correlation between model predictions and both measured and simulated data."}, {"label": 1, "content": "The synchronization ATSC DTV signals are designed for channel modeling and multipath mitigation, and consequently can be used for accurate positioning. In this paper, we present our study of a high accuracy range estimation method for DTV signal based positioning system. Our proposed method boasts of being robust, fast, accurate, and easy to implement into conventional RF receivers for indoor positioning and tracking applications. Performance analysis was performed using Matlab simulated data, and the results were corroborated with raw field data."}, {"label": 1, "content": "This paper presents a compact and highly-efficient inverted-F antenna (IFA) suitable for Internet of Things (IoT) applications. In order to enhance the radiation efficiency, we integrated a slotted ground structure into the antenna. Compared to a conventional IFA on a solid ground plane, the proposed structure enhances radiation efficiency approximately 100%, from 14% to 27%, at 750 MHz. Simulated and measured results are in an excellent agreement, fully validating the IFA response."}, {"label": 1, "content": "This paper introduces a novel approach to the detection and classification of buried landmines in sandy soil with various soil conditions and at different depths. Prony's method is used to extract the complex natural resonance (CNRs) frequencies from the reflected landmine signals. These CNRs are used as new features for detection and classification of buried landmines. Different classification methods are employed to investigate the validity of the proposed method."}, {"label": 1, "content": "In this paper, an array of multiband Wi-Fi antenna using genetic algorithm (GA)is proposed and optimized. The antenna array used for Wi-Fi communications is designed and simulated using CST Microwave Studio. We present the detailed design and simulation results of the two-element antenna array, including the return loss and gain of the antenna array. Our proposed array significantly improves the gain in both 5.2 GHz band and 5.8 GHz bands, which are commonly used for Wi-Fi communications."}, {"label": 1, "content": "A well-known numerical integration scheme for weakly near-singular integrands on triangle domains, is the Radial-Angular-RI-Sqrt near-singularity cancellation transformation quadrature scheme. Such integrals feature routinely in the method of moments (MoM), for integral equation based numerical electromagnetic field calculations. Recently, a closed-form error estimation for the quadrature scheme has been suggested. In this paper, the estimate is further improved, such that its range of applicability is extended."}, {"label": 1, "content": "We demonstrate the application of Double Debye theory combined with Finite-Difference Time-Domain technique to model terahertz wave interaction with breast tumor tissues. It has been shown that terahertz signals are strongly absorbed by water. In particular, terahertz technology presents a promising method for contrast-based imaging of breast cancer tissues. However, while our simulations have revealed a clear contrast between fatty and cancerous breast tissues, the low contrast between cancerous and fibroglandular tissues (normal tissue) represents a significant challenge. Therefore, computer simulations are needed to investigate a method to manipulate and enhance the contrast between these tissues for the sake of using terahertz technology in tumor margin assessment."}, {"label": 1, "content": "This article presents the measurement results of the Off-Body channel characterization for Ultra-Wide-Band Communications in an underground gold mine. The frequency measurements ranging from 3.1 GHz to 10.6 GHz are presented in terms of the channel parameters, impulse response, and channel capacity. The research highlights that the channel parameters tend to degrade in non-line-of-sight configurations compared to line-of-sight ones. The time dispersion, path loss and capacity results demonstrate that the UWB channel is better suited for the in-mine communications than the 2.4 GHz band at line of sight and non-line-of-sight scenarios."}, {"label": 1, "content": "Scattering properties of multiple-layer anisotropic metasurfaces are characterized based on generalized boundary conditions. Once surface susceptibilities of all layers are determined, reflection and transmission coefficients could be readily derived for arbitrary incident angles and polarizations. This analytical method has been thoroughly tested and validated through comparison with full-wave simulations."}, {"label": 1, "content": "Coherent distributed arrays are composed of widely separated elements which are coherently coordinated for distributed phase-coherent operations. However, their inherent sparsity leads to significant sidelobe energy in radiation patterns. In this work, we explore the use of a two-step optimization routine to mitigate sidelobe energy within a window around the mainbeam in extremely sparse arrays. We use a linear sparse linear array consisting of 9 distributed elements within a span of 1000A, with a minimum distance of 10\u03bb separation. The two-step optimization consists of an initial genetic algorithm (GA) optimization routine as a coarse layout design after which a particle swarm optimization (PSO) routine is used to refine the solution. We show that the GA achieves > 10 dB sidelobe suppression within a 10\u00b0 region, and the PSO step improves this by > 1.5 dB."}, {"label": 1, "content": "We propose a cost-effective method for optimizing the design of antenna structures with multiple objectives. Our approach exploits variable-fidelity EM simulations, kriging surrogates, and domain segmentation to reduce the volume of the design space to be sampled for the sake of surrogate model construction. Design space compartments created during segmentation are enforced to be of equal volume which reduces the number of training samples as compared to the standard segmentation. The operation and performance of the proposed technique is demonstrated using a UWB monopole antenna example."}, {"label": 1, "content": "Nowadays, deterministic channel modeling methods play an increasingly important role in the research of wireless communication. As the modeling process demands a higher degree of precision, researchers are driven to explore ways to increase computational speed and reduce complexity. Through the improved vector algorithm proposed in this paper, the ray-tracing simulation is able to achieve better collision detection in complex 3D scenarios. Moreover, these methods will guide the construction of fast collision detection algorithm in ray-tracing propagation simulation."}, {"label": 1, "content": "This paper addresses shooting-bouncing rays (SBR) ray-tracing techniques and their applications in computational electromagnetics. It specifically discusses uniform random, low-discrepancy deterministic, and uniform sampling techniques for the standard SBR formulation and compares their effects for a simple waveguide model."}, {"label": 1, "content": "This paper presents a dual-beam wearable antenna based on AMC- FSS technology with a transmission type polarizer to convert linear to circular polarization at 5.8 GHz. The design is composed of 3 layers, an AMC substrate, a planar monopole and a tilted 45\u00b0 cross-slots FSS superstrate. The structure provides a 1.69 dB of axial ratio purity polarization and a valuable gain of 7.32 dBi at the ISM band. The dual beam radiation pattern of the antenna makes it an excellent option for Off-Body underground communications."}, {"label": 1, "content": "According to the operation mode of sky wave over-the-horizon radar(OTHR), the detection of targets is greatly affected by the ionosphere. The ionosphere's instability can create a range of distortions and contaminate the echo signal. Here we propose a method based K-means Clustering to correct doppler frequency shift in the range-doppler domain. Our extensive experiments using measured data demonstrate the effectiveness of the proposed approach. Additionally, we also compare this algorithm with other methods."}, {"label": 1, "content": "A 60GHz Yagi-Uda circular array antenna with an omni-directional pattern for millimeter-wave WBAN applications has been proposed. Center fed octagonal plates is used for feeding 8 Yagi-U da antennas located at the vertices of top and bottom octagonal plates. To enhance the gain, each arm of the dipole element has been bent. The simulated 10-dB return loss bandwidth is 4085 MHz (57.17 GHz - 62.02 GHz) which covers a potential frequency band of 5G communication. The simulated results show that the proposed antenna has an omni-directional radiation pattern toward the body surface with the maximum gain of 2.86 dBi at 60 GHz."}, {"label": 1, "content": "This research paper introduces a novel particle swarm optimization (PSO) algorithm that utilizes simulated annealing (SA) to synthesize sparse linear arrays. This algorithm is used to optimize the element positions to suppress the peak side lobe level (PSLL) of the array with many constraints. With this method the freedom increase amounts are used as optimization variables, which plus the minimum element spacing constraint is deemed as element spacing of the array. And the method has downsized the searching region by indirect expression of element spacing. The simulated optimization results substantiate the superior suppression capability of PSLL by the proposed algorithm."}, {"label": 1, "content": "Controlling the propagation of electromagnetic (EM) waves through a dielectric medium has applications in antenna miniaturization and beam-forming. In a recent study, researchers have introduced an innovative approach by embedding conducting micron-sized particles into the conducting strips. The work presented in this paper demonstrates a new way to implement these conducting strips. More specifically, conducting micron-sized particles are embedded into the design of the conducting strips and are used to control the EM response of the overall host dielectric. As the particles are conducting, this results in conducting columns that connect various conducting strips. When a field is applied, the particles within a dielectric cavity form columns in the direction of the field lines. The theoretical and simulation results validate the effectiveness of this method. The advantage of using these particles is that a directly connected biasing circuit is not required and allows for the placement of these particles in very complex geometries. Finally, theory and simulations are shown to agree and validate the overall results."}, {"label": 1, "content": "In this study, our focus is on exploring the application of the supervised descent method in solving two-dimensional electromagnetic inverse problems. The inversion process contains two stages: offline training and online prediction. In the offline stage, the average descent directions of cost functions are learned from a set of training data; and in the online stage, model reconstruction is achieved through iterative minimization based on learned descent directions. This scheme offers a new perspective to incorporate prior information into inversion, and reduce the computational complexity in the online inversion. Numerical examples demonstrate the effectiveness and efficiency of this approach, validating its accuracy in solving electromagnetic inverse problems successfully."}, {"label": 1, "content": "The development of educational information technology (IT) training is crucial to ensure progress in education. As a developed country of education, Japan has many excellent achievements in the field of faculty development (FD). In order to enhance the progression of faculty development, our study has examined the characteristics and trends prevalent in Japanese universities by drawing upon historical development and literature reviews. Our study delves into a thorough investigation of the historical context, characteristics and trends of faculty development in Japanese universities."}, {"label": 1, "content": "Learning Analytics has gained widespread popularity in various research domains. Although many researches have been done in LA, there are still numerous issues and challenges such as how to utilize LA results to support learning. To address this challenge, our paper suggests a feedback system that operates on a digital book platform. We collected user feedback regarding the performance of the said feedback system."}, {"label": 1, "content": "We present NavWalker, a flexible random walk-based approach for learning the representations of vertices in an information network. This approach allows different walk strategies to be incorporated into the sampling process, thereby boosting network embedding techniques. Our method integrates the adjacency matrix of a network with a pre-defined information augmentation matrix. In contrast to SkipGram-based methods such as DeepWalk and Node2vec, which rely solely on local network information, NavWalker is adaptable to include global or auxiliary network information when guiding the sampling process. Empirical results on six real-world datasets confirm NavWalker's flexibility and superior performance in classification and recommendation tasks, outperforming current state-of-the-art network embedding algorithms."}, {"label": 1, "content": "There has been a significant increase in the installation of public Internet of Things (IoT) devices in urban areas, providing users with a range of tasks that can be performed. However, due to the nature of public spaces, these devices must support groups of users rather than just individuals. Unfortunately, the types and quality of IoT devices can vary, making it difficult for groups of users to identify opportunities to perform tasks. Additionally, group users are often unfamiliar with the public space and have not previously used IoT devices in such environments. In this paper, we propose a two-phase task recommendation approach for groups of IoT users in public environments. In the first phase, we employ a random walk with restart (RWR) algorithm to overcome the problem of sparse historical data for the performance of user tasks in public IoT environments. The second phase predicts a set of operations (IoT device functionalities) that are most appropriate for each candidate task. In this phase, to more effectively predict IoT operations for a user task we consider the contextual semantics of users via a classification model. Our approach was evaluated using real-world datasets collected from practical IoT testbed environments. In addition, we show that an appropriate set of task operations can be predicted effectively by considering task types and contextual semantics."}, {"label": 1, "content": "Exploring people's perception of urban areas has been a fascinating research endeavor that requires a multidisciplinary approach. To this end, we propose an approach that explores spatial and semantic aspects in free-text messages shared on location-based social networks (LBSNs) for uncovering and mapping the perception reflected regarding urban outdoor areas. By studying the outdoor areas of Chicago, we learned that LBSN data contains valuable information about different places and can help extract urban perception, enabling better understanding of urban areas from a variety of perspectives. We demonstrate, through a survey with volunteers, that our approach has the potential to correctly capture the opinion considered by the users regarding the reflected perception of those areas, indicating that it could be a feasible alternative for the task under study."}, {"label": 1, "content": "Recommendation methods usually use users' historical ratings on items to predict ratings on their unrated items to make recommendations. However, the limited amount of rating data available can impact the quality of recommendations. To address this issue, additional information is often used to supplement the user's preferences and enhance the recommendations. This paper introduces a new recommendation model that utilizes adversarial learning between auto-encoders to enhance recommendation quality by minimizing the disparity between user and item reviews and their corresponding ratings. The proposed approach is shown to improve recommendation performance based on empirical studies conducted on real-world datasets."}, {"label": 1, "content": "Scientific workflows define computational processes needed for carrying out scientific experiments. There are numerous repositories of scientific workflows, which contain a wealth of materials and knowledge to help scientists design workflows for running related experiments. Identifying reusable fragments in growing workflow repositories has become increasingly important. In this paper, we present PSM-Flow, a probabilistic subgraph mining algorithm designed to discover commonly occurring fragments in a workflow corpus using a modified version of the Latent Dirichlet Allocation algorithm. The algorithm employs a modified version of the Latent Dirichlet Allocation algorithm and encodes the geodesic distance between workflow steps to implicitly model fragments. PSM-Flow captures variations of frequent fragments and maintains its space complexity bounded polynomially, as it requires no candidate generation. The authors applied PSM-Flow to three real-world scientific workflow datasets containing over 750 workflows for neuroimaging analysis. Our results show that PSM-Flow outperforms three state of the art frequent subgraph mining techniques. The authors also discuss possible future improvements to the proposed method."}, {"label": 1, "content": "In recent years, the area of sentiment analysis has undergone significant developments, particularly in the domain of aspect-based sentiment analysis. More specifically, there has been growing interest in aspect-based sentiment analysis in which the goal is to extract, group, and rate the overall opinion about the features of the entity being evaluated. Techniques for aspect extraction can produce an undesirably large number of aspects - with many of those relating to the same product feature. This problem is further compounded when reviews are written in multiple languages. In this paper, we address the novel task of multilingual aspect clustering which aims at grouping together the aspects extracted from reviews written in several languages. We contribute with a proposal of techniques to tackle this problem and test them on reviews written in five languages. Our experiments demonstrate that our unsupervised clustering technique outperforms a semi-supervised baseline in several cases, providing promising results for this important aspect of sentiment analysis."}, {"label": 1, "content": "Excessive alcohol consumption is a worldwide problem, and social networks such as Twitter can provide valuable data that help understanding factors related to alcoholism, particularly among youngsters. The identification of drunk tweets (i.e. Identifying drunk tweets (tweets posted under the influence of alcohol) is challenging due to the platform's limitations, including short length, scarce text, varied Internet-specific terms, and possible language errors from alcohol influence. To address this, we propose a framework that integrates conceptual and semantic features to augment vocabulary and provide context to terms, improving detection accuracy. The framework also accounts for misspellings and chooses discriminative features from context. We outperformed the baseline, achieving improvements of 13.79 percentage points in recall, with no significant harm to precision. We explore the value of drunk tweet classification by analyzing demographics and tweet properties of this group."}, {"label": 1, "content": "The identification of prerequisite relationships among concepts is a fundamental step toward the organization of knowledge for educational purposes. In the context of a learning process, simplest concepts that are requirements to understand and address more complex concepts should be presented first. Therefore, the identification of prerequisite relationships is a fundamental step for effective course design and automatic learning path generation systems. Although there have been recent advances in machine learning methods for the automatic identification of prerequisite relationships between concepts, little research has been done on whether these automatic strategies can be extended to establish precedence relationships among learning resources. The strategy analyzes the prerequisites among the concepts covered by the resources to estimate the precedence relation. In this paper, we approach this problem and propose a strategy to identify the precedence relation. The results indicate that it is possible to identify the precedence relation between learning resources through the automatic identification of prerequisite relationships between concepts. A set of 1588 pairs of learning resources extracted from MOOCs refined by human experts is used to evaluate the strategy. The experimental results show that it is possible to identify the precedence relation between learning resources through the automatic identification of prerequisite relationships between concepts."}, {"label": 1, "content": "Verbalizing the knowledge base (KB) facts of an entity helps users understand the information contained in the KB more easily. The drawback of most previous work is that they cannot generalize to unseen frames. To overcome this limitation, this paper presents the task of precise KB verbalization, which involves generating an exact description of given factual triples. To achieve this, a novel sequence-to-sequence (seq2seq) model is proposed, which incorporates a local pointer network. Additionally, the approach to training data construction is explored. Experimental results show our method improves the performances in terms of Meteor and slot error rates. Human evaluation is also performed to confirm the effectiveness of our model."}, {"label": 1, "content": "In this work, we introduce LASAGNE, a method for unsupervised learning of graph node embeddings that takes into account locality and structure awareness. In particular, we show that the performance of existing random-walk based approaches depends strongly on the structural properties of the graph, e.g., the size of the graph, whether the graph has a flat or upward-sloping Network Community Profile (NCP), whether the graph is expander-like, whether the classes of interest are more k-core-like or more peripheral, etc. For larger graphs with flat NCPs that are highly expander-like, the quality of the vector representations obtained through these methods is often lower due to the rapid expansion of random walks touching dissimilar nodes. LASAGNE circumvents this issue by utilizing localized Approximate Personalized PageRank stationary distributions to incorporate more precise local information into node embeddings, rather than relying on global random walks or fixed hop distances. This leads, in particular, to more meaningful and more useful vector representations of nodes in poorly-structured graphs. We show that LASAGNE leads to significant improvement in downstream multi-label classification for larger graphs with flat NCPs and that it is comparable for smaller graphs with upward-sloping NCPs."}, {"label": 1, "content": "This paper investigates the effect of social influence on collective intelligence, which can result in the deviation of individual decisions from the expected collective decision. An important application of collective intelligence is national election predictions, which may encounter spectacular failures. While multiple explanations have been put forth by experts, the paper suggests that the primary reason for these failures is social influence among different types of voters. The 2015 UK Election as a case was studied, which demonstrates that such influence is intrinsic to collective intelligence. To address this issue, the paper proposes a new social influence-based prediction model, which is designed to address the shortcomings of current models. The experiments conducted on this model show that it can effectively account for the influence of social factors in collective decision-making."}, {"label": 1, "content": "This research leads to the development of a system that generates minutes by using repeated questions and answers between a user and the system. The goal of this system is to provide users with an interactive analysis of meeting records with a specific emphasis or perspective. To realize the system, we propose a novel tree structure dedicated to representing the hierarchical discussion structure of a meeting based on the relative importance of utterances underlain by the semantic structure of meeting records. The system utilizes a number of flexible extraction methods, which include intention extraction, summarization, and viewpoint switching, among others, to extract the necessary information from meeting records. The results of a user experiment showed that the system allows for effective exploratory information retrieval. Additionally, the study revealed that users of the system showed a significant improvement in their level of comprehension as opposed to when they did not use the system."}, {"label": 1, "content": "Tracking technologies and location-acquisition have led to the increase of the availability of trajectory data. Many efforts are devoted to develop methods for mining and analysing trajectories due to its importance in lots of applications such as traffic control, urban planning etc. This research paper introduces a new trajectory analysis and visualization framework for analyzing massive movement data. This framework leverages formal concepts, sequential patterns, emerging patterns, and analyses the evolution of mobility patterns through time. Tagged city maps are generated to display the resulting evolution analysis and directions at different spatio-temporal granularity values. Experiments on real-world dataset show the relevance of the proposition and the usefulness of the resulting tagged city maps."}, {"label": 1, "content": "As predicted by Internet Data Center (IDC), the amount of global language data will exceed 40ZB by 2020. With the globalization of information, it has become an urgent matter for current web retrieval to break the barriers between languages. To that end, this paper proposes the integration of semantic and lexical information to facilitate cross-language information retrieval (CLIR). The approach does not rely on external knowledge bases thus to avoid that knowledge bases cannot deal with net neologism. Experiments on Sogou dataset show the feasibility of the approach."}, {"label": 1, "content": "In recent years, deep neural networks (DNNs) have become increasingly popular due to their exceptional performance in various visual and speech recognition tasks. As the scale of tasks that need to solve is increasingly big, the networks used also become wider and deeper, resulting in millions or even billions of parameters needed. Deep and wide networks with large number of parameters bring many problems, including memory requirement, computation cost and overfitting, which severely hinder the application of DNNs in practice. To overcome these challenges, researchers have proposed training sparse networks with fewer parameters and floating point operations while maintaining performance. This paper surveys sparsity-promoting techniques in DNNs proposed in recent years, which can be broadly divided into three categories: pruning, randomly reducing complexity, and optimizing with sparse regularizers. Pruning involves removing connections or entire neurons from a network, while random reduction involves randomly removing a portion of the network's connections or neurons. These approaches are roughly divided into three categories, including pruning, randomly reducing the complexity and optimizing with sparse regularizer. Pruning techniques will be introduced first and others will be described in the following section. For each kind of methods, we present approaches in this category, strengths and drawbacks. In the final, we will discuss the relationship of these three categories of methods."}, {"label": 1, "content": "Air pollution has become a global concern that has a significant impact on human health. Consequently, there is a need to explore methods for forecasting air pollutant severities in advance. This study proposes the use of a bi-directional LSTM model for predicting air pollutant levels ahead of time. We have shown that the predictions can be significantly improved using an ensemble of three Bi-Directional LSTMs (BiLSTM) that model the long-term, short-term and immediate effects of PM2.5 (the key air pollutant) severity levels. Further, weather information data has been taken into account while modelling, since they are found to boost prediction accuracies. Experimental results for multiple locations in New Delhi, India are presented to demonstrate model superiority over earlier techniques."}, {"label": 1, "content": "We investigate automated methods for generating tag-clouds for Computer Science researchers using keyphrase extraction methods and learning-to-rank models. To extract keyphrases, we extract links to PDFs of papers from a bibliographical database (currently DBLP) based on an author's identifier. We then apply keyphrase extraction methods to extract multi-term tags from the text. In order to select the most important tags for the researcher, we propose a set of features that serve as input for a variety of learning-to-rank models. We evaluate our methods using 12 Computer Science professors who score a selection of keyphrases extracted from their papers based on relevance. These scores are used to train and compare various learning-to-rank models for reordering the most important keyphrases, which generate the final tag clouds. We validate our approaches by asking professors to evaluate the final tag-clouds."}, {"label": 1, "content": "Sentiments are feelings, emotions likes and dislikes or opinions which can be articulate through text, images or videos. Sentiment Analysis on web data is now becoming a budding research area of social analytics. Users express their sentiments on the web by exchanging texts and uploading images through a variety of social media like Instagram, Facebook, Twitter, WhatsApp etc. A lot of research work has been done for sentiment analysis of textual data; there has been limited work that focuses on analyzing the sentiment of image data. Image sentiment concepts are ANPs i.e. Adjective Noun Pairs automatically discovered tags of web images which are useful for detecting the emotions or sentiments conveyed by the image. The major challenge is to predict or identify the sentiments of unlabelled images. Some of the noteworthy models of deep learning in image sentiment analysis include Deep Neural Network (DNN), Convolutional Neural Network (CNN), Region-based CNN (R-CNN), and Fast R-CNN. Each of these models has strengths and limitations in different applications. Overall, image sentiment analysis is a rising field that presents challenges and opportunities for future research. Deep learning techniques have shown promise in this area and are likely to play a significant role in the development of new models and approaches for image sentiment analysis."}, {"label": 1, "content": "We introduce Grail, a framework designed to equip developers with the necessary tools to implement Artificial Intelligence (AI) in games. Given the diversity of games, the usage of AI in each game can vary significantly. Thus, the main challenge is to create a system allowing for meeting various design goals with relatively easy to use interfaces. The conceptual architecture of Grail is presented, along with the algorithms that have been carefully selected to cover a wide range of use cases such as Planning, Utility System, Simplified Games with Tree Search, and scripting. We are confident that these algorithms together create a flexible AI engine capable of meeting the demands of various games."}, {"label": 1, "content": "This paper demonstrates how different machine learning techniques performed on a recent, partially labeled dataset (based on the Locked Shields 2017 exercise) and which features were deemed important. Additionally, the outcomes were analyzed by a cybersecurity expert, who validated the fact that the models were proficient in distinguishing between known and malicious intrusions, and even uncovered novel attacks. The study included the detection of 500 anomalies, out of which 50 stood out as previously undiscovered incidences. Given that such observations are uncommon, this indicates how well an unlabeled dataset can be used to construct and to evaluate a network intrusion detection system."}, {"label": 1, "content": "The use of SCADA systems in modern industrial processes represents a high cost to the industrial sector due to licensing payments, difficulty in connecting to RTUs from different manufacturers, support only for industrial communication protocols and limited scalability when using a structured database. In this work, a low cost SCADA system was developed, with the possibility of connecting to an RTU based on IoT technology that makes universal communication with field devices through TCP / IP and supported on a NoSQL database. The system's versatility was evaluated by incorporating a semaphore field device into a traffic management process, demonstrating response times comparable with those of commercial alternatives."}, {"label": 1, "content": "This paper presents a design for a rectangular microstrip antenna patch that has been applied to a WBAN (Wireless Body Area Network). The antenna has been designed to operate at a frequency of 2.4 GHz, utilizing FR-4 substrate material with a dielectric constant of 4.3. The designed antenna can be used for ISM (industrial, scientific and medical) band and UWB applications. The designed antenna has low profile, low cost, easy fabrication and good isolation. To design the antenna, CST simulation software was used, resulting in a return loss of less than -10 dB. The proposed antenna has been designed specifically to obtain low SAR (Specific Absorption Rate) models of the human body, taking into account both the electromagnetic effect and the shell model in human tissues."}, {"label": 1, "content": "The use of wireless sensor networks for collecting geolocated sightings of marine fauna can provide useful information for wildlife conservation agencies and scientific studies. Both, networking services and data quality can benefit from the integration of social paradigms in the design of these networks. This study proposes the implementation of a wireless sensor network that incorporates social interactions among network nodes, also known as the Social Internet of Things (SIoT). The network employs smartphones as sensors to monitor wildlife, with the support and input of the community, creating a communality between the nodes. The information obtained is centralized and managed through a web platform, allowing the community to share and visualize data. The results demonstrate the collection of hundreds of high quality sightings in the project's initial months of operation."}, {"label": 1, "content": "Diagnosing childhood pneumonia by analyzing chest radiographies is a crucial task that physicians perform to determine further clinical decisions and treatments. Pattern recognition techniques that would help automate the classification of chest radiographies into absence and presence of pneumonia with high reliability are being researched and tested, but it is still an open problem and this automate process have a high computational cost. To address this problem, this paper introduces a CUDA-based parallel algorithm that can significantly improve the speed of a feature extraction process using wavelets applied to high-resolution DICOM images. By using wavelet features to detect pneumonia and utilizing the proposed parallel technique, the computing speed can be increased more than 12.75 times."}, {"label": 1, "content": "Piscirickettsia salmonis is a highly transmissible pathogens that cause high mortality in farmed salmonids. In this way, new techniques based on mass spectrometry (MS) and machine learning were applied and combined in an automatized platform in order to classify and predict this pathogen, in a faster and effective way. By analyzing serum samples from healthy and diseased salmonids using MALDI-MS coupled with machine learning analysis, researchers were able to obtain a specific and sensitive pattern (m/z) for every pathogen, ensuring high reproducibility. Results showed that the combination of these two techniques was a powerful tool in early detection of the pathogen, with an accuracy rate of over 80%. This platform has the potential to serve as an effective tool for early disease control in the salmon farming industry."}, {"label": 1, "content": "This article presents a navigation simulation based on computer vision of the Khepera IV robot model (KH4VREP library) in the V-REP simulator. The KH4VREP library is utilized, and images acquired by the robot are externally processed with the OpenCV library using a script in Python. This library has implemented many optimized machine learning algorithms and will now be implemented in the discipline of machine vision, so some robot speed control experiments are implemented to test this approach. The main focus of this work is to introduce students to the control of mobile robots through the use of artificial vision."}, {"label": 1, "content": "Frequency identification is an essential consideration when it comes to power converters that are connected to the grid, particularly in instances where the grid is distorted or unbalanced. Therefore, this paper proposes a novel frequency estimator with fast convergence for balance and unbalanced grid-voltage faults. The new algorithm leverages a Phase Locked Loop (PLL) that's been boosted with a positive and negative sequence detector, allowing for quick convergence. Additionally, the Delayed Signal Cancellation methodology with fast convergence is looked at in cases where a voltage dip occurs in either one or two phases. The effectiveness of the proposed PLL is validated using simulations and experimental results."}, {"label": 1, "content": "Conventional methods for diagnosing anemia involve drawing blood, which can be a major problem for patients who are afraid of needles or have sensitivities. This generates a great problem in patients due to the fear of contracting a disease through syringes, or sensitivity to this element. The palpebral conjunctiva is an indicator of diseases such as the hordeolum, chalazion, marginal blepharitis, bacterial conjunctivitis, trachoma, and anemia. To achieve this goal, a team of researchers developed an Android application and utilized image processing techniques to automatically segment the palpebral conjunctiva membrane. The results were impressive, with a 92.2% success rate in the segmentation process. This breakthrough has the potential to significantly improve the diagnosis and treatment of anemia while providing a more comfortable experience for patients."}, {"label": 1, "content": "In recent years, there has been an increased interest in the non-intrusive monitoring of electrical systems due to lower costs and space requirements. Machine learning techniques have proved their ability to predict the parameters under monitoring and consequently improve the performance of power electronics systems. The present work seeks to determine the combination of machine learning techniques and dimensionality reduction that efficiently predicts the inductance value for a Voltage Source Inverter's Modulate Model Predictive Control (VSI_M2PC). The problem is modeled as a classification task with three classes, and it has a high dimensionality of 5000 attributes. Consequently, its reduction is needed to make it tractable at the cost of slightly sacrificing the accuracy of the model. Seven machine learning methods were tested: Support Vector Machine, K-Nearest Neighbors, Na\u00efve Bayes, Linear Discriminant Analysis, Classification and Regression Trees, C4.5, and Random Forest. Additionally, the strategies for dimensions reduction Correlation Elimination, Principal Component Analysis, and Boruta were experimentally studied on VSI_M2PC Matlab simulations. It was found that Random Forest combined with the Boruta provided the best results regarding classification efficiency."}, {"label": 1, "content": "Simulation-based training is an interdisciplinary approach that has been widely adopted in the medical profession to enhance the learning process of students which, in turn, ensures their professional competence. While this education methodology employs the use of new technologies in simulated scenarios such as hospitals and classrooms, it should be noted that it involves teamwork and communication skills, rather than mere technology. This article focuses on the development of a training system for auscultation processes utilizing a phantom and a modified stethoscope to generate body sounds. In this context, this work is about the development of a training system for auscultation processes using a phantom and an adapted stethoscope to generate body sounds. The phantom network is also capable of generating its own wireless connection, allowing students to connect their smartphones and use a mobile application to configure settings within the system. The phantom creates its own wireless network to connect a smartphone and use a mobile application to configure the settings of the system. The implementation and first practical results of this system are presented here, with a discussion about future work. Overall, this simulation-based training system serves as an innovative and effective approach to enhance the learning process of students, particularly those in the medical profession, resulting in improved professional competence."}, {"label": 1, "content": "Driving simulators have various applications in training vehicle drivers as well as in the gaming and entertainment industries. By minimizing the use of actual vehicles, these platforms help to conserve resources. Here, control strategies for nonlinear systems applied in parallel robots using electropneumatic actuators are developed. These actuators are more cost-effective but present challenges in control due to air compressibility, friction, and uncertainties. To address these issues, a nonlinear PD-fuzzy + I control strategy has been developed, which is able to handle highly nonlinear elements and uncertainties. Also, control algorithm soften the controls and thus protect the valves and pneumatic pistons. Simulation results demonstrate the effectiveness and potential of the proposed control strategy."}, {"label": 1, "content": "This paper introduces the usage of a sequence-to-label network with Long-Short Term Memory (LSTM) architecture to classify whether a user is chewing or waiting to receive food by detecting 3 different states of the mouth: closed, intermediate closed-open, and open. To develop this task, 2 databases were built, one of training with 260 sequences of 3 seconds of the states with 15 time steps, within which there are 2 characteristics: opening and the relationship between the height and width of the mouth, and another of test, of 145 sequences with the same parameters as the previous one. Initially, the network was designed to detect 2 states, chewing and waiting, but due to the variations in chewing behavior, it was divided into two sub-states: closed and intermediate. With this, the neural network is trained, reaching an 84.8% accuracy by classifying the 3 states and 97.9% merging the two states belonging to \u201cchewing\u201d, which verifies the ability of this type of network to recognize the states of the mouth through sequences over time."}, {"label": 1, "content": "This paper addresses the online identification problem of uncertain systems. By using a neural identification model with feedback and a weight law based on Lyapunov theory, an online identification algorithm is proposed to make the residual state error arbitrarily small and related to two design matrices. In addition, it is shown that the transient can be controlled by other parameters which are not related to the residual state error. Hence, it is possible to decouple the transient and the steady state error. The study also examined the impact of primary design parameters on algorithms' performance."}, {"label": 1, "content": "Wireless Sensor Networks (WSNs) frequently face issues related to spectrum scarcity, however, Cognitive Radio (CR) technology has commonly been used to address the same. This has brought about the emergence of Cognitive Radio Sensor Networks (CRSN). As mobile sensor networks for the Internet of Things (IoT) are expected to be implemented in the near future, this work has been motivated to tackle this challenge. Cooperative Spectrum Sensing (CSS) is the most widely used strategy for solving the hidden terminal problem. In this regard, we determine the number of awake sensors over a dynamic environment to spectrum sensing. In this context, we have described a mobility model for the sensors using Markov chains. Additionally, a first approximation of such scenarios has been described using convex optimization. The proposed solution minimizes the energy consumption subject to constraints on detection performance. The analysis and performance of the proposed system are discussed and illustrated with the aid of simulations."}, {"label": 1, "content": "In this paper, we propose an automated method for generating ground truth labels in order to train neural networks for the automatic segmentation of the urinary bladder in CT images. Our approach utilizes Positron Emission Tomography (PET) acquisitions to generate the training data set, as deep neural networks require a large amount of training data that is currently difficult and time-consuming to create through manual labeling. However, deep neural networks require a large amount of training data, which is currently the main bottleneck in the medical field, because ground truth labels have to be created by medical experts on a time-consuming slice-by-slice basis. We achieve this by using thresholding on the PET data to identify regions where the urinary bladder appears distinct, and then using these regions to automatically generate ground truth labels for the CT images. Although our method currently relies on PET imaging, our long-term goal is to reduce patient radiation exposure by eliminating this step entirely and relying solely on CT images for segmentation. However, the ultimate goal is to entirely skip PET imaging and its additional radiation exposure in the future, and only use CT images for segmentation."}, {"label": 1, "content": "Assisting blind individuals with the recognition of bus routes can be made possible with the development of a smart phone application. To facilitate bus route number reading, obstacles along the road should first be identified. In order to achieve this, the identification of static obstacles is a crucial aspect that requires two processes. Furthermore, an artificial neural network is employed to select a group of k-means that contains the road area. Next, the straight lines on the road are detected via Hough line transformation. This leads to the line selection step where the boundary of the road area is defined. The second process involves static obstacle detection and is addressed through segmentation, vertical projection of the road area boundary, and consideration of the vertically projected signal. The experimental results demonstrate a high performance of the proposed method with an F-measure of 0.912."}, {"label": 1, "content": "One critical issue in hospitals is patient injuries from falling out of bed, which can even result in death. To address this problem, a monitoring and alarming system called NEFs (Never Ever Falls) is being introduced. Considering this type of incident, a monitoring and alarming system called NEFs (Never Ever Falls) is introduced to prevent patients from falling out of the bed. The experiment involves collecting data from 6 people in 5 different on-bed patterns, including Sitting inside the bed, Supine, Lateral on the left, Lateral on the right, and sitting at bedsides and corners. In the experiment, data from 6 persons in 5 different on-bed patterns (Sitting inside the bed, Supine, Lateral on the left, Lateral on the right and sitting at bedsides and corners) is recorded. According to the confusion matrix, training and validation confusion tables show 99.5% and 89.1% accuracy, respectively."}, {"label": 1, "content": "This paper investigates lag synchronization between two uncertain complex dynamical network with time-varying coupling delay, fully unknown parameters, and disturbances in finite-time. A nonsingular terminal sliding surface is proposed and its finite convergence is proved. Then, appropriate adaptive laws are derived to estimate the unknown parameters of the networks. Subsequently, based on the finite-time stability theory and adaptive laws, an adaptive sliding mode control is designed for achieving finite-time lag synchronization. The proposed control is also effective in overcoming the unknown bounded disturbances, as demonstrated by the analytical results. Finally, analytical results show that the states trajectory of the networks error converge to the sliding phase within finite-time. Furthermore, numerical simulation results demonstrate the applicability and the effectiveness of the designed method."}, {"label": 1, "content": "With the change of epoch, the standard of male attractiveness is also changing. By analyzing the changes in facial features over time in different environments, we can see how social development affects our perception of facial attractiveness. This paper proposes a method for analyzing trends in facial features for Chinese males. To start, a face database with male subjects from different periods is established and rated by individuals of different ages. Machine learning algorithms are then used to rate facial images from different time periods in order to verify the changes in male aesthetics. Then, the retrained Inception v3 model is used to realize facial shape classification. After that, the change trend of face shape is analyzed by using massive data. This proposed method provides a deeper understanding of the evolving cultural standards of male attractiveness over time. Compared to other research on facial attractiveness, this method allows for a more thorough understanding of the guiding trends of popular culture on facial attractiveness."}, {"label": 1, "content": "In recent years, object detection tasks have been dominated by deep learning algorithms. However, in real-time, systems having memory or computing limitations very wide and deep networks with numerous parameters constitute a major obstacle. To address this issue, we propose a method for detecting pedestrians in surveillance systems with limited resources. Our method applies a model compression technique based on the teacher-student framework to a random forest (RF) classifier instead of a wide and deep network, which requires significant memory and processing resources. The proposed compression method trains a student shallow RF (S-RF) to mimic the performance of the teacher RF's output, using a softened version of the latter. Second, a deep network cannot easily detect small and closely located pedestrians in a surveillance video captured from a high perspective because of frequent convolutions and pooling processes. In this paper, adaptive image scaling and region of interest with S-RF were therefore combined to allow fast and accurate pedestrian detection in a low-specification surveillance system. In experiments, our proposed method achieved faster speeds and higher compression rates than the teacher RF, demonstrating better detection performance than other state-of-the-art methods on benchmark datasets such as Performance Evaluation of Tracking and Surveillance 2006, Town Centre and Caltech."}, {"label": 1, "content": "To improve human detection in an office work scenario, this paper proposes two ideas which utilize top-view depth cameras. Geometric human shapes can change due to body posture (sitting, standing, crouching) and for this reason, two features, namely roundness and size of a height-continuous region, are proposed to describe the human upper-back shape. Additionally, the proposed adaptive feature adjustment algorithm is used to mitigate the influences of partial loss of depth information caused by occlusions and infrared light absorption. We implemented the proposed algorithm on a system with 13 depth cameras. Application to 100-hours (10 workdays) of actual office data demonstrated that the upper-back features complement the existing head-shoulder features. Results showed that the upper-back features complemented the existing head-shoulder features and that the proposals significantly improved human detection accuracy to 97.7%."}, {"label": 1, "content": "Brain-computer interface technologies, such as steady-state visually evoked potential, P300, and motor imagery are methods of communication between the human brain and the external devices. Motor imagery-based brain-computer interfaces are popular because they avoid unnecessary external stimuli. Although feature extraction methods have been illustrated in several machine intelligent systems in motor imagery-based brain-computer interface studies, the performance remains unsatisfactory. To address this issue, researchers have been exploring the use of fuzzy integrals, such as the Choquet and Sugeno integrals, which are suitable for applications that require data fusion that considers possible data interactions. To enhance the classification accuracy of brain-computer interfaces, we adopted fuzzy integrals, after employing the classification method of traditional brain-computer interfaces, to consider possible links between the data. Subsequently, we proposed a novel classification framework called the multimodal fuzzy fusion-based brain-computer interface system. Ten volunteers performed a motor imagery-based brain-computer interface experiment, and we acquired electroencephalography signals simultaneously. The multimodal fuzzy fusion-based brain-computer interface system enhanced performance compared with traditional brain-computer interface systems. Additionally, the system achieved the highest accuracy (up to 78.81% and 78.45% with the Choquet and Sugeno integrals, respectively) when the input features were derived from the motor imagery-relevant electroencephalography frequency alpha and beta bands. The results of this study present a new concept for enhancing brain-computer interface systems through the use of fuzzy integrals, particularly in the fusion for classifying brain-computer interface commands."}, {"label": 1, "content": "The measurement of software trustworthiness is a widely-discussed topic in the industry. Software component technology is the mainstream technology of software development. How to get the trustworthy degree of software component efficiently and accurately is a challenging issue for the component-based software development. Obtaining the level of trustworthiness required for software components necessitates numerous successful cases from multiple users. In this paper, we propose an innovative updating model for software component trustworthiness. Initially, we compute the trustworthy degree of the software component based on users' feedback. Then we determine the weight of updating based on the number of users. Finally, we cluster different companies using the Euler distance method. In conclusion, we demonstrate that this method is reasonable and effective by presenting a case study."}, {"label": 1, "content": "This paper focuses on physical layer security in non-orthogonal multiple access (NOMA) based uplink massive machine type communication (mMTC) networks. Aiming at the maximization of the system secrecy capacity, with the presence of eavesdroppers, we propose a joint power and sub-channel allocation for secrecy capacity (JPSASC) algorithm to obtain the suboptimal solution of the joint problem. The algorithm obtains a suboptimal solution for the joint problem by allocating power and sub-channels in a non-cooperative game with a distributed perspective. The existence of Nash equilibrium (NE) is proved and a sufficient condition to ensure the uniqueness of NE is given. The Nash equilibrium (NE) is proven to exist, and a sufficient condition for ensuring the uniqueness of NE is given. Additionally, the paper proposes distributed power allocation and preference secrecy capacity maximum (PSCM) algorithms for the power allocation and sub-channel allocation problems, respectively. Furthermore, the secrecy capacity in NOMA-based mMTC is improved compared with that in orthogonal multiple access schemes."}, {"label": 1, "content": "Multi-access edge computing (MEC) has emerged as a promising technique for low-latency services, in light of its proximity to users and embedded cloud computing capability. However, in order to improve network efficiency and fairness, there is a need to jointly optimize resource management and user association mechanisms, taking into consideration the heterogeneities of services and network conditions. Moreover, the existing studies always neglect the heterogeneities of services on the requirements of both computation capability and storage capability. To solve this issue, we derive a strategy to improve the overall delay-aware performance of heterogeneous services with the MEC capability of computation and storage and the choices of the users' association. To achieve this, a coalition-game-based algorithm has been developed to form user coalitions for association scheme and resource sharing policy. The proposed algorithm is capable of convergence and optimality, as demonstrated by the mathematical presentation of its performance. The simulation results show that the proposed algorithm delivers good performance efficiently, reducing the weighted sum of delays of users by an average of 27.8% and 82.1%, while continuously improving delay-aware fairness, compared with those of the priority-based assignment scheme and the nearest assignment scheme, respectively. Furthermore, it reduces the weighted sum of delays of users by average 27.8% and 82.1%, while continuously improving delay-aware fairness, compared with those of the priority-based assignment scheme and the nearest assignment scheme, respectively."}, {"label": 1, "content": "A convolutional neural network (CNN) is a popular approach for image recognition and classification. It involves a series of convolutions and other non-linear phases to transform input images. With CNNs, the softmax loss is used as the traditional loss function. The traditional loss function for CNNs is softmax loss, which separates deep features of distinct classes and promotes effective training. An improvement on CNNs' discriminative power for face recognition was recently reported, where softmax and center loss were jointly used as supervisory a loss signal. However, for human activity recognition, such a supervisory loss function is not optimal. Therefore, this paper proposes a new likelihood regularization term based on a Bayesian distribution, intended to enhance the feature discriminative power of CNN models. The regularization term improves different class discrimination, maximizing the distance between different classes and minimizing distances within the same class. The results obtained on the KTH and Weizmann datasets were encouraging."}, {"label": 1, "content": "Circuit switching is a de facto switching technology widely employed in today's networks where the conventional approaches to routing have remained unchanged for many years. This paper develops a new and very different methodology, by incorporating a supervised na\u00efve Bayes (NB) classifier, to assist least loaded (LL) routing and to further improve its performance that has remained the best among all the routing approach for the past several decades. Specifically, by iteratively learning the information of historical network snapshots, the NB classifier predicts potential future circuit blocking probability between each node pair if a service connection is established via a certain route between the node pair. The snapshots are taken for each service request arriving at an operating network that keeps on accepting and releasing dynamic service connections and records the number of busy capacity units on each link at each snapshot instance. The candidate route for serving a new request is chosen based on both link loads and the potential future blocking probability throughout the network. The proposed approach's performance is evaluated through simulations and compared with the conventional LL algorithm. The results show that the NB classifier-assisted LL routing algorithm significantly reduces the blocking probability of service connections and outperforms the conventional LL routing approach. To speed up the learning process (which is based on a large number of network snapshots), we also develop a framework to incorporate the proposed approach in a parallel learning system. A network control system also supports online NB classifier-assisted LL routing algorithms."}, {"label": 1, "content": "In this letter, an innovative convexified linear program is introduced to address the optimal directional overcurrent relay (DOCR) coordination problem (CP). The DOCR CP is a highly constrained nonlinear nonconvex problem which was solved using various traditional and heuristic optimization techniques in the past. In this letter, the CP is formulated as a linear programming problem without fixing the current pickup settings of DOCRs. The bilinear terms are transformed into linear inequalities using McCormick envelopes, making the problem convex. A sequential tightening algorithm is then employed to adjust the boundary limits of every parameter based on the solutions obtained in each iteration, thereby refining the convex hulls, thereby bringing us closer to global optimum. The performance of the proposed method is applied to various test systems among which the results of three bus test system are presented."}, {"label": 1, "content": "Datasets advance research by posing challenging new problems and providing standardized methods of algorithm comparison. In the fields of robotics and computer vision, high-quality datasets exist for some important problems such as egomotion estimation and motion/scene segmentation, but not for techniques that estimate every motion in a scene. Metric evaluation of these multimotion estimation techniques requires datasets consisting of multiple, complex motions that also contain ground truth for every moving body. That's why we have the Oxford Multimotion Dataset, which provides a variety of complex and simpler problems to support algorithm development. This dataset includes observations from both static and dynamic sensors with a varying number of moving bodies and different three-dimensional motions. It also offers experiments designed to isolate specific challenges of the multimotion problem such as rotation about the optical axis and occlusion. It also provides a number of experiments designed to isolate specific challenges of the multimotion problem, including rotation about the optical axis and occlusion. The dataset culminates in a complex toy car segment representative of many challenging real-world scenarios. This letter focuses on the relevance of each experiment to the multimotion estimation problem, aiming to advance research in this field. The Oxford Multimotion Dataset serves as a valuable resource that can help researchers tackle real-world challenges in multimotion estimation with confidence."}, {"label": 1, "content": "In this letter, a novel approach for fast frequency estimation is proposed using frequency-shift filtering. The original sampled signal is first frequency-shifted to 0 Hz nearby via multiplying a reference signal. Subsequently, a convolution average filter is applied on the shifted signal to effectively eliminate spectral interference that is caused by asynchronous sampling. Finally, the frequency of power system can be estimated using the phase difference between arbitrary two points of the filtered signal. The approach has major advantages of low computational burden and strong anti-noise performance, which makes it appropriate for high precision and reporting rate frequency measurement in embedded systems. Comprehensive simulations are conducted to validate the accuracy and efficiency of the proposed approach."}, {"label": 1, "content": "With the rapidly increasing demand for security and E-health applications, device-free human detection has attracted interest because it does not require a wearable device or camera setup. This paper proposes a deep-learning-based approach that monitors wireless signals to learn three human modes, i.e., absence, working, and sleeping, in realistic indoor environments. The approach integrates amplitude and phase of channel state information to create a hybrid complex feature, which results in robust and efficient human detection even when fewer data samples are available. The proposed algorithms are tested in two unmodified WiFi networks, and the results demonstrate their effectiveness. Among the four machine learning algorithms that were tested, deep neural networks were found to perform the best. Results show that by using 6% training samples, the proposed hybrid feature still achieves 93% accuracy and can even outperform three typical machine learning algorithms that use full training samples. Moreover, the proposed feature significantly improves detection accuracy by 11.62%-27.76% than traditional amplitude feature with fewer training samples."}, {"label": 1, "content": "In this paper, we consider a mission-critical control system, where an unstable dynamic plant is monitored by a number of distributed sensors connected to the controller over the wireless fading channels. We focus on the dynamic sensor scheduling to stabilize the unstable dynamic plant. The dynamic sensor scheduling is modeled as a non-convex drift-plus-penalty minimization problem. To improve the scheduling efficiency, the proposed scheme adapts to both the fading channel state as well as the dynamic plant state. A novel transformation technique for scheduling variables and Lyapunov drift for the objective function was proposed to overcome the non-convexity of the minimization problem. Based on that, we can derive a low complexity dynamic sensor scheduling scheme and also obtain a closed-form stability analysis (despite the non-convexity) of the mission-critical control system via a randomized state-independent policy. Compared with various baselines, the proposed scheme has higher power efficiency and superior scalability performance."}, {"label": 1, "content": "It is with great pride and sadness that we present this special issue of the Computational Intelligence Magazine (CIM) dedicated to Lotfi A. Zadeh, the renowned founder of fuzzy set theory [1], who passed away in September 2017. In a previous issue of CIM [2], we gathered the perspectives of many pioneers of fuzzy sets, sharing their thoughts on Lotfi's remarkable contributions to science, mentorship, and his personal-life. In this special issue of CIM, we take a closer look at his legacy, impact, and the evolution of his work."}, {"label": 1, "content": "The use of fuzzy sets and fuzzy logic concepts has been proven to be successful in various fields of science and engineering. This paper elaborates on the use of fuzzy sets in the broad field of data analysis and statistical sciences, including modern manifestations such as data mining and machine learning. This branch of research has become increasingly important in the fuzzy logic community due to the emergence of data science as a new scientific discipline and the relevance of machine learning as a key methodology in artificial intelligence. There has been a shift from knowledge-based to data-driven fuzzy modeling and systems design. Looking back at the historical dimension and evolution of the area, the role of fuzzy logic in data analysis and related fields is discussed, highlighting existing contributions of fuzzy sets and outlining interesting directions for future work."}, {"label": 1, "content": "Lotfi Zadeh, the father of fuzzy sets and fuzzy logic, introduced many of their important concepts, two of which are discussed in this article namely, type-2 fuzzy sets and computing with words. Substantial developments have occurred since their introduction, and this article aims to provide readers with updated insights on these topics. It is also hoped that this article will whet the reader's appetite to read further on one or both of these topics."}, {"label": 1, "content": "Detection of high-speed maneuvering targets has been a topic of great interest. There are two main problems to be solved: improving detection ability under the condition of complicated range migration and Doppler frequency migration effects, and reducing computational load. This paper presents a computationally appealing method with excellent detection performance, unlike existing fast algorithms that compromise on detection ability. First, the keystone transform is carried out to remove linear range migration. Thereafter, a fast discrete chirp-Fourier transform (FDCFT) based on radix-4 decomposition is proposed to compensate the undersampled linear Doppler frequency migration and quadratic Doppler frequency migration. Exploiting inherent symmetry and periodicity, the FDCFT reduces computational complexity without performance loss, similar to the fast Fourier transform (FFT). The innovation of this algorithm lies in combining linear transform with the decimation-in-time FFT concept, thus avoiding demanding multi-dimensional searches and severe performance loss caused by introducing non-linear transforms. It is shown that the proposed method has an approximately optimal detection performance but with relatively low computational cost."}, {"label": 1, "content": "Detecting falls in the elderly population is of utmost importance as it can lead to hypothermia and dehydration, which in turn may result in death if left undetected for an extended time. For this reason, it is critical to have a real-time fall-detection system in place. Therefore, real-time detection of falls is critical. We previously proposed a fall-detection system based on a microwave Doppler sensor, which can capture the object's velocity. Our system performs template matching based on the dynamic time warping distance; hence, the processing time depends on the number of template datasets. We utilize a genetic algorithm to select better performing templates with higher accuracy. The fitness of the selected templates is evaluated by dividing the accuracy by the number of templates used. The fitness of the selected templates is evaluated by dividing the accuracy by the number of templates."}, {"label": 1, "content": "In this paper, we present a novel automatic autonomous vision-based power line inspection system that uses unmanned aerial vehicle inspection as the main inspection method, optical images as the primary data source, and deep learning as the backbone of the data analysis. To implement the system effectively, we tackle three major challenges of deep learning in vision-based power line inspection, namely the scarcity of training data, class imbalance, and detection of small components and faults. First, we create four medium-sized datasets for training component detection and classification models. Furthermore, we apply a series of effective data augmentation techniques to balance out the imbalanced classes. Lastly, we propose the multi-stage component detection and classification using the Single Shot Multibox detector and deep Residual Networks, targeting the identification of small components and faults. The results show that the proposed system is fast and accurate in detecting common faults on power line components, including missing top caps, cracks in poles and cross arms, woodpecker damage on poles, and rot damage on cross arms. The field tests suggest that our system has a promising role in the intelligent monitoring and inspection of power line components and as a valuable addition to smart grids."}, {"label": 1, "content": "Ion-current density is a significant indicator of electromagnetic environment under high voltage direct current lines. In order to enhance measurement accuracy and convenience, this study has identified key factors that can impact the measurement outcomes and introduced a wireless measurement method. The Wilson plate was used as the sensor, which was designed in compliance with the IEEE standard. The measurement device facilitated the measurement and digitization of the ion current, which is then transmitted to the computer via wireless sensor networks. The data were analyzed and recorded by automatic measurement software. The measurement system was calibrated and several field tests were conducted which confirmed that the system possesses remarkable accuracy, simple architecture, easy operation, and high stability and reliability. The maximum current measurement error for the system was found to be less than 3%."}, {"label": 1, "content": "This article presents a new Bayesian inference technique for estimating dynamic parameters in decentralized power systems utilizing measurement data from phasor measurement units. The response surface for the decentralized generator model is formulated through a polynomial-chaos-based surrogate. This surrogate allows us to efficiently evaluate the time-consuming dynamic solver at parameter values through a polynomial-based reduced-order representation. The method also includes a polynomial-chaos-based analysis of variance to screen model parameters while ensuring system observability. In dealing with sampling the non-Gaussian posterior distribution for the parameters, the Metropolis-Hastings sampler is adopted. Simulation studies conducted on the New England system show that the proposed method attains a significant speedup of two orders of magnitude compared to traditional approaches while providing full probabilistic distribution of model parameters and maintaining the same level of accuracy."}, {"label": 1, "content": "The inhomogeneous wires are made up of various metals that allow for the benefits of specific materials to be utilized. One of the most relevant electrical parameter used to describe the conductor is its internal impedance. Existing methods for calculating the internal impedance rely on approximations, limiting the analysis of only two layers of different materials. The currently existing methods of analytical calculation of the internal impedance are based upon the approximations to keep the numerical stability and are limited to two layers made of different materials. The presented method utilizes modified scaled Bessel functions and further division into sub-layers, ensuring numerical stability for thick conducting layers, even at high frequencies. Compared to finite-element models, the computation time and memory usage required by the presented method of internal impedance are remarkably less. The time of computations done by the presented method of internal impedance and required memory is much lower than in the case of the finite-element models."}, {"label": 1, "content": "In order to overcome the shortcomings of traditional back propagation (BP) and single genetic algorithm (GA), a method based on quantum GA (QGA) is proposed to optimize the BP neural network for fault detection of liquid rocket engines. In this QGA-BP method, a dynamic improvement strategy is adopted to adjust the rotation angle according to the evolution situation, and a quantum catastrophe strategy is used as an operation criterion during evolution. Then, the improved QGA is used to optimize the weight and threshold of the BP neural network from multiple spots. The QGA-BP model has been applied to a typical fault detection process of a liquid rocket engine using representative history test data of engine state. Representative history test data of engine state is used to verify this method, and the results show that the convergence speed, the evolution generation, and the accuracy of fault detection of the QGA-BP model are all improved compared with the traditional BP neural network and the single GA."}, {"label": 1, "content": "In this paper, we introduce a 5G K-Simulator consisting of a link-level simulator (5G K-SimLink), a system-level simulator (5G K-SimSys), and a network simulator (5G K-SimNet). The 5G K-Simulator is open source to solve the waste of human and material resources consumed in simulator development in the continuous management and performance analysis/verification of wireless communication technology. The code of the 5G K-Simulator is developed with a modular and flexible methodology so that users can easily change or extend it according to their purpose. In addition, we provide a web-based simulator platform that allows users to more easily use. The 5G K-Simulator assures objectivity through verification by a beta test of experts group. The use of 5G K-Simulator enables universities, research institutes, and industries to develop and verify 5G technology quickly. Furthermore, it facilitates the creation of industry-specific applications and services technologies related to 5G wireless/mobile communication. The 5G K-Simulator holds promise as a foundation for the simulator of communication systems beyond 5G."}, {"label": 1, "content": "This paper presents the policy and technology case for decentralizing the architecture for dynamic spectrum management. A specific system concept called SMAP (distributed spectrum management architecture and protocol) is intended to enable wireless devices and networks to coordinate their spectrum use through an Internet-based common spectrum control plane. The design requirements for this distributed spectrum control plane are efficiency, scalability, decentralized decision making, support for local policy, service-level agreements, and market mechanisms. The architecture also provides interfaces to higher level cloud services including spectrum aggregators which facilitate broader cooperation and business relationships between wireless domains in the same area, or to regional spectrum databases such as the SAS (spectrum access system) being used for the 3.5 GHz innovation band. Design requirements for the proposed distributed spectrum control plane are discussed, including efficiency, scalability, decentralized decision making, support for local policy, service-level agreements and market mechanisms. Proof-of-concept simulation and/or experimental results demonstrate the technical feasibility of the proposed techniques in different scenarios. These scenarios include logically centralized spectrum coordination via regional spectrum brokers, distributed coordination between colocated wireless domains using either single or multiple radio technologies (such as Wi-Fi and LTE), and distributed coordination with additional local policy constraints. In conclusion, the paper discusses future work and opportunities for broader cooperation and business relationships between wireless domains in the same area through spectrum aggregators and regional spectrum databases like the SAS being used for the 3.5 GHz innovation band. The proposed SMAP architecture provides a flexible and adaptable solution for dynamic spectrum management that can enable efficient and fair use of the radio spectrum in a decentralized manner."}, {"label": 1, "content": "The number of IoT devices is increasing rapidly, resulting in explosive growth of mobile network traffic. In order to meet the human desire to explore unknown areas, edgeless communications have always been one of the directions for the development of wireless networks. However, integrating communication protocols, routing issues, and resource allocation in a vast and heterogeneous network architecture is challenging. This article proposes a possible H-STIN architecture based on the development trends of the Internet of Things, mobile networks, and satellite networks. Therefore, this article proposes a potential H-STIN architecture based on the development trends of the Internet of Things, mobile networks, and satellite networks. AS is used to achieve regional self-optimization. SSTIS consists of the perception layer, the cognition layer, and the intelligence layer. It integrates IoT, SDN, and network functions virtualization technologies to achieve self-monitoring, crisis forecasting, and optimal control. Finally, proposed technical challenges include integrated route planning and large-scale resource allocation."}, {"label": 1, "content": "With the gradual deployment of space-based wireless networks, security risks in data communication between satellites and even the internal structure of a satellite have become increasingly significant. In this article, we use the satellite's internal communications security as an example to illustrate the space network security threats. We first provide a summary of the security requirements of space-based wireless networks. Then three typical attack approaches for satellite platforms based on MIL-STD-1553B bus are described. Subsequently, we present some attack simulation results and suggest some protective mechanisms."}, {"label": 1, "content": "Telecommunication networks are evolving toward a data-center-based architecture, which includes physical network functions, virtual network functions, as well as various types of management and orchestration systems. The primary purpose of this type of heterogeneous network is to provide efficient and convenient communication services for users. However, the diverse factors of a heterogeneous network such as bandwidth, delay, and communication protocol, bring great challenges for routing recommendations. The explosive deployment of big data and heterogeneous networks has ushered in a new era of big data technologies to implement routing recommendations. This article proposes a tensor-based big-data-driven routing recommendation framework that includes the edge plane, fog plane, cloud plane, and application plane. In this framework, a tensor-based, holistic, hierarchical approach is introduced to generate efficient routing paths using tensor decomposition methods. A tensor matching method, utilizing the controlling tensor, seed tensor, and orchestration tensor, is employed for routing recommendations. Finally, a case study is used to demonstrate the key processing procedures of the proposed framework."}, {"label": 1, "content": "Enhancements to the IEEE 802.15.4(TM) smart utility network (SUN) orthogonal frequency division multiplexing (OFDM) physical layers (PHYs) that enable support for data rates up to 2.4 Mb/s are defined by this amendment to IEEE Std 802.15.4(TM)-2015. Additionally, this amendment defines new channel plans as necessary to facilitate emerging applications."}, {"label": 1, "content": "The deployment of Internet-of-Things (IoT) devices in large quantities can lead to congestion in cellular networks, which ultimately affects the energy consumption of IoT devices and the efficiency of the cellular network. Thus, extending IoT communications into the unlicensed band is a promising solution. In this study, we focus on uplink cellular IoT networks, where IoT devices serve as cluster heads (CHs) to collect and aggregate data from other devices in their cluster. To be specific, the IoT devices can either transmit the sensory data to the base station (BS) directly by cellular communications, or aggregate the data to a CH through Machine-to-Machine (M2M) communications before the CH uploads the aggregated data to the BS. To support the massive connection of IoT devices, the unlicensed spectrum is utilized for M2M communications. We propose a scheduled number maximization problem for IoT devices with minimum transmit power and solve it by decoupling it into two subproblems using integer linear programming and convex optimization techniques successively. Simulation results show that the proposed unlicensed scheme can support more IoT devices than that only using licensed spectrum."}, {"label": 1, "content": "Emergence of shared spectrum such as CBRS 3.5 GHz band promises to broaden the mobile operator ecosystem and lead to proliferation of small cell deployments. We consider the inter-operator interference problem that arises when multiple small cell networks access the shared spectrum. To address this, a communication-free approach has been proposed that seeks implicit coordination between operators. The key idea is for each operator to sense the spectrum through its mobiles to be able to model the channel vacancy distribution and extrapolate it for the next epoch. We use reproducing kernel Hilbert space kernel embedding of channel vacancy and predict it by vector-valued regression. This predicted value is then relied on by each operator to perform independent but optimal channel assignment to its base stations taking traffic load into account. Via numerical results, we show that our approach, aided by the above channel vacancy forecasting, adapts the spectrum allocation over time as per the traffic demands and more crucially, yields as good as or better performance than a coordination based approach, even without accounting the overhead of the latter."}, {"label": 1, "content": "Detecting anomalous behavior in wireless spectrum is a complex and demanding task due to the sheer number of electromagnetic spectrum uses. Wireless spectrum anomalies can take a wide range of forms from the presence of an unwanted signal in a licensed band to the absence of an expected signal, which makes manual labeling of anomalies difficult and suboptimal. We present, Spectrum Anomaly Detector with Interpretable FEatures (SAIFE), an Adversarial Autoencoder (AAE) based anomaly detector for wireless spectrum anomaly detection using Power Spectral Density (PSD) data which achieves good anomaly detection and localization in an unsupervised setting. In addition, we investigate the model's capabilities to learn interpretable features such as signal bandwidth, class and center frequency in a semi-supervised fashion. Additionally, the SAIFE model exhibits promising results for lossy PSD data compression up to 120X and semi-supervised signal classification accuracy close to 100% on three datasets, using only 20% labeled samples. Finally the model is tested on data from one of the distributed Electrosense sensors over a long term of 500 hours showing its anomaly detection capabilities."}, {"label": 1, "content": "The aim of this paper is to efficiently and accurately detect changes in observations, specifically in the context of localizing using time difference. This is achieved through a method called sequential detection, or quickest detection. In our model, we consider a system being monitored by distributed sensors, where an abrupt change, such as a jamming signal, could occur at any unknown moment. We propose a framework using the quickest detection with cumulative sum (CUSUM) test which is well-known to be optimal for a non-Bayesian statistical change-point detection formulation. At each time, the distributed sensors decide about the presence or absence of any jamming signal. Once the sensors get a decision and transmit it to fusion center, then fusion center localizes the jammer. In the end, the results are evaluated using computer simulations."}, {"label": 1, "content": "This paper focuses on the study of a cellular system using unmanned aerial vehicles (UAVs) that coexist with a WiFi network. By spectrum sharing and traffic offloading with the WiFi access point, our objective is to minimize the average delay of the users served by the UAV base station, while ensuring that the delay of the WiFi users is not greater than certain threshold, via jointly optimizing the spectrum allocation, the set of offloaded users and their offloaded traffic rates. The optimization problem is non-convex and difficult to be directly solved. Due to the complexity of the optimization problem, an efficient sub-optimal solution is proposed using the block coordinate descent method. Numerical results are presented to illustrate the effectiveness of the proposed design."}, {"label": 1, "content": "Cellular Internet of Things (CIoT) technologies have been designed for reliability, security, and scalability and provide a strong foundation for existing cellular networks connectivity with a unique combination of functionality and performance. By leveraging existing infrastructure through software upgrades, CIoT provides unparalleled IoT coverage and fast deployment capabilities. This ecosystem, based on 3GPP standards, is both extensive and continuously evolving. This paper emphasizes a comparative study between the emergent CIoT namely; the Narrow-band IoT (NB-IoT) and the LTE-M technology (named also eMTC)."}, {"label": 1, "content": "The upward trend in using both Cloud computing and Internet of Things (IoT) is changing the way of conceiving information and communication systems. We can talk about IoT Cloud to indicate a distributed complex system which is able to provide IoT-as-a-Service (IoTaaS). At the edge of the system, multisensors electronic devices make smart the \u201cthings\u201d they control also enabling the interconnection with a remote Cloud infrastructure, platform, or software through the Internet. In such scenario, achieving energy-aware IoTaaS is a challenge which needs appropriate choices in all the environmental, economic and ethical aspects of sustainability. To address this issue, utilizing Fog computing can help businesses discover new opportunities while simultaneously working towards a \"green\" goal. In this paper, we explore the benefits of implementing Fog computing to provide energy-aware IoTaaS in green smart environments. Additionally, we present a real-life use case that demonstrates the feasibility of our solution."}, {"label": 1, "content": "The Internet of vehicles, or IOV, is widely recognized as an important trend in Intelligent Transportation Systems with the aim of improving transport safety and reducing accidents in our automotive society. However, forecasting that a large fraction of the population seeks connected vehicles, the vehicular communications may be overdue owing to the limited channels available on the IEEE 802.11p standard spectrum. In this work, we analyze how dynamic access spectrum in cognitive radio can promote the spectrum resources opportunities in IOV communications for unlicensed users by averting detrimental interferences for primary users. A comparative study between cognitive IOV approaches based on spectrum sensing is the nub of this work."}, {"label": 1, "content": "This paper investigates and computes dust particle alignment in relation to cross polarization discrimination. Relevant forces influencing the particle alignment such as inertial torques and turbulence shear are calculated. Our results shed light on how these forces influence the behavior of dust particles and can inform future research in this area."}, {"label": 1, "content": "With the increasing use of Big Data, security has become a top concern for businesses. Cyberattacks continue to grow, making information security a global problem. However, simply relying on prevention methods is not enough to protect against intrusions. Therefore, intrusion detection systems have been developed. Intrusion refers to hacking the system or a network that harms the security of information. Intrusion detection systems (IDS) are an essential element of the network security infrastructure and play a very important role in the detection of a large number of attacks. This paper introduces an analysis of network security issues and also represents the various works of current research in Big Data intrusion and detection systems. In this article, we discuss the recent approaches used for intrusion detection in Big Data and especially the predictive analysis proposed to minimize security problems."}, {"label": 1, "content": "This article presents a new technique for steganography, which involves embedding secret messages in grayscale images. The proposed method is an adaptive edge scheme that effectively conceals the secret data in the edge pixels. The selection of the edge pixels is based on oversegmentation using Modified Simple Linear Iterative Clustering (M-SLIC). The M-SLIC algorithm makes it possible to segment an image into K regions called superpixel. This segmentation is very useful for the detection of objects and the contours between these objects. The number of superpixels required depends on the amount of data to be embedded and the regularity of the cover image. The experiment results have shown that the proposed technique improve the performance of the stego image in terms of capacity, imperceptibility and robustness compared to recent steganographic techniques."}, {"label": 1, "content": "The goal of this paper is to present a parallel implementation of Haralick features extraction technique for our unsupervised texture image segmentation approach. This process involves using the CUDA environment on an NVIDIA GPU to compute the gray-level co-occurrence matrix (GLCM) and Haralick features for each pixel in the image simultaneously. This enhanced implementation is followed by our clustering approach, which based on the representation of a Kohonen map trained by features parallel extracted from each pixel of the image, and extraction of modal regions from that map. To demonstrate the effectiveness of our approach, we conducted experiments comparing the performance of the parallel GPU implementation to that of the CPU-based sequential technique. In addition, segmentation rate results obtained by applying our approach are compared to the result of the K-means method."}, {"label": 1, "content": "This work considers the simulation of an array consisting of identical, interconnected elements using dynamically constructed macro basis functions. The construction of these macro basis functions is done through the iterative Jacobi technique, which enhances the convergence of the method."}, {"label": 1, "content": "Breast cancer is one of the most common cancers among women in the world, accounting for the majority of new cancer cases and cancer-related deaths according to global statistics, making it a significant public health problem in today's society. To address this public health issue, this paper presents an overview of the evolution of big data in the health system and applies four learning algorithms to a breast cancer dataset. The aim of this study is to predict breast cancer using machine-learning algorithms, including Random Forest, Na\u00efve Bayes, Support Vector Machines (SVM), and K-Nearest Neighbors (K-NN), to reduce the risk of death through early detection and prevention. The experimental results revealed that SVM provided the highest accuracy levels of 97.9%. This study's findings will aid in selecting the most effective classification machine-learning algorithm for breast cancer prediction."}, {"label": 1, "content": "Terrain classification plays a critical role in all robot systems especially in unknown environments. In recent years, researchers have proposed various algorithms to improve the efficiency and accuracy of terrain classification. Nevertheless, these methods still have some deficiencies in classification efficiency. In this paper, a double-tower convolutional neural network has been designed to implement end-to-end underwater terrain classification. The proposed method uses matched sonar and visual images, obtained simultaneously by the robot's sonar and visual sensors, as input. The corresponding image pairs are set to be the input of the convolutional neural network, and the output of the network is the classification of the terrain. Sonar and visual image features are applied simultaneously in the network to achieve accurate terrain classification. Therefore, this paper establishes an end-to-end convolutional neural network equipped with a classification function for underwater terrain classification."}, {"label": 1, "content": "Most brain-computer interface (BCI) systems use the synchronization paradigm to detect specific brain activities to control external devices. However, for more varied control options, a switch based on spontaneous brain activities may be necessary. EEG data during motor imagery of right hand movement were collected by 64 electrodes from 4 healthy subjects. After pre-processing, a linear discriminant classifier was employed to extract the feature related to motion, and the system on/off discrimination rate was found to be around 90% for offline analysis. Additionally by the maximum redundancy minimum correlation analysis, the most relevant channel was obtained. In the future, the brain-controlled switch may play an important role in brain-computer interface capacities in practical applications."}, {"label": 1, "content": "Human-powered lower exoskeletons have garnered attention from academia and industry as a means of augmenting human locomotion and strength. In order to improve the control performance of these systems, machine learning techniques such as reinforcement learning have been employed. However, real-life applications of reinforcement learning require the discretization of continuous observation spaces, limiting the effectiveness of traditional strategies. In real-life applications, Almost all tasks of interest and most notably physical control tasks have continuous and high-dimensional observation spaces. The continuous states can characterize the state of exoskeleton system and the continuous actions can control the exoskeleton system accurately. Therefore, this paper proposes an Inter-active Learning based on Actor-Critic (ILAC) to solve the problems with continuous high-dimensional observation spaces. In proposed ILAC, the actor-critic algorithm that can learn optimal policies in high-dimensional continuous domains is used to learn the controller's coefficients of exoskeleton system, in which the sensitivity factors of Sensitivity Amplification Control (SAC) and the coefficients of the compensation controller are learned at the same time. The experiments on both single Degree Of Freedom (DOF) exoskeleton and HUman-powered Augmentation Lower EXoskeleton (HUALEX) is shown. Overall, this paper highlights the potential of ILAC as a means of improving the control performance of exoskeleton systems in real-life applications with continuous high-dimensional observation spaces."}, {"label": 1, "content": "As a basic and key attribute of human beings, race plays an indispensable role in face analysis. In the past, traditional machine learning methods have addressed the issue of race classification by taking two steps: extracting artificially designed features and training a suitable classifier with these features. Some convolutional neural networks have also been proposed to deal with this problem, but get unsatisfactory accuracies. In this research, we present an enhanced deep convolutional neural network based on an existing network. The network uses a branch structure to merge networks of different depths, such that it can see multi-scale features (features in the low layers are more global and general than those in the high layers). To train this network, we collect a private race database using the available search engines on the Internet, which is larger and more balanced than publicly available databases. Experimental outcomes reveal that the proposed network can not only extract features and classify them simultaneously in contrast to conventional methods but also achieve a remarkable accuracy of almost 99% on both public and proprietary databases. Lastly, it is critical to emphasize the significance of state-of-the-art face detection and face alignment for the ultimate outcome."}, {"label": 1, "content": "This paper investigates the fault-tolerant control problem of a multi-input and multi-output robotic system with uncertainties. Neural networks based on Gaussian radial basis functions are used to compensate for the dynamic uncertainties. To address the issue of prescribed performance, the paper introduces a transformation of the problem into a time-varying constraints problem and utilizes the barrier Lyapunov function (BLF) to guarantee both steady state and transient performance. Additionally, the study considers partial loss of actuator effectiveness and presents an approach to compensate for it. Simulations using a two-link manipulator have confirmed that the proposed adaptive neural control scheme can ensure prescribed tracking performance even when unknown actuator failures occur."}, {"label": 1, "content": "In this study, we present our novel infrared visual inertial odometry (VIO) algorithm designed for unmanned aerial vehicles (UAVs) that can operate both during the day and at night. It utilizes images from a single downward-looking thermal camera, and uses a laser scanner to measure relative distance to determine the scale factor. The scale recovery process consists of a comprehensive logic flow to try out measurements into a homography decomposition with a multi-plane constraint, obtaining the results with minimum error, making the algorithm more robust. Moreover, other modifications, such as keyframe idea and pruning KLT tracker with sub-regions further improves the performance. Besides, the results are fused with an on-board IMU through an EKF-based sensor fusion framework. This algorithm enables UAVs to navigate through missions in the absence of GPS or under poor illumination conditions."}, {"label": 1, "content": "Fixed-structure low-order feedback control is a simple and effective damping approach for piezo nanopositioning thanks to the merits of ease of implementation and low computational complexity. However, the tracking performance of scanning motions for nanopositioners is weaken due to the unexpected phase lag and residual periodic errors caused by feedback control alone. To address this issue, this paper proposes a feedback integral resonant control (IRC) integrated with a feedforward PID type iterative learning control (ILC) for vibration damping and precision scanning simultaneously. For a TITO system that exhibits cross coupling around the first resonance, a static decoupling matrix is designed to obtain decoupled dynamics. A standard IRC along with ILC is then applied. The parameters in the composite controller are determined to satisfy the convergence condition based on lifted matrix computation. For evaluations, simulations are conducted to check the effectiveness of the parameters. Compared with feedback IRC alone, experimental results demonstrate the superiority of the composite design with more than 85% improvements in RMS errors for both raster and spiral scanning at 20 and 40 Hz."}, {"label": 1, "content": "The problem of pose estimation is very important for ground-based visual servoing of multicopters. Existing Kalman filter methods for this problem often adopt a linear process model that is suitable for many rigid objects. In this paper, with a nonlinear process model customized for multicopters and the pinhole camera model, a correspondence-based extended Kalman filter (EKF) method and a correspondence-free EKF method for pose estimation are proposed. Our experimental findings reveal that our new methods are more efficient and durable than current filtering methods, and they offer comparable or even higher accuracy."}, {"label": 1, "content": "Aiming at the difficulty of extracting turbine vibration fault features under strong noise background, a fault diagnosis method based on stochastic resonance, multi-dimensional permutation entropy and optimized Support Vector Machine diagnostic model was proposed. Firstly, the signal is denoised by stochastic resonance, then multidimensional permutation entropy is used to extract the signal feature vector, and finally the fault is identified by optimized SVM model. Our simulation results suggest that this approach delivers superior diagnostic accuracy in comparison to existing techniques."}, {"label": 1, "content": "In order to combat the issue of complacency and encourage active involvement from patients during robot-assisted rehabilitation, the team has developed a method for online assessment of neural engagement and functional performance. In this work, a fine motor control task has been designed for functional training of the upper limbs. The participant is required to control the position of their hand and gripping force, while receiving visual and haptic feedback. Then subjects\u2019 neural engagement and functional performance were assessed online based on the measured EEG and motion information. Experiments with healthy subjects have been conducted, and the relationship between subjects\u2019 neural engagement and functional performance with the task configuration parameters was studied through statistical analysis."}, {"label": 1, "content": "Facial expression recognition has played a significant role in harmonious human-computer interaction, and feature extraction is the key step for the final recognition result. Although single feature extraction techniques are commonly used, fusion of multiple features can offer several advantages. The traditional serial features fusion technique is the most common fusion method, but there is a constant problem existing in the above fusion method: the curse of the dimension. For solving the problem, a new fusion method is proposed. Experimental results demonstrate that the proposed method yields higher recognition rates and better real-time performance."}, {"label": 1, "content": "In the intelligent parking lot, the robot has to get the accurate position of the wheels in the process of automatic parking. However, conventional image processing algorithms are unsuitable for this application due to their sensitivity to environmental noise. Originating from natural language processing, RNN has now been used in the computer vision for its robustness and excellent performance. This paper proposes a method to position vehicle wheels from point cloud based on the RNN model. The experimental results demonstrate that the bidirectional RNN offers significantly higher prediction accuracy than the unidirectional RNN. In a small-scale training set, the effect of BiLSTM/BiGRU is close, while the training speed of BiGRU is faster, which can be used in the wheel detection system."}, {"label": 1, "content": "Multicopters have been attracting increasing attention in recent years while it is important to consider the flight safety of multicopters in the presence of propulsor faults or failures. To mitigate such risks, it is important to estimate the fault information of the vehicle by investigating the observability of loss of effectiveness information for multicopters. Nonetheless, it has been revealed that the loss of effectiveness information for multicopters with more than four propulsors is unobservable. Simulation results are presented to show the loss of effectiveness information observability of a hexacopter. The study has also highlighted related works in the literature that focus on improving multicopter flight safety."}, {"label": 1, "content": "In order to help patients with lower-limb dysfunction, a movable cable-driven lower-limb rehabilitation robot was designed to restore their motor function and improve their quality of life. The robot can control the position, posture and force of the lower limb, thus facilitating the comprehensive training of the lower limbs. The robot can assist patients in different rehabilitation stages to carry out rehabilitation training with passive, assistant and active mode. Kinematics modeling, statics modeling and workspace analysis were conducted, and the robot configurations were analyzed and optimized based on these. Continuous cable tension was calculated using the 2-norm of the cable tension optimization algorithm. The foundation for subsequent research work of parallel cable-driven lower-limb rehabilitation robot was laid in this paper, which had reference significance for the research of flexible medical rehabilitation equipment."}, {"label": 1, "content": "In this paper, we present a global localization approach for mobile robots equiped with 2D laser range finder(LRF), which combines depth-first search and grid submaps for computing scan-to-submap matches. For global localization, the maximum likelihood submap of the given scan obtained from the multi-level lookup tables, using the depth-first search method adopt the coarse-to-fine upper bound constraints. Then, a set of candidate poses and their matching scores are obtained, and the proposed algorithm could lead to the true pose convergence gradually through the mobile robot motion guided by topological information. Unlike traditional scan matching methods, our approach fuses pose tracking and global localization by utilizing prior submaps. The mobile robot localization in current submap costs low memory, this makes the workspace easy to be expanded. Finally, we verified the performance of our global localization algorithm through experiments on unique and similar scenes."}, {"label": 1, "content": "In Elastic Optical Networks (EON), the problem of routing and spectrum Allocation (RSA) has been solved along the years through the use of optimization techniques or heuristics, with the aim of minimizing the use of network resources or maximizing the capacity for future requests. The primary objective of these approaches has been to minimize the use of network resources or improve the capacity for future requests. On considering the linear and non-linear noise effects, it is possible to include quality of transmission (QoT) in the optimization process. The inclusion of quality of transmission (QoT) in the optimization process is vital when considering the linear and non-linear noise effects. The proposed modification takes into account the channel input power and SNR Margin to choose all modulation formats and to assign resources. The proposed heuristic considers the channel input power and Signal-to-Noise Ratio (SNR) Margin in selecting the modulation format and assigning resources to increase spectral efficiency and channel throughput while mitigating blockages at the physical layer. Simulations carried out across various network topologies indicated an improvement in terms of the number of blocked channels and the spectrum usage of the new proposal compared to the traditional BSR heuristics."}, {"label": 1, "content": "The exponential increase in traffic has pushed conventional optical systems based on standard single-mode fibers to their limits. In this scenario, transmission by few-mode fibers (FMFs) appears as a strong candidate. These systems require adaptive digital signal processing (DSP) algorithms with multiple inputs and multiple outputs (MIMO) at the receiver in order to recover the signals multiplexed in the various spatial and polarization modes. To properly function, these equalizers require the pre-compensation of chromatic dispersion using static filters. The DSP algorithms used in mode-multiplexed systems have slow convergence, making scanning algorithms difficult to apply. In this work, we investigate the applicability of an algorithm based on the auto-correlation of the signal power waveform to estimate the CD in FMF systems. The results indicate an algorithm vulnerability with the increase of the inter-mode crosstalk level."}, {"label": 1, "content": "The elastic optical network (EON) was proposed as a solution to avoid the spectrum waste in conventional WDM networks, since it improves the network utilization. However, the elastic optical network is still based on an optical circuit switching (OCS) paradigm, which tends to waste bandwidth resources due to idle times of the reservation process. In response, hybrid solutions that include the optical packet switching (OPS) paradigm have been introduced, offering greater efficiency in resource usage, particularly in specific applications like data center optical networks. To enable future studies, we have developed an adaptation of the Optical Network Simulator (ONSim) that can work within hybrid switching paradigms. As an example, we examine blocking probability within the context of a hybrid OCS/OPS optical network operating under varying traffic proportions."}, {"label": 1, "content": "We modeled Multilayer Perceptron (MLP) Artificial Neural Network for predicting band diagrams (BD) of bi-dimensional photonic crystals. To create the training datasets for MLP, we linked geometric and material features to BDs of photonic crystals with triangular and square lattices. We demonstrate that fast-training MLP models are able to estimate accurate BDs and existing photonic band gaps through rapid computations."}, {"label": 1, "content": "This work presents a novel formulation for traffic grooming, routing, and spectrum allocation in Elastic Optical Networks (EON) using Mixed Integer Linear Programming (MILP). In order to compare our method with classic Routing and Spectrum Allocation (RSA) formulation without grooming, the evaluation of the benefits of exact traffic grooming and RSA formulation are presented over three small networks. For each network, the objective function is set as minimizing the maximum slot index among all fibers. Then, the collected results show the benefits of traffic grooming in EON networks. However, due to the NP complexity of the MILP approach, we proposed a heuristic algorithm for large networks, and we also investigated the benefits of grooming using this algorithm."}, {"label": 1, "content": "A flexible spectral allocation method is proposed in this study, which takes into account the effect of the number of channels on the required spectral bandwidth. Elastic optical networks (EONs) are investigated in terms of their performance using a multilevel modulation format and coherent transmission. Network design parameters such as spectral bandwidth and channel symbol error rate (SER) are adopted as metrics for analysis. Transmission of quadrature phase shift-keying (QPSK) signals modulated at 56 and 100 Gbps is simulated and results for B-B, 50 and 200km are discussed."}, {"label": 1, "content": "In this paper, a control design problem for a nonlinear ball and beam system is discussed. A state feedback linear quadratic regulator (LQR) based control scheme is discussed using the flatness property of the system. A state estimation is carried out with Kalman filter while utilizing differentially flat nonlinear system written in the Brunovsky canonical form. The proposed control strategy is practically validated through tracking results, demonstrating its effectiveness."}, {"label": 1, "content": "This paper presents a novel miniaturized planar antenna for Ultra-wideband (UWB) Communication. The proposed antenna has novel semi-elliptical shape with sine-curve based parasitic structures and defected ground plane. The design covers the unlicensed (3.1-10.6 GHz) UWB band and provides impedance match over the licensed UWB band (10.6 GHz and above). Furthermore, the antenna achieves good radiation characteristics over the desired band while maintaining over 3-4 dBi gain and compactness. The employed substrate is low profile FR-4 having miniaturized dimensions of 30 mm \u00d7 25 mm. The scattering parameters, radiation characteristics surface current distributions and overall better gain make it a potential candidate for UWB applications."}, {"label": 1, "content": "In this paper, we present a new approach to design photonic crystal based optical filters using machine learning based mathematical model. The presented optical filter device finds its application in near infrared spectral range. The design and spectral response of the filter can be predicted using the proposed mathematical model which can considerably reduce simulation time and efforts. We elaborate on the numerical simulation of the optical filter device and also present the spectral results and mathematical modeling."}, {"label": 1, "content": "Wireless sensor networks are widely used in today's world, with sensor nodes being the basic components of these networks. However, such nodes are resource-constrained, with less memory, processing power, and battery capacity. As such, it is crucial to efficiently use the battery to increase the network's lifespan. This is where hierarchical routing protocols come in to allow sensor nodes to communicate in an energy-efficient way. Hierarchical routing protocols are energy efficient routing protocols which allows the sensor nodes to communicate with an energy efficient way. LEACH (Low Energy Adaptive Clustering hierarchical protocol) is clustering based hierarchical routing protocol which helps to reduce the power consumption by distributing the load among each sensor node and putting these sensor nodes into sleep mode when not in use. These techniques save energy consumption and provide accurate data that is useful in making better decisions. So, this leads to save the energy consumption and accurate data for making accurate decision by the base station for better utilization of the sensor network and fulfil the goal of implementation of the sensor network."}, {"label": 1, "content": "Fault tolerance is a paramount challenge faced by wireless sensor networks (WSNs) due to their mobile nature. These networks are mobile in nature hence achieving fault free function in these networks is always a challenge. As a result, it is crucial to effectively diagnose these networks to ensure their efficient operation. This research paper has delved into three major modules of fault diagnosis: monitoring, detection, and recovery. Extensive literature survey has been conducted to survey and analyze different techniques used in all these modules. The article has also highlighted some unresolved research problem areas. The given paper presents a comprehensive survey of techniques used to achieve fault tolerance in WSNs."}, {"label": 1, "content": "Cloud computing now-a-days used as means to enhance Information Technology's (IT) existing capabilities dynamically without putting extra cost in the name of infrastructure, highly skilled personnel, or buying licensed software. It offers the virtualized availability of resources at a lower cost. Here in this paper we are going to discuss important virtualization techniques being recently used along with critical security threats to virtualization."}, {"label": 1, "content": "Advancement in trends provokes new ideas in our mind. This paper presents a successful outline of the methodologies and approaches that can be utilized to create a robot capable of monitoring plant growth in an environment devoid of human life. Just as humans need adequate sustenance and nutrients to remain energetic and work efficiently, plants also require proper nourishment to survive and maintain growth. As humans need proper food and nutrients to keep us energetic and working, similarly plants require proper nutrients in order live longer. The proposed method consists of a mobile robot integrated with a novel algorithm (implementing the concepts of Computer Vision) to segment out the plants from the image in the best possible way. The practical application of this research extends beyond the improvement of plant and tree conditions. It has the potential to create significant advancements in our surroundings and atmosphere, leading to a better quality of life for ourselves and our planet."}, {"label": 1, "content": "Heart disease is the major cause of the death. Medical treatment and diagnosis of the heart disease is the major factor to improve the death rate. The main risk factors of the heart disease are obesity, tobacco, alcohol consumption and age factor etc. This paper aims to provide a big data analysis for predicting Coronary Artery heart disease. This paper presents a big data analysis for prediction of the Coronary Artery heart disease. The analysis of huge amount of a patient by using data mining and machine learning algorithms improves a hospital administration. Therefore, big data technologies and tools are now being employed to manage, store and extract meaningful insights from data."}, {"label": 1, "content": "Communication plays a vital role whether we talk about formal or informal but when it comes to device communication or objects communication. IoT plays very important role to understand real world objects after transforming into virtual objects and generate huge amount of data which is in structured as well as unstructured form. In this paper, the author focuses on several attributes that can provide feasible and optimal solutions for effective integration of IoT with Big Data to solve critical data analysis challenges. Author would also highlights various challenges faced in IoT and Big Data integration by providing reviews of various authors."}, {"label": 1, "content": "As an innovative networking paradigm in the Internet of Things, information-centric networking (ICN) offers improved security compared to traditional IP networks. However, it still faces numerous security threats, particularly those from internal attacks. The trust management technology is an effective approach for defending against internal attacks. In this paper, our contributions lie in our investigating the requirements of cybersecurity in ICN and analyzing the typical attack behaviors and defense schemes. Then, we propose a fast and efficient trust management scheme (FETMS) for defending against the on-off attack, which is an intelligent internal attack. Simulation results demonstrate that FETMS can efficiently detect and remove malicious nodes in a short time, while also achieving lower latency and improving mobility."}, {"label": 1, "content": "The classification of networked data is an interesting and challenging problem. Most traditional relational classifiers that are based on the principle of homophily have an unsatisfactory classification performance in networks with heterophily. This is because these methods treat inhomogeneous networks homogeneously. The first step of this new method involves estimating the class distribution of an unlabeled node based on the class distribution of its neighbors' neighbors. This estimation is performed separately on both known and unknown neighbors. The second step involves combining these two parts using multinomial na\u00efve Bayesian classification. Additionally, the relaxation labeling collective inference method, which utilizes simulated annealing, is paired with this new method to update class distributions as needed. Experimental results demonstrate that the proposed method performs significantly better when the network exhibits heterophily. Comparisons of the experimental results demonstrate that the proposed method performs better when the networks are heterophilous."}, {"label": 1, "content": "Inspired from the idea that the contexts in which a word occurs are of different significance, this paper proposes a novel method, called word representation with Salient Features (SaFe), to represent words using salient features selected from the context words. The SaFe method employs the point-wise mutual information (PMI) method with scaled context window to measure word association between a target word and its context. The number of salient features for a given word is determined by the ratio between the number of unique contexts and the total occurrences in the corpus. The SaFe approach can be applied to the positive PMI matrix (PPMI) and can be further decomposed using truncated singular vector decomposition to obtain dense vectors. Experimental results demonstrate that SaFe-PPMI achieves remarkable improvements in seven semantic relatedness tasks and outperforms state-of-the-art models. Besides being computationally efficient, SaFe provides a powerful tool for representing words in natural language processing applications."}, {"label": 1, "content": "Fog computing has risen as a promising technology for augmenting the computational and storage capability of the end devices and edge networks. However, there are challenges related to fog node planning, resource allocation, and offloading strategies that need to be addressed to achieve optimal performance. This paper aims to formulate a mathematical model which jointly tackles these issues. The goal of the model is to optimize the tradeoff (Pareto front) between the capital expenditure and the network delay. To solve this multiobjective optimization problem and obtain benchmark values, we first use the weighted sum method and two existing evolutionary algorithms (EAs), nondominated sorting genetic algorithm II and speed-constrained multiobjective particle swarm optimization. The effectiveness of this algorithm was evaluated using hypervolume and inverted generational distance indicators. The effectiveness of the proposed algorithm is evaluated by the hypervolume and inverted generational distance indicators. The optimized tradeoff between network delay and capital expenditure can improve the performance of fog computing, making it more attractive for deployment in a wide range of applications."}, {"label": 1, "content": "This paper presents a novel approach for diagnosing fault type and faulty phase of series compensated transmission lines. The proposed method employs the standard deviation (SD) principle along with the fast discrete orthonormal s-transform (FDOST) and the decision tree (DT) technique for fault classification. The FDOST, as an efficient signal processing tool, is used for extracting the features from a half cycle window of voltage and current signals sampled from one end of the power system network. The SD of a half cycle post-fault samples of the FDOST coefficients is then computed to generate the input feature vector for the DT-based classifier. The DT processes the features to accurately classify faults. The practicability of the proposed method is validated by modified Western System Coordinating Council 3-machine 9-bus system simulated in the PSCAD/EMTDC software and field fault data captured from a real transmission network of Chhattisgarh state, India. Experimental results demonstrate that the proposed method can successfully and reliably classify all types of faults, with high efficacy."}, {"label": 1, "content": "Traditional and current event-based adaptive control usually require designing integral-type adaptive laws to estimate a number of unknown parameters. However, this can be computationally expensive and time-consuming, particularly when online updating of parameter estimates is required. To address this, this paper proposes a novel event-triggered mechanism that generates switching-type adaptive laws instead of the traditional integral-type ones. Using nonsmooth Lyapunov analysis, the paper shows how this mechanism achieves preset tracking precision while ensuring the boundedness of all closed-loop signals. Finally, simulation results demonstrate the efficiency of the proposed algorithm."}, {"label": 1, "content": "We present a novel active learning approach for shape cosegmentation based on graph convolutional networks (GCNs). Our method represents collections of three-dimensional shapes as graph-structured data, with each node representing a primitive patch of an oversegmented shape and associated with a feature-based representation. Then, the GCN operates directly on the graph to update the representation of each node based on a layer-wise propagation rule, which aggregates information from its neighbors, and predicts the labels for unlabeled nodes. We also propose an active learning strategy that selects the most informative samples to expand the initial GCN training set and produce more accurate predictions. Our experimental results on the Shape COSEG dataset demonstrate the effectiveness of our approach."}, {"label": 1, "content": "Cross-validation (CV) is a widely used approach for selecting the best model. Its computation of empirical cross-validation error (CVE), however, is complex as it requires multiple rounds of training. To address this issue, we present a novel approximation theory of CVE and an approximate approach to CV based on the Bouligand influence function (BIF) for kernel-based algorithms. We first represent the BIF and higher order BIFs in Taylor expansions, and approximate CV via the Taylor expansions. An upper bound of the discrepancy between the original and approximate CV is derived. We provide a unique computing method to calculate the BIF for general distribution and evaluate BIF criterion for sample distribution to approximate CV. The proposed CV only requires training on the full data set once and is suitable for a wide variety of kernel-based algorithms. Experimental results demonstrate that the proposed approximate CV is sound and effective."}, {"label": 1, "content": "Discriminative correlation filter (DCF)-based trackers have recently exhibited high efficiency and impressive robustness to challenging factors, such as illumination change and partial occlusion. However, both have their limitations in detecting objects in certain situations. To overcome these limitations, we propose a real-time complementary tracker (RCT) that integrates DCF and Siamfc into a two-stage tracking framework. In this paper, we propose a real-time complementary tracker (RCT) by integrating DCF and Siamfc into a two-stage tracking framework where DCF and Siamfc share mutual advantages and complement each other. In the second stage, the derived coarse location is refined by DCF for higher accuracy. In the second stage, the derived coarse location is refined by DCF for higher accuracy. Efficiency is achieved by activating Siamfc occasionally based on the tracking status inferred from the correlation response map of DCF in the second stage. We conducted comprehensive experiments on three benchmark datasets: OTB2013, OTB2015, and VOT2016. On OTB2013, RCT runs with over 40 f/s and achieves an absolute gain of 4.8% and 5.2% in mean overlap precision compared with two base trackers (Staple and Siamfc). On VOT2016, RCT ranked fifth in EAO and first in EFO compared to the top five trackers, striking a good balance between performance and efficiency."}, {"label": 1, "content": "To address the issue of limited samples in supervised convolutional neural network (CNN) models for medical hyperspectral images (MHSI) classification, a two-channel CNN has been developed. The first channel, EtoE-Net, uses unsupervised learning to obtain representative and global fused features with fewer noise, achieved by mapping pixel-by-pixel between the original MHSI data and its principal component. Meanwhile, a traditional CNN is incorporated to provide local detailed information. The features extracted from different underlying layers of two channels (i.e., EtoE-Net and typical CNN) are concatenated into a vector, which is expected to preserve global and local informations simultaneously. The proposed network, EtoE-Fusion, utilizes full connection for feature dimensionality reduction. To assess the efficacy of the proposed framework, experiments conducted on two different MHSI data sets have shown promising results for classification accuracy."}, {"label": 1, "content": "The Internet of Things (IoT) provides a beautiful and intelligent landscape for humanity's future. However, due to the vast network of sensors and devices, the energy consumption of IoT can be significant. As such, it is crucial to research and develop energy-saving and energy-efficient applications for IoT. In particular, sustainable operation for wireless sensors in IoT is essential as they have limited battery capacity. This paper attempts to study an IoT network containing wireless sensors and base stations. Wireless power transfer techniques are becoming increasingly popular for charging sensors' batteries, and a charging vehicle can supply electrical power to these sensors. For wireless sensors, a charging vehicle is responsible for the electrical power supply. To save electrical energy, data transfer of the discussed IoT network scenario is expressed as a minimization problem. To solve the subproblems of the data transfer model, a restart artificial bee colony (RABC) method is proposed. This method is proven to asymptotically converge to the optimal solution of the problem, making it an effective optimization technique. It is proved that the RABC method asymptotically converges to the optimal solution of the problem. Numerical simulations show that energy consumption in the studied network scenario can be minimized using the proposed method with a good, robust property."}, {"label": 1, "content": "In this paper, we explore the use of explicit user feedback, such as 5-star numerical ratings, to improve top-N recommendation systems. This approach has been largely overlooked in the past decade, but we propose a novel and generic transfer learning based recommendation framework, CoFiToR, which extends a recent method called ToR. Our key idea is to model user behavior by simulating their shopping processes, and we convert the ranking problem into three subtasks that correspond to specific questions: whether an item will be examined by a user, how an item will be scored by a user, and whether an item will be purchased by a user. Therefore, we convert the studied ranking problem to three subtasks corresponding to three specific questions, including (i) whether an item will be examined by a user, (ii) how an item will be scored by a user, and (iii) whether an item will finally be purchased by a user. Based on this new conversion, we then develop a three-staged solution that progressively models users' preferences from a coarse granularity to a fine granularity. Empirical studies on two large public datasets demonstrate the superiority of our solution over state-of-the-art methods. Empirical studies on two large and public datasets showcase the merits of our solution in comparison with the state-of-the-art methods."}, {"label": 1, "content": "Due to the unavailability of Vehicle-to-Infrastructure (V2I) communication in current transportation systems, Traffic Light Detection (TLD) is still considered an important module in autonomous vehicles and Driver Assistance Systems (DAS). To address the low flexibility and accuracy of vision-based heuristic algorithms and the high power consumption of deep learning-based methods, we present a lightweight and real-time traffic light detector for autonomous vehicle platforms. Our model consists of a heuristic candidate region selection module to identify all possible traffic lights, and a lightweight Convolution Neural Network (CNN) classifier to classify the results obtained. Offline simulations on the GPU server with the collected dataset and several public datasets show that our model achieves higher average accuracy and less time consumption. By integrating our detector module on NVidia Jetson TX1/TX2, we conduct on-road tests on two full-scale self-driving vehicle platforms (a car and a bus) in normal traffic conditions. Our model can achieve an average detection accuracy of 99.3 percent (mRttld) and 99.7 percent (Rttld) at 10Hz on TX1 and TX2, respectively. The on-road tests also show that our traffic light detection module can achieve <; + 1:5m errors at stop lines when working with other selfdriving modules."}, {"label": 1, "content": "In this letter, we consider secure transmissions of millimeter wave simultaneous wireless information and power transfer unmanned aerial vehicle (UAV) relay networks, where a source transmits confidential information to an energy-constrained destination on the ground with the help of a UAV-based relay in the presence of multiple independent homogeneous Poisson point process eavesdroppers. First, we derive closed-form expression of the lower bound of average secrecy rate using 3-D antenna gain model. Then, we study the secrecy rate lower bound maximization problem by optimizing the source/UAV transmit power, power splitting ratio, and UAV's location. This algorithm involves optimizing the source/UAV transmit power, power splitting ratio, and UAV's location, and employs an alternate optimization approach by iteratively solving three sub-problems. Simulation results show that the proposed algorithm can greatly improve the average secrecy rate."}, {"label": 1, "content": "Intelligent offloading of computation-intensive tasks to a mobile cloud server provides an effective mean to expand the usability of wireless devices and prolong their battery life, especially for low-cost internet-of-things (IoT) devices. Nevertheless, implementing this technology in multiple-input multiple-output (MIMO) systems requires a sophisticated design of joint computation offloading and other network functions, such as channel state information (CSI) estimation, beamforming, and resource allocation. In this paper, we study the computation task offloading and resource allocation optimization in MIMO based mobile edge computing systems considering perfect/imperfect-CSI estimation. Our objective is to minimize the maximum weighted energy consumption while adhering to practical constraints on available computing and radio resources and allowable latency. To tackle the underlying mixed integer non-linear problems (MINLP), we propose optimal and low-complexity algorithms. For perfect-CSI, we use bisection search to find the optimal solution. Meanwhile, for low-complexity algorithms, we break down the original optimization problem into offloading optimization (OP) and power allocation (PA) subproblems, and solve them iteratively. For imperfect-CSI, we employ the difference of convex functions (DC) method to manage the non-convex structure of (PA) subproblems. Numerical results confirm the advantages of proposed designs over conventional local computation strategies in energy saving and fairness."}, {"label": 1, "content": "The direct use of deep learning for remote sensing image scene classification is greatly limited by the lack of annotated samples. Although previous research has attempted to address this issue by using data augmentation with various image transformations, the quantity and diversity of samples remain restricted. Recently, the advent of the unsupervised learning-based generative adversarial networks (GANs) brings us a new way to generate augmented samples. However, such GAN-generated samples are currently only served for training GANs model itself and for improving the performance of the discriminator in GANs internally (in vivo). It becomes a question of serious doubt whether the GAN-generated samples can help better improve the scene classification performance of other deep learning networks (in vitro), compared with the widely used transformed samples. SiftingGAN extends the traditional GAN framework through Online-Output method for sample generation, Generative-Model-Sifting method for model sifting, and Labeled-Sample-Discriminating method for sample sifting. Experiments conducted on the well-known aerial image data set reveal that the proposed SiftingGAN approach not only effectively improves the performance of the scene classification baseline without data augmentation but also significantly outperforms comparison methods based on traditional geometric and radiometric transformation operations. Experiments on the well-known aerial image data set demonstrate that the proposed SiftingGAN method can not only effectively improve the performance of the scene classification baseline that is achieved without data augmentation but also significantly excels the comparison methods based on traditional geometric/radiometric transformation operations."}, {"label": 1, "content": "This paper presents an automatic content-based image retrieval (CBIR) system for brain tumors on T1-weighted contrast-enhanced magnetic resonance images (CE-MRI). The key challenge in CBIR systems for MR images is the semantic gap between the low-level visual information captured by the MRI machine and the high-level information perceived by the human evaluator. The traditional feature extraction methods focus only on low-level or high-level features and use some handcrafted features to reduce this gap. It is necessary to design a feature extraction framework to reduce this gap without using handcrafted features by encoding/combining low-level and high-level features. Deep learning, which is excellent at feature representation, is more than capable of depicting low-level and high-level information in full, embedding the phase of feature extraction in self-learning. Therefore, we propose a deep convolutional neural network VGG19-based novel feature extraction framework and apply closed-form metric learning to measure the similarity between the query image and database images. Furthermore, we adopt transfer learning and propose a block-wise fine-tuning strategy to enhance the retrieval performance. We conducted extensive experiments on a publicly available CE-MRI dataset comprising three types of brain tumors (glioma, meningioma, and pituitary tumor) from 233 patients and 3064 images spanning the axial, coronal, and sagittal views. Our method is more versatile in that it requires minimal preprocessing, doesn't use any handcrafted features, and outperforms the state-of-the-art CBIR systems on the CE-MRI dataset, achieving a fivefold average precision of 96.13% while being robustly tested using fivefold cross-validation."}, {"label": 1, "content": "Localization is perhaps one of the most interesting research subjects in signal processing. Traditional localization methods rely on the prior information whether the source is located in the near-field or far-field. However, in practice, obtaining this prior knowledge can be difficult, which can lead to unreliable performance for both positioning and bearing. To solve this contradiction, the modified polar representation (MPR) has been proposed as a uniform framework that eliminates the thresholding effect as the source range increases. While the maximum likelihood estimator (MLE) in current research provides excellent performance reaching the Cram\u00e9r-Rao lower bound (CRLB), it is computation-complex and time-consuming. Furthermore, it can have a divergence problem if the initial value is not close enough to the true value. This paper focuses on the localization problem using time difference of arrival measurements in MPR. We propose a two-step least squares (LS) estimator for the MPR model to address these issues. The weighted total LS is applied in the first step and the weighted LS for the final solution, where the second step is based on the natural constraint of the unknowns. The proposed method is closed-form and able to reach CRLB in low noise power situation irrespective of the source range. We analyzed the covariance and proved that it provides CRLB accuracy for Gaussian noise theoretically. Our simulations support these theoretical results, demonstrating that our proposed method is comparable with MLE but simpler and more computationally efficient. It outperforms the classical closed-form solution."}, {"label": 1, "content": "Decisions may not solely rely on the performance of alternatives, but also on how they satisfy the decision makers' aspiration levels. To address this, the paper proposes a linguistic aspiration-based solution for qualitative decision-making (QDM), which deals with complex problems involving multi-criteria, multi-groups of experts, and multi-granular linguistic information. Complex linguistic expressions (CLEs), including hesitant fuzzy linguistic term sets and weakened hedges, can express aspiration levels and performances. Based on the conventional aspiration-based approaches, the value function is defined by the probability of a CLE achieving its linguistic aspiration level. The performance of the proposed QDM approach is then demonstrated by solving the problem regarding the provider evaluation and selection. To demonstrate performance, the proposed QDM approach resolves the evaluation and selection of a provider."}, {"label": 1, "content": "Measurement of the electricity consumption of major appliances in different time segments is of crucial significance to demand-side management and energy conservation. Non-intrusive load monitoring (NILM) is a technique that can determine the power usage information of target appliances by solely analyzing the aggregate power data at a single entrance point. Inspired by the success of deep neural network in other fields, some researchers have applied it to NILM with promising results. However, the present research calls for labeled real aggregate data to train the networks, which is challenging to obtain in realistic situations due to the difficulty of synchronized measurements for labeling the target appliance. To overcome this challenge, the paper proposes a novel approach to train networks using only synthetic aggregate data. Furthermore, a training data generation method via background filtering is proposed, and the obtained training data is used to train the network for estimating electricity consumption. The proposed estimation method has been tested on a public dataset and proved to have a higher accuracy than the current methods, demonstrating the effectiveness of background filtering. Overall, this study highlights the potential of deep neural networks in NILM and proposes an innovative method that can reduce the complexity of training data acquisition, thus providing a practical solution for implementing NILM in real-world situations."}, {"label": 1, "content": "This paper presents a novel method for HSI classification utilizing limited labeled samples in both domains, called heterogeneous domain adaptation (HDA). The method is achieved through cross-domain collaborative learning (CDCL), which includes cluster canonical correlation analysis (C-CCA) and random walker (RW) algorithms. To be specific, the proposed CDCL method is an iterative process of three main components, i.e., RW-based pseudolabeling, cross-domain learning via C-CCA, and final classification based on extended RW (ERW) algorithm. First, given the initially labeled target samples as the training set (TS), the RW-based pseudolabeling is employed to update TS and extract target clusters (TCs) by fusing the segmentation results obtained by RW and ERW classifiers. Second, cross-domain learning via C-CCA is applied using labeled source samples and TCs. The unlabeled target samples are classified using estimated probability maps, which are generated using the model trained in the projected correlation subspace. The newly estimated probability map and TS are used for updating TS again via RW-based pseudolabeling. Finally, when the iterative process converges, the result obtained by the ERW classifier using the final TS and estimated probability maps is regarded as the final classification map. Experimental results on four real HSIs demonstrate that the proposed method can achieve better performance compared with the state-of-the-art HDA and ERW methods."}, {"label": 1, "content": "Spatiotemporal processing is a complex task that involves speech recognition, object tracking, and natural language processing. The developmental network-1 (DN-1) was developed to address this challenge with a general-purpose algorithm that functions similar to Turing machines. DN-1 employs emergent motor vectors, which act as contexts for top-down attention, enabling effective temporal processing. This also enables finite automaton (FA), which is the control of TM, to incrementally emerge inside DN-1. However, DN-1 has limitations as its hidden areas have fixed boundaries. To address this issue, a new model called developmental network-2 (DN-2) has been proposed. In DN-2, hidden neurons have the ability to automatically discover their neuron-wise inhibition zones based on incremental statistics. This feature enables the number of hidden areas and their interconnections to be dynamic. The authors of the paper first present DN-2 using a visual toy problem and then conduct phoneme recognition experiments to investigate its performance in various settings. Based on the experimental results, we analyzed how the new mechanisms enabled DN-2 to automatically generate a dynamic hierarchy and improve the performance. The ultimate goal is to use DN-2 for a wide range of spatiotemporal processing tasks."}, {"label": 1, "content": "Energy is an important resource in wireless sensor nodes which gets depleted soon, especially in harsh environments, where the sensor's batteries once deployed cannot be reached easily for replacement or recharging activities. Therefore, the development of energy-efficient routing protocols for wireless sensor networks is a critical objective in applications like structural health monitoring and military surveillance. In this paper, energy optimization is performed by the clustering technique. In clustering, the energy harvesting sensor nodes, which are close to the sink and having a higher probability of energy arrival rate, are elected as cluster head nodes. The remaining nonelected node reserves a portion of their harvested energy to be used by them when they are elected as cluster heads. Thus, utilizing this reserved harvested energy, the cluster head nodes can survive longer. Additionally, optimal sensor node placement in SHM applications is essential for cost-effectiveness. Also, sensor node placement optimization is important in SHM applications, as if the desired information quality is achieved with less number of sensor nodes, then it is a cost-effective method. Overall, this research demonstrates the potential of optimizing energy consumption and sensor node placement through clustering, and it can significantly improve the performance of wireless sensor networks in harsh environments."}, {"label": 1, "content": "Recent studies have highlighted the potential for improving the performance of convolutional neural networks by incorporating a learning block that captures spatial correlations. In this paper, we propose a novel multiple feature reweight DenseNet (MFR-DenseNet) architecture. The MFR-DenseNet improves the representation power of the DenseNet by adaptively recalibrating the channel-wise feature responses and explicitly modeling the interdependencies between the features of different convolutional layers. First, in order to perform dynamic channel-wise feature recalibration, we construct the channel feature reweight DenseNet (CFR-DenseNet) by introducing the squeeze-and-excitation module (SEM) to DenseNet. Then, to model the interdependencies between the features of different convolutional layers, we propose the double squeeze-and-excitation module (DSEM) and construct the inter-layer feature reweight DenseNet (ILFR-DenseNet). In the last step, we designed the MFR-DenseNet by combining the CFR-DenseNet and the ILFR-DenseNet with an ensemble learning approach. Our experiments demonstrate the effectiveness of CFR-DenseNet, ILFR-DenseNet, and MFR-DenseNet. More importantly, the MFR-DenseNet drops the error rate on CIFAR-10 and CIFAR-100 by a large margin with significantly fewer parameters. Our 100-layer MFR-DenseNet (with 7.1M parameters) model achieves competitive results on CIFAR-10 and CIFAR-100 data sets, with test errors of 3.57% and 18.27% respectively, achieving a 4.5% relative improvement on CIFAR-10 and a 5.09% relative improvement on CIFAR-100 over the best result of DenseNet (with 27.2M parameters)."}, {"label": 1, "content": "This paper describes fast versions of model predictive control (MPC) algorithms and their practical implementation using the STM32 ARM microcontroller. Two popular MPC approaches, Dynamic Matrix Control (DMC) and Generalized Predictive Control (GPC), were examined. Computationally efficient Fast DMC (FDMC) and Fast GPC (FGPC) algorithms are derived in which the values of the manipulated variables are calculated from uncomplicated explicit formulas. The efficacy of the proposed algorithms was tested using two benchmark processes and a laboratory servo. Additionally, the effects of various tuning parameters on the computation time of the algorithms were investigated. It is shown that for short sampling periods the classical DMC and GPC algorithms fail to work since calculations last longer than the sampling period, which is unacceptable. Conversely, the FDMC and FGPC algorithms require only hundreds or tens of microseconds and single microseconds, respectively. Hence, proposed algorithms may be used for very fast processes, with very short sampling time."}, {"label": 1, "content": "Cross-camera label estimation from a set of unlabeled training data is an extremely important component in the unsupervised person re-identification (re-ID) systems. It allows for the utilization of advanced supervised learning methods to learn discriminative re-ID models. In this paper, we utilize the graph matching technique for accurate label estimation due to its advantages in optimal global matching and intra-camera relationship mining. However, the non-learned similarity measurement used in constructing the graph structure cannot handle large cross-camera variations, leading to inaccurate label outputs. This paper designs a dynamic graph matching (DGM) framework, which improves the label estimation process by iteratively refining the graph structure with better similarity measurement learned from the intermediate estimated labels. In addition, we design a positive re-weighting strategy to refine the intermediate labels, which enhances the robustness against inaccurate matching output and noisy initial training data. Furthermore, a co-matching strategy is incorporated to reduce false matchings and fully utilize abundant video information. Comprehensive experiments conducted on three video benchmarks demonstrate that DGM outperforms the state-of-the-art unsupervised re-ID methods and yields the competitive performance to fully supervised upper bounds."}, {"label": 1, "content": "The growth of the first generation Internet of Things (IoT) systems for low-end devices with limited memory and infrastructure is being hindered by the lack of engineering support in terms of architecture, technology, hardware, privacy, security, and business. This paper explores the relevant system engineering issues in IoT processors and suggests that the use of rough set as an approximate computing technique could be the solution for the next generation IoT hardware. The proposed approach has a potential of dealing with inconsistencies arising at layers in hardware infrastructure for IoT applications with a dominance at cloud and fog processing. The authors bring out a research direction in IoT as an engineering discipline with an emphasis on the impact of processor hardware extension."}, {"label": 1, "content": "Detecting event-related potential (ERP) is a challenging problem because of its low signal-to-noise ratio and complex spatial-temporal features. The conventional ensemble averaging technique used for detection may eliminate subtle but important information in the signals, ultimately leading to poor detection performance. To address this issue, we propose a spatial-temporal discriminative restricted Boltzmann machine (ST-DRBM) that extracts spatial and temporal features for ERP detection, inspired by the discriminative performance of DRBM in feature extraction and classification. The experimental results and statistical analyses show that our proposed method achieves state-of-the-art ERP detection performance. The ST-DRBM not only serves as an effective ERP detector but also proves to be a practical tool for ERP analysis. Based on the proposed model, similar scalp distribution and temporal variations were found in the ERP signals of different sessions, which indicated the feasibility of cross-session ERP detection. Given its state-of-the-art performance and effective analytical technique, ST-DRBM is promising for ERP-based brain-computer interfaces and neuroscience research."}, {"label": 1, "content": "This paper proposes a new algorithm to evaluate the performance of the mobile edge computing system. The algorithm proposes a new metric called computation efficiency, which is the ratio of the number of calculated data bits to the corresponding energy consumption. To ensure that the data is computed in a timely manner, the algorithm combines the local computing and data offloading schemes into a joint computation algorithm. To maximize the computation efficiency among users, an optimization problem with weighting factors is formulated. The problem is efficiently solved using iterative and gradient descent methods. Simulation results show that the proposed scheme outperforms traditional approaches. In addition, the tradeoff study between local computing and data offloading reveals that when data size is small, local computing plays a more important role, but when the size grows, data offloading becomes preferable."}, {"label": 1, "content": "Monitoring for industrial machines can get benefit from Cloud Computing and Internet of Things capabilities. This research proposes a new approach to monitor machines based on the low coast board (Omega2 Plus). The Omega2 Plus is an embedded system and its shape and size like a credit card work as a computer. In industrial factories, numerous machines perform different functions. In any industrial factory there are multiple types of machines, this approach coupled machines logical outputs to Omega2 Plus GPIO and send data to web-based server to be monetarized. The Cloud Monitoring System (CMS) gives a real-time information about production data, current work, the employee ID working on a given machine, the production rate, and downtime. All machines monitored by supervisors or specialists sitting in a remote place virtually any part of the world. Moreover, this simple system can be operated by untrained personnel employed at the basic level."}, {"label": 1, "content": "Teeth play an integral role in the physiology of vertebrates. The accurate classification of teeth is beneficial for dentists to diagnose. This manuscript proposes a novel algorithm that utilizes principle component analysis and extreme learning machine to classify teeth. Principle component analysis is employed to extract features from teeth images, while extreme learning machine serves as the classifier. Our experimental results demonstrate the efficacy of this method in identifying four distinct categories of teeth."}, {"label": 1, "content": "There are numerous prospective applications for floated Wireless Sensor Networks (WSN), including sea surveillance and ship detection. One of the ways to detect the existence of a ship is by observing the waves it creates. In this paper, we develop a framework using WSN to measure sea and ship waves. This framework involves both hardware and software components. We use accelerometer and Arduino Uno for hardware. The software component involves the design and implementation of a spectrum measurement algorithm using Processing programming language. In the study, experiments were conducted in the Java sea to measure the spectrum of both sea and ship waves. The results show that the spectrum of sea waves and ship waves are 0.63 Hz and 1.25 Hz, respectively."}, {"label": 1, "content": "Cloud computing is revolutionizing biological systems by providing integrated solutions that offer ownership, streamlined layout, configuration, quantification, and computerization. However, this transformation presents several security concerns that must be addressed. These are some of the main challenges of the cloud computing environment: measuring trust, multi-tenure, loss of administration. This document reviews current advances and offers a broad range of cloud security and assurance solutions. Additionally, it examines information security concerns in distributed computing and outlines strategies to tackle these issues effectively."}, {"label": 1, "content": "Leaf dimensions, specific mass, and composition are important traits that are closely linked to the ability of leaves to exchange gases with the environment. These variables are influenced by many factors and vary according to changing growth conditions. Plant growth models are effective tools for exploring a wide range of climatic scenarios, management practices, and genotypes. However, most models lacks process-based formalisms allowing simulating shoot architecture plasticity. We propose a functional-structural wheat model that couples carbon and nitrogen metabolism with leaf morphogenesis during the vegetative stage. The originality of our model relies on the interaction between leaf growth and the metabolism of carbon and nitrogen in the growing zone, which is possible thanks to an explicit and detailed formalism of the processes at organ level. The model simulates the appearance of successive leaves using coordination rules instead of a constant phyllochron as a driving mechanism. Our model simulates the appearance of successive leaves using coordination rules, rather than a constant phyllochron as the driving mechanism. The model shows interesting emergent properties: phyllochron stability, pattern of mature leaf length along the culm and realistic kinetics of length, dry mass and concentrations in both growing and mature zones. To assess the efficacy of our completely integrated model at the plant scale, we propose a qualitative evaluation strategy. As a conclusion, the model appears to be a useful concept, which could be transposed to other grasses."}, {"label": 1, "content": "Penetration testing (PT) is a proactive method for evaluating and assessing the security of digital assets by executing various attacks that can exploit vulnerabilities. Current PT practice is becoming repetitive, complex and resource consuming despite the use of automated tools. The goal of this paper is to design an intelligent PT approach using reinforcement learning (RL) that will allow regular and systematic testing, saving human resources. The system is modelled as a partially observed Markov decision process (POMDP), and tested using an external POMDP-solver with different algorithms. Although this paper is limited to only the planning phase and not the entire PT process, the results support the hypothesis that reinforcement learning can enhance PT beyond the capabilities of any human expert in terms of accurate and reliable outputs."}, {"label": 1, "content": "Motivated by the increasing complexity of modern society and the need to improve the efficiency of highly connected teams and networks, multiple disciplines have come together to understand how these teams operate. However, team building is complicated due to structural diversity, network heterogeneity, and scope differences. In order to build a universal recommendation system, we apply biological phenomena, known as the catfish effect, to built a team recommendation system. By extracting the factors which have a direct influence on team performance and modelling RBF neural network, we can predict the relationship between team performance and factors. We design a CTI(Catfish Identification) algorithm based on RBF i.e., RBF -CTI algorithm, which aims to calculate catfish index CI i.e., the intensity of cat-fish effect of each factor. According to CI, we can easily identify the skill of catf ish, namely the most potential impact factor for the team, as well as the indicator of team recommendation system. We apply the idea of the combination of multi-disciplines and give a universal framework of team recommendation system."}, {"label": 1, "content": "The prevalence of media news sources has increased significantly in recent years, leading to a growing concern about the credibility of news content. This brings to the service news credibility issue which emerges to become one of the most important issues concerning assessment of news items with respect to either the news editors' personnel or the readers of news items. In an effort to address this issue, we have developed a model known as NCMOSWS that leverages established semantic weighing methods and ontology to accurately assess the credibility of news items. We apply our model on the RSS of a set of news agencies and show that the model is highly reliable on measuring partial credibility of news items."}, {"label": 1, "content": "We proposed a method to compute photosynthesis in response to environmental factors, applying in stadia environment. We managed to visualize heterogeneity of photosynthesis level for turfgrasses used in football pitch. This allows us to not only identify the spatial variation in photosynthesis, but also track its temporal evolution throughout the year. Our model takes into account a number of key environmental factors including air temperature, daylength, intercepted irradiance and leaf area index, to evaluate the gross photosynthesis of the turfgrass on a daily basis. With a resolution of 7140 surface units, we can obtain daily gross photosynthesis on the whole pitch. Using this cartography, we can identify areas of the pitch where the turfgrass is experiencing a deficit of photosynthesis, allowing pitch managers to prioritize these areas for further attention and care."}, {"label": 1, "content": "In wireless sensor networks, data messages containing sensor data achieved by a sensor module in a wireless sensor node is transmitted to a stationary wireless sink node along a wireless multihop transmission route in which wireless sensor nodes themselves forward the data messages. Each intermediate wireless sensor node broadcast data messages in its wireless transmission range to forward them to its next-hop intermediate wireless sensor node. Unfortunately, this also means that eavesdropper wireless nodes within this range can easily overhear the data messages. To prevent illegal eavesdropping, wireless sensor nodes that have overlapping wireless transmission ranges avoid passing data messages to next-hop intermediate wireless sensor nodes within these overlapping regions, and deliberately cause collisions at possible eavesdropper wireless nodes. This paper proposes an algorithm that assigns specific time slots for transmitting data messages, which maximizes the opportunities for interfering with eavesdroppers and enhances the regions where concurrently forwarded data messages collide, thereby preventing their overhearing."}, {"label": 1, "content": "Digital Health Care not only offers benefits but also widens the socio-economic gap between developed and underdeveloped countries. Revenue could gain only some number of countries, which will get the more attractive position at global market of trans border online patient-doctor communications. This intense competition encourages every country to embrace digital Health Care and innovate to stay at the forefront of the industry. Also digital technologies can quickly destroy an existing Health Care system in a country. It is imperative to establish the necessary digital infrastructure to reduce the risk of negative effects. Besides the global competition issue there is the problem of future uncertainty in society. It is necessary to understand all challenges and develop measures to prevent a negative scenario."}, {"label": 1, "content": "To build a successful machine learning model, a vast amount of training data is required, which is often manually labelled. We propose a method to efficiently build an action recognition dataset in the handball domain, focusing on minimizing the manual labor required to label the individual players performing the chosen actions. The method uses existing deep learning object recognition methods for player detection and combines the obtained location information with a player activity measure based on spatio-temporal interest points to track players that are performing the currently relevant action, here called active players. The proposed method was successfully tested on real-world handball practice videos, leading to an 84% accuracy of correctly tracked and labelled active players, making the process of building a reliable dataset more efficient."}, {"label": 1, "content": "One of the major challenges in hyperspectral imaging (HSI) is the selection of the most informative wavelengths within the vast amount of data in a hypercube. To address this, band selection can be employed to minimize data and computational costs while addressing the negative impacts of superfluous and incorrect information. In this paper, we propose an unsupervised, embedded band selection algorithm that utilises the deep learning framework. Specifically, autoencoders are employed to rebuild measured spectral signatures. By putting a sparsity constraint on the input weights, the bands that contribute most to the reconstruction can be identified and chosen as the selected bands. Additionally, segmenting the input data into several spectral regions and distributing the number of desired bands according to a density measure among these segments, the quality of the selected bands can be increased and the computational time reduced by training several autoencoders. Results on a benchmark remote sensing HSI dataset show that the proposed algorithm improves classification accuracy compared to other state of the art band selection algorithms and thereby builds the basis for a framework of embedded band selection in HSI."}, {"label": 1, "content": "This paper proposes a solution to the learning problem for data-adaptive transform that offers sparse representation in a space with dimensions larger than the dimensions of the original space. We present an iterative, alternating algorithm that has two steps: (i)transform update and (ii)sparse coding. In the transform update step, we focus on novel problem formulation based on a lower bound of the objective that addresses a trade-off between (a) how much are aligned the gradients of the approximative objective and the original objective, and (b)how much the lower bound is close to the original objective. This formulation enables the authors to propose an approximate closed form solution that can lead to accelerated local convergence and provides a satisfactory solution with a small amount of data. Moreover, since the approximate closed form solution preserves the gradient in the transform update, and exact closed form solution is used in the sparse coding step, the resulting algorithm is convergent. On the practical side, we evaluate on image denoising application and demonstrate promising denoising performance together with advantages in training data requirements, accelerated local convergence and the resulting computational complexity."}, {"label": 1, "content": "The black box arc model is suitable to describe the arcing process of hybrid circuit breaker, since it provides an effective way to integrate the arc model into an electrical circuit. However, determining the parameters of the arc model based on circuit breaker performance is a more challenging task than solving the analytical equations of the model. A library of arc models including Mayr, Cassie, Kema and other models has been built and verified based on MATLAB Simulink/Simscape. To estimate the parameters of the arc models, heuristic optimization methods such as Genetic Algorithm, Simulated Annealing, and Particle Swarm Optimization have been utilized. The results indicate that the inverse modeling technique effectively identifies the appropriate parameters to represent the changing conductance of the arc plasma. It is also found that the GA with well-selected parameters has the advantage over other methods."}, {"label": 1, "content": "The field of gait analysis has seen recent advancements in techniques for identifying individuals based on their unique gait patterns. One of appearance-based methods based on GEI (Gait Energy Image) has been used for forensic purpose in Japan. However, when these conditions are not met, identification accuracy is compromised, rendering analysis impossible. To address these limitations, we propose a novel human identification method that combines the strengths of both appearance-based and model-based methods to compare dynamic features. Here, we construct a novel human identification method based on comparison of dynamic features, which takes advantage of features of both appearance-based method and model-based method. Feature points (resemble to joint-points in model-based method) and those local shape features are semi-automatically extracted from silhouette sequences, and then the matching probability of two footages is calculated by comparing the dynamics of extracted features. Therefore, while the GEI-based method characterizes \"figure\" information, our proposed method is superior for characterizing \"dynamic\" information of human gait. The results suggested that GEI-based method is superior to characterizing `figure' information, whereas the proposed method is superior to characterizing `dynamic' information of human gait."}, {"label": 1, "content": "Essentially, many digital cameras have the ability to capture extended spectral information beyond the visible spectrum by modifying the device to remove the internal ICF (Infrared Cut-off Filter) filter. So, using a conventional digital camera and stripping off the internal ICF (Infrared Cut-off Filter) filter, we use the captured Visible + NIR images (also called full-spectrum or VNIR images) for the classical face recognition problem. These images are stored as 3-channel RGB files and can be used to train and evaluate CNNs, resulting in surprisingly good results. Interestingly, using RGB+NIR images (4 channels) does not perform as well - this is because the contribution of the blue channel to this task is weak. The paper shows that the contribution of the blue channel to this task is weak, and the recognition rate raises significantly when NIR is added to the channels, adding information and increasing signal to noise ratio especially in the blue channel."}, {"label": 1, "content": "In today's modern cities, public transportation plays a crucial role in facilitating mobility. The main objective is to propose solutions to a given, practical problem, which specifically concerns the bus arrival time at various bus stop stations, by taking to account local traffic conditions. We show that a global prediction approach, under some global macro-parameters (e.g., total number of vehicles or pedestrians) is not feasible. Hence, a more specific approach is needed, where traffic conditions are represented by a traffic density matrix. The results obtained from both linear and neural network (NN) approaches demonstrate that the NN method provides a 24% increase in accuracy compared to basic linear regression. Overall, this finer granularity approach shows promising results, providing a solution to the bus arrival time prediction problem."}, {"label": 1, "content": "The paper introduces a new method for suppressing linear motion blur and out-of-focus blur in photographic images. Typical image deconvolution methods have a regularization parameter that balances between removing blur and preventing artifacts such as ringing and noise. The idea of the proposed image deblurring method is to apply grid warping approach to improve image sharpness after conventional image deconvolution algorithms used with strong regularization. Grid warping algorithm moves pixels in edge neighborhood area towards the edges making them sharper without introducing artifacts like ringing and noise. The method is expected to produce the same level of sharpness as conventional image deconvolution, but with a reduced risk of producing artifacts. In order to validate the proposed scheme, we have applied it to artificially blurred images and images with real blur, with different levels of noise and blur radii, directions and lengths."}, {"label": 1, "content": "RFID networks face certain limitations due to the interference caused by concurrent operation of readers. This paper explores a comprehensive approach to determine if a given system load allows for stable operation, which means that queue backlogs do not keep increasing. Our assumption is that the RFID network functions in a time-slotted manner, where traffic comes in the form of tag batches to RFID cells. The service rate (necessary to determine stability regions) of a RFID cell, given the batch size distribution and any arbitrary FSA policy is computed by simulation. To demonstrate this framework, we provide several examples."}, {"label": 1, "content": "Physical distortions alongside digital artefacts are frequently observed in document images. Their presence sabotages the optical character recognition (OCR) process which not only leads to a reduced amount of automatically retrievable content, but also deteriorates the performance of other document analysis algorithms that rely on layout analysis or content recognition. In this study, a method for identifying and eliminating specific types of physical distortions from document images is proposed. By exploiting the intensity and spatial relation of distorted pixels, we construct a conditional random field (CRF) based method for distortion identification. Additionally, a peak searching approach is introduced to learn the energy functions' model parameters in the CRF model from the image automatically. Discrimination between pixels originating from the genuine document content and those related to physical noise is achieved by maximizing the CRF model's conditional probability. Real-life image samples illustrate the efficacy of the proposed method."}, {"label": 1, "content": "The problem of making the Kalman filter robust in a case of undetermined noise statistic has been considered in the paper. Firstly, the equivalence between the Kalman filter and a specific linear dynamic stochastic approximation recursive algorithm is established. In addition, the dynamic stochastic approximation algorithm is considered as a regression problem, and the regression problem is solved in a robust manner using the M-robust statistical approach. In order to improve the convergence rate, the gain matrix of the proposed state estimator is derived from the minimization of the particular criterion. A real-life example of maneuvering target tracking is considered to illustrate the usefulness of the proposed robustified Kalman filtering technique."}, {"label": 1, "content": "Multi-spectral image acquisition has immense potential benefits for computer vision and image processing applications. Using a single-sensor approach aids in overcoming the issues that arise due to misalignments that occur in multi-sensor acquisition. However, the single-sensor approach poses the problem of interpolation of missing values. To address this problem, we propose an adapted version of a residual U-Net, with a focus on demosaicing. The results from our experiments reveal that the proposed approach achieves state-of-the-art results with excellent generalization capabilities to suit different color filter array patterns."}, {"label": 1, "content": "Direction of arrival (DOA) estimation is an important issue in many applications such as radars, wireless communications, objects detection and imaging. The knowledge of DOAs allows the use of control algorithms to enhance systems performances namely capacity growing in wireless communication. Improving DOA estimation methods and reducing computational time are thus of most interest. We present an optimization of a Support Vector Machine (SVM) based approach for DOA estimation. Our approach utilizes correlation matrix analysis to select essential predictors, reducing the training dataset and improving the system's generalization capability by up to 80%. We successfully tested our approach in a two-dimensional DOA estimation scenario."}, {"label": 1, "content": "Cloud IoT solutions are becoming more accessible as off-the-self plug and play services. This is done through reducing the amount of transmitted data via pre-processing and data aggregation. Hence, this paper proposes a low-cost solution powered by open standards that utilizes Arduino and Raspberry Pi for environmental and utilities monitoring applications. The solution boasts a multiple level processing architecture and employs a simple data aggregation algorithm to increase its operational efficiency."}, {"label": 1, "content": "We introduce reenterable models of networks which contain only one instance of each component and accept a given topology as a parameter. Tokens that represent dynamic elements of the model, such as packets and records of switching and routing tables, are equipped with topology location tags. These tags allow us to simulate all the devices in a given topological scheme. The network model is supplied with special measurement subnets which allow fast computation of the network bandwidth, the packet delivery time, and jitter on-fly directly in the process of simulation, without storing intermediate statistical data, for the network performance and QoS evaluation. The key advantage of this approach is its ability to quickly reconfigure models, which is highly beneficial for model-driven network design. To illustrate this technique, we present a case study of a reenterable model for a Provider Backbone Bridge that replaces Multiprotocol Label Switching technology."}, {"label": 1, "content": "This paper describes the test platform for verifying the functionality of network protocols and for optimization of their parameters. A combination of OPNET simulator and MATLAB development environment is utilized to create this test bed. The platform connects OPNET protocols simulator to MATLAB development environment in a way that allows OPNET to carry out simulations of network traffic using predetermined parameter values, while MATLAB executes a mathematical algorithm script to optimize parameters listed in OPNET simulator."}, {"label": 1, "content": "The paper presents the development of geoinformation technology for estimating the inflow of Atlantic-origin waters and polar front parameters. It is based on common issues, best practices, modern theoretical studies, heuristics and software development tools applied to research in the Barents Sea. The research was conducted in the Barents Sea and utilized the main hydrographic datasets of long-term in situ measurements values from the World Ocean Database and the Murmansk Marine Biological Institute database. Detailed investigations of the spatial distribution were carried out in the areas of Kola Transect, providing important insights into seasonal and inter-annual variability of salinity. The results obtained revealed promising prospects for further research in this field."}, {"label": 1, "content": "In this paper a new approach is proposed to decompose the basis functions in a piece-wise modeling technique for nonlinear radio frequency (RF) power amplifiers. This approach addresses the discontinuity issue of the model output at the joint points between different operating points, while still preserving the linear and nonlinear characteristics of the original model within each region. Experimental results have demonstrated that this technique outperforms the conventional piece-wise model in terms of model errors."}, {"label": 1, "content": "We have developed a novel technique for altering the frequency response and impedance matching of inkjet-printed antennas. This approach involves replacing traditional ground planes with patterned structures comprising hexagonal cells arranged in optimal configurations. The cells are switched on or off, while the optimal distributions are found via genetic algorithms to reach desired characteristics. A full-wave solver is used for the required analysis in the optimization trials without sacrificing the accuracy. The resultant antennas are fabricated using inkjet-printing technology, providing a low-cost and effective means of reconfiguring antennas for applications in radio-frequency identification."}, {"label": 1, "content": "This paper presents the findings of a study that employed numerical simulation to investigate the behavior of a nanosecond discharge in a pin-to-plate diode that was filled with nitrogen at atmospheric pressure. The simulation was carried out on the basis of the hydrodynamic model of the discharge, taking into account both the process of electron ionization and photoionization of the gas. Results indicated that in the absence of photoionization, the discharge developed through an unstable scenario; however, photoionization suppressed this instability. When the theoretical results were compared to experimental data, there was a strong correlation observed in the spatial structures of the discharge."}, {"label": 1, "content": "We investigate the influence of internal impedance-matching network for an axial-mode helical antenna on its radiation pattern. Our previous work proposed a single wire attached to the helix for this network. We show that the matching wire, mounted on a tubular dielectric support and attached to the helix close to the reflector, does not degrade the antenna radiation pattern."}, {"label": 1, "content": "Metamaterials involve small details that create challenges in their numerical analysis. As commonly practiced, homogenization of such complex structures may simplify and facilitate their numerical solutions. When dealing with finite structures, accuracy is crucial. In this paper, we present an accurate homogenization approach for three-dimensional metamaterials utilizing split-ring resonators (SRRs). To obtain electromagnetic characteristics of finite SRR structures, we utilized genetic algorithms for rigorous optimization and the multilevel fast multipole algorithm for accurate numerical simulations. The results demonstrate the effectiveness of this approach for realistic metamaterial structures."}, {"label": 1, "content": "For the physical layer (PHY) key generation, two distant nodes exploit the reciprocal channel coefficients as a shared secret. These nodes may obtain erroneous channel estimates and generate highly correlated but not identical secret keys. This study examines how different wavelet families can affect the Key Error Rate (KER) performances during the pre-processing stage of PHY key generation. The analysis reveals that the selection of wavelets can greatly influence the performance, as demonstrated by measurement-based KER results."}, {"label": 1, "content": "The article describes general structure and results of the experimental approbation of the remote decision support system for diagnosing of arterial hypertension. The system comprises of two key components, a database for storing and remotely accessing data and decision-making algorithms based on heart rate variability signal features. Results of experimental approbation have shown that among 5 diagnostically significant features sets, 2 have a decent potential for a generalization."}, {"label": 1, "content": "Recently, Machine Learning has gained considerable momentum and become the most talked-about technology in Computer Science since the inception of the Internet. At the same time, Graphical Processing Units (GPUs) have evolved from being video game rendering devices to being the primary computing devices for AI applications. This paper provides a historical perspective and a comprehensive overview of how and why this phenomenon occurred. Furthermore, the technological stack and performance of the most popular Deep Learning frameworks are analyzed."}, {"label": 1, "content": "We are studying pilot pattern optimization for transmission using single-user single-input-single-output orthogonal frequency division multiplexing, where we assume the least squares channel estimation with linear interpolation and extrapolation. Our focus is on small data packet transmission which is typical for machine-type communications. We compare diamond-shaped pilot patterns to rectangular-shaped pilot patterns. We optimize the density and spacing of pilot symbols in both pilot patterns with respect to the constrained capacity and demonstrate that the rectangular pilot pattern can outperform the diamond-shaped pattern in case of small data packet transmission."}, {"label": 1, "content": "Object detection plays a crucial role in the development of autonomous cars. However, traditional computer vision and machine learning methods for object detection are prone to delays in response time. Modern algorithms and architectures based on artificial neural networks, such as YOLO (You Only Look Once) algorithm, solve this problem without precision losses. In this paper, we showcase the use of the newest YOLOv3 algorithm for detecting traffic participants. After training the network for 5 object classes (car, truck, pedestrian, traffic signs, and traffic lights), we demonstrate the effectiveness of this approach in a variety of driving conditions, including bright and overcast skies, snow, fog, and nighttime scenarios."}, {"label": 1, "content": "Deploying complex student projects in a realistic production environment can pose a significant challenge for university computing centers. While alternatives such as using public/private clouds exist, enabling the students to focus on core project deployment instead of system administration of a production environment in general requires an alternative approach. To address this issue, a proposed platform as a service solution must meet three essential requirements: scalability, ease of use, user isolation, and security."}, {"label": 1, "content": "In this paper we present a wideband model of the polarization insensitive traveling-wave and reflective semiconductor optical amplifiers, with the active regions based on the bulk material and the multiple-quantum well structure. We analyze the steady-state and dynamic behavior of these devices, and explore the impact of various material, geometrical, and operational parameters on their performance. Our study provides valuable insight into performance optimization, and we demonstrate electro-optical remodulation from a 10 Gbps downstream signal to a 2.5 Gbps upstream signal using semiconductor optical amplifiers."}, {"label": 1, "content": "An analysis was conducted on the GA-based jigsaw puzzle solver, which revealed that the reproduction stage crossover operator played a crucial role in the algorithm's success. The operator made use of the best buddy property, which facilitated the rapid convergence of solutions. Number and validity of best buddy pieces depend on the compatibility metric used. LPQ compatibility metric provides the best result, achieving improvement of 5% with respect to the used SSD metric. A crossover modification is proposed resulting in 8% increase in average accuracy."}, {"label": 1, "content": "This article delves into the development and usage of a speech emotion recognition system. The system is trained in Czech emotionally coloured speech. The output of the system is the evaluation one of the four emotional states. To achieve this, a multi-classifier with three sub-classifiers is employed, whose results are fused together using the Bayes Believe rule. The proposed system was deployed in the Secure Mobile Communication Infrastructure developed for the Czech Republic Security Components."}, {"label": 1, "content": "Churn is an issue that affects many companies, particularly in the telecommunications sector. This paper describes experiment on data provided by the telecommunications company - Orange, for predicting churn. The preprocessing phase of the experiment included removal of missing values and redundant data, Lasso and manual feature engineering. The preprocessed one-dimensional dataset was then classified using a Convolutional Neural Network, achieving an impressive accuracy of 98.85%. Our proposed model has potential applications in telecommunication systems for detecting churn."}, {"label": 1, "content": "In this work, particle swarm optimization (PSO) based approach to the synthesis of a cylindrical-rectangular ring microstrip conformal antenna using support vector regression (SVR) models is presented. Resonant frequency of the antenna is obtained by PSO of trained SVR models. To build the SVR models, we have utilized two different kernel functions: radial basis function (RBF) and wavelet kernel function. Simulation examples are given and the results are compared."}, {"label": 1, "content": "In recent years, there has been a tremendous increase in the world of smart devices, including intelligent homes. These allow interaction between people and everyday activities in the home that can be automated. Through sensory activity and data analysis, the household can autonomously respond to situations at home and warn users of possible anomalies and shortcomings. The concept of a smart home described in this paper uses low-energy wireless IoT elements with simple installation and implementation of sensors to create a smart home without the need for reconstruction. This project's concept is based on our solution using Google cloud services and provides an overview of current data and information about the smart home anytime and anywhere within a single application for use on Android devices. By using this application, a user can control the lighting, regulate brightness levels, and detect object intrusion or protect their property from natural elements with ease."}, {"label": 1, "content": "In this paper, we propose a method to capture the electrical properties of a lossy dielectric slab by using reflection coefficient measurements. Different from the previous works, the problem is set as a minimization problem over multifrequency-monostatic/monoview frequency domain measurements. The cost function takes conductivity, dielectric permittivity and thickness of the slab as its variables and measures the squared distance between measured reflection coefficients and analytic formulas. To minimize this cost function, we use Newton's method. Our results demonstrate the stability and accuracy of this proposed scheme."}, {"label": 1, "content": "In this paper we consider an error-detecting code. Our main objective is to determine the number of errors that the code can detect with certainty. To achieve this goal, we conduct simulations with linear quasigroups of order 4 that offer the lowest probability of undetected errors for coding. Through our analyses, we reveal the exact number of errors that can be reliably detected by this code."}, {"label": 1, "content": "In this paper, the impact of the self-correction (SC) technique on the performance of posterior probability (APP) based decoding algorithms for LDPC codes is explored. The authors investigated the influence of the self-correction method on decoders with 3, 2, and 1 bit input quantization. Results showed that the performance of quantized input decoders can be significantly improved compared to the regular APP-based decoding algorithm, and in some cases, even with the Min-Sum algorithm. Convergence of algorithm is almost the same as for Min-Sum decoding. Performance and convergence results are presented for two CCSDS (8176, 7156) and IEEE 802.3an (2048, 1723) LDPC codes for APP-based, SC APP-based and Min-Sum decoding algorithms."}, {"label": 1, "content": "A solution for automatic detection and classification of buried objects by implementing Faster Region Convolutional Neural Network (Faster R-CNN) with Ground Penetrating Radar (GPR) system is presented. Specifically, Faster R-CNN Inception-v2 was chosen, as a compromise between computational load and accuracy, compared with other Faster R-CNNs. The solution can be retrained for various classes, but this study focuses on distinguishing anti-tank (AT) mines signatures from standard hyperbolic signatures of other objects, including anti-personnel (AP) mines. The image dataset used for training and testing the R-CNN network consists of GPR B-scans obtained both by gprMax based simulations and from real measured GPR data. The solution's performance can be evaluated using Confusion matrices and ROC curves. Post processing approach based on object size and depth below ground surface enables discrimination of AP mines."}, {"label": 1, "content": "Efficient recurrence expressions are proposed in this paper as promising tools for accurate dispersive analysis of symmetric coplanar structures on uniaxial electric/magnetic anisotropic composite substrates. By using the formalism of mathematical operators, the proposed formulation can accurately evaluate the admittance operator in the modal domain moment method while simplifying its numerical implementation. With an appropriate choice of trial functions, the proposed approach was demonstrated by successfully characterizing multilayered coplanar structures."}, {"label": 1, "content": "This paper presents a novel software architecture designed for Android operating system that enables the integration of Google Assistant in TV applications. Nowadays, Google Assistant is becoming more and more popular personal assistant and people freaquently use it's features in the applications on their, smart\u2019 devices such as smartphones, smart digital television receivers, smartwatches, etc. While existing speech recognition tools provide speech processing output in the form of structured or free-form textual data, this paper's approach focuses on detecting command patterns, such as channel up/down, volume up/down, mute/unmute, and other commands to enhance the overall user experience. With this architecture, users will be able to use voice commands to control the TV application, without the need to use a remote control device. The goal of this approach is to offer an improved user experience that relies on voice activation for TV applications."}, {"label": 1, "content": "The Brain Computer Interface (BCI) is a promising solution for people with disabilities to communicate and control their environment. BCI technology decodes a person's intention through brainwaves generated during motor imagery activities. Machine learning models are used to classify the intention, and the accuracy of BCI systems mainly depends on the quality of the features extracted from the EEG signals. To decode someone's intention from brainwaves during motor imagery activities, machine learning models trained on features extracted from the acquired EEG signals have been used. To address this issue, researchers have proposed using deep learning models, such as Convolutional Neural Networks (CNNs), to directly learn features from raw EEG data. In this study, a CNN model was trained on raw EEG signals obtained from four subjects using a 64 channel EEG device. 64) to capture spatial information which are necessary during training a machine learning model. In this study, Convolutional Neural Network (CNN) is used to decode five motor imagery intentions from EEG signals obtained from four subjects using 64 channels EEG device. This study demonstrates the potential of CNNs for BCI systems and suggests that deep learning models could improve the accuracy and efficiency of BCI technology. Channel selection based on learned weights extracted from a trained CNN model has been performed with subsequent models trained on only two selected channels with higher weights attained a high accuracy (average of 98%) among three participants out of four."}, {"label": 1, "content": "Robotic neuro-rehabilitation has the potential to significantly improve the recovery process for post-stroke patients. By providing support movement for the affected limb triggered by brain signals, a hand rehabilitation robotic system using a robotic hand orthosis moved by Near-Infrared Spectroscopy is currently being studied. In this paper, a new method has been proposed to classify the motion intention from the NIRS signal. The classification accuracy that is an essential factor to extract the users' motion intension, was significantly improved by parameterizing the individual hemodynamic response."}, {"label": 1, "content": "Previous studies have primarily utilized the nonnegative matrix factorization algorithm (NNMF) to extract muscle synergies during reaching tasks or relatively stable activities to observe muscle coordination in humans. However, few studies have used this algorithm in the tracking task since its high complexity and unpredictability, which means that the muscle synergies become more unstable and are hard to embody the neural mechanisms behind the human motion. In this study, we applied NNMF to tracking tasks and calculated two synergy indices - the synergy stability index (SSI) and synergy coordination index (SCI). SSIW and SSIC measured the similarity between synergies and activation coefficients, respectively, while SCI indicated the size of the synergy space. In our results, SSIW was about 0.8, SSIC was nearly 0.3 and SCI was 0.6. These values suggest that humans tend to maintain stable synergies and flexibly control them by adjusting the activation time and amplitudes during complex and variable tasks."}, {"label": 1, "content": "The stability and robustness of an electromyography-based pattern recognition (EMG-PR) control system when it comes to feature extraction and electrode configuration has been a neglected area in research until now. Aiming at developing a robust, stable, and accurate EMG-PR system, a new pattern recognition method, Linear Regression Classifier (LRC) is proposed in this study. The study conducted tests on 12 TBI patients, which demonstrated that the LRC approach resulted in significantly higher classification accuracy compared to commonly used LDA and KNN methods (achieving above 99% accuracy when 56 monopolar electrodes were used). Moreover, the LRC scheme was robust to the selection of feature sets, and to the electrode configurations especially when TD and TDAR feature sets were used. These outcomes suggest the comparative advantage of the LRC."}, {"label": 1, "content": "In this paper, we try a novel approach to detect human movement intentions based on electromyography(EMG) signals. The Convolutional Neural Network (CNN) is used to automatically extract EMG features, followed by dueling deep Q-learning, a reinforcement learning technique, which learns a classification policy to select the most helpful subset features while filtering out irrelevant or redundant features. We show that the deep learning method outperforms the multi-layer perceptron in the several subjects EMG data classification situation and the reinforcement networks can use less features to reach a relatively high classification precision."}, {"label": 1, "content": "The objective of Prosthetic Research for upper extremities is to create a prosthetic limb that resembles a human arm in terms of flexibility, speed of response, and appearance. The paper describes the design and assembling of a prosthetic arm that has the flexibility and motion as that of a human arm. A finger of the designed arm is modeled using Neural Networks. The outcomes of the study confirm that Neural Networks can be used for system identification purposes."}, {"label": 1, "content": "The Convolutional Neural Network (CNN) is a powerful deep learning algorithm widely used in image processing and pattern recognition, thanks to its feature invariance. However, it is also computation-intensive that results in a bad real-time performance. To address this issue, the Field Programmable Gate Arrays (FPGA) are used for their energy efficiency, parallel processing flexibility, and pipelined operations. Thus it is expected to be used for accelerating deep learning algorithm. In this research, a FPGA based system is developed to realize the real-time Hand Gesture Recognition. We train a designed CNN model with caffe framework and obtain the model's parameters on PC. The researchers then used Xilinx SDx tools to design an accelerator and implement the inference process of Hand Gesture Recognition with the obtained parameters on the FPGA. Then we use FPGA to implement the inference process of Hand Gesture Recognition with obtained parameters by designing an accelerator using Xilinx SDx tools."}, {"label": 1, "content": "For upper limb multiple degrees of freedom prostheses to be clinically viable, their control performance should be accurate and consistently stable over time. Factors such as the feature extraction methods and window conditioning parameters play an important role in this context. To provide information on optimal feature/windowing parameters, this study investigated the accuracy and stability of notable time-domain (TD) and frequency-domain (FD) features across different windowing conditions. The study investigated the interaction effect of a range of window lengths (50ms~300ms) and window increments of 25ms, 50ms, and 100ms on the classification performance, stability, and computation time of TD and FD features based on electromyogram (EMG) recordings of four able-bodied subjects performing seven classes of limb motions. Experimental results show that TDAR (consisting of 4th Order autoregressive coefficient and root mean square) achieved the lowest classification error (CE) among the TD features at an optimal window size of 300ms and increment of 100ms, while MNP (mean power) recorded the best accuracy among the FD features. Despite the significant reduction in CE of TDAR and MNP over the other features, their computation time was relatively high, indicating a trade-off between accuracy and computation time among the different feature extraction methods. Thus, the findings from this study may provide potential insight on the proper choice of features and window conditioning parameters in the context of research and practical applications in myoelectric control systems."}, {"label": 1, "content": "Artificial neural networks (ANN) have been applied effectively in numerous fields for the aim of prediction, knowledge discovery, classification, time series analysis, modeling, etc. ANN training can be assorted into Supervised learning, Reinforcement learning and Unsupervised learning. There are some limitations using supervised learning. These limitations can be overcome by using unsupervised learning technique. This gives us motivation to write a review on unsupervised learning based on ANN. One main problem associated with unsupervised learning is how to find the hidden structures in unlabeled data. This paper reviews on the training/learning of unsupervised learning based on artificial neural network. However, challenges remain in selecting the appropriate number of hidden nodes and designing effective ANN architectures for unsupervised learning. Moreover, the status, benefits and challenges of unsupervised learning are also summarized."}, {"label": 1, "content": "Gait analysis is widely used in the field of entertainment, sport, public welfare, and medical services. In this paper, three generation foot pressure analysis systems are developed to analyze gait patterns. The whole system is wireless connected with Zigbee. Three pressure sensors for each foot are used to detect the foot pressure of different area."}, {"label": 1, "content": "In service robot applications, it is essential for an automated in-door mobile robot to be able to accurately localize itself through sensor measurements. This paper is concerned with the wheeled mobile robot self-localization using the digital compass and photoelectrical encoder measurements. A traditional odometric localization algorithm is utilized and the bias analysis of this method is derived based on the statistic theory. To improve estimation performance, a low-cost bias compensation strategy is developed. Also, a classical extended Kalman filter (EKF) is applied as a comparison. The effectiveness of the proposed self-localization algorithm is verified with simulation examples."}, {"label": 1, "content": "Cardiac autonomic regulation is impacted by respiratory training, as has been evidenced through linear analysis of heart rate variability (HRV). The two most important bands in frequency domain analysis of HRV, the low frequency (LF) and high frequency (HF) analysis, which respectively represent parasympathetic and sympathetic activity, have been widely used in deep breathing research. The use of deep breathing in research has been widespread, although the interaction between LF and HF during deep breathing has not been extensively studied. In this sense, we recruited 12 healthy young subjects (7 males and 5 females) for deep breathing experiment. Transfer entropy (TE) of HRV was applied to analyze the changes in information transmission between HF and LF at baseline and deep breathing. Our results indicate that the mean TE(LF\u2192 HF) value was significantly lower during deep breathing compared to baseline. Compared with the mean TE(HF\u2192 LF) value, the mean TE(LF\u2192 HF) value was significantly higher at baseline. The results also showed a significant increase in the LF/HF ratio in deep breathing. Our results demonstrated that deep breathing attenuates the information transmission from the sympathetic nervous system (SNS) to the parasympathetic nervous system (PNS)."}, {"label": 1, "content": "This paper presents models to calculate refueling and restoration delays for cell sites in extreme events. Delays are modeled using a triangular distribution whose parameters indicate the minimum travel delay, most likely travel and maximum travel delay to a given location. This study uses graph theory to compute the delay parameters for a transportation network graph that is affected by a catastrophic event. A simulated attack on Pittsburgh, Pennsylvania is used to exemplify the delay model results. The study investigates the impact of the road network's geographical characteristics on its structure, connectivity, and travel delays. The findings reveal that graph-theoretic approaches, as well as physical and geographical characteristics, are crucial to understanding network behavior during post-disaster conditions."}, {"label": 1, "content": "Computer room air handling (CRAH) bypass (BP) method utilizes fan-equipped perforated tiles to force or induce room air through relatively lower flow resistance than that in CRAH units, as CRAH fans operate at lower speed. Increasing the BP airflow rate reduces the total fan power and increases the chiller power since the CRAH unit needs to provide colder air when a certain fraction of warm room air is allowed to mix with cold CRAH airflow. However, recent studies indicate temperature non-uniformities at higher fractions of BP flow, which challenges the uniform temperature assumption of simplified airflow network and thermodynamic modeling tools. These tools assume uniform air temperature at the server inlets, even though recent experimental and computational studies indicate temperature non-uniformities at higher fractions of BP flow. This study addresses energy implications of the uniform temperature assumption of the reduced order modeling tools on the cooling energy saving potential of EA data centers by using computational fluid dynamics (CFD) modeling. Moreover, existing literature does not thoroughly investigate the application of CRAH BP in the open aisle (OA) data centers, which requires CFD models to resolve complex airflow patterns due to recirculation of hot air into the cold aisle and leakage of cold air into the hot aisle. This study utilizes CFD to investigate both induced and forced CRAH BP methods in a representative quadrant of a 1MW data center with and without aisle containment."}, {"label": 1, "content": "The battery management system (BMS) is tasked with determining the current charge level, as a miscalculation of this can have consequential effects on the operation of various communication systems. Due to the complexity of the analytical model of multi-cell lithium-ion storage devices, when developing algorithms for controlling them, it is effective to use fuzzy inference systems (FIS) that allow for the correction of the state of the charge level. The article presents the results, further improvement of such systems by combining several FIS depending on the state of the battery, which increases the accuracy of determining the charge state of the lithium-ion energy storage by modern BMS."}, {"label": 1, "content": "Solar power generation systems require maximum power point tracking (MPPT) control to get maximum power using low efficient and high cost PV modules. Traditional MPPT algorithms are based on the slope-tracking concept, with one common method being the Perturb and Observe (P&O) algorithm. One of a typical slope-tracking method is Perturb and Observe (P&O) algorithm. If the perturbation voltage is set too high, the MPPT controller can quickly move to a new maximum power point with changing insolation. When the perturbation voltage is set to large, the MPPT controller quickly moves to the new maximum power point at insolation change, while the error of output power will be huge in the steady state even when insolation is not changing. When the MPPT control period is set for short, the dynamics of the MPPT controller can be accelerated even though the perturbation voltage is set for small. However, too short MPPT control period does not contribute improvement of the MPPT performance but consumes the MPPT controller resources. Therefore, in order to determine the optimum MPPT control period and the magnitude of the perturbation voltage, it is necessary to analyze the performance of the MPPT controller for actual insolation conditions in real weather environment. This paper proposes an optimum MPPT control period that maximizes MPPT efficiency by measuring and analyzing actual insolation profiles in typical clear and cloudy weather in central Korea."}, {"label": 1, "content": "Satellite networks are able to provide seamless services for ground users with wide coverage, which is intrinsically suitable for providing broadcasting or multicasting services. Although terrestrial networks have undergone rapid development in recent years, the integration of terrestrial-satellite networks in the next generation of communications promises to allow users to access all types of services seamlessly. In this article, we examine the issue of cooperative transmission in integrated terrestrial-satellite networks, analyzing both unicast and multicast transmission separately. This article's objective is to present a detailed discourse on cooperative transmission and devise a general cooperative transmission framework for future terrestrial-satellite networks."}, {"label": 1, "content": "Fully convolutional neural network is a special deep neural networks based on convolutional neural networks and are often used for semantic segmentation. This paper introduces an enhanced fully convolutional neural network, which incorporates the feature maps of both deeper and shallower layers to enhance the performance of image segmentation. Adaptive parameters are employed to allow different layers to contribute differently to the feature fusion process. The deeper layers of neural network concentrate on extracting the abstract information of the object, while the shallower layers of neural network concentrate on refining the features of the object, such as edge information and precise shape. Adaptive parameters can speed up the training speed and improve the prediction accuracy. In the early stages of training, the feature maps of shallow layers have a larger fusion coefficient, which allows the neural network to learn the feature of object's location and shape quickly. As the training progresses, gradually weakening the fusion coefficient of shallow layers and increasing the fusion coefficient of deep layers which can enhance the network's ability of predicting the details of the objects. This paper uses Scene Parsing Challenge 2016 dataset presented by MIT for training. Preliminary experiments demonstrate that this new approach can significantly improve the pixel prediction accuracy and speed up the training process."}, {"label": 1, "content": "With the rapid growth of the Internet of Things (IoT), storing and retrieving its vast amount of data has become increasingly challenging due to its diverse sources and heterogeneous structure. Those characteristics bring great difficulties to the storage and rapid retrieval of IoT data. This framework efficiently stores and retrieves massive, heterogeneous IoT data, and outperforms other solutions based on RDBMS. Extensive testing has confirmed the effectiveness of the HSFRH-IoT framework for efficient storage and retrieval of IoT data."}, {"label": 1, "content": "Relevance vector machine (RVM) is a typical sparse learning model. However, the use of the Radial Basis Function (RBF) kernel in RVM, which is based on Euclidean distance, does not consider the distribution characteristics of the training set, leading to suboptimal performance. To address this issue, this paper proposes the use of a mixed Mahalanobis kernel for Sparse Bayesian classification. The kernel matrix is built based on the weighted Mahalanobis distances from all Gaussian components in a GMM. Applying the proposed kernel to RVM, a new version of RVM, MM-RVM, is formed. Experimental results using several UCI datasets demonstrate the superior performance of the proposed method and the effectiveness of the mixed Mahalanobis kernel."}, {"label": 1, "content": "Polarization sphere is an ideal model for describing geological targets such as ore bodies, and its forward response signals can provide useful orientation and lithologic information for related geological interpretation in borehole geophysical exploration work. In this paper, we study the fast forward modeling of surface-hole measurement data of polarized spheres. We utilized the analytical solution in the spherical coordinate system as the fundamental algorithm for forward motion to simulate the responses of various borehole induced polarization devices in different conditions, including the five-direction observation device and the symmetric direction observation device. Then, we get systematic summary of pattern for surface-hole measurement data of polarization sphere from the forward modeling results and conversion parameters. The forward calculation result and analysis conclusion serve as critical references for relevant researches or actual production work."}, {"label": 1, "content": "In this paper, we introduce a novel technique for diagnosing arteriosclerotic disease utilizing the photoplethysmogram (PPG) signal. The proposed method incorporates three types of parameters, namely, waveform features in the temporal domain, harmonic amplitudes in the frequency domain, and wavelet energy characteristics. We have utilized genetic algorithms to select relevant characteristics for diagnosing arteriosclerosis. The support vector machine (SVM) model is used to fuse these parameters to establish a non-invasive arterial sclerosis recognition model. Our experimental results demonstrate that the precision of our algorithm on the test set is not lower than 98%, while the accuracy rate on the training set attains 100.0%. Our proposed algorithm is precise, efficient, and is promising for practical applications."}, {"label": 1, "content": "This paper utilizes abstracts and keywords from aviation literature as the corpus. Firstly, analyze the word formation characteristics of domain term in corpus, these characteristics are used to construct the part-of-speech combination model that adapted to the specific domain. And then, the distance between two adjacent word tokens in the candidate terms was calculated with mutual information, and set the threshold to convert the term extraction into a classification problem of whether the candidate term is domain specific. The experimental results demonstrate that this method is effective in automatically extracting domain-specific terms."}, {"label": 1, "content": "Maintenance plays a crucial role in keeping machines in good condition. Inadequate machine maintenance scheduling can result in unforeseen impacts, leading to production process stoppage. With continued stoppage, machine downtime losses can become a significant concern for the company. This research focused on comparing Artificial Neural Network (ANN) and other failure rate distributions to determine the appropriate maintenance interval. A reliable machine is less likely to experience breakdowns. The analysis of reliability involves using statistical methods to calculate the probability of a component's failure rate over time. For our study, we used Time Between Failure (TBF) and Time To Repair (TTR) data from the machine's historical data between 2015-2017. Our findings indicate that ANN provides the least error compared with other distribution models."}, {"label": 1, "content": "At present, sharing of files and other media is a common use case which involves cloud storage services. However, the privacy of cloud services has become questionable as certain free cloud service providers use personal content for analytical purposes. Also, in order to share content over cloud storages, local files must be uploaded to cloud services, even for minor use cases such as directory browsing. This paper presents a novel solution which provides similar sharing capabilities to that of existing cloud services without having to store content in a cloud storage. The solution enables direct content browsing, uploading and downloading using a web interface. The solution also provides the capability to generate links for locally stored content in desktop or personal computers. The presented solution utilizes peer-to-peer networking technologies which are scalable and more secure. Performance measures have shown that this solution is competitive with existing cloud service providers for content sharing."}, {"label": 1, "content": "At present, all CNN-based fire identifications identify whether a fire is blazed with a single frame image, all of which have low accuracy under a strong interference or complex backgrounds such as flickering light or a high-brightness background. To address the issue, this paper proposes a neural network model which combines CNN with SRU. This approach enables the extraction of scene content through CNN and the dynamic characteristics of the flame through SRU, which results in improved accuracy of fire detection. The paper presents three models: Resnet18+SRU, Resnet34+SRU, and resnet18(Maxpool)+SRU. The models were validated on a test set containing intensive indoor environmental interferences and compared with CNN-based single-frame and multi-frame fire identification methods. The results show that the proposed methods are more accurate than the single-frame CNN fire recognition method by more than 25% and are more suitable for indoor environment fire alarms."}, {"label": 1, "content": "A novel method for identifying different types of network traffic in encrypted hybrid data has been proposed based on sequential pattern mining. After dimensionality reduction for traffic series, the data are cut into short sequences. By sequential pattern mining and pruning, the characteristics sequential sets, which are extracted from transaction database comprised of these sequences, are used to calculated and analyzed the identification results. With removing each traffic's characteristic away from the hybrid data, the final results are verified and concluded. The experiments show that the recall and precision of identification results are more than 60%."}, {"label": 1, "content": "As parallelism is becoming increasingly prevalent on various levels in modern computers, it is beneficial to examine the extensive range of parallel computing advancements that have emerged in recent decades. Although we will not provide a comprehensive overview, we will instead focus on parallel programming patterns, parallel program design, parallel programming models, parallel programming languages, the design of parallel algorithms, and offer a perspective on parallel computing. Besides presenting the patterns, models, design frameworks, we also refer to languages, implementation, and tools."}, {"label": 1, "content": "Many real-world problems are modeled as multi-agent problems, which are often partially observable. But multi-agent problems will make the environment became nonstationary from the point of view each agent, which causes the combination of experience replay with IQL to be problematic. While DDRQN has been proposed as a solution to partially observable RL problems, it still faces limitations such as the inability to enable memory replay. In this article we propose a method based on importance sampling and address DDRQN's disadvantages which can't enable memory replay. Results on the SC2LE environment confirm that this method significantly improve performance compared to original DDRQN"}, {"label": 1, "content": "Deep convolutional neural network (CNN) is known to be the first truly successful deep learning approach for image processing and understanding, e.g., the handwritten digits discrimination. However, in real-world scenarios, such as the recognition of handwritten zip codes, images are often collected with smudged backgrounds. This study aims to understand the effect of CNN on recognizing smudged digits by comparing its results with a three-layered perceptron. The experimental results based on the MNIST dataset with simulated salt-and-pepper and Gaussian noise showed a drastic decline in recognition accuracy for CNN, suggesting that the feature extraction using convolutional operation and max pooling is highly sensitive to noise."}, {"label": 1, "content": "In this paper, we present a self-positioning method for mobile robots that is based on a rotary encoder. Initially, we analyze the robot's wheel speed information using the rotary encoder. Secondly, we use the micro-offset accumulation-based trace estimation algorithm to measure the distance and direction deviation information of the current robot relative to the start position. Finally, we apply an error correction method to reduce excessive self-positioning deviation resulting from accumulated errors, leading to successful mobile robot self-positioning. The experiments show that the proposed self-positioning method can achieve high accuracy under the effect of the error correction method, and it has a certain degree of robustness."}, {"label": 1, "content": "Text representation is a key task in machine learning, allowing for the conversion of varying lengths of text into feature vectors. While early methods used discrete and sparse lexical and syntactic features, these approaches struggled to capture the semantic relationships between words. Recent advances in deep learning, which represents text segments into dense and continuous vectors, has shed light on this problem. However, the main limitation is they are usually based on complex neural network structure, which are resource-consuming to train and make inference. To address this issue, we propose a novel approach to text representation that considers the importance of words on both a local and global scale using the BM25 weighing schema. We use word vectors pretrained from large text corpus to capture the latent semantic relatedness between words. Experimental results show that our approach is effective and efficient compared with existing feature-based, unsupervised and supervised baselines."}, {"label": 1, "content": "In order to focus the trainees' attention on the operation of equipment and tool rather than the specific model motions such as rotation and translation, desktop virtual training system needs to automatically recognize the purpose of the user's operations and simulate model motion in real time. This paper proposes a novel method based on spatial calculation to implement interactive operations in a user-friendly way. It accurately calculates model motion parameters based on the user's operations and adjusts to different equipment positions, attitudes, and user behavior. The method has been successfully applied in an interactive maintenance training system of actual hydroelectric generating equipment and proved to be effective."}, {"label": 1, "content": "An online adaptive iterative learning control (AILC) algorithm has been proposed for a class of nonlinear discrete-time systems based on the neural network approximator. The algorithm takes into account the random iteration initial error and trajectory reference. The nonlinear system is transformed to a predictor form, and the desired control signal is obtained through the implicit function theorem. A neural network using only a norm adaptation law is utilized to approximate this desired control signal iteratively. By using Lyapunov analysis, it is proven that all the system signals are bounded and the system tracking error converges to a neighborhood around zero as iteration number goes to infinity. The proposed neural network-based AILC is highly advantageous as it significantly reduces the number of adjustable parameters as compared to the existing AILC results."}, {"label": 1, "content": "Modal parameter estimation for large structures has always been a significant focus of research. Its core content is to obtain the eigenvalues of large-scale structural system. Obtaining high-accuracy system free response based on environmental excitation is particularly crucial. This paper proposes a random reduction de-noising-ARMA method (RDT-ARMA) that analyzes the free response root and imaginary parts under healthy and damaged conditions. By analyzing the characteristics of free response root and imaginary part under the condition of health and damage, the noise modal of free response obtained by stochastic reduction method can be effectively eliminated to obtain a more realistic system free response signal. Using the ARMA method, the modal of the large structural system is identified. And through the actual acquisition of the WSN signal processing measured acceleration, indicating the effectiveness of the proposed method can be proved."}, {"label": 1, "content": "In this paper, an optimization method, Golden Ratio Algorithm (GRA), is introduced for biometric fusion at score level. The experiments conducted reveal that GRA can effectively balance the contributions of different modalities to achieve better biometric person recognition. And the test results have also indicated that an Audio-Visual (AV) bimodal biometric system normally outperforms the respective single modality systems when biometric fusion is done properly. Furthermore, the experiments demonstrate that the performance of a biometric system relies on the accurate extraction of features from original biometric sources."}, {"label": 1, "content": "Recently many researches have focused on the lie detection (LD) using the event-related-potentials (ERPs) of EEG signals. However, most of the current systems only extract features from a small number of channels, ignoring the fact that deception involves multiple brain regions. However, most of current ERP-based LD systems only focus on extracting the various features from the EEG signals on one or few channels. In this study, we used the phase lag index (PLI) to establish brain network connections and applied graph theory approach to investigate structure features in functional networks. Statistical analysis revealed significant differences in the graph-based features between the two groups. Statistical analysis indicates that the differences in the extracted graph-based features between the two groups were significant. This suggests that using graph-based features in combination with brain network analysis could provide a powerful new approach for automatically identifying deception."}, {"label": 1, "content": "In order to solve the problem that the accuracy of time synchronization in power consumption information collection system is insufficient, a method of clock synchronization of power users' power information acquisition terminal based on a new timing algorithm is proposed. This paper first analyzes the traditional NTP time synchronization algorithm. In view of the shortcomings of the algorithm in clock synchronization, the channel asymmetry is fully considered and the algorithm is optimized in this paper. Additionally, a synchronization calculation method based on minimal round trip difference of NTP is presented. By implementing this method in the synchronized structure of the electricity info-collecting acquisition terminal clock, the new synchronization calculation method shows significant improvement in both accuracy and convergence when compared with older methods."}, {"label": 1, "content": "Network coding has reached a goal of significant throughput improvement. However, some scenarios have shown that it does not work well on encoding. To enhance the throughput of the COPE protocol for wireless transmission, this study proposes a new approach based on \"opportunistic asynchronous transmission coding CACM.\" The researchers conducted a comparative simulation experiment on four different transport protocols and found that the throughput of COPE, CACM-nP, and CACM protocols has significantly improved by 122%, 133%, and 139%, respectively, compared to the Unicast protocol. This work provides guidance for further designing routing/MAC on network coding."}, {"label": 1, "content": "Obtaining fused state estimation for multi-sensor multi-delay systems with correlated noises can be a troublesome task. The previous conventional fused estimation method uses the information, sent by several different filter, smoothers and predictors, to get one-step estimation, which increases the complexity of the method, so that it is not suitable for real application. In order to get more convenient estimators, the augmented state equation is introduced, and then an augmented steady-state Kalman estimator can be got, which conceals the time delays. Extracting the partial component of that augmented steady-state estimator yields the suboptimal estimator, which ignores the correlation between the components of the augmented estimator, but possesses more excellent rapidity and convenience compared with the previous fused estimator. Then by Sequential Covariance Intersection (CI) fusion method, a fast fusion steady-state suboptimal Kalman filter is obtained. Although the proposed fusion steady-state estimator is suboptimal, simulation examples demonstrate that it offers higher accuracy than each of the local estimators and is approximate to the optimal information fusion estimator."}, {"label": 1, "content": "The Arabic language has evolved over time and given rise to several linguistic registers. They can be classified as: Old Arabic, Classical Arabic and Modern Standard Arabic. In this work, we propose a method that aims to disambiguate words in Modern Standard Arabic. To accomplish this, we propose a method that measures the semantic relation between the context of use of the ambiguous word and its sense definitions. Within the context of creating a historical dictionary for Arabic, and to disambiguate a word, we need to take into consideration the historical period in which the word appeared. By applying this method, we can effectively disambiguate words in Modern Standard Arabic and provide a more accurate understanding of the language's development over time."}, {"label": 1, "content": "With the advances in Internet technologies and services, the social media has been gaining excessive popularity, especially because these technologies provide anonymity where they use nicknames to post their messages. Nonetheless, this feature has become a haven for cybercriminals to mask their identities and activities. Consequently, cybersecurity researchers have developed an interest in identifying the authors of malicious messages and activities. Internet Relay Chat (IRC) channels are widely used to exchange messages and information among malicious users involved in cybercrimes. In this paper, we present an autonomic author identification technique based on personality profile and analysis of IRC messages. We first monitor the IRC channels using our autonomic bots and then create a personality profile for each targeted author. We establish that personality analysis for author detection/identification is an efficient approach with high detection rates."}, {"label": 1, "content": "With the emergence of Internet of Things (IoT) and the growth of cloud computing technology, Electronic Health systems are integrated as a significant and active domain, which enables the development of medical practices with an affordable cost. Thus, the questions about preserving the security and privacy of the sensitive user's information are raised. To convince users to move their health record data to cloud networks, strong and secure access control schemes must be employed to protect their data. Access control schemes play a critical role in protecting e-Health records, where several policies coexist to provide security and privacy in a network. That is why, one of the essential targets in this work is to retain a secure access to health cloud services. Thus, this paper offers a certificate-based access control strategy, where we introduce an access method for e-health cloud that is mainly based on trust assessment. The main features of the proposed scheme are the integration of trust concept with the monitoring process in order to ensure more security in access control. We more explained the methodology of the proposed approach through appropriate evaluation results, which improves system security and performance by minimizing the time spent to have permissions to access services and optimizing the overall system resource utilization."}, {"label": 1, "content": "Mobile Cloud Computing (MCC) is a technique that allows mobile users to use cloud services, which can reduce costs, increase flexibility, and provide on-demand scalability. In fact, MCC enables to share the hardware, data and storage. However, access network verification is vital to ensure the security of the system. Traditional cloud computing architecture has specific security requirements that aren't captured by typical network access control models, such as Virtual Firewalls (VF). The latter uses an effective static verification method that is not adapted to the MCC, where existing threats increase because of the combination of different types of networks (Mobile and Internet). To address these issues, researchers have introduced distributed firewalls to secure complex networks such as MCC. However, creating, managing and implementing firewall policies can be challenging. In this paper, we propose an architecture for MCC that encourages the use of distributed firewalls/controllers, with two levels of cooperation: horizontal cooperation in the cloud and vertical cooperation between mobile devices and the cloud. This architecture is in charge of administering and updating rules between distributed firewalls and their neighbors via the Controller component. For validation, we use a cloud implementation based on the Openstack platform. The results show promising delays regarding traffic burden, indicating that our proposed architecture is effective in securing MCC systems."}, {"label": 1, "content": "Fuzzy rule-based classification systems (FRBCS) have gained popularity due to their ability to build linguistically interpretable models while automatically generating fuzzy if-then rules for classifying new observations. However, in these supervised learning systems, a high number of predictive attributes leads to an exponential increase of the number of generated rules. To tackle this issue, we propose using ensemble methods for FRBCS that combine the decisions of different classifiers to form the final classification model. We particularly focus on ensemble methods that divide the attributes into subgroups and treat each subgroup separately. To address this problem, we propose to use ensemble methods for FRBCS where the decisions of different classifiers are combined in order to form the final classification model. We are interested in particular in ensemble methods which split the attributes into subgroups and treat each subgroup separately. We propose to regroup attributes by correlation search among the training set elements that belongs to the same class, such an intra-classes correlation search allows to characterize each class separately. Furthermore, classification rates improved. In conclusion, the proposed method addresses the challenge of generating rules in FRBCS, with ensemble methods being proven to be a useful technique for improving the accuracy and interpretability of rule-based classification systems."}, {"label": 1, "content": "We present a SAT-based bounded model checking (BMC) method for timed interpreted systems (TIS) and for properties expressible in the existential fragment of a Real-Time Computation Tree Logic with epistemic components (RTECTLK). Our BMC method is implemented using the standard algorithm and evaluated on two multi-agent systems: a timed train controller system and a timed generic pipeline paradigm. To compare the effectiveness of our new technique, we also implemented an SMT-based method and used various solvers, including YicesSAT and Cryptominisat5 for SAT-solving and Z3 and YicesSMT for SMT-solving. For experiments we used the SAT solvers YicesSAT and Cryptominisat5, and the SMT solvers Z3 and YicesSMT."}, {"label": 1, "content": "This paper discusses our work on using software engineering metrics (i.e., source code metrics) to classify an error message generated by a Static Code Analysis (SCA) tool as a true-positive, false-positive, or false-negative. Specifically, we compare the performance of Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Random Forests, and Repeated Incremental Pruning to Produce Error Reduction (RIPPER) over eight datasets. The performance of the techniques is assessed by computing the F-measure metric, which is defined as the weighted harmonic mean of the precision and recall of the predicted model. The overall results of the study show that the F-measure value of the predicted model, which is generated using Random Forests technique, ranges from 83% to 98%. Additionally, the Random Forests technique outperforms the other techniques. Lastly, our results indicate that the complexity and coupling metrics have the most impact on whether a SCA tool with generate a false-positive warning or not."}, {"label": 1, "content": "People experience mental stress on a daily basis from a variety of different reasons, including environmental reasons (traffic, noise, or bad weather), social reasons (family issues, friends, and financial problems), or from events such as wedding planning or giving a presentation in front of large audience. A manageable amount of stress is healthy and can motivate a person; however, a large amount of continuous stress or a strong response to stress can be harmful. Therefore, the identification and prediction of mental stress has become a crucial area of research, and there are numerous approaches in the literature for stress detection using machine learning. In this paper, we review and summarize various approaches found in the literature for stress detection using machine learning and suggest directions for future research and interventions."}, {"label": 1, "content": "This paper focuses on the classification of breast images using the Number of Texture Unit as a feature extractor. Local Binary Pattern technique was developed based on the Number of Texture Unit, which is popular in facial recognition. We compared the proposed strategy with the Gray Level Co-occurrence Matrix which is the most used texture analysis technique in the literature. With this work we have been able to show that the combination of the two techniques of feature extraction improves the final result of classification. Moreover, we used the Support Vectors Machine classifier, which achieved an impressive 96.15% Area Under the Curve (Receiver Operating Characteristic Curve) in the tests."}, {"label": 1, "content": "This study proposes a unique framework for speaker recognition over VoIP network using a discriminative auditory feature extractor. The auditory model that simulates the mid-external and inner ear is incorporated into the conventional Mel Frequency Cepstral Coefficients (MFCC) scheme to produce new parameters that we named Ear Frequency Cepstral Coefficients (EFCCs). The experiments are conducted on the TIMIT corpus and the EFCC, used as input parameters tothe i-vector algorithm. Results of the experiments showed significantly improved performance compared to the conventional MFCC parameters, while using a lower number of parameters (43 EFCCs vs 60 MFCCs). The findings of this study are promising and could pave the way for more effective speaker recognition methods in the future."}, {"label": 1, "content": "This paper presents a novel super-resolution method for degraded images of documents captured by mobile devices. This is an improvement of a non-linear existed method but limited by its high complexity and low quality on degraded images, caused in general by the JPEG compression. In such scenarios, it is necessary to expand the local analysis for better visual rendering, without increasing the complexity. Our contribution seeks to address these constraints by linearizing the approach via bio-inspired methods that entail the use of multilayer perceptron neural networks. Our results demonstrate that these networks can learn the mechanisms of a super-resolution approach, and extend it to enhance the quality of output. This serves as an alternative to conventional neural approaches used for image magnification."}, {"label": 1, "content": "The goal of this manuscript is to present a new hybrid system based on the fusion of gaze data and Steady State Visual Evoked Potentials (SSVEP) to command a powered wheelchair. The motivation behind this research is to overcome the limitations of using gaze-based and SSVEP-based wheelchair command techniques separately. The proposed framework is based on two modules : a gaze module to select command and activate the flashing stimuli. An experimental protocol was developed and the prototype was tested on five individuals with paraplegia. An experimental protocol was set up and the prototype was tested on five paraplegic subjects. The results showed that the run accuracy reached an average of 98%. Overall, the results demonstrate the effectiveness of the proposed hybrid system for controlling powered wheelchairs."}, {"label": 1, "content": "For the last decade, a rising need for emotion recognition has been noticed in several domains, such as virtual reality, human-computer interaction, video games and health monitoring, etc. Emotion recognition through facial expressions has become increasingly popular. In this paper, a new facial emotion recognition method based on geometrical facial features is proposed. We collected a novel dataset of 17 subjects facial performance of six emotional states (anger, fear, happiness, surprise, sadness, and neutral) using Kinect (vi) and Kinect (v2) and RGB HD camera. New positional features, including a combination of angle and distance features, were used to train the classifier. The K nearest neighbors (k-NN) is used as the main classification technique. To assess our proposed method performance, we use the leave-one-out subject cross-validation. A comparison between RGB and RGB-D data is provided. The results obtained demonstrate the superior performance of the RGB-D features provided by Kinect (v2). The researchers noted that, during their experiment, 2D images were not robust enough for facial emotion recognition due to the RGB camera's sensitivity to the surrounding conditions."}, {"label": 1, "content": "Digital forensics has become a pressing and critical issue to investigate due to the exponential increase in computer crimes. The main objective of the digital forensics analysis is to retain all collected evidence in its unadulterated form by gathering, identifying and evaluating digital data to reconstruct past events. The majority of digital crime evidence is stored within the computer system files. This article explores and evaluates the applicability of Neural Network techniques in DF analysis by analysing information related to computer's file system to determine whether they have been manipulated by a specific application program. A data set described as a vector of attributes related to file system activities thru a specific time is collected and utilized for creating a neural network classification model. The experimental outcomes show promising results in terms of various performance assessment metrics."}, {"label": 1, "content": "Hybrid Networks, defined as networks that include both SDN and IP nodes, were considered as a natural consequence of the incremental deployment of SDN in the current all-IP world. However, there are situations where the centralized control plane of SDN offers benefits over traditional distributed approaches. This prompted our study, where we developed a hybrid network that can dynamically switch between centralized and distributed control depending on network conditions. Our proposed optimization model yielded the expected performance: the network operated fully centrally when optimal, and fully distributed when appropriate. Furthermore, for unpredictable conditions, our model captured network node behavior over time and varying decision thresholds."}, {"label": 1, "content": "Business process and requirement specification are crucial phases in the software development process. Unfortunately, these crucial steps are often performed separately by different teams, resulting in misaligned models. The degree of misalignment grows continuously with their independent evolution. To address this issue, researchers have proposed using integration techniques to bridge the gap between the various heterogeneous models. Our approach is based on this technique and aims to align the business world, represented by BPMN, with the software requirement world, represented by the UML use case. Therefore, this approach is based on this technique to align the business world represented by BPMN and the software requirement world represented by the UML use case. We define an integrated meta-model that incorporates all BPMN and use case elements as well as new others to map traceable elements. Our approach supplements CASE tools with additional information and relationships, effectively maintaining global system consistency. Our approach supplements CASE tools with additional information and relationships to maintain the global system consistency. By integrating business process and requirement specification phases, our technique ensures that software development is efficient and that the full potential of model-driven software development is realized."}, {"label": 1, "content": "Cloud computing is the major paradigm in today's IT world with the capabilities of security management, high performance, flexibility, scalability. However, customers who value these features can further improve their benefits by choosing a cloud environment built using an HPC fabric architecture. However, security is still a major concern, not only on the software side but also on the hardware side. There are multiple studies showing that the malicious users can affect the regular customers through the hardware if they are co-located on the same physical system. Therefore, solving possible security concerns on the HPC fabric architecture will clearly make the fabric industries leader in this area. In this study, we propose an autonomic HPC fabric architecture that harnesses both resilient computing capabilities and adaptive anomaly analysis to improve security."}, {"label": 1, "content": "In today's world, the Internet has become the primary source of information. Consequently, end users need to be knowledgeable about how to use search engines in order to locate relevant information in a reasonable time with minimal effort. Conversely, search engines should provide alternate ways of displaying search results to aid visually impaired (VI) users' access to information. Our research aim is to produce a new representational model for the search engine results targeting VI users. The output of our study will be a functional prototype that summarizes search results into key concepts. Formal Concept Analysis (FCA) defines a concept as the maximum number of objects that are sharing the maximum number of features or attributes. Concepts are discovered by analyzing data patterns for the text of the study. This step significantly reduces the number of websites and URLs that match a user's search parameters. This scenario of summarization can give the user different directions for the shortest path to reach the target information with the minimum amount of time and effort required. The user can elect either to read the entire document or continue searching other related materials that match the inquiry. Experiments run on an iterative testing basis until VI users find proper results that satisfy their needs for the search context. We will use their observations and interpretations to evaluate the user interface and suggest modifications accordingly. This study will guide us for designing a new model for summarizing search results based on the FCA algorithm to the VI end users, and with a new representation interface based on the discovered concepts' weights."}, {"label": 1, "content": "In this study, our objective is to explore the feasibility of predicting defects in plastic injection molding by leveraging predictive models built from time-series process data obtained from a molding machine. The model of our choice is an RNN (recursive neural network) model using LSTM (long short-term memory) units in its hidden layer. This model is recognized for its ability to analyze and process time-series data effectively. Since defects are rare and thus the dataset is highly skewed, we try to achieve a high average recall rather than a high classification accuracy. Additionally, we provide preliminary results from our experiments and describe the direction of our future research."}, {"label": 1, "content": "Pseudo-Random Number Generators (PRNGs) play a vital role in many cryptography functions such as encryption, authentication, and identification. Producing a Pseudo Random Number (PRN) including high randomness is a big challenge for researchers. This paper introduces a model for PRNG via employing Hopfield Neural Network (HNN) that has produced unpredictable output under specific circumstances. The random numbers generated by means of HNN are evaluated through the National Institute of Standards and Technology (NIST) statistical test and ENT test. Effectiveness of the proposed model has been revealed based on the results recorded over the evaluation metrics."}, {"label": 0, "content": "To solve the problems of the data reliability for NAND flash storages, a variable-node-based belief-propagation with message pre-processing (VNBP-MP) decoding algorithm for binary low-density parity-check (LDPC) codes is proposed. The major feature is that, by making use of the characteristics of the NAND flash channel, the proposed algorithm performs the message pre-processing (MP) scheme to effectively prevent the propagation of unreliable messages and speed up the propagation of reliable messages. To further speed up the decoding convergence, the treatment for oscillating variable nodes (VNs) is considered after the MP scheme being employed. Simulation results show that the proposed VNBP-MP algorithm has a noticeable improvement in convergence speed without compromising the error-correction performance, compared with the existing algorithms."}, {"label": 0, "content": "To solve the simultaneous localization and mapping (SLAM) problem, many techniques have been proposed, and the Particle Filter (PF) is one of effective ways. However, the PF algorithm needs a large number of samples to approximate the posterior probability density of the system, which makes the algorithm complex. What's more, the judgment of resampling is imperfect. Based on this, an improved PF algorithm which introducing population diversity factor and genetic algorithm into the process of re-sampling is proposed in this paper. The effective sample size and the population diversity factor are used to determine whether to re-sampling. When re-sampling is needed, the genetic algorithm is used to optimize the particle set. The simulation result shows that estimation accuracy of the improved algorithm is better than that of traditional particles filter, not only in accuracy, but also in efficiency."}, {"label": 0, "content": "In the future scenario of multiple wireless network coverage, the choice of vertical handoff decision algorithm will directly affect the continuity of the session, the mobility of the user, and seamless roaming under heterogeneous wireless networks. Therefore, the study of vertical handover related algorithms is the key to the success of various wireless access networks in the future. This paper proposes an optimized algorithm which combines two multiple attribute decision making (MADM) methods, the Entropy and the improved Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS). The Entropy method is applied to obtain objective weights and the improved TOPSIS method is used to rank the alternatives. The simulation results show that the proposed technique can make the distribution of weights more reasonable, and effectively reduce the number of handoffs."}, {"label": 0, "content": "Passive sound source localization (SSL) using time-difference-of-arrival (TDOA) measurements is a non-linear inversion problem. In this paper, a data-driven approach to SSL using TDOA measurements is considered. A neural network (NN) is viewed as an architecture constrained non-linear function, with its parameters learnt from the training data. We consider a three layer neural network with TDOA measurements between pairs of microphones as input features and source location in the Cartesian coordinate system as output. Experimentally, we show that, NN trained even on noise-less TDOA measurements can achieve good performance for noisy TDOA inputs also. These performances are better than the traditional spherical interpolation (SI) method. We show that the NN trained offline using simulated TDOA measurements, performs better than the SI method, on real-life speech signals in a simulated enclosure."}, {"label": 0, "content": "We consider a two user Gaussian multiple access channel with an additive Gaussian state process. The past values of both the state and the received symbols are strictly causally made available to the encoders at each instant. The capacity region for the noiseless case, without any feedback, was recently solved in literature. Here we study the model with noise as well as feedback. We propose a communication scheme which effectively utilizes the feedback symbols as well as the state information to enhance the achievable region. In particular, Wyner-Ziv binning on the state information and Ozarow feedback scheme for the MAC are effectively utilized, using a suitable interleaving technique. The obtained region is significantly better than the feedback capacity region with no state information."}, {"label": 0, "content": "Many machine learning techniques and social engineering methods have been adopted and devised to combat phishing threats. In this paper, a novel hybrid deep learning model is proposed to identify phishing attacks. It incorporates two components: an autoencoder (AE) and a convolutional neural network (CNN). The AE is adopted to reconstruct features that enhances correlation relationship among the features explicitly. The results from the experiments show that the model is able to detect phishing attacks with a mean accuracy over 97.68%, yet it has high generalization ability and can detect phishing attacks in the receivable time scale."}, {"label": 0, "content": "With the development of the Internet, social bots are increasingly spreading on social platforms. Therefore, an effective detection algorithm is demanded to detect these social bot accounts that endanger social networks. In this paper, a social bots detection model based on deep learning algorithm (DeBD) is proposed. The model mainly includes three layers. The first layer is the joint content feature extraction layer, which focuses on the feature extraction of the tweets content and the relationship between them. The second layer is the tweet metadata temporal feature extraction layer, which regards the tweet metadata as temporal information and uses this temporal information as the input of the LSTM to extract the user social activity temporal feature. The third layer is the feature fusing layer, which fuses the extracted joint content features with the temporal features to detect social bots. To evaluate the effectiveness of the DeBD model, we conducted experiments on three different types of new social bot data sets from the real world and the experiment results also demonstrate the effectiveness of our proposed model."}, {"label": 0, "content": "In this paper, we consider a two-way relay (TWR) visible light communication (VLC) system consisting of two users which communicate to each other with the help of a relay. To achieve efficient transmission, we introduce network coding (NC) into VLC system and we develop two energy-efficient NC-based strategies, namely straightforward network coding over finite-alphabet sets (SNCF) scheme and physical-layer network coding over finite-alphabet sets (PNCF) scheme, which need three time slots and two time slots respectively to achieve the communication. Simulation results indicate that our proposed SNCF scheme and PNCF scheme both can achieve great performance gains over the traditional four time-slot transmission scheme."}, {"label": 0, "content": "We introduce Generative Adversarial Network (GAN) into the radio machine learning domain for the task of modulation recognition by proposing a general, scalable, end-to-end framework named Radio Classify Generative Adversarial Networks (RCGANs). This method naively learns its features through self-optimization during an extensive data-driven GPU-based training process. Several experiments are taken on a synthetic radio frequency dataset, simulation results show that, compared with some renowned deep learning methods and classic machine learning methods, the proposed method achieves higher or equivalent classification accuracy, superior data utilization, and presents robustness against noises."}, {"label": 0, "content": "In this paper, we study the physical layer security and transmission reliability problem where there is an active eavesdropper (AE) in the D2D underlaying cellular networks. We formulate the cooperation between the cellular user equipment (CUE) and the D2D user equipment (DUE), the completion between legitimate users and the AE to be a secrecy anti-jamming game. In the proposed game framework, DUE launches the cooperative relaying or the friendly jamming mode to help CUE to improve its anti-eavesdropping and anti-jamming performance. CUE gives different-level rewards for the assistance of the DUE. And AE shifts its attacking modes between actively jamming and passively eavesdropping to maximize the destruction for the D2D underlaying cellular networks. Under the perfect information, we prove the existence of the pure-strategy equilibrium of the proposed game. Under the imperfect information, we analyze the existence of the mixed-strategy equilibrium of the proposed game and propose a distributed Q-Iearning-based algorithm to converge to a mixed-strategy equilibrium. Simulation results show that the proposed algorithm is convergent and verify that average utilities of legitimate users are improved by the cooperation between CUE and DUE."}, {"label": 0, "content": "In recent years, deep learning object detectors including Fast/Faster R-CNN, SSD, R-FCN and Mask R-CNN have shown significant performance for general object detection except for pedestrians. The Region Proposal Network (RPN) in Faster R-CNN works well yet lacks of adaptability. Therefore, we propose an adaptive real-time pedestrian detection and attribute identification scheme based on Caffe. The first contribution is the adaptive threshold adjustment (ATA) algorithm for intelligent monitoring, utilizing the pedestrian movement information to adjust the threshold. Moreover, to overcome the time-consuming defect, we analyze the influences of the number of layers, the size of convolution kernels and the number of feature maps to reduce redundant computation while maintaining satisfactory performance. By optimizing the neutral network structure, choosing model parameters and data augmentation, a stable and well-performed model with fast detection rate and high accuracy is obtained. Besides, pedestrian information can also be identified in our program, offering better service in security monitoring, intelligent robots and other fields. Extensive experimental results demonstrate that even in complex and athletic scenarios, our method can make an improvement in quality and speed over state-of-the-art."}, {"label": 0, "content": "Human heart is a vital organ therefore proper diagnosis of heart activities is essential. Various parameter estimation techniques have been developed to estimate heart parameters. In this work, we use Ensemble Kalman Filter (EnKF) and Particle Filter (PF) for dynamic assimilation of human heart parameters. EnKF and PF are modified filters specifically designed for state prediction of nonlinear systems with large data samples. A third order mathematical heart model is used to estimate three heart parameters that includes movements of heart muscle fiber, tension in heart muscle and electrochemical activity of the heart. EnKF and PF are applied to heart model and different case studies are performed to observe the prediction accuracy by comparing sum squared error values. Case studies are performed with variable state and measurement noise values. The proposed approach demonstrates promising results in accurately predicting the human heart parameters."}, {"label": 0, "content": "In recent years, Deep Learning based method for 3-Dimension (3D) geometry perception tasks, like dense depth recovery, optical flow estimation and ego-motion estimation, is attracting significant attention. Inspired by recent advances in unsupervised strategies to learning from video datasets, we present a reasonable combination of constrains and a finer architecture, used for unsupervised ego-motion and depth estimation. Specifically, we introduce our effective neural networks Depth-Net (for monocular depth estimation) and Pose-Net (for ego-motion estimation), which are trained with monocular images. Depth-Net is proposed by us, improving the accuracy of estimation with as few parameters as possible. Finally, extensive experiments are implement on the KITTI driving dataset, proving our method outperforms some state-of-the-art results in unsupervised even supervised method."}, {"label": 0, "content": "Images have always had a significant effect on their viewers at an emotional level by portraying so much in a single frame. These emotions have also been involved in human decision making. Machines can also be made emotionally intelligent using \u2018Affective Computing\u2019, giving them the ability of decision making by involving emotions. Emotional aspect of machine learning has been used in areas like E-Health and E-learning etc. In this paper, the emotional aspect of machines has been used to perform Geo-tagging of an image. The proposed solution concentrates on a hybrid approach towards Affective Image Classification where the Elements-of-Art based emotional features (EAEF) and Principles-of-Art based emotional features (PAEF) are combined. Firstly, experiments are performed on these two sets of features individually. Then, these two sets are combined to obtain a Hybrid feature vector and same experiments are performed on this feature vector. On comparison of results, it is indicated that the hybrid approach gives better accuracy then either individual approach. Images in this research work are downloaded from Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset which contains the co-ordinates of millions of images and are free to use."}, {"label": 0, "content": "The HF radio communication has long been a big problem in channel selection since the spectrum environment is dynamic. To verify the feasibility of detecting idle channels by spectrum prediction, the data in this paper are based on realworld measurements collected by USRP in different time periods. The received signal power is converted to continuous sequences through a new channel state model reflecting spectrum availability. We then develop a prediction algorithm using simplified frequent pattern mining which can predict channel availability based on past channel states with considerable accuracy. The experimental results show that the measured data are more fluctuant in the afternoon which increase the predicted difficulty, nevertheless, the proposed algorithm is superior to neural network and Markov model in this situation, and the larger samples the better prediction performance."}, {"label": 0, "content": "As a multicarrier modulation system, OFDM/OQAM system is especially sensitive to the system synchronization errors. In this paper, we first introduce the existing data-aided joint carrier frequency offset and time offset estimation methods for OFDM/OQAM systems in time domain, and point out their shortcomings. On this basis, combining the advantages of the existing methods and introducing an iterative link, an improved time-domain data-aided joint carrier frequency offset and time offset estimation method for OFDM/OQAM system is proposed. Simulation results show that this method can effectively overcome the shortcomings of the existing methods and enhance the time-frequency offset estimation performance for OFDM/OQAM systems, which is an effective time-frequency offset estimation method for OFDM/OQAM systems."}, {"label": 0, "content": "Malware detection is more challenging due to the increase in android malicious programs and the current problems of android malicious detection. This paper proposes an android mobile malware detection system based on deep neural network, a novel malware detection method which uses optimized deep Convolutional Neural Network to learn from opcode sequences. In the proposed detection system, the optimized Convolutional Neural Network is trained multiple times by the raw operation code sequence extracted from the decompiled android file, so that the feature information can be effectively learned and the malicious program can be detected more accurately. More critically, the k-max pooling method with better results was adopted in the pooling operation phase, and which improves the detection effect of the proposed method. The experimental results show that the detection system achieved the accuracy of 99%, which is 2%-11 % higher than the accuracy of the machine learning detection algorithms when using the same dataset. It also ensures that the indicators such as Fl-score, Recall and Precision are maintained above 97%."}, {"label": 0, "content": "The development of the city transportation system provides us lots of conveniences, but it also can brings traffic congestion causing environmental pollution and increasing the travel cost. The current research mainly focuses on traffic volume prediction, route recommendation. However, it is also very meaningful and instructive to select a suitable less time-consuming alternative route on a specified congested road. In general, skilled taxi drivers who are relatively familiar with the traffic condition would like to choose the less time routes to avoid peak congestion road segments. Inspired by above ideas, in this paper, we propose a novel hybrid framework which integrates both urban traffic flow characteristics theory and machine learning techniques. We first describe the problem definition, then capture a typical set of congestion road features from the GPS trajectories, the features include traffic volume, road speed limit, route distance, traffic light, and weather features. After that, the most commonly used top-k candidate alternate routes based on historical data are generated, then the feature representations for congestion are feed to train the deep learning model, and the best alternative route is selected after the training process. Extensive experiments on realistic datasets derived from realistic car services demonstrate the superiority of our methodologies."}, {"label": 0, "content": "During the pilot project that is constructed based on the edge ecology incubation and edge service platform, China Unicom works along with ZTE, Intel, Tencent video to build an edge data center test bed in the university town located in Tianjin, China. This is a combination of OTT business and edge computing technology for the first time. This paper focuses on the full introduction of overall architecture and construction scheme of edge vCDNs. Also the introduction of infrastructure and business flow are included. The test results of vCDN demonstrate that the deployment of applications to the network edge not only reduces the bandwidth pressure (resulting in network transmission and multiple layers of forwarding) and latencies but also optimizes user experience and helps content providers reduce costs. This pilot project begins the edge computing collaboration between China Unicom and OTT, and is of great guidance value for the reconstruction of the equipment rooms of edge DCs and the commercial incubation of 5G innovative services."}, {"label": 0, "content": "To solve the problem of joint channel estimation for two-way multiple-input multiple-output (MIMO) relay systems, we propose a low complexity algorithm in this paper. At each user, the proposed channel estimation algorithm uses a unified formulation of the received signal as a Tucker-2 model. A joint channel estimation process is derived out by resorting to the proposed low complexity iterative algorithm. The proposed algorithm can provide each user with full knowledge of all channel matrices in the considering communication system. Moreover, the proposed algorithm can estimate channel effectively even when the channel becomes strongly correlated. Simulation results demonstrate the effectiveness of the proposed algorithm."}, {"label": 0, "content": "At present, using distance function to learn image pairs is a widely used method in the field of computer vision, and Euclidean distance is widely used. But traditional Euclidean distance has disadvantage of distinguishing ability in the feature similarity measure. In this paper, we propose a weighted Pairwise Constrained Component Analysis (wPCCA) algorithm based on weighted Euclidean distance for person reidentification (Re-ID). The algorithm of wPCCA is based on the PCCA using the weighted Euclidean distance to measure the characteristics. The experiments were conducted on two challenging datasets named i-LIDS and CAVIAR, and gained good results."}, {"label": 0, "content": "Route randomization is an important research focus for moving target defense which seeks to proactively and dynamically change the forwarding routes in the network. In this paper, the difficulties of implementing route randomization in traditional networks are analyzed. To solve these difficulties and achieve effective route randomization, a novel route randomization approach is proposed, which is implemented by adding a mapping layer between routers' physical interfaces and their corresponding logical addresses. The design ideas and the details of proposed approach are presented. The effectiveness and performance of proposed approach are verified and evaluated by corresponding experiments."}, {"label": 0, "content": "Jamming identification is the precondition of taking targeted anti-jamming measures, and it is very important to improve the adaptability of electronic information system to electromagnetic environment. The traditional recognition method of jamming is based on the feature extraction based on expert knowledge, but due to the jamming pattern diversity and different parameter, in practice it is difficult to determine the appropriate feature set. Therefore, this paper introduces a deep learning approach, which automatically extracts features from the original data to identify the jamming factors of electronic information system. In order to demonstrate the effectiveness and practicability of this approach, the noise jamming factor identification of the superheterodyne receiver is introduced."}, {"label": 0, "content": "In this paper, we present a method using an unmanned aerial vehicle (UAV) to track the specified walking person and automatically capture a frontal photo of the target. The proposed method is composed by three parts: person detection and recognition, face detection and feature points localization, and vision based UAV control. In the person tracking part, we employ the deep neural network YOLOv3 for person detection and Locality-constrained Linear Coding (LLC) method to match the specified target person. In terms of frontal face perception, we use Multi-task Cascaded Convolutional Neural Networks (MTCNN) for face detection. Based on the vision information obtained from the two modules, the UAV can fly around the target person and obtain the target's frontal face image. The outdoor experiments based on a Parrot Bebop2 drone verify the effectiveness and practicability of our method."}, {"label": 0, "content": "In the recent past, we have witnessed steep growth in mobile data consumption. To address the capacity requirements resulting from the huge growth in mobile data traffic, the mobile network operators (MNOs) are adding more base stations and allocating more spectrum layers including outdoor and indoor small cells. Since the capacity requirement of the network varies over time, the scaling up of the network may increase the energy consumption of the Radio Access Network (RAN). Hence, we need to optimize the network to reduce the overall power consumption through Cloud based models, and deployment of power-efficient radio nodes. In this paper, we analyze the network evolution towards Cloud based Radio Access Network (CRAN) for a heterogeneous set of base stations such as those with Macro RRUs, Micro RRUs and Pico radio units. We derive the computational complexity using a flexible and \u2018future-proof\u2019 power model and apply it for the network. We also compare the computation complexity for various cases of User Equipment (UE) channel conditions, different sub-components within the given base station type and provide the results. We further use the Bin-Packing algorithm to analyze the number of base station cloud servers needed for this network and the power consumption of the base station cloud. We further evaluate whether the newer cloud servers with higher CPU cores are power efficient for a given load. We observe from the simulations, that the currently available base station cloud servers have more capacity and still are more power efficient than the baseline Compute Node servers used with the earlier power model."}, {"label": 0, "content": "This paper presents a shape descriptor-based approach to human activity classification in devices such as iPod Touch, smartphones, and other similar devices. In this work, signals acquired from the built-in accelerometer and gyroscope sensors of iPod Touch are analyzed to recognize different activities performed by a user. In order to extract the discriminative information, shape descriptor-based features are computed from the captured signals. These features are then normalized and concatenated to form a consolidated feature vector. To recognize an activity performed by the user, k-nearest neighbor classifier is employed. The proposed approach is evaluated on the publicly available dataset namely, physical activity sensor data. Our experimental results demonstrate the effectiveness of the proposed shape descriptors for activity classification. Additionally, the experimental results on the aforementioned dataset show significant improvement in classification accuracy as compared to the existing work."}, {"label": 0, "content": "Orthogonal frequency division multiplexing (OFDM) has been widely used in modern wireless communication systems. In OFDM system, channel estimation is a key technology to improve the system performance. However, most conventional channel estimation methods cannot suppress the noise effectively, which affects the quality of the final received OFDM signals. To solve this problem, this paper utilizes the raise cosine (RC) filter and square root raise cosine (SRRC) filter to suppress the inter-symbol-interference (ISI) and the noise in OFDM-RC and OFDM-SRRC systems. The proposed method first filters out the noise by using RC filter or SRRC filter, while obtaining the noise standard deviation as the threshold. Then, the impact of noise is further reduced by setting the noise suppression threshold. Simulation results are shown to verify the effectiveness of the OFDM-RC and OFDM-SRRC systems over multipath propagation conditions."}, {"label": 0, "content": "Coronary artery disease (CAD) is one of the leading causes of mortality and morbidity globally. Nowadays, it is spreading at an alarming rate. Recently, there is an increasing interest to develop simple and non-invasive automated methods for reliable diagnosis of CAD. Studies reported that the use of single-channel phonocardiogram (PCG) signal for detecting weak CAD murmurs caused by the stenosed coronary arteries due to turbulent blood flow. In this work, we introduce a new framework with multi-channel data acquisition system to classify CAD and normal subjects. The proposed method does not require any reference signal such as an electrocardiogram (ECG) signal for PCG signal segmentation as reported in the earlier studies. Subsequently, the study has used five different features, such as spectral moments, spectral entropy, moments of PSD function, autoregressive (AR) parameters, and instantaneous frequency derived from frequency representations of PCG signals. These features have captured the specific details related to the disease. We use an artificial neural network (ANN) for the classification task. Experimental results show that the AR features well-performed. We achieve an accuracy of 74.24% by using multi-channel recorded data where as the best performance obtained using single-channel signal is 69.69%."}, {"label": 0, "content": "In order to speed up the convergence of distributed online optimization algorithms, a Fast Distributed Online Conditional Gradient Algorithm (F-DOCG) is proposed in this paper. The Erdos-Renyi (ER) stochastic model is firstly established and an Edge Addition (AE) algorithm is proposed. Secondly, the Edge Addition algorithm and Distributed Online Conditional Gradient Algorithm are combined to propose a F-DOCG. The F-DOCG algorithm not only avoids the high cost projection problem with a linear approximation, but also improves the Regret bound based on the relationship between the underlying topology and the algebraic connectivity, and thus results in a faster convergence rate. Finally, compared with the existing Distributed Online Conditional Gradient Algorithm (DOCG), numerical simulation experiments show that the proposed F-DOCG has better performance."}, {"label": 0, "content": "In order to increase the capacity and improve the spectrum efficiency of wireless communication system, this paper proposes a rate-based iterative one-to-one matching game algorithm to realize multi users access an energy-harvesting small cells considering NOMA (non-orthogonal multiple access) in heterogeneous cellular networks. First, we use a heuristic clustering based channel allocation algorithm to assign channels to small cells and reduce the interference. Then we model user access problem as an iterative one-to-one matching game with rate as its utility, where one user matches with one small cell at each matching game and so is the small cell. After that, we propose an algorithm to iteratively enable users to access small cells in terms of proposed matching game and prove its stability. Finally, a power allocation algorithm to reallocate transmission power for each user is presented to fully use the harvesting power. Simulation results show that this algorithm outperforms OMA (orthogonal multiple access) system in efficiency besides improving the system capacity."}, {"label": 0, "content": "Multiple Signal Classification (MUSIC), Steered Response Power-PHAse Transform (SRP-PHAT) and Generalized Cross Correlation (GCC) are the well known techniques for Direction of Arrival (DoA) estimation, using microphone array. However, in real time scenarios, these techniques encounter limitations such as computational complexity and thresholding difficulties. In this paper, a novel and robust method is introduced in which DoA is estimated using the concept of subarray decomposition to provide better performance with effective thresholding and minimal computational complexity."}, {"label": 0, "content": "The localization is the one of the most promising technology in modern society. Wireless sensor network (WSN) as a popular method to solve the problem is being studied by many scholars. The main challenge in localization problem is the non-line of sight (NLOS) propagation. To overcome the issue, we present a method combing the particle filter and residual analysis. The residual analysis is a posteriori reliable algorithm. Particle filter is a powerful technique because it does not make any presumptions about the probability density function or the linearity of the system model. The mix of the residual analysis and particle filter could improve the localization accuracy. At the same time, the randomness of the particle improves the robustness of the method. Simulation results evaluate the effectiveness and the robustness of the proposed method."}, {"label": 0, "content": "Often, images captured by digital camera in outdoor vision system may be significantly distorted by bad weather conditions. Such visual distortions may negatively affect the performance of the system. One such bad weather condition is rain, which randomly makes intensity fluctuations in the images. This paper proposes a new low rank recovery based algorithm to remove the rain streaks from single image taken in rainy weather. This method makes the use of weighted nuclear norm (WNN) and total variation (TV) regularization for efficient rain removal. WNN assigns different weights to different singular values based on the details each singular value holds. TV regularization is used to discriminate most of natural image content from sparse rain streaks by preserving piecewise smoothness of images. Simulation result shows that the rain streaks are more efficaciously eliminated by our method."}, {"label": 0, "content": "In a long term evolution (LTE) based cellular network, the mobility management entity (MME) is responsible for non-data signaling between user equipment of multiple base stations in a geographic region and the core network. Thus, the MME residence time (MRT) is a key parameter required to improve the performance of an LTE based cellular network. The impact of various mobility and network scenarios on cell residence time has been studied in the literature. However, the MRT has not been suitably modeled. Hence, in this paper, we consider diverse mobility and network scenarios. For these scenarios, we model the MRT using various probability distributions. We analyze and evaluate the statistical performance of these distributions in modeling MRT. Finally, we show through exhaustive simulations that the Lognormal and Generalized Pareto distributions are best suited to model the MRT for specific network and mobility scenarios."}, {"label": 0, "content": "In view of the traditional BP neural network, high-dimensional complex data is prone to slow detection rate and low accuracy in network intrusion detection. To reduce data dimension and improve BP neural network performance, an intrusion detection method of KPCA-BP neural network is proposed. Firstly, the KPCA's good dimensionality reduction capability is used to reduce the dimension of network data. Then, by changing the initialization initial value method and loss function of traditional BP neural network, the learning performance of BP neural network is improved, and the learning effect of improved BP neural network is better. Experiments show that the KPCA-BP based intrusion detection method proposed in this paper has a better improvement effect on detection rate and accuracy."}, {"label": 0, "content": "The objective of the present work is to improve the epoch extraction performance from emotive speech by proposing a post processing approach to the conventional zero frequency filtering (ZFF) method using variational mode decomposition (VMD) based spectral smoothing. Due to the fast uncontrolled variations of the pitch in emotive speech signals, the reliable estimation of epochs is always challenging. In the proposed method, the spectra of the short frames of zero frequency filtered signal (ZFFS) is subjected variational mode decomposition to get component spectra in five modes. A smoothed short time spectra is then obtained by excluding the spectra from the two higher VMD modes which essentially have the high spectral variations. The modified ZFFS is then reconstructed using the sinusoidal parameters corresponding to single dominant frequency present in the smoothed spectra using VMD by parameter interpolation based sinusoidal synthesis. The resulting re-synthesized ZFFS has reduced spurious zero crossings as compared to that obtained from the conventional ZFF method for emotive speech signals. The effectiveness of the proposed VMD based spectral post processing is confirmed from the improved epoch identification rate and epoch identification accuracy across all the emotive utterances (with 7 emotions) present in German emotion speech database having simultaneous speech and electroglottographic (EGG) signal recordings. The performance of the proposed method is found to be better or comparable with the other existing ZFF based post processing methods proposed for emotive speech signals in terms of the epoch identification accuracy with respect to the corresponding reference epochs estimated from EGG signals."}, {"label": 0, "content": "We analyze the performance of a dual-hop downlink cellular amplify-and-forward cooperative system in the presence of both channel estimation errors and radio frequency (RF) impairments, where multiple antennas are deployed only at the base station (BS) and single-antenna at relays and mobile stations (MS). Specifically, we derive approximate as well as exact closed-form expressions for the outage probability and expected spectral efficiency. In addition, simple asymptotic expressions at the high signal-to-noise ratio (SNR) regime are obtained, which facilitate the characterization of the achievable diversity order of the system. To validate the derived analytical expressions, we presented simulation results which are sufficiently tight across the entire range of SNRs. Findings of the paper suggest that full diversity order can be achieved only when the RF front-end hardware is assumed to be perfect, while in practice the imperfections in hardware are inevitable and reduce diversity order of the system. Moreover, the influence of key parameters such as the number of antennas, users, and relays on the system performance has been presented with the influence of the level of RF impairments."}, {"label": 0, "content": "Ultra-wideband (UWB) is a very promising technology for accurate indoor localization. Time of arrival (TOA) and time difference of arrival (TDOA) are two of the most widely used algorithms for UWB to localize the mobile station (MS). However, the accuracy performance of the position estimation based on these algorithms dramatically degrades under Non-Line-of-Sight (NLOS) condition. Inertial measurement unit (IMU) is not effected by NLOS. However, the localization results based on IMU are only accurate for a short period of time due to the drift errors. The integration of these two systems will allow to profit from their advantages. In this paper, the Extended Kalman Filter (EKF) is used to combine IMU and UWB with TOA or TDOA approach. Based on the IMU measurements, the inaccurate UWB range measurements or range difference measurements due to NLOS could be detected in the EKF. Furthermore, in order to further improve the detection accuracy, the channel impulse response (CIR) from UWB are also used for NLOS detection during the real field test. The support vector machine (SVM) is used to train the data. Only the identified accurate measurements will be used for further calculation in the EKF. The performance of the proposed method will be evaluated in the simulation and real field tests. The proposed method shows both in simulation and real field tests very promising results."}, {"label": 0, "content": "Lately, the problem of code-switching has gained a lot of attention and has emerged as an active area of research. In bilingual communities, the speakers commonly embed the words and phrases of a non-native language into the syntax of a native language in their day-to-day communications. The code-switching is a global phenomenon among multilingual communities, still very limited acoustic and linguistic resources are available as yet. For developing effective speech-based applications, the ability of the existing language technologies to deal with the code-switched data cannot be over emphasized. The code-switching is broadly classified into two modes: inter-sentential and intra-sentential code-switching. In this work, we have studied the intrasentential problem in the context of code-switching language modeling task. The salient contributions of this paper includes: (i) the creation of Hindi-English code-switching text corpus by crawling a few blogging sites educating about the usage of the Internet, and (ii) the exploration of the parts-of-speech features towards more effective modeling of Hindi-English code-switched data by the monolingual language models trained on native (Hindi) language data."}, {"label": 0, "content": "Today, wireless sensor networks are widely used in the field of intelligent transportation system. For areas with different vehicle densities, the amount of data that needs to be wirelessly transmitted to the central server is different. In this paper, we propose a new data transmission scheme, i.e., the Sensor On/off scheme based on Polling Algorithm (SOP). In specific, we are going to realize one no packet loss scheduling transmission mechanism in different service density areas, and have effectively integrated switching technology into SOP. That is, when the sensor is idle, we can turn it off to achieve the goal of reducing energy consumption. Finally, we compare the SOP with the random scheme and sequential scheme. It is proved by the analysis results that SOP can effectively reduce the energy consumption of data transmission in wireless sensor networks."}, {"label": 0, "content": "At present, the improved social network analysis methods are mainly based on homogeneous networks. However, the actual social networks are heterogeneous in nature. Heterogeneous social networks can better reflect the composition of the system and the associated relationships than the homogeneous social network model. The previously proposed ranking clustering provides a new idea, however, the algorithm only completes the clustering results of specific target types and cannot cover the complete heterogeneous network type. By introducing collaborative clustering algorithm and combining it with ranking, we propose a ranking collaborative clustering algorithm. Firstly, based on ranking clustering, the ranking distribution matrix is obtained, and then collaborative clustering is used to complete different types of clustering, and the relationship between different types and the same types is fully utilized. The experimental results on the real twitter and foursquare datasets show that the ranking collaborative clustering algorithm has better performance on the modularity compared to the ranking clustering."}, {"label": 0, "content": "In this paper, we consider the downlink in a K-tier heterogeneous network in the presence of Nakagami-m fading and noise. For such a system, we derive closed-form approximations of coverage probability and average rate achievable. A piece-wise linear approximation is employed in obtaining the simplified expressions. The proposed results are verified numerically through simulations. A comparison with existing work shows that the proposed work is a good approximation."}, {"label": 0, "content": "This study is aimed at the problem that the detection rate of intrusion detection technology based on Extreme Learning Machine (ELM) algorithm is not high and the intrusion detection technology based on Support Vector Machine (SVM) algorithm is slow. An intrusion detection method based on Kernel Principal Component Analysis (KPCA) and extreme learning machine algorithm is proposed. Using the KPCA algorithm to reduce the dimension of the extracted feature matrix, and using the ELM algorithm to perform multi-classification detection on four common types of attacks. Simulation results show that the proposed method is more efficient and faster than intrusion detection based on extreme learning machine algorithm and intrusion detection based on support vector machine algorithm. Finally, the accuracy, false alarm rate, detection rate, and detection time in intrusion detection technology are improved."}, {"label": 0, "content": "Evolutionary circuit design, is a time-intensive process, especially for large scale circuits. In this paper, we design a distributed computation framework for evolutionary circuit design via parallel genetic algorithm, which actualizes the tasks-distributing and tasks-collecting with the employment of communication network structure using client-sever mode. A series of experiments are conducted and the results show that the framework presented performs better on seeking for the circuits that satisfy designer specified performance goals with high efficie-ncy, great flexibility, strong fault tolerance."}, {"label": 0, "content": "Nowadays, more and more people have their own accounts in different social networks, and they might use the different email addresses or phone numbers in different networks, so how to identify the same person among different social networks become a vital problem, called network alignment. Users with different accounts are called anchor users, researches showed that using some known anchor users to predict the potential anchor links for the full network is an effective way. To predict more accurate anchor links, the paper proposes a new prediction framework ISS, based on a reality of partially aligned social networks, it applies supervised learning based on social feature extraction and strict stable matching, which improve the accuracy of the prediction result, what is more, we apply an iterative framework to refine known information and maximize the prediction results. Experiments have conducted in two realworld heterogeneous social networks, Foursquare and Twitter, and it demonstrates that ISS can predict anchor links among heterogeneous social networks very well and outperform other similar prediction methods."}, {"label": 0, "content": "Aiming at the shortcomings of traditional speaker segmentation and clustering methods, this paper proposes a multilevel speaker re-segmentation and re-clustering algorithm based on GMM-UBM. The algorithm is based on the method of statistical modeling in the field of speaker recognition, and makes full use of the speaker information after segmenting and clustering in traditional methods to re-segment and re-cluster speech files, which improves the performance of the system effectively."}, {"label": 0, "content": "Research in the field of co-prime arrays and samplers has been mainly focused on reconstructing the autocorrelation and the spectral content of a signal at the Nyquist rate from sub-Nyquist data. This has found applications in power spectrum estimation, beamforming, direction-of-arrival estimation, and system identification. However, the use of coprime samplers for cross-correlation estimation has not received much attention. We describe cross-correlation estimation using co-prime samplers and consider two scenarios. In the first, both signals are acquired using co-prime samplers, while in the second scenario we assume one of the signals to be a known signal and thus available at the Nyquist rate, and the second signal is acquired using a co-prime sampler. We determine the number of contributors available for cross-correlation estimation at each difference value as this is a key parameter in determining the estimation accuracy. The work presented in this paper will have applications in time-delay, range, velocity, acceleration, and cross-spectrum estimation, which require cross-correlation estimation."}, {"label": 0, "content": "The behavior of APT attack has been the hot topic in recent network security study. It is critical to understand the implementation principle of APT attack. In this paper, we analyze the behavior of APT attack in the Ngay campaign from two aspects: network traffic and code implementation. We first set up the attack chain by using network traffic analysis. Then, the vulnerability exploitation process is detailed through reverse code analysis. After that, we illustrate the process of building back door. Lastly, we introduce the obfuscation technology applied in the APT malware samples."}, {"label": 0, "content": "Channel estimation for Multi-input Multi-output/Orthogonal Frequency Division Multiplexing (MIMO/OFDM) in fast linear-time-varying (LTV) multi-path channel using special frequency-division (FD) pilot is proposed. Unlike linear -time-invariant (LTI) channel, OFDM systems in LTV channel may suffer significant inter-carrier interference (ICI), which strictly depend on Doppler frequency shift caused by relative movement of transmitters and receivers. In this paper, we redesign frequency-division pilot for LTV channel, so we can effectively estimate the channel tap of intermediate instant of each OFDM symbol with relatively low ICI. Finally, we use well-known basis expansion model (BEM) to fit the whole channel. Numerical results indicate that our new frequency-division pilot combined with BEM fitting can obtain high precise channel estimation for fast LTV multi-path channels."}, {"label": 0, "content": "A refined phase estimation based parallel carrier recovery algorithm for high speed wireless communication systems is proposed in this paper. This parallel algorithm is based on a serial DPLL (digital phase locked loop) carrier recovery feedback architecture and a novel refined phase estimation module. To archive high speed communication, parallelization of serial DPLL carrier recovery algorithm is presented; to guarantee high accuracy, a refined phase estimation design is proposed. A 32 parallel baseband simulation model of 16QAM modulation is performed on MATLAB platform to validate the proposed algorithm. Simulation results demonstrate that the performance loss of EVM (Error Vector Magnitude) introduced by the proposed algorithm is less than 0.3%, which is only half of the traditional coarse compensation algorithm."}, {"label": 0, "content": "Social media have become increasingly popular components of our everyday lives in today's globalizing society. They provide a context where people across the world can communicate, exchange messages, share knowledge, and interact with each other regardless of the distance that separates them. This research trend, extraction of events for specific domain from these social media is emerging speedily ranging from business intelligence to nation security field. The short length of Twitter messages and frequent use of informal and ungrammatical language challenge many long standing approaches for automatically detecting and categorizing events using streamed data in Event Message Identification system. A semi-supervised approach with Support Vector Machine (SVM) in combination with the corpus to identify the events from twitter for targeted domain in specific location is proposed in this paper. The experimental results show that the proposed semi-supervised SVM model is more efficient than a strong state-of-the-art semi-supervised classification model of Logic Regression, Naive Bayes and Decision Tree."}, {"label": 0, "content": "Fine-grained classification is challengeable due to the small inter-class variance and large intra-class distance between fine-grained categories. The key to solve this problem is to locate the discriminative part in the image. In this paper we propose a weakly supervised method, which only need image-level label for fine-grained classification. In our model, the convolutional neural network (CNN) can location the discriminative region through attention and automatically focus on subtler features by zooming the discriminative region and feeding it to the next CNN. A Squeeze and Excitation (SE) module is employed for channel-wise attention, and a spatial constrain loss is utilized to keep the diversity of located part. We conduct experiments on CUB-2011-200, Stanford Dogs and Stanford Cars datasets to evaluate the performance of our model. The experimental results demonstrate the effectiveness of the proposed method as compared other methods."}, {"label": 0, "content": "As the high-bandwidth and data-intensive applications evolving rapidly, traditional electrical networks are no longer able to meet the increasing traffic requirements. In order to improve the communication performance of data center network(DCN), the hybrid electrical/optical architecture has become a new research topic. Moreover, traditional architectures are often too complicate to manage. As a new technology, Software Defined network (SDN) addresses this issue effectively. In this paper, we propose a hybrid architecture based on SDN. The control manager is used to monitor and allocate traffic, then configure the network by SDN. There is a hybrid network platform under the implementation of virtual machine migration. The experiment shows that the proposed scheme reduces the total time of the virtual machine migration effectively compared to that running on the electrical architecture. In addition, our scheme can configure the network topology flexibly and achieve load balancing."}, {"label": 0, "content": "Many audio forensic applications would benefit from the ability to classify audio recordings, based on characteristics of the originating device, particularly in social media platforms where an enormous amount of data is posted every day. This paper utilizes passive signatures associated with the recording devices, as extracted from recorded audio itself, in the absence of any extrinsic security mechanism such as digital watermarking, to identify the source cell-phone of recorded audio. It uses device-specific information present in low as well as high-frequency regions of the recorded audio. On the only publicly available dataset in this field, MOBIPHONE, the proposed system gives a closed set accuracy of 97.2% which matches the state of art accuracy reported for this dataset. On audio recordings which have undergone double compression, as typically happens for a recording posted on social media, the proposed system outperforms the existing methods (4% improvement in average accuracy)."}, {"label": 0, "content": "With the success of Convolutional Neural Networks (CNN) in computer vision tasks, Steganalysis, the technique of detecting hidden secret messages within images, is moving away from Feature Engineering to Network Engineering. Deep neural networks are being proposed to model and capture the weak embedded signals, in such a low Signal-to-Noise (SNR) scenario. In this paper, we propose a novel Convolutional Neural Network based on aggregated residual transformations, which generate stronger image representations helpful for steganalysis. The architecture has very few hyperparameters to set and focus on increasing the classification accuracy while keeping the depth and number of parameters fixed. The residual skip connections further help preserve the weak embedded signals and improve the gradient flow. We evaluated our proposed CNN on BOSSbase against S-UNIWARD and HILL steganographic algorithms with different payloads. Comparing with the state-of-the-art Deep Residual Learning (DRN) based on Residual Learning and the SRM plus Ensemble, our proposed CNN gives a better classification Accuracy."}, {"label": 0, "content": "Mixing matrix estimation is very important for underdetermined blind source separation. To solve the problems of existing methods for mixing matrix estimation such as low estimation accuracy, a detection method for single source points (SSPs) is proposed based on local stationarity and distribution symmetry in this paper, and then mixing matrix estimation is obtained through clustering algorithm. The proposed method does not require region division of hypersphere and is easy to operate, so as to effectively eliminate pseudo SSPs and improve the clustering features of observed signals. The simulation results show that the proposed method has higher accuracy than the traditional methods."}, {"label": 0, "content": "A robust MAC protocol is required to provide Vehicle to Vehicle (V2V) and Vehicle to Infrastructure (V2I) data traffic communication for Vehicular Ad Hoc Networks (VANETs). The data traffic in VANETs is classified as high priority safety messages, control messages and non-safety infotainment related messages. For this purpose, three regional standards have been proposed. However, their performance has been questioned because of their inability to ensure timely delivery of safety messages. Therefore, most of recent research work has been focused on designing an optimal MAC protocol. This paper surveys significant TDMA-based MAC protocols proposed for VANETs. The purpose of this survey is to outline the current status of research work in MAC for VANETs by evaluating the basic idea, operation and performance of these protocols. Furthermore, certain requirements which must be considered for future work in development of optimal TDMA-based MAC protocols for VANETs have also been proposed."}, {"label": 0, "content": "Physical-layer secret key generation (PSKG) technology based on reciprocal wireless channel has been widely studied in point-to-point (P2P) scenarios as it can effectively solve the key distribution problem in traditional security mechanisms. However, the computation and energy cost of PSKG is high when it is extended to group key distribution. The problem of applying PSKG to ensure group secret communication remains open. In this paper, we propose a lightweight group key distribution (LGKD) method for a star network topology environment. In our proposed method, center node and each child node first extract high correlated channel characteristics instead of generating identical P2P keys, respectively. Then, the group key is broadcasted to each child node protected by the P2P channel characteristics with high similarity. Our simulation results verify the feasibility and effectiveness of our proposed group key distribution method."}, {"label": 0, "content": "High speed railway communications has received wide attention in the world. The Doppler shift from the motion of train induces inter-carrier interference. Meanwhile the doubly selective channel increases the difficulty of training and beamforming. In this paper, we propose an angle domain channel tracking scheme. The base station is equipped with large-scale uniform linear antennas array (ULA) to provide high angular resolution. The spatial property was investigated to decompose channel into angular information and beam gain. The former is acquired by aligning beams towards the direction of signals, based on which the Doppler frequency offset (DFO) is compensated. The latter is tracked by using linear Kalman filter, which is optimal for minimizing the mean square error (MSE). By combing the angular information and beam gain, the CSI is recovered. Simulation results show the superiority of proposed scheme."}, {"label": 0, "content": "We focused on a problem where balanced use of sensor nodes' battery power is considered to maximize the overall lifetime of ad-hoc Wireless Sensor Networks. A process in which utilizing less attended sensor nodes compare to sensor nodes which are used more frequently enhances the overall network lifetime. To perform this process, we propose a joint optimization problem to select a subset of active sensor nodes and a multi-hop routing structure interconnecting all selected sensor nodes, which helps to route the aggregated information to a querying node. Our optimization problem becomes non-convex over the subset selection and the multi-hop routing paths selection, thus belonging to the class of NP-hard problems. We solve our problem by relaxing one of the variable so that optimization problem becomes convex over this variable, which can be solved efficiently. We also propose an iterative algorithm to solve this problem distributively. We demonstrate by extensive simulation that the above mentioned both the approaches increase the overall network lifetime for a given power budget. One another important result is that the distributed approach provides an optimal routing structure considering over the well known shortest path tree based routing structure."}, {"label": 0, "content": "In this paper, a comparative analysis of various performance enhancement techniques in two-dimensional (2-D) atmospheric optical code division multiple access (OCDMA) system is studied in presence of beam divergence, multiple access interference (MAI), noise and atmospheric turbulence. Lognormal and gamma-gamma probability density functions (pdfs) are considered for evaluating fading process due to atmospheric turbulence. Further, double hard limiters, spatial diversity and error correcting code (ECC) are used for performance improvement of the 2-D atmospheric OCDMA system. Double hard limiters and ECC improve performance substantially as compared to spatial diversity. In addition, double hard limiters are cost-effective than the spatial diversity and ECC. Thus, double hard limiters are superior to the other performance improvement techniques in 2-D atmospheric OCDMA system."}, {"label": 0, "content": "Mechanical equipment fault signals are mostly nonlinear and non-stationary signals. Processing these signals by Fourier transform and wavelet transform usually cannot obtain desired fault diagnosis results. In this paper, a machine fault diagnosis method based on Industrial Internet of Things (IIoT), industrial wireless sensor networks (IWSNs), Hilbert-Huang transform (HHT), and support vector machine (SVM) is proposed, in which HHT and SVM are used for fault feature extraction and fault diagnosis respectively. The proposed fault diagnosis approach by SVM is implemented and tested on the IWSN sensor node, while the fault feature extraction method using HHT is verified by MATLAB simulation. The effectiveness of the presented approach is evaluated by a set of experiments using bearing vibration data. The result indicates that the fault diagnosis accuracy of the presented method reaches 100% for four machine working conditions and 92% for five working conditions."}, {"label": 0, "content": "For serving traffic in the inter data centers which provide services such as, duplication of data and migration of the virtual machines, it is requisited to transfer voluminous data for which, under guarantee of a finishing time within the stipulated (i.e., pre-set) deadline, specific latency is tolerable. In the current work, we propose offline routing and spectrum assignment (RSA) schemes in view of the transfer of deadline complying voluminous data demands in elastic optical networks. The proposed schemes, which jointly optimize time and frequency domains, are initially formulated as an integer linear program (ILP). Subsequently, in view of practicality, we propose scalable scheduling techniques which combine three methods of ordering demands, and two schemes of RSA. To evaluate the proposed ILP model and the scheduling methods, we conduct simulations considering realistic network parameters and topologies. The obtained results demonstrate that in comparison to ILP model, scalable methods obtain similar spectrum usage performance within reasonable times. Lastly, based on the results, we also provide a `rule-of-thumb' on the selection of appropriate scheduling technique."}, {"label": 0, "content": "Automatic Modulation Recognition (AMR) is of great significance in civil and military applications. Cumulant-based recognition is one of the effective methods for AMR. However, different from that in conventional SISO systems., Cumulant-based AMR in space division multiplexing MIMO systems faces the problem of lower recognition rate since the statistical characteristics of signals are different in these systems. In order to solve this problem., we propose to adopt auto-encoding network (AEN) as the data dimension reduction algorithm and artificial neural network (ANN) as the classifier of several kinds of digital modulation signals in MIMO system. In our simulations., the proposed scheme is used to classify five types digital modulation signals., which are 2PSK., 4PSK., 8PSK., 16QAM., 32QAM. Simulation results indicate that the proposed method will realize substantially higher recognition rate compared with direct classification by cumulants."}, {"label": 0, "content": "With the development of renewable energy technology, it is possible that base stations (BSs) are powered by renewable energy. In this paper, we study the problem of minimizing the on-grid energy consumption in heterogeneous cellular networks with hybrid energy supplies, and propose an energy-aware user association algorithm. The key idea of our proposed algorithm is to adaptively set the biasing factor for each small BS (SBS) according to the renewable energy storage and make more utilization of the renewable energy for minimizing the on-grid energy consumption. The proposed algorithm is compared with the max-RSRP algorithm and the traditional cell range expansion (TCRE) algorithm. The simulation results show that the proposed algorithm can effectively reduce the consumption of on-grid energy and improve the utilization of renewable energy in the network."}, {"label": 0, "content": "Wireless sensor networks are susceptible to report fabrication attacks, where adversary can use compromised nodes to flood the network with false reports. En-route filtering is a mechanism of dropping bogus/false reports while they are being forwarded towards the sink. Majority of the proposed en-route filtering schemes are probabilistic, where the originality of forwarded reports is checked with fixed probability by intermediate nodes. Thus, false reports can travel multiple hops before being dropped in probabilistic en-route filtering schemes. Few deterministic based en-route filtering schemes have also been proposed, but all such schemes need to send the reports through fixed paths. To overcome the above mentioned limitations of existing en-route filtering schemes, we propose a novel deterministic enroute filtering scheme. In the proposed scheme, secret keys are allocated to sensor nodes based on combinatorial design. Such design ensures direct communication between any two nodes without adding more key storage overhead. We provide in-depth analysis for the proposed scheme. The proposed scheme significantly outperforms existing schemes in terms of expected filtering position of false reports and is more buoyant to selective forwarding and report disruption attacks. Our scheme also performs neck-to-neck with existing schemes in terms of protocol overheads."}, {"label": 0, "content": "Learning to predict human body motion has emerged as a meaningful research in computer vision and artificial intelligence. This paper presents the study on predicting human body motion from video sequences. We propose a human body motion prediction network integrating the recent advanced 2D feature extraction and video sequences prediction. Based on the temporal characteristics extracted from video sequences, our network realizes the prediction of the human motion. We train the network using the video based human pose datasets and demonstrate good performance of our network on 2D human body motion prediction through quantitative and qualitative results. Experimental results prove the feasibility of our method."}, {"label": 0, "content": "As a special form of Mobile Ad hoc NETwork (MANET), vehicles in the network can connect to other vehicles on the road and the Internet, and can provide stable and high-speed wireless data access services for vehicles with high velocity. VANET has become an effective technology and important means to guarantee vehicle safety, provide intelligent traffic management of high-speed data communication and vehicle entertainment. However, the typical characteristics of VANET, including rapidly changing network topology, highly dynamic channel condition and node competition in channel accessing, etc., raise difficulties and challenges on data transmission in VANET. To stress the problem, a cluster algorithm based on vehicle mobility for VANET is proposed, in the proposed algorithm, the object function of cluster head selection is formulated based on the vehicle mobility including position, velocity and packet forwarding capability, the process of the clustering algorithm is also presented. Simulations demonstrate that comparing to previously proposed algorithms, the proposed algorithm offers better performance in terms of packet delivery rate, average transmission delay and total throughput."}, {"label": 0, "content": "In this paper, we propose a progressive spectral mapping learning algorithm for throat microphone (TM) speech enhancement. Unlike previous full-band spectra mapping algorithms, this algorithm divides the spectra mapping from TM speech to Air-conducted (AC) speech into two tasks, one is the voice conversion task, and the other is the artificial bandwidth extension task. Long short-term memory recurrent neural network (LSTM-RNN) is further deployed as the mapping model. Objective evaluation results show that the TM speech quality is improved when compared with conventional full-band spectra mapping framework and DNN-based mapping model."}, {"label": 0, "content": "Aiming at the irrational size of LEACH protocol cluster and the imbalance of network energy consumption, an energy-equalized unequal clustering routing protocol is proposed. By introducing the energy decision factor and the density decision factor of node, the proposed method improves a threshold calculation formula of candidate cluster heads, and it is more likely that the nodes with more residual energy and more neighbor nodes become candidate nodes. The candidate cluster heads are elected to become the real cluster heads according to a certain competition radius. Since we consider the influence of the number of cluster heads and the distance between the nodes and the base station on the network energy consumption when determining the competition radius, then, the algorithm can divide the nodes into clusters of different sizes and make the size of the cluster close to the base station smaller, thereby effectively solving the \u201chot spots\u201d problem. Finally, the number of cluster heads and the cluster head distribution are more reasonable. The simulation results show that the improved algorithm can effectively balance the network consumption and prolong the network life cycle."}, {"label": 0, "content": "The rise of Bring Your Own Device (BYOD) now poses new challenges to the traditional intranet, which used to deploy boundary-based defenses to guarantee internal security. The bringing of personal devices has threatened the internal security. Based on the idea of isolation and dynamic, this paper designs and implements a Software-defined Intranet Dynamic Defense System (SIDD) to harass cyber kill chain. Firstly, to solve the issue that network can be easily reconnoitered due to its static attributes, we allocate virtual IP address space for intranet terminals and implement the dynamic mapping between real IP addresses and virtual IP addresses to hide the real IP address. Secondly, we propose a software-defined dynamic defense architecture scheme, which manages to provide a general control of the intranet, including three core modules (e.g. DNS, virtual & real address assignment and virtual address maneuvering). Finally, we implement a dynamic defense system oriented to the production environment, based on the OpenDaylight controller. Our experiments indicate that this method can achieve a definable IP address, which frequency and space are maneuverable, thus it could significantly reduce the availability of network reconnaissance and increase the difficulties of attacker's realtime attack without affecting network applications."}, {"label": 0, "content": "In this paper, we propose LayerOS, a proactive framework that can be used to schedule apps in order to improve the scalability of wearable devices. LayerOS generates scheduling strategies which wille be deployed to devices ahead of time based on the analysis of user behavior, device capability, etc. According to these policies, LayerOS can offload apps to the cloud or load apps from the cloud dynamically. Hence, users can execute any app provided by the cloud directly, and there is no need to modify installed apps and interrupt users' normal operations. In addition, LayerOS uses app state migration to ensure consistency of view and state of the application on different mobile devices. Experiments based on a prototype system have shown that, at the same level of limited space, LayerOS enables users to run virtually unlimited applications directly and increases the running fluency by 25.6% from the perspective of FPS. At the same time, LayerOS can effectively reduce the waiting time when launching one application. More specifically, the average waiting time can be reduced by up to 13.0% in the experiments."}, {"label": 0, "content": "Recently, accurate target tracking is widely used in the field of Unmanned Aerial Vehicles (UAV). In this paper, we focus on the application of detecting and following a walking pedestrian in real time from the moving platform with many interferences. We present a scheme that uses CNN model (YOLO-V2) to detect pedestrian and matches the walking pedestrian with a postprocessing and feature queue and Locality constrained Linear Coding algorithm. After that the ground station receives and analyses the video stream from the parrot and sends back commands to control the motion of UAV. At the beginning of the tracking process, the UAV is hovering when one pedestrian will be selected as the special target. Visual information is acquired only through a front camera without assistant sensors. A parrot Bebop 2 is adopted in the experiment, which is the basis for doing experiments outdoors and experimental result verify the effectiveness of our solution."}, {"label": 0, "content": "In wireless sensor network (WSN), a secure link in the key pre-distribution (KP) scheme may be compromised when sensor nodes are captured. Accordingly, the KP q- composite scheme was proposed to tackle this node-capture problem. Recently, Bechkit et al. proposed a hash chain based KP (HCKP) q-composite scheme to further enhance network resiliency against node capture. However, Bechkit et al.'s HCKP q-composite scheme has to perform too many hash operations to establish a secure link between two nodes for the case that the difference of node identifiers is large. This computational overhead is more serious for the large value of q. In this paper, we propose a computational overhead invariant HCKP q-composite scheme by storing one additional hashed value in sensor node. When compared with Bechkit et al.'s HCKP q-composite scheme, the proposed scheme reduces the number of hash operations, and meanwhile the storage overhead remains insignificant."}, {"label": 0, "content": "In satellite-borne Terahertz wave ground detection situation, the quantitative estimation of Terahertz wave atmospheric absorption attenuation loss, especially with designated satellite position and down-looking angle information, has always been fundamental and a key technology application for various Terahertz communication modes, such as wideband & high-speed network, interstellar communication, satellite-ground station link, stratosphere aero-craft communication, long distance vast data transfer, short range wireless security communication etc. This paper designed an atmospheric absorption loss estimation software on satellite global THz wave ground detection. The realized functions of this software including scene establishment, basic functions and calculation methods were explained in detail. Finally, the monthly change calculation results of satellite ground detection with 0.34THz working band in 10\u00b0\u201390\u00b0 down-looking angle are given."}, {"label": 0, "content": "Aiming at the issue how users in Windows domain cross-realm access cloud computing resources, a cross-realm authentication scheme based on federated identity was proposed. Based on the idea of the declaration, the scheme uses the federated identity provider to replace the gateway in the traditional gateway-based cross-realm authentication model, so as to realize the users in Windows domain access the cloud resources without re-authentication. The scheme uses SAML protocol to exchange user identity information between different domains, which ensures versatility and security of the system and realizes seamlessly secure communication between different security domains. Finally, based on claim provider, federated identity provider and application service provider, we give the design of the key components of the three modules, then the feasibility of the scheme is verified with the popular cloud platform OpenStack."}, {"label": 0, "content": "Phoneme lattices have been shown to be a good choice to encode in a compact way alternative decoding hypotheses from a speech recognition system. However the optimal phoneme sequence is produced by tracing all the phoneme identities in the lattice. This not only makes the search space of the decoder huge but also the final phoneme sequence may be prone to have false substitutions or insertion errors. In this paper, we introduce the split lattice structures that is generated by splitting the speech frames based on the manner of articulation. Spectral flatness measure (SFM) is exploited to detect the two broad manner of articulation sonorants and non-sonorants. The manner of sonorants includes broadly the vowels, the semivowels and the nasals whereas the fricatives, stop consonants and closures belong to non-sonorants. The conventional way of speech decoder produces one lattice for one test utterance. In our work, we split the speech frames into sonorants and non-sonorants based on SFM knowledge and generate split lattices. The split lattice generated are modified according to the manner of articulation in each split so as to remove the irrelevant phoneme identities in the lattice. For instance, the sonorant lattice is forced to exclude the non-sonorant phoneme identities and hence minimizing false substitutions or insertion errors. The proposed split lattice structure based on sonority detection decreased the phone error rates by nearly 0.9 % when evaluated on core TIMIT test corpus as compared to the conventional decoding involved in the state-of-the-art Deep Neural Networks (DNN)."}, {"label": 0, "content": "Indoor localization is often challenging due to the non-availability of GPS signals. Recently, various radio frequency fingerprinting techniques have been proposed to identify indoor locations using simply received signal strength (RSS) measurements. In general however, RSS measurements are time-varying and are difficult to model for complex environments. This paper proposes the use of dictionary learning (DL) to generate high quality fingerprints that depend also on the channel characteristics for each location. An enhanced DL algorithm is proposed that utilizes prior information about the channel distribution, and can generate the fingerprints in an online fashion. Simulation results demonstrate the efficacy of the proposed approach."}, {"label": 0, "content": "Software Defined Networking (SDN) breaks the vertical integration of existing Internet architecture and makes the network programmable from a logically centralized control point. Even though the centralized network control provides several advantages, attacks toward SDN framework remain as a challenge. In this paper, we propose a method based on machine learning to detect Denial of Service (DoS) attack in data plane devices, i.e., the OpenFlow switches, resulting from flow-table overflow. We created an SDN dataset using Mininet and features are extracted from switch-controller communication trace as well as flow-table snapshots of OpenFlow switches. Further, we use three algorithms, (i) Neural Network, (ii) Support Vector Machines, and (iii) Naive Bayes, to classify the network to either malicious or benign. The results show that neural network and Naive Bayes provide 100% accuracy with the extracted features."}, {"label": 0, "content": "In an Orthogonal Frequency Division Multiple Access (OFDMA) based multi-cellular WiMAX system, a suitable base station is obtained by cell search. In the cell search technique, in addition to timing and frequency synchronization, the detection of the frame start position is another basic task that the mobile terminal must successfully complete before establishing a communication link with the base station. In this work, we propose a dual correlation algorithm to improve the accuracy of frame detection synchronization. In order to reduce the influence of channel fading under low SNR, we also propose a scheme for joint blind channel estimation and equalization. Compared with the existing scheme, the obtained scheme can obtain better frame detection system performance at a low signal to noise ratio."}, {"label": 0, "content": "In this paper, the authors propose a novel range-free localization method to localize the sensor nodes in anisotropic networks. The basic methods of range-free localization assume the hop-size of all links to be the same. This assumption is valid only in scenarios where the node distribution is fairly balanced. This is not practically accurate due to the random deployment of nodes in wireless sensor networks. Hence, the method of finding hop-size using the expected distance and hop-count between the sensor nodes is used in our work. This method is applied to anisotropic networks where obstacles are present. Extensive simulation studies have been conducted to validate the accuracy of the proposed method under the effects of log-normal shadowing which is practically relevant. The results are compared with DV-Hop and Reliable anchor pair selection method (RAPS). This method gives up to 35% performance improvement over DV-Hop technique and 15% performance improvement over RAPS technique in the literature."}, {"label": 0, "content": "Humans seamlessly perceive a massive amount of information while observing a scene. Though humans recognize real-world scenes easily and accurately but its not the same for computers due to scene images variability, ambiguity, and diverse illumination and scale conditions. Scene classification is a fundamental problem which provides contextual information to guide other processes, such as browsing, content-based image retrieval and object recognition. A baseline model based on traditional bag of words model is built to better evaluate the proposed solution. Model based on the idea of fine to coarse category mappings is proposed, whose information is combined with the fusion of feature descriptors resulting in a single feature representation. This additional information enhances performance by exploiting hierarchical relationship among the scene categories. Effectiveness of the proposed approach is validated using different evaluation metrics. Proposed model performs considerably better compared to the given baseline as well as several state-of-the-art methods. Proposed framework ensures appropriate balance between time and accuracy of the model"}, {"label": 0, "content": "Management and configuration of optical networks, implementing new policies to keep up with ever-changing network etc. have always been tedious tasks. Software-defined networking (SDN) has provided many network solutions in the electrical counterpart. SDN for optical networks can provide new opportunities to make the above mentioned tasks easier and faster. As a first step towards this goal, we develop an optical network description language (ONDL). We use it to describe various network components, and their configuration and run-time states, such as modulation schemes, wavelength and spectral-width of a transponder, switching matrix of an optical switch etc. The language is based on resource description framework (RDF). Furthermore, we develop a controller which understands and sends instructions in this language to different network devices to provide/change their states. We show the applicability of ONDL by simulating a network, controlling and managing its nodes using ONDL and developed controller."}, {"label": 0, "content": "The revolution in information technology has lead to the availability of vast and varied collections of music on the digital platform. With the widespread use of smartphones and other personal digital devices, there has been a growing interest in accessing music, based on its various characteristics using information retrieval technologies. But the unavailability of meta-tags or annotations has lead to the need for developing technologies to automatically extract relevant properties of music from the audio. Automatically identifying meta-data from audio like, artist information - especially instrument artists - is a very tough task, even for humans. In this paper, automatic identification of percussion artist is attempted on mridangam audio from Carnatic music concert using probabilistic models. Unlike speaker identification where the voice of the speaker is unique, the timbre of the percussion instruments will be more or less the same across instruments. The distinctive characteristics of a musician can be found in the style of him/her playing the instrument. A single Gaussian mixture model (GMM) is built across all musician data using tonic normalized cent-filterbank-cepstral-coefficients (CFCC) features. Each artist's percussion audio is converted to a sequence of GMM tokens. Sub-string matching between train and test data is used to identify the musician. The performance is evaluated on a dataset of 10 mridangam artist and could identify the artist with an accuracy of 72.5%."}, {"label": 0, "content": "Collisions between vehicles and pedestrians usually result in the fatality to the vulnerable road users (VRUs). Thus, new technologies are needed to be developed for protecting the VRUs. Based on the high density of pedestrians and limited computing capability of base stations, in this paper the cloud computing technologies are adopted to handle the huge amounts of safety-critical messages. Moreover, the wireless multi-hop backhaul technology is adopted to overcome the bottlenecks of limited transmission capability and queueing delay of the transmitted safety-critical messages between base stations and clouds. Based on the multi-hop wireless transmission scheme, the signal transmission success probability and delay between pedestrians and clouds are derived for performance analysis. Furthermore, numerical simulations are performed to illustrate the relationship between the transmission success probability and the received signal to interference plus noise ratio (SINR) threshold."}, {"label": 0, "content": "Unmanned aerial vehicle mounted base stations (UAV - BSs) can provide wireless cellular service to ground users in a variety of scenarios. The efficient deployment of such UAV-BSs while optimizing the coverage area is one of the key challenges. This work investigates the 3D UAV -BS placement that maximizes the numbers of covered users with different Quality-of-Service (QoS) requirements using the minimum power. In this paper, we first highlight the properties of the 3D placement problem and we model the problem as a multiple concentric circles placement problem with the objective of maximizing the numbers of covered users. We decouple the UAV-BS deployment problem in the vertical and horizontal dimensions without any loss of optimality, after some mathematical manipulations, we formulate a Mixed Integer Second Order Cone Problem (MISOCP) and propose an improved Multi-Population Genetic Algorithm (MPGA) for horizontal dimensions placement problem. Numerical simulations are presented showing that improved MPGA can obtain better performance compare to Standard Genetic Algorithm (SGA) in this problem."}, {"label": 0, "content": "The customized wireless devices play a key role in information exchanging for various electrical facilities. However, the development of the devices is hindered by diversity of interfaces (RS-232/485, Ethernet, USB, etc.), high cost, outdoor environment, etc. In this work, we proposed a \u2018system on module\u2019 solution to address above issues. We optimize and spare the storage space of baseband chip for embedded control system to replace the independent chips, which dramatically reduces the size, cost and power consumption of devices. Meanwhile, our SOM wireless devices can support multi-interfaces and thus realize \u2018plug and play\u2019. Moreover, our developed wireless devices allow multimode wireless communications including operator's wireless networks LTE FDD/TDD, UMTS, GSM and self-built LTE-G 1800MHz network. Last but not least, we can embedded positioning chip and the client of management system in the wireless devices for the sake of operation and maintenance."}, {"label": 0, "content": "Aimed at the road service level prediction problem for real-time traffic flow, a real-time evaluation method for road service level based on traffic model driven is proposed. Firstly analyze the basic feature model of traffic flow, using flow-time occupancy model as a reference model for congestion assessment, and based on K-means clustering algorithm to complete traffic-based congestion definition. Then use the BP neural network algorithm to build congestion assessment model, finally establish a real-time stream processing framework based on Spark Streaming to realize real-time evaluation for road service level. The experiment results show that the method could effectively describe the state of congestion, and be able to evaluate road service in real time based on traffic flow data, with decision support for intelligent traffic control system to improve the service level."}, {"label": 0, "content": "Wireless Sensor Network (WSN) is an emerging next-generation sensor network that has a wide range of application prospects. The localization technology is one of the most important key technologies for WSN. However, in a complex indoor environment, fluctuations in received signal strength can seriously degrade positioning accuracy. In this paper, we propose a fingerprint localization method based on received signal strength (RSS) distance and improved weighted k-Nearest Neighbor (KNN) algorithm. The fingerprint database is established in the off-line phase. The real-time RSS values of the on-line measurement points are measured, and the two-stage RSS distance is calculated using the Euclidean distance. Finally, in order to solve the problem of non-Gaussian distribution of measurement noise, we use an improved weighted KNN algorithm to calculate the final position coordinates of the measurement point. Simulation results show that this method can reduce the influence of signal strength fluctuations and improve the positioning accuracy."}, {"label": 0, "content": "IP source spoofing is a consequence of lack of packet level authentication in the Internet which allows attackers to carry out Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks. Source address validation filtering is one of the most important scheme that is deployed in the Internet to deter such attacks by filtering the spoofed IP packets. In this paper, we propose a novel scheme to study the deployment of source address validation-filtering by using some special path backscatter messages that are generated by the spoofed traffic. We use the long term absence of such messages from an Autonomous System (AS) to classify it as non-spoofer AS. We use Caida's backscatter dataset for our study. We provide the list of spoofer and non-spoofer ASes from the given dataset. We also provide detailed mathematical analysis for calculating the amount of time we need to wait before declaring an AS as a non-spoofer. Besides, we use the normal approximation of binomial distribution to calculate confidence interval for the proportion of ASes allowing spoofing and to test the hypothesis regarding the spoofing activity in the Internet."}, {"label": 0, "content": "The multiple security domains management and the security gateways that protect the security domain boundary are applied in space-ground integration information network, so we can adjust network security functions in the gateways dynamically by reconfiguration as the response to various security threats and malicious attacks. To solve the decision making problem about when to reconfigure and what to reconfigure, we presented privilege transition graph of internal security domain that can show us the vulnerabilities utilization between hosts in the domain. The algorithm combining forward breadth search and depth backtracking was proposed to get the attack paths from one host to key resources in the domain. And a quantification method for the risk of attack paths was presented. The method was tested which could provide effective data for recoufiguration decision."}, {"label": 0, "content": "With the rapid development of the Internet, the amount of network data was exponentially increased and based on a dynamic society the network at any time will produce many hot topics. According to the requirements of the Hidden Markov Model, the paper provided and modeled a general framework based on the statistical modeling of HMM for solving the problem of hot topics on networks, then assessed the probability of a network theme to be a hot topic. Experiments show that the framework is superior to the traditional algorithm of network hot topics discovery considering the convergence speed and accuracy."}, {"label": 0, "content": "In this article, we study the problem of anti-jamming through channel selection in fading environment. Different from the most existing works which ignored the fading characteristic of the channels, we use a Markov channel model to capture the influence of variable channel transmission rate in fading environment. Then, we model the anti-jamming channel selection problem as a Markov decision process. On this basis, an online reinforcement learning algorithm (Q-learning) is proposed to select the optimal channel intelligently. Simulation results show that the proposed algorithm outperforms the sensing algorithm. The reason is that it can learn both the pattern of jamming signal and the condition of channel, so as to select the channel with better condition for data transmission. Based on the proposed algorithm, we develop a real-life dynamic spectrum anti-jamming testbed based on the USRP platform in the indoor environment to demonstrate the effectiveness of the algorithm."}, {"label": 0, "content": "To solve the problem of the low detection rate of minority samples in imbalanced datasets in network intrusion detection, a deep learning intrusion detection model based on optimized imbalanced data is proposed. Firstly, a hybrid sampling method is adopted in data processing. Synthetic Minority Over-sampling Technique (SMOTE) was used to increase the numbers of samples in minority categories and the majority of the samples were under-sampled by Neighborhood Cleaning Rule (NCL). Secondly, on the preprocessed balanced dataset, the high-dimensional data was reduced by Deep Belief Network (DBN) to obtain the lower low-dimensional representation of the preprocessed data. Finally, the classification work was completed by Probabilistic Neural Network (PNN). The experiment on NSL-KDD dataset showed that hybrid sampling can improve the detection rate and classification accuracy of minority categories. And the performance of DBN-PNN is obviously superior to the traditional method."}, {"label": 0, "content": "In wireless sensor network (WSN), the appearance of coverage hole reduces the efficiency of data collection and has a serious impact on network quality of service. Detection of coverage holes is foundation of patching the sensor network to guarantee network quality of service. The paper proposes a distributed coverage hole detection algorithm based on hole boundary nodes (HPNs-CHD). The algorithm firstly uses sensing disk model to identify the HBN nodes in WSN and then exploits probabilistic message mechanism to detect coverage hole. The simulation results indicate that the proposed algorithm outperforms other two algorithms in terms of average energy consumption and average time of coverage holes detection."}, {"label": 0, "content": "With the increase of cyber-attacks, the risks of information leakage, falsification and forgery, and bypass control are intensified, and attackers can attack the primary station circuitously through the terminal, resulting in a wider range of security threats. Quantum cryptography has a higher level of security than traditional cryptography. In this paper, a quantum cryptosystem suitable for power communication access network is proposed, and various practical quantum communication devices are designed, and proposes a key reading mode between the service terminal and the quantum key mobile storage device, an interface protocol between the service terminal device and the quantum key mobile storage device, and a key management method applicable to the terminal for the service of the electric service. The application of quantum communication technology to power systems plays a vital role in ensuring the safe, stable and efficient operation of the power grid."}, {"label": 0, "content": "Local matching approaches are still common tools in real-time applications. Mismatch is a common situation in stereo vision, especially in local approaches. In this paper, we propose a truncated majority voting method (TMVM) to discriminate and reduce mismatches for local matching approaches in stereo. Experiments on Middlebury benchmark show that mismatches can be discriminated and reduced correctly by deploying the proposed method without losing real-time properties."}, {"label": 0, "content": "Pilot contamination (PC) is one of the main obstacles that limit the performance of massive multiple-input multiple-output (MIMO) systems. In this paper, we propose the asynchronous scheduling which is based on the fractional pilot reuse so that the users can be free from the pilot contamination during the uplink transmission. According to the level of interference, the users are divided into two groups, which are referred as the center users, who suffer from the mild pilot contamination, and the edge users, who suffer from the severe pilot contamination. Based on this distinction, a cell-center pilot set is reused for all the center users in all cells, whereas a cell-edge pilot set is applied for the edge users in the adjacent cells. In this case, the pilots used by the cell-edge users are orthogonal to each other. So the edge users can transmit the pilots at any time. But the pilot set for the center users are reused for all the cells, the cell-center users send their pilots in the non-overlapped time periods in order to avoid the pilot contamination. With this scheduling, the cost of the orthogonal pilots for each cell is reduced obviously. And the base station (BS) can easily recover the estimation of the pilots as it knows there is no pilot contamination. Simulation results show that the proposed asynchronous fractional pilots scheduling (AFPS) outperforms the other conventional pilot assignment schemes."}, {"label": 0, "content": "The accurate estimation of power system frequency and amplitude is essential for power system monitoring, stability, control, and protection. This work proposes a novel approach for power system frequency and amplitude estimation based on variational mode decomposition (VMD) algorithm and Cheb-function (Chebfun) approximation system. In this work, the spectral information of power signals is extracted using VMD as sub-signals or modes. Each mode is further interpolated by Chebyshev polynomials in continuous domain using Chebfun system. The instantaneous frequency and amplitude are estimated based on zero crossings and local extrema locations of the continuous function. The robustness of the approach is evaluated on various power system scenarios and the results are compared with other existing methods. The promising results suggest that the proposed approach can be used as an efficient candidate for power system frequency and amplitude estimation."}, {"label": 0, "content": "The Space-Ground Integrated Network (SGIN) plays an important role for the future development of the country. The cyber-attacks against it are the focus of the research. In this paper, an association analysis algorithm based on knowledge graph of cyber security attack events is proposed to present the attack scenario for the Space-Ground Integrated Network. The construction of knowledge graph and association analysis can show the scene of cyber-attacks in the form of graphs. During the build process, the construction of an event ontology is an important part of it. Event ontology is used to represent various relationships in the network attack procedure. At last, we present a space-ground integration network security analysis system based on the knowledge graph of cyber security attack events, and uses the association analysis algorithm to analyze the attack scenario."}, {"label": 0, "content": "To realize ubiquitous wireless network environment, the unmanned aerial vehicle (DAV) is considered as good candidate for the next generation network communication infrastructure. UAVs wireless sensor network is composed of low cost and extremely power constrained sensor nodes scattered over spatial region. In this network, users can use UAVs to transfer data. In addition, good maneuverability and wide range of coverage improve UAVs communication efficiency. Mobile Ad-hoc Network (MANET) and Delay / Disruption Tolerant Network (DTN) are considered good supporter for UAVs network. However, the performance of UAVs network may change with the change of surrounding environment, such as UAV density and mobility. Neither single network architecture can always perform well in UAVs network. Therefore, we propose a novel method to improve UAVs network capabilities. In our proposed method, each source node selects the network architecture (MANET or DTN) according to the feature of the data that need to be sent and the network environment. Additionally, we have implemented this architecture and measured data in real world to verify the reliability of the theory. Experimental results show that the bandwidth that our proposed adaptive architecture is 150\\% better than DTN architecture. In addition, compare to MANET, the adaptive architecture can effectively transfer data in the high latency and easily interrupted network."}, {"label": 0, "content": "Opportunistic Networks, with users as a vital component, appears as a natural evolution of mobile Ad Hoc Networks field, and to some extent, an example of how the Internet of Things work. So, Opportunistic Networks due to their characteristics, need users- centric considerations when it comes to design Opportunistic Networks' routing, privacy, and authentications schemes. However, most mutual authentication schemes proposed for Opportunistic Networks do not consider, on the one hand, the gregarious aspect of users, and on the other hand, the parameter that pre-established contacts could be used in a mutual authentication process. Considering the factors pre-established contacts, Seed OppNet Identity, and traditional cryptography's principles, this paper proposes a realistic multiple levels authentication scheme for short-term and limited-time wireless network environment. The proposed scheme is realistic and achieves anonymity and privacy."}, {"label": 0, "content": "A joint communication and state estimation problem in a Gaussian multiple access channel with common additive state is considered. The state process is assumed to be IID Gaussian, and known non-causally at both the transmitters. The receiver not only has to decode the messages from the transmitters, but also needs to estimate the state process to within some prescribed squared error distortion. We provide a complete characterization of the optimal sum-rate versus distortion performance."}, {"label": 0, "content": "Spectrum is a scarce and precious resource in a battlefield. Multiple frequencies spread over different bands are available for usage in a battlefield. The frequencies need to be assigned to the nodes/Base Stations(BSs) in a battlefield in such a way as to meet the Signal-to-Interference-Noise Ratio(SINR) and throughput requirements of all the users. Previous attempts for frequency assignment abstract away the notion of SINR and throughput and usually constrain the distance between the frequencies required between pairs of interfering BSs. In this work, the frequency assignment that optimizes the SINR and throughput of all the users in a system while ensuring fairness among the users is found. A genetic algorithm is used to perform the unconstrained optimization and this method is termed as the efficient rate and SINR matcher with hyperbolic cost (ERSMHC). A high SINR initialization algorithm (HSIA) and a high throughput greedy rate matching algorithm (HTGRM) that can be used for quickly initializing the system are also presented. The ERSMHC algorithm is shown to aid in meeting the stipulated rate and throughput requirements of more number of users as compared to HSIA and HTGRM."}, {"label": 0, "content": "Modeling of spectrum occupancy is important for better channel utilization, accurate spectrum sensing, and enhanced Quality of Service (QoS) to the primary user (PU) in a cognitive radio (CR) system. Existing models are highly dependent on the spatio-temporal variations of the PU activity as the statistical behavior of the PU changes with respect to the location, spectrum band, and the varying load time. In this work, a generalized Gaussian Mixture model (GMM) has been investigated for characterizing the spectrum occupancy of the PU in three spectrally different CR scenarios, viz. VHF/UHF band, GSM band, and ISM band. The goodness of fit performance of GMM is compared with the widely used spectrum occupancy model based on Beta distribution. Further, the robustness of GMM has been validated through learning based prediction via Recurrent Neural Networks (RNN), thereby proposing a hybrid approach of statistical and predictive modeling of spectrum occupancy for enhanced dynamic spectrum access."}, {"label": 0, "content": "Dynamic spectrum assignment of cognitive heterogeneous wireless networks is a typical integer programming problem which is difficult to obtain optimal solutions within the limit computing time. In order to obtain optimal dynamic spectrum allocation scheme, a novel discrete optimization algorithm described as quantum harmony search algorithm (QHSA) is put forward. By means of the harmony search algorithm and quantum optimization theory, the quantum harmony and harmony of the quantum harmony algorithm are developed through designing of new quantum evolutionary equations. The proposed dynamic spectrum allocation method based upon QHSA has better convergent accuracy and speed. Computer simulation indicates that the proposed dynamic spectrum assignment method based upon QHSA is superior to the dynamic spectrum assignment methods based upon previous intelligence algorithms in different cognitive heterogeneous network environments."}, {"label": 0, "content": "Multi-objective route planning is a hot issue in current research, and it applies all aspects of life. With the expansion of the scale of the problem, large numbers of approximate algorithms and heuristic algorithms proposed to solve the problem. In this paper, a solution of a multi objective route planning with a balanced assignment of tasks is proposed. The solution can divide into two steps. First., a clustering algorithm cbk-means (cluster balance k-means) is proposed, which improves the similarity measurement in the clustering process, and overcomes the shortcomings of traditional k-means algorithm, such as uncertain number of points and inflexible measurement criteria, which is the key step to achieve fair assignment of tasks. Second, this paper use genetic algorithm to obtain an optimal route planning for each cluster. Experimental results show that the cbk-means algorithm makes the workload of each cluster more balanced at the expense of negligible cost, which improves the fairness of task assignment greatly. Besides, this hybrid solution can save computational time and get better results."}, {"label": 0, "content": "Internet of Things (IoT) is playing an important role in our lives. It connects lots of embedded devices, which can deal with very complicated and difficult tasks to facilitate our work. However, the security of IoT faces many challenges due to the wireless broadcasting nature and the energy constraint of the physical objects. In order to provide a secure environment for IoT, in this paper, we investigate two opportunistic relay selection schemes to further enhance physical layer security, which are single relay selection(SRS) and multi relay selection(MRS) respectively. We analyze the outage probability (OP) and intercept probability (IP) as well as the system tradeoff performance (STP) for SRS and MRS under Nakagami-m fading channel. Simulation results show that the OP and IP of MRS are all better than that of SRS. Besides, the STP for MRS is also more perfect than that of SRS. What's more, the STP of these two opportunistic relay selection schemes improve with the increasing of relay numbers."}, {"label": 0, "content": "The increasingly popularity of vehicles embeded with high computational devices and resources has attracted great interests as a mean for sharing resources in a new or existing cloud infrastructure. Vehicles can be clustered together to ease the management, increase the effectiveness and capability of vehicular cloud. Clustering would allow these vehicles to pool their resources to serve the upper layers of the network infrastructure. This paper investigates the trends and state of the art of vehicular clustering in the past 5 years especially for parked vehicles in fog computing paradigm. It was observed that static and dynamic clustering are the two common methods, albeit dynamic clustering is more common. The challenges and issues are discussed."}, {"label": 0, "content": "To solve the resource optimization issue for the related power-protection services under the background of Energy Internet, this paper proposes a two-layer resource-balanced optimization Model in the planning design of the power backbone communication network. Firstly, the communication requirements of the related power-protection service are analyzed based on the description of the power-protection system. Secondly, the proposed resource-balanced model is formulated as a mathematic optimization model based on Optical Transport Network layered architecture, which comprises the optimal resource-balanced funciton and related restraints considering both the routing hops and the Quality-of-Service requirements. Finally, the simulation experiment is implemented to analyze the resource-balanced performance of the proposed model using the typical power-protection communication network topology. Simulation Results illustrate that the proposed model is of effectiveness for the power-nrotection services."}, {"label": 0, "content": "In this paper, for the defects which the basic genetic algorithm can get local optimum easily and converge slowly, the genetic algorithm is improved by constructing a suitable fitness function and improving genetic operators. Meanwhile, the improved genetic algorithm is applied to select optimal access network in integrated heterogeneous wireless networks. According to Quality of Service (QoS) requirements of different business types, compared with combined the analytic hierarchy process (AHP) and the technique for order preference by similarity to an ideal solution (TOPSIS) and the basic genetic algorithm, the simulation results indicate that the improved genetic algorithm can find a network with higher fitness and better meet different QoS."}, {"label": 0, "content": "The performance of automatic speaker identification (ASI) systems on Voice over Internet Protocol (VoIP) speech varies with the type of codec used in the VoIP communication. The type of codec used depends on the service provider of the user. Thus there is a need for the codec-independent ASI systems to identify the speaker. Three modeling approaches based on UBM-GMM framework and i-vector framework are proposed to identify the speaker independent of codec used. These frameworks are also evaluated for the mismatch conditions with respect to the codec used in training and testing. The proposed approaches are evaluated on VoIP speech from four codecs with different bit rates along with uncoded speech."}, {"label": 0, "content": "Visible light communication (VLC) transmits the wireless data through optical scintillation. However, the joint problem of both illumination and communication remains the tradeoff challenge. For both illumination and communication, this paper adopts dimming control to flexibly adjust the brightness for human eyes, and introduces non-orthogonal multiple access (NOMA) to improve the system throughput. First, we establish a model combining signal power allocation with dimming control. Second, for the enhancement of spectral efficiency, we introduce gain ratio power allocation (GRPA) of NOMA and variable on-off keying dimming control. Third, in the indoor user scenario, we analyse the relationship of luminescent angles, user data rate, and luminous intensity. Finally, experimental results indicate that the proposed GRPA scheme achieves the better performance than the previous strategy in user data rate for the same dimming factor. Furthermore, the optimal indoor VLC cell deployment of semi-angles and dimming factors is discussed."}, {"label": 0, "content": "A lot of people have been concerned about the problem of maximizing influence in social networks, which is aimed to find a set of nodes to get the influence spread maximized. However, the existing reasearches mainly focus on that a node influences its neighbors once without considering time and cost constraints. But in real world, people often try to influence their friends repeatedly during a time interval. Sometimes, the spread of information will cost a certain price as well. In this paper, we study the Time-sensitive Influence Maximization Problem and propose a Time and Cost constrainted Influence model with users' Online patterns (TCIO model). In TCIO model, the selection of seed nodes is limited to the budget and each node can influence its neighbors repeatedly according to their online patterns with different probability until a given message expire time is reached. We then show that the problem is NP-hard and our model satisfies monotonicity and submodularity for influence spread. Based on this, we develop a greedy algorithm to solve the problem. To reduce the computation complexity and optimize seed node selection with cost, we propose an efficient method GMAI for approximately calculating added influence using influence weight. Our experiments show that our model is effective and practical since it takes into account time factors, and GMAI faster and more effecient than other evaluated algorithms."}, {"label": 0, "content": "Nowadays, audio generation plays an important role in human-computer interactive applications. However, the audio generated by machine is far from nature sound, especially in expressiveness and complexity. Currently, conditional variational Auto-encoder (cVAE) has achieved excellent results in data generation, but original cVAE cannot avoid the defects caused by KL divergence which used in stochastic distribution measurement. This paper introduced Hellinger distance into cVAE model. First of all, the experiment shows that using Hellinger distance can improve the weakness of KL divergence effectively. And then, the relationship between the latent space parameters and the generated music quality is analyzed by experiments, and we found the best generative parameter is the distribution centroid. Finally, the generated music is subjectively evaluated and the results show that it is significantly better than the original model."}, {"label": 0, "content": "The ever-growing of Internet of Things (IoT) data and the new spectrum of data applications have stimulated IoT clients to outsource their data to cloud servers or datacenters. Apart from storage service, the IoT clients also desires the servers to execute functional operations per client's request. In this paper, we aim to design the secure mechanisms that allow the IoT clients to outsource their encrypted data to geographically distributed servers while supporting homomorphic computation functions. We leverage the distributed index framework to disassemble and spread data evenly across geographically distributed servers while employing the key-value store as the underlying structure for fast data retrieval. To support computing over encrypted data, we customize Shamir's secret sharing into our mechanisms to design a tunable scheme for the adaption of different IoT application scenarios. In particular, we design three tunable protocols to achieve the effective additive homomorphic computations while approaching efficiency in terms of servers utilization, computation, and storage overhead. Even the designs focus on the additive computation, we show that it can be readily extended to other types of homomorphic computations as well as verifying the correctness of stored data. Based on the proposed protocols, we design system prototypes, deploy them in Amazon Web services, and evaluate our construction experimentally. Through experimental results, we show that our designs can achieve the efficiency in various perspectives."}, {"label": 0, "content": "The global positioning system (GPS) has become an indispensable navigation sensor for field operations with unmanned surface vehicles (USVs) in marine environments. However, GPS may not always be available, even in open outdoor areas, because it is vulnerable to natural interference and malicious jamming attacks. Thus, an alternative navigation system is required when the use of GPS is restricted or prohibited. In such circumstances, a marine radar, which is a standard sensor in a marine vehicle including USV, can be used for localization in coastal areas. The marine radar can extract landmark features of the surrounding coastlines. These features can be utilized for relative navigation with respect to the detected coastlines. However, coastline maps based on radar signatures may be unavailable in unexplored areas, and they may be unreliable in coastal areas with high tidal elevations. In this study, the relative navigation with respect to the surrounding coastlines is performed in the framework of simultaneous localization and mapping (SLAM) for a USV operation in coastal waters. In particular, coastline features are parameterized by using B-splines for efficient map management, instead of the conventional point cloud representation. To verify and demonstrate the performance of the proposed coastal SLAM algorithm, field experiments were conducted in actual coastal environments. The results are presented and discussed in this paper."}, {"label": 0, "content": "A vehicular ad-hoc network (VANET) can improve the flow of traffic to facilitate intelligent transportation and to provide convenient information services, where the goal is to provide self-organizing data transmission capabilities for vehicles on the road to enable applications, such as assisted vehicle driving and safety warnings. VANETs are affected by issues such as identity validity and message reliability when vehicle nodes share data with other nodes. The method used to allow the vehicle nodes to upload sensor data to a trusted center for storage is susceptible to security risks, such as malicious tampering and data leakage. To address these security challenges, we propose a data security sharing and storage system based on the consortium blockchain (DSSCB). This digital signature technique based on the nature of bilinear pairing for elliptic curves is used to ensure the reliability and integrity when transmitting data to a node. The emerging consortium blockchain technology provides a decentralized, secure, and reliable database, which is maintained by the entire network node. In DSSCB, smart contracts are used to limit the triggering conditions for preselected nodes when transmitting and storing data and for allocating data coins to vehicles that participate in the contribution of data. The security analysis and performance evaluations demonstrated that our DSSCB solution is more secure and reliable in terms of data sharing and storage. Compared with the traditional blockchain system, the time required to confirm the data block was reduced by nearly six times and the transmission efficiency was improved by 83.33%."}, {"label": 0, "content": "Person re-identification has gradually become a popular research topic in many fields such as security, criminal investigation, and video analysis. This paper aims to learn a discriminative and robust spatial\u2013temporal representation for video-based person re-identification by a two-stage attribute-constraint network (TSAC-Net). The knowledge of pedestrian attributes can help re-identification tasks because it contains high-level information and is robust to visual variations. In this paper, we manually annotate three video-based person re-identification datasets with four static appearance attributes and one dynamic appearance attribute. Each attribute is regarded as a constraint that is added to the deep network. In the first stage of the TSAC-Net, we solve the re-identification problem as a classification issue and adopt a multi-attribute classification loss to train the CNN model. In the second stage, two LSTM networks are trained under the constraint of identities and dynamic appearance attributes. Therefore, the two-stage network provides a spatial\u2013temporal feature extractor for pedestrians in video sequences. In the testing phase, a spatial\u2013temporal representation can be obtained by inputting a sequence of images to the proposed TSAC-Net. We demonstrate the performance improvement gained with the use of attributes on several challenging person re-identification datasets (PRID2011, iLIDS-VID, MARS, and VIPeR). Moreover, the extensive experiments show that our approach achieves state-of-the-art results on three video-based benchmark datasets."}, {"label": 0, "content": "Desirable properties of extensions of non-negative matrix factorization (NMF) include robustness in the presence of noises and outliers, ease of implementation, the guarantee of convergence, operation in an automatic fashion that trades off the balance between data approximation and model simplicity well, and the capability to model the inherently sequential structure of time-series signals. The state-of-the-art methods typically have only a subset of these aforementioned properties and seldom simultaneously possess them all. In this paper, we propose a novel approach that provides all these desirable properties by extending the automatic relevance determination framework in NMF from Tan and F\u00e9votte. Starting from an objective function derived from the maximum a posterior estimation of a Bayesian model, we develop majorization-minimization algorithms that work effectively to determine the correct model order, regardless of the impact of noise and outliers. Subsequently, we give a rigorous convergence analysis of the proposed algorithms. Moreover, convolutive bases are also incorporated in the basic model so that it is able to capture the richness of temporal continuity. We perform experiments on both synthetic and real-world data sets to show the efficiency and robustness of our approach."}, {"label": 0, "content": "Intelligent transportation systems (ITSs) will be a major component of tomorrow's smart cities. However, realizing the true potential of ITSs requires ultralow latency and reliable data analytics solutions that combine, in real time, a heterogeneous mix of data stemming from the ITS network and its environment. Such data analytics capabilities cannot be provided by conventional cloud-centric data processing techniques whose communication and computing latency can be high. Instead, edge-centric solutions that are tailored to the unique ITS environment must be developed. In this article, an edge analytics architecture for ITSs is introduced in which data is processed at the vehicle or roadside smart sensor level to overcome the ITS's latency and reliability challenges. With a higher capability of passengers' mobile devices and intravehicle processors, such a distributed edge computing architecture leverages deep-learning techniques for reliable mobile sensing in ITSs. In this context, the ITS mobile edge analytics challenges pertaining to heterogeneous data, autonomous control, vehicular platoon control, and cyberphysical security are investigated. Then, different deep-learning solutions for such challenges are revealed. The discussed deep-learning solutions enable ITS edge analytics by endowing the ITS devices with powerful computer vision and signal processing functions. Preliminary results show that the introduced edge analytics architecture, coupled with the power of deep-learning algorithms, provides a reliable, secure, and truly smart transportation environment."}, {"label": 0, "content": "Transportation and locomotion mode recognition from multimodal smartphone sensors is useful for providing just-in-time context-aware assistance. However, the field is currently held back by the lack of standardized datasets, recognition tasks, and evaluation criteria. Currently, the recognition methods are often tested on the ad hoc datasets acquired for one-off recognition problems and with different choices of sensors. This prevents a systematic comparative evaluation of methods within and across research groups. Our goal is to address these issues by: 1) introducing a publicly available, large-scale dataset for transportation and locomotion mode recognition from multimodal smartphone sensors; 2) suggesting 12 reference recognition scenarios, which are a superset of the tasks we identified in the related work; 3) suggesting relevant combinations of sensors to use based on energy considerations among accelerometer, gyroscope, magnetometer, and global positioning system sensors; and 4) defining precise evaluation criteria, including training and testing sets, evaluation measures, and user-independent and sensor-placement independent evaluations. Based on this, we report a systematic study of the relevance of statistical and frequency features based on the information theoretical criteria to inform recognition systems. We then systematically report the reference performance obtained on all the identified recognition scenarios using a machine-learning recognition pipeline. The extent of this analysis and the clear definition of the recognition tasks enable future researchers to evaluate their own methods in a comparable manner, thus contributing to further advances in the field. The dataset and the code are available online. http://www.shl-dataset.org/."}, {"label": 0, "content": "Most software defect prediction models usually assume that enough historical training instances with labels are available. Additionally, the training data and the predicted instances should share the same features to ensure the prediction accuracy. However, in practice, there are many datasets with different granularities containing information in different dimensions. Therefore, it is valuable to effectively use the small scale and different dimensions of data as training instances to improve the prediction performance of the model. We propose a heterogeneous data orienting multiview transfer learning for software defect prediction, denoted as MTDP, which can achieve different dimensions and granularities features to automatically learn labels through neural network models. With this multiview transfer method, lots of training instances are provided for software defect prediction model to ensure the effectiveness of training labels. The proposed MTDP method has four main stages: 1) build heterogeneous transfer models; 2) transfer heterogeneous instances to generate quasi-real instances; 3) label quasi-real instances through co-training and then expand the training set; and (4) construct improved software defect prediction models. The experimental results show that the quasi-real instances have similar effects compared with real instances. Moreover, the software defect prediction performance can be improved by introducing the quasi-real instances into the training dataset."}, {"label": 0, "content": "In this paper, we propose an uplink scheduling scheme via downlink signal design for wireless powered communication networks (WPCNs). Although harvest-then-transmit protocols and related optimal resource allocation problems were studied, explicit methods of transmitting the scheduling information have not been considered in prior works. For uplink scheduling, we propose a design of the downlink energy signal with a power level modulation, which conveys the scheduling information to users. Hybrid-access point allocates different power levels to the subslots of the downlink energy signal, and the users recognize their uplink subslot lengths from their corresponding downlink subslots' power levels. The scheduling can be optimized based on the user channel state with respect to the sum rate. We formulate the sum throughput maximization problem for the proposed scheme, which is shown to be a convex optimization problem. We also study the proposed scheme in a noisy environment. The solution to the problem provides the optimal downlink and uplink slot lengths. The numerical results confirm that the throughput of the proposed WPCN scheme outperforms that of the conventional schemes. The improvement is shown even in imperfect synchronization scenario."}, {"label": 0, "content": "Texture analysis is used in a very broad range of fields and applications, from texture classification (e.g., for remote sensing) to segmentation (e.g., in biomedical imaging), passing through image synthesis or pattern recognition (e.g., for image inpainting). For each of these image processing procedures, first, it is necessary to extract\u2014from raw images\u2014meaningful features that describe the texture properties. Various feature extraction methods have been proposed in the last decades. Each of them has its advantages and limitations: performances of some of them are not modified by translation, rotation, affine, and perspective transform; others have a low computational complexity; others, again, are easy to implement; and so on. This paper provides a comprehensive survey of the texture feature extraction methods. The latter are categorized into seven classes: statistical approaches, structural approaches, transform-based approaches, model-based approaches, graph-based approaches, learning-based approaches, and entropy-based approaches. For each method in these seven classes, we present the concept, the advantages, and the drawbacks and give examples of application. This survey allows us to identify two classes of methods that, particularly, deserve attention in the future, as their performances seem interesting, but their thorough study is not performed yet."}, {"label": 0, "content": "In this paper, we consider the finite-state approximation of a discrete-time constrained Markov decision process (MDP) under the discounted and average cost criteria. Using the linear programming formulation of the constrained discounted cost problem, we prove the asymptotic convergence of the optimal value of the finite-state model to the optimal value of the original model. With further continuity condition on the transition probability, we also establish a method to compute approximately optimal policies. For the average cost, instead of using the finite-state linear programming approximation method, we use the original problem definition to establish the finite-state asymptotic approximation of the constrained problem and compute approximately optimal policies. Under Lipschitz-type regularity conditions on the components of the MDP, we also obtain explicit rate of convergence bounds quantifying how the approximation improves as the size of the approximating finite-state space increases."}, {"label": 0, "content": "Despite outstanding performance in image recognition, convolutional neural networks (CNNs) do not yet achieve the same impressive results on action recognition in videos. This is partially due to the inability of CNN for modeling long-range temporal structures especially those involving individual action stages that are critical to human action recognition. In this paper, we propose a novel action-stage (ActionS) emphasized spatiotemporal vector of locally aggregated descriptors (ActionS-ST-VLAD) method to aggregate informative deep features across the entire video according to adaptive video feature segmentation and adaptive segment feature sampling (AVFS-ASFS). In our ActionS-ST-VLAD encoding approach, by using AVFS-ASFS, the keyframe features are chosen and the corresponding deep features are automatically split into segments with the features in each segment belonging to a temporally coherent ActionS. Then, based on the extracted keyframe feature in each segment, a flow-guided warping technique is introduced to detect and discard redundant feature maps, while the informative ones are aggregated by using our exploited similarity weight. Furthermore, we exploit an RGBF modality to capture motion salient regions in the RGB images corresponding to action activity. Extensive experiments are conducted on four public benchmarks-HMDB51, UCF101, Kinetics, and ActivityNet for evaluation. Results show that our method is able to effectively pool useful deep features spatiotemporally, leading to the state-of-the-art performance for video-based action recognition."}, {"label": 0, "content": "Most of existing traffic simulation methods have been focused on simulating vehicles on freeways or city-scale urban networks. However, relatively little research has been done to simulate intersectional traffic to date despite its broad potential applications. In this paper, we propose a novel deep learning-based framework to simulate and edit intersectional traffic. Specifically, based on an in-house collected intersectional traffic dataset, we employ the combination of convolution network (CNN) and recurrent network (RNN) to learn the patterns of vehicle trajectories in intersectional traffic. Besides simulating novel intersectional traffic, our method can be used to edit existing intersectional traffic. Through many experiments as well as comparative user studies, we demonstrate that the results by our method are visually indistinguishable from ground truth, and our method can outperform existing methods."}, {"label": 0, "content": "A novel method for semantic action recognition through learning a pose lexicon is presented in this paper. A pose lexicon comprises a set of semantic poses, a set of visual poses, and a probabilistic mapping between the visual and semantic poses. This paper assumes that both the visual poses and mapping are hidden and proposes a method to simultaneously learn a visual pose model that estimates the likelihood of an observed video frame being generated from hidden visual poses, and a pose lexicon model establishes the probabilistic mapping between the hidden visual poses and the semantic poses parsed from textual instructions. Specifically, the proposed method consists of two-level hidden Markov models. One level represents the alignment between the visual poses and semantic poses. The other level represents a visual pose sequence, and each visual pose is modeled as a Gaussian mixture. An expectation-maximization algorithm is developed to train a pose lexicon. With the learned lexicon, action classification is formulated as a problem of finding the maximum posterior probability of a given sequence of video frames that follows a given sequence of semantic poses, constrained by the most likely visual pose and the alignment sequences. The proposed method was evaluated on MSRC-12, WorkoutSU-10, WorkoutUOW-18, Combined-15, Combined-17, and Combined-50 action datasets using cross-subject, cross-dataset, zero-shot, and seen/unseen protocols."}, {"label": 0, "content": "The relational database service in cloud usually achieves energy efficiency by using virtualization technology, in which it consolidates multiple independent database systems into a single physical machine while enforcing the hardware-level isolation among them. However, the disk I/O performance is inevitably hurt due to the resource contention on the shared device. We propose VMSQL, a novel disk I/O model for the virtualized relational database management system (RDBMS). VMSQL has two innovations over the original disk model of virtualized database systems. First, VMSQL enforces the synchronous operation in guest operating system to handle with the transaction commitment. Due to its simplicity, a portion of CPU cycles is decoupled from I/O buffer management and then used to serve the upcoming requests, thereby improving their response times. Second, in host system, VMSQL asynchronizes the storage path of transactions which are committed from the different co-located guest databases. An obvious advantage of this procedure is that systems can apply host-level improvements into the disk I/O performance of virtualized RDBMS, relieving the random I/O and enhancing the throughput of whole system. We implement a prototype of this Sync-Async model in QEMU-KVM hypervisor, in which the InnoDB engine is deployed in the guest operating system. Extensive experiments are conducted to verify its advantages and the results are positive without any loss of ACID-compliance. In the meanwhile, VMSQL incurs moderate overhead at the hypervisor layer."}, {"label": 0, "content": "Rendering translucent materials is costly: light transport algorithms need to simulate a large number of scattering events inside the material before reaching convergence. The cost is especially high for materials with a large albedo or a small mean-free-path, where higher-order scattering effects dominate. We present a new method for fast computation of global illumination with participating media. Our method uses precomputed multiple scattering effects, stored in two compact tables. These precomputed multiple scattering tables are easy to integrate with any illumination simulation algorithm. We give examples for virtual ray lights (VRL), photon mapping with beams and paths (UPBP), Metropolis Light Transport with Manifold Exploration (MEMLT). The original algorithms are in charge of low-order scattering, combined with multiple scattering computed using our table. Our results show significant improvements in convergence speed and memory costs, with negligible impact on accuracy."}, {"label": 0, "content": "Accurate load forecasting can create both economic and reliability benefits for power system operators. However, the cyberattack on load forecasting may mislead operators to make unsuitable operational decisions for the electricity delivery. To effectively and accurately detect these cyberattacks, this paper develops a machine learning-based anomaly detection (MLAD) methodology. First, load forecasts provided by neural networks are used to reconstruct the benchmark and scaling data by using the k-means clustering. Second, the cyberattack template is estimated by the naive Bayes classification based on the cumulative distribution function and statistical features of the scaling data. Finally, the dynamic programming is utilized to calculate both the occurrence and parameter of one cyberattack on load forecasting data. A widely used symbolic aggregation approximation method is compared with the developed MLAD method. Numerical simulations on the publicly load data show that the MLAD method can effectively detect cyberattacks for load forecasting data with relatively high accuracy. Also, the robustness of MLAD is verified by thousands of attack scenarios based on Monte Carlo simulation."}, {"label": 0, "content": "Neuromorphic is a relatively new interdisciplinary research topic, which employs various fields of science and technology, such as electronic, computer, and biology. Neuromorphic systems consist of software/hardware systems, which are utilized to implement the neural networks based on human brain functionalities. The goal of neuromorphic systems is to mimic the biologically inspired concepts of the nervous systems, envisioned to provide advantages, such as lower power consumption, fault tolerance, and massive parallelism for the next generation of computers. This brief presents a neural computing hardware unit and a neuromorphic system architecture based on a modified leaky integrate and fire neuron model in a spiking neural network for a pattern recognition task in register-transfer level. The neuron model and the spiking network are explored, considering digital implementation, targeting low-cost high-speed large-scale systems. Results of the hardware synthesis and implementation on field-programmable gate array are presented as a proof of concept. Accordingly, the maximum frequency of the implemented neuron model and spiking network are 412.371 MHz and 189.071 MHz, respectively."}, {"label": 0, "content": "By jointly conducting sparse coding and classifier training, supervised sparse coding has shown its effectiveness in a variety of recognition tasks. However, the existing supervised sparse coding methods often consider linear classification, which limits their discrimination in handling highly nonlinear data. In this letter, we propose a new supervised sparse coding model by incorporating decision tree classifiers. Since decision trees can well deal with the non-linear properties of data, the introduction of decision trees to sparse coding can noticeably improve the discrimination of coding. Meanwhile, sparse coding is able to produce sparse de-correlated features that decision tree is in favor of. For further improvement, we close the loop of sparse coding and decision tree learning with an ensemble framework, which alternatively learns a dictionary for sparse coding and a decision tree for classification. The resulting series of decision trees as well as series of dictionaries are used to construct a decision forest for classification. The proposed method was applied to face recognition and scene classification, and the experimental results have demonstrated its power in comparison with recent supervised sparse coding methods."}, {"label": 0, "content": "In this work, we consider the problem of inferring links in a communication network, using limited, passive observations of network traffic. Our approach leverages transfer entropy (TE) as a metric for quantifying the strength of the automatic repeat request (ARQ) mechanisms present in next-hop routing links. In contrast with existing approaches, TE provides an information-theoretic, model-free approach that operates on externally available packet arrival times. We show, using discrete event simulation of a wireless sensor network, that the TE based topology inference approach described here is robust to varying degrees of connection quality in the underlying network. Compared to an existing approach which uses the linear regression based formulation of Granger Causality for network topology inference, our approach has better asymptotic time complexity, and shows significant improvement in network topology reconstruction performance. Our approach, though sub-optimal, also has better time complexity, while still retaining reasonable performance, compared to a causation entropy based optimal algorithm proposed in the literature."}, {"label": 0, "content": "High-resolution optical imagery can provide detailed information of urban land objects for impervious surface extraction, while airborne light detection and ranging (LiDAR) data can provide height features of land objects. Therefore, synergistic use of high-resolution imagery and LiDAR data is considered as an effective method to improve impervious surfaces extraction. In this paper, a novel hierarchical multiscale super-pixel-based classification method is proposed and applied to the urban impervious surfaces extraction from WorldView-2 and normalized digital surface model (nDSM) images derived from airborne LiDAR data. Three subsets in rural, rural-urban, and urban subsets are selected as the study areas. First, we split nonground and ground objects based on nDSM thresholds. Second, a hierarchical multiresolution segmentation method is used to generate nonground and ground super pixels. Then, we determine the multiscale input images based on the size of super pixels. Third, we construct optimal deep residual network (ResNet) and Spatial Pyramid Pooling (SPP-net) to train the model using multiscale input images. Finally, we use our deep models to predict hierarchically total super pixels in three subsets and generate the classification and impervious surfaces results. Our proposed method adopts hierarchical classification based on LiDAR nDSM height, which significantly improves the impervious surfaces extraction accuracies. Then, the deep residual network is applied further on multispectral and height fused data to extract urban impervious surfaces. Moreover, we propose an adaptive method to determine multiscale input images based on the segmentation of super pixels, which are inputs into the ResNet+SPP-net to train the deep model. Our proposed method reduces the uncertainty of multiscale input images and extracts better multiscale features. The results of the experiment show that our proposed method has a significant superiority to traditional pixel-based method and single scale method for urban impervious surfaces extraction."}, {"label": 0, "content": "Direction-based methods are the most powerful and popular palmprint recognition methods. However, there is no existing work that completely analyzes the essential differences among different direction-based methods and explores the most discriminant direction representation of a palmprint. In this paper, we attempt to establish the connection between the direction feature extraction model and the discriminability of direction features, and we propose a novel exponential and Gaussian fusion model (EGM) to characterize the discriminative power of different directions. The EGM can provide us with a new insight into the optimal direction feature selection of palmprints. Moreover, we propose a local discriminant direction binary pattern (LDDBP) to completely represent the direction features of a palmprint. Guided by the EGM, the most discriminant directions can be exploited to form the LDDBP-based descriptor for palmprint representation and recognition. Extensive experiment results conducted on four widely used palmprint databases demonstrate the superiority of the proposed LDDBP method over the state-of-the-art direction-based methods."}, {"label": 0, "content": "Although face recognition systems have achieved impressive performance in recent years, the low-resolution face recognition task remains challenging, especially when the low-resolution faces are captured under non-ideal conditions, which is widely prevalent in surveillance-based applications. Faces captured in such conditions are often contaminated by blur, non-uniform lighting, and non-frontal face pose. In this paper, we analyze the face recognition techniques using data captured under low-quality conditions in the wild. We provide a comprehensive analysis of the experimental results for two of the most important applications in real surveillance applications, and demonstrate practical approaches to handle both cases that show promising performance. The following three contributions are made: (i) we conduct experiments to evaluate super-resolution methods for low-resolution face recognition; (ii) we study face re-identification on various public face datasets, including real surveillance and low-resolution subsets of large-scale datasets, presenting a baseline result for several deep learning-based approaches, and improve them by introducing a generative adversarial network pre-training approach and fully convolutional architecture; and (iii) we explore the low-resolution face identification by employing a state-of-the-art supervised discriminative learning approach. The evaluations are conducted on challenging portions of the SCface and UCCSface datasets."}, {"label": 0, "content": "In this paper, we propose a deep reinforcement learning (DRL)-based method that allows unmanned aerial vehicles (UAVs) to execute navigation tasks in large-scale complex environments. This technique is important for many applications such as goods delivery and remote surveillance. The problem is formulated as a partially observable Markov decision process (POMDP) and solved by a novel online DRL algorithm designed based on two strictly proved policy gradient theorems within the actor-critic framework. In contrast to conventional simultaneous localization and mapping-based or sensing and avoidance-based approaches, our method directly maps UAVs' raw sensory measurements into control signals for navigation. Experiment results demonstrate that our method can enable UAVs to autonomously perform navigation in a virtual large-scale complex environment and can be generalized to more complex, larger-scale, and three-dimensional environments. Besides, the proposed online DRL algorithm addressing POMDPs outperforms the state-of-the-art."}, {"label": 0, "content": "Precision viticulture (PV) aims to improve the grapevine production efficiency, quality, and profitability, while reducing the environmental impact. The promises of PV are realized only if large areas are monitored with high spatial and temporal resolutions. This paper considers the integration of a wireless sensor network and a smart unmanned aerial vehicle platform. To this end, local variations of factors that influence grape yield and quality are measured and site-specific management practices are applied. This approach achieves real-time, uninterrupted monitoring of the vine growth environment, and on-demand imaging and high-resolution data collection from any specific location, thereby optimizing the production efficiencies and the application of inputs in a cost-effective way."}, {"label": 0, "content": "Single image super-resolution (SISR) has witnessed great progress as convolutional neural network (CNN) gets deeper and wider. However, enormous parameters hinder its application to real world problems. In this letter, We propose a lightweight feature fusion network (LFFN) that can fully explore multi-scale contextual information and greatly reduce network parameters while maximizing SISR results. LFFN is built on spindle blocks and a softmax feature fusion module (SFFM). Specifically, a spindle block is composed of a dimension extension unit, a feature exploration unit. and a feature refinement unit. The dimension extension layer expands low dimension to high dimension and implicitly learns the feature maps which are suitable for the next unit. The feature exploration unit performs linear and nonlinear feature exploration aimed at different feature maps. The feature refinement layer is used to fuse and refine features. SFFM fuses the features from different modules in a self-adaptive learning manner with softmax function, making full use of hierarchical information with a small amount of parameter cost. Both qualitative and quantitative experiments on benchmark datasets show that LFFN achieves favorable performance against state-of-the-art methods with similar parameters."}, {"label": 0, "content": "This paper presents Navion, an energy-efficient accelerator for visual-inertial odometry (VIO) that enables autonomous navigation of miniaturized robots (e.g., nano drones), and virtual reality (VR)/augmented reality (AR) on portable devices. The chip uses inertial measurements and mono/stereo images to estimate the drone's trajectory and a 3-D map of the environment. This estimate is obtained by running a state-of-theart VIO algorithm based on non-linear factor graph optimization, which requires large irregularly structured memories and heterogeneous computation flow. To reduce the energy consumption and footprint, the entire VIO system is fully integrated on-chip to eliminate costly off-chip processing and storage. This paper uses compression and exploits both structured and unstructured sparsity to reduce on-chip memory size by 4.1\u00d7. Parallelism is used under tight area constraints to increase throughput by 43%. The chip is fabricated in 65-nm CMOS and can process 752 \u00d7 480 stereo images from EuRoC data set in real time at 20 frames per second (fps) consuming only an average power of 2 mW. At its peak performance, Navion can process stereo images at up to 171 fps and inertial measurements at up to 52 kHz, while consuming an average of 24 mW. The chip is configurable to maximize accuracy, throughput, and energy-efficiency tradeoffs and to adapt to different environments. To the best of our knowledge, this is the first fully integrated VIO system in an application-specified integrated circuit (ASIC)."}, {"label": 0, "content": "Local binary descriptors, such as local binary pattern (LBP) and its various variants, have been studied extensively in texture and dynamic texture analysis due to their outstanding characteristics, such as grayscale invariance, low computational complexity and good discriminability. Most existing local binary feature extraction methods extract spatio-temporal features from three orthogonal planes of a spatio-temporal volume by viewing a dynamic texture in 3D space. For a given pixel in a video, only a proportion of its surrounding pixels is incorporated in the local binary feature extraction process. We argue that the ignored pixels contain discriminative information that should be explored. To fully utilize the information conveyed by all the pixels in a local neighborhood, we propose extracting local binary features from the spatio-temporal domain with 3D filters that are learned in an unsupervised manner so that the discriminative features along both the spatial and temporal dimensions are captured simultaneously. The proposed approach consists of three components: 1) 3D filtering; 2) binary hashing; and 3) joint histogramming. Densely sampled 3D blocks of a dynamic texture are first normalized to have zero mean and are then filtered by 3D filters that are learned in advance. To preserve more of the structure information, the filter response vectors are decomposed into two complementary components, namely, the signs and the magnitudes, which are further encoded separately into binary codes. The local mean pixels of the 3D blocks are also converted into binary codes. Finally, three types of binary codes are combined via joint or hybrid histograms for the final feature representation. Extensive experiments are conducted on three commonly used dynamic texture databases: 1) UCLA; 2) DynTex; and 3) YUVL. The proposed method provides comparable results to, and even outperforms, many state-of-the-art methods."}, {"label": 0, "content": "Existing inefficient traffic light cycle control causes numerous problems, such as long delay and waste of energy. To improve efficiency, taking real-time traffic information as an input and dynamically adjusting the traffic light duration accordingly is a must. Existing works either split the traffic signal into equal duration or only leverage limited traffic information. In this paper, we study how to decide the traffic signal duration based on the collected data from different sensors. We propose a deep reinforcement learning model to control the traffic light cycle. In the model, we quantify the complex traffic scenario as states by collecting traffic data and dividing the whole intersection into small grids. The duration changes of a traffic light are the actions, which are modeled as a high-dimension Markov decision process. The reward is the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is employed to map states to rewards. The proposed model incorporates multiple optimization elements to improve the performance, such as dueling network, target network, double Q-learning network, and prioritized experience replay. We evaluate our model via simulation on a Simulation of Urban MObility simulator. Simulation results show the efficiency of our model in controlling traffic lights."}, {"label": 0, "content": "Multispectral image change detection based on deep learning generally needs a large amount of training data. However, it is difficult and expensive to mark a large amount of labeled data. To deal with this problem, we propose a generative discriminatory classified network (GDCN) for multispectral image change detection, in which labeled data, unlabeled data, and new fake data generated by generative adversarial networks are used. The GDCN consists of a discriminatory classified network (DCN) and a generator. The DCN divides the input data into changed class, unchanged class, and extra class, i.e., fake class. The generator recovers the real data from input noises to provide additional training samples so as to boost the performance of the DCN. Finally, the bitemporal multispectral images are input to the DCN to get the final change map. Experimental results on the real multispectral imagery datasets demonstrate that the proposed GDCN trained by unlabeled data and a small amount of labeled data can achieve competitive performance compared with existing methods."}, {"label": 0, "content": "Current anomaly detection systems (ADSs) apply statistical and machine learning algorithms to discover zero-day attacks, but such algorithms are vulnerable to advanced persistent threat actors. In this paper, we propose an adversarial statistical learning mechanism for anomaly detection, outlier Dirichlet mixture-based ADS (ODM-ADS), which has three new capabilities. First, it can self-adapt against data poisoning attacks that inject malicious instances in the training phase for disrupting the learning process. Second, it establishes a statistical legitimate profile and considers variations from the baseline of the profile as anomalies using a proposed outlier function. Third, to deal with dynamic and large-scale networks such as Internet of Things and cloud and fog computing, we suggest a framework for deploying the mechanism as Software as a Service in the fog nodes. The fog enables the proposed mechanism to concurrently process streaming data at the edge of the network. The ODM-ADS mechanism is evaluated using both NSL-KDD and UNSW-NB15 datasets, whose findings indicate that ODM-ADS outperforms seven other peer algorithms in terms of accuracy, detection rates, false positive rates, and computational time."}, {"label": 0, "content": "Network modeling of high-dimensional time series data is a key learning task due to its widespread use in a number of application areas, including macroeconomics, finance, and neuroscience. While the problem of sparse modeling based on vector autoregressive models (VAR) has been investigated in depth in the literature, more complex network structures that involve low rank and group sparse components have received considerably less attention, despite their presence in data. Failure to account for low-rank structures results in spurious connectivity among the observed time series, which may lead practitioners to draw incorrect conclusions about pertinent scientific or policy questions. In order to accurately estimate a network of Granger causal interactions after accounting for latent effects, we introduce a novel approach for estimating low-rank and structured sparse high-dimensional VAR models. We introduce a regularized framework involving a combination of nuclear norm and lasso (or group lasso) penalties. Subsequently, we establish nonasymptotic probabilistic upper bounds on the estimation error rates of the low-rank and the structured sparse components. We also introduce a fast estimation algorithm and finally demonstrate the performance of the proposed modeling framework over standard sparse VAR estimates through numerical experiments on synthetic and real data."}, {"label": 0, "content": "For linear systems in the observer canonical form, we introduce a state observer with time-varying gains that tend to infinity as time approaches a prescribed convergence time. The observer is shown to exhibit fixed-time stability with an arbitrary convergence time, which is prescribed by the user irrespective of initial conditions. The output estimation error injection terms are also shown to remain uniformly bounded and converge to zero at the prescribed time."}, {"label": 0, "content": "In this letter, a sensor localization technique for highly deformed partially calibrated arrays with multiple moving targets is proposed. The deformed array is divided into several subarrays. The first subarray is composed of the pre-calibrated sensors, whereas the sensors in the other subarrays are uncalibrated. The positions of the sensors are estimated from the phase differences between the pre-calibrated sensors and the uncalibrated sensors. The phase ambiguities caused by the highly deformed sensor positions can be solved using the subspace orthogonality and the movement of multiple targets. Simulation results evaluate the performance of the proposed method, and the Cramer-Rao bounds are compared."}, {"label": 0, "content": "Increasing longevity remains one of the open challenges for Lithium-ion (Li-ion) battery technology. We envision a health-conscious advanced battery management system, which implements monitoring and control algorithms that increase battery lifetime while maintaining performance. For such algorithms, real-time battery capacity estimates are crucial. In this paper, we present an online capacity estimation scheme for Li-ion batteries. The key novelty lies in: 1) leveraging thermal dynamics to estimate battery capacity and 2) developing a hierarchical estimation algorithm with provable convergence properties. The algorithm consists of two stages working in cascade. The first stage estimates battery core temperature and heat generation based on a two-state thermal model, and the second stage receives the core temperature and heat generation estimation to estimate state-of-charge and capacity. Results from numerical simulations and experimental data illustrate the performance of the proposed capacity estimation scheme."}, {"label": 0, "content": "Head detection plays an important role in localizing and identifying persons from visual data. Most existing methods treat head detection as a specific form of object detection. Head detection is nontrivial due to the considerable difficulty in building the local and global information under conditions of unconstrained pose and orientation. To address these issues, this paper presents an effective adaptive relational network to capture context information, which is greatly helpful to suppress missed detection. We show that the fundamental contextual properties, such as the global shape priors from different heads and the local adjacent relationship between the head and shoulders, can be systematically quantified by visual operators. Specifically, we propose a two-step search algorithm to quantify the global intergroup conflict with adaptive scale, pose and viewpoint. Meanwhile, a structured feature module is introduced to capture the local relation of intraindividual stability. Finally, the global priors and local relation are integrated seamlessly into a single-stage head detector that is end-to-end trainable. An extensive ablation analysis demonstrates the effectiveness of our approach. We achieve state-of-the-art results on two challenging datasets, i.e., HollywoodHeads and Brainwash."}, {"label": 0, "content": "There is a numerous color spaces with different properties in literature. In order to find the appropriate and relevant color space for the fabric defect classification problem, we propose to investigate the performance and robustness of the Local Binary Pattern (LBP) descriptor in supervised context by using SVM classifier. The experimental results show that the luminance-chrominance spaces are suitable for coding fabric defect with the classification accuracy obtained is 92.1%."}, {"label": 0, "content": "The principal observed progressive swim types of sperm cells are linear mean and circular swim. Using motility characteristic parameters produced by CASA systems, we perform a parameter subset search to produce distinct clusters of the different swim types. For this task, the artificial bee colony algorithm (an iterative search algorithm modeled after the collective behavior of bees) and the well-studied k-means clustering algorithm were used on simulated and human sperm swim data. The result is distinct clusters with features of each types of swim. The clustering approach displays potential as a tool for automated sperm swim subpopulation analysis."}, {"label": 0, "content": "Kinship verification from faces is a challenging task that is attracting an increasing attention in the recent years. The proposed methods so far are not robust enough to predict the kin between persons via facial appearance only. The initial studies using deep convolutional neural networks (CNN) have not shown their full potential as well, mainly due to limited training data. To mitigate this problem, we propose a new approach to kinship verification based on color features and extreme learning machines (ELM). While ELM aims to deal with small size training sets, color features are proven to provide significant enhancement over gray-scale counterparts. We evaluate our proposed method on three benchmark and publicly available kinship databases, namely KinFaceW-I, KinFaceW-II and TSKinFace. The obtained results compares favorably against some state-of-the-art methods including those based on deep learning."}, {"label": 0, "content": "Audiovisual speech synchrony detection is an important part of talking-face verification systems. Prior work has primarily focused on visual features and joint-space models, while standard mel-frequency cepstral coefficients (MFCCs) have been commonly used to present speech. We focus more closely on audio by studying the impact of context window length for delta feature computation and comparing MFCCs with simpler energy-based features in lip-sync detection. We select state-of-the-art hand-crafted lip-sync visual features, space-time auto-correlation of gradients (STACOG), and canonical correlation analysis (CCA), for joint-space modeling. To enhance joint space modeling, we adopt deep CCA (DCCA), a nonlinear extension of CCA. Our results on the XM2VTS data indicate substantially enhanced audiovisual speech synchrony detection, with an equal error rate (EER) of 3.68%. Further analysis reveals that failed lip region localization and beardedness of the subjects constitutes most of the errors. Thus, the lip motion description is the bottleneck, while the use of novel audio features or joint-modeling techniques is unlikely to boost lip-sync detection accuracy further."}, {"label": 0, "content": "This paper focuses on target localization problem in a multi-station redundancy system which finds broad applications in sonar, radar, wireless sensor networks, and location based service. Previous solutions can only be applied to the minimum system, such as TOA method with three sensors or need matrix inversion. To solve this problem, a simple closed-form solution for a multi-station redundancy localization system is proposed by using the estimation variance as the weighting coefficient to compute an average of each group's localization result. The proposed method, with simple algebraic solution, requires no matrix inversion and can be used for low-cost hardware devices. The method is derived in TOA solution. It can also be extended to other locating technologies. Numerical examples are provided to illustrate the performance of the proposed method in root-mean-square error. The positioning accuracy of the proposed method is close to Cram\u00e9r-Rao low bound."}, {"label": 0, "content": "The aim of this paper is to develop an intelligent event-driven Electrocardiogram (ECG) processing module in order to achieve a computationally efficient solution for diagnosis of the cardiac diseases. The suggested method acquires the signal with an event-driven A/D converter(EDADC). The output of EDADC is passed through the activity selection and interpolation blocks. It allows focusing only on the important signal parts and resampling it uniformly by using the Simplified Linear Interpolator. Later on, the signal is de-noised. The autoregressive (AR) method is used to extract the classifiable features of the de-noised signal. Afterwards, the output is classified by employing different robust classification techniques such as support vector machines (SVMs), K- Nearest Neighbor (KNN) and Artificial Neural Network (ANN). The event-driven feature enables to adapt the system processing load according to the signal temporal variations. This interesting feature of the devised system aptitudes a drastic reduction in its processing activity and therefore in the power consumption as compared to the counter traditional ones. A comparison of the performance of different classifiers is also made in terms of accuracy. Results show that the proposed system is a potential candidate for an automatic diagnosis of the cardiac diseases."}, {"label": 0, "content": "Attitude motion periods of the unstable satellites are important parameters for space target surveillance. Traditional period estimation methods can only be used for single period. Aiming at rotation and precession of unstable satellite, a double-period estimation method based on variational mode decomposition (VMD) and mutual information is proposed. First, the radar cross section (RCS) of unstable satellite is processed by VMD to obtain intrinsic mode functions (IMFs) and corresponding center-frequencies. Then, through calculating and comparing the mutual information of IMFs and RCS sequence, the rotation period and precession period of satellite are obtained. The experimental results indicate that both rotation period and precession period can be estimated correctly. Compared with spectrum analysis, autocorrelation function and empirical mode decomposition (EMD), the phenomenon of frequency multiplication and mode mixing can be restrained effectively, and the accuracy of period estimation is improved."}, {"label": 0, "content": "In this paper, the improved face recognition method based on two-directional 2DPCA (two-dimensional principal component analysis) in each block of face images is proposed. Firstly, the face image is divided into several sub-images, and then the sub-image features of each corresponding block are extracted by two-directional 2DPCA according to the number of sub-images. Finally, using the support vector machine to improve the recognition rate. Experimental results on ORL face database and YALE face database show that the proposed method is superior to any other 2DPCA methods in face recognition rate."}, {"label": 0, "content": "Railway detection task produces a large number of images, the lack of effective image classification method makes it difficult to analyze detection image deeply. Using convolutional neural networks (CNN) to realize railway image scene classification is an effective technical means. This paper propose a method to reduce the bias of database by Gradient-weighted Class Activation Mapping(Grad-CAM) to effectively improve scene classification accuracy, and achieve accuracy of 95.3%(top3) on Railway12 database. Our approach combines two insights: (1) Small quantity of railway scene database make it hard for CNN to achieve high performance, transfer pre-trained ImageNet-CNN to fine-tune on railway scene database (2) Introduce Grad-CAM visualization method to analyze model's classification pattern and intuitively show the possible bias of database, provide an intuitive way to reduce bias of dataset."}, {"label": 0, "content": "The variations of photoplethysmography (PPG) morphology for the pregnancies with preeclampsia (PE) were studied in this paper. PPG data from 16 hypertensive pregnancies with PE and 26 normotensive pregnancies were acquired by the standard medical monitor. A novel hierarchical area ratio (HAR) parameter was invented to segment and quantitate the descending domain of a pulse based on the data acquired. The algorithm and features of HAR are fully explained and discussed in the paper. A rough PE distinction based on the statistics of HAR calculated from the original PPG signals was conducted with the precision of 72.7%, sensitivity of 100%, specificity of 76.9% and accuracy of 85.7%. The HAR we proposed in the paper showed favorable prospects in the quick distinction of PE."}, {"label": 0, "content": "In this paper, we present a real-time deep neural network architecture (called DiFRuNNT) for disguised face verification. The proposed model consists of two neural networks, first one being a convolutional neural network (CNN) that predicts 20 facial key-points in the image and the second neural network classifies the subject based on the angles and ratios calculated from the predicted points. The accuracies are 67.4% and 74.8% for prediction and classification respectively and the results have been compared with the state-of-the-art methodologies also."}, {"label": 0, "content": "Synthetic Aperture Radar(SAR) and optical remote sensing image registration is the prerequisite for image fusion and it is of important theoretical significance and practical value. The image registration methods are mainly divided into the methods based on feature, the methods based on Gray-scale and others. This article systematically sorts out feature-based optical and SAR remote sensing image registration techniques, summarizes all types of image registration, points out their advantages and disadvantages and predicts the prospects of their future."}, {"label": 0, "content": "In this article, a novel interferometric synthetic aperture radar (InSAR) baseline estimation method based on ground control points (GCPs) and partitioning is proposed. Instead of introducing the existing low-resolution digital elevation model (DEM) to correct the phase jumps between high and low coherence regions in the process of phase unwrapping, we use the high coherence regional block to calibrate the interferometric parameters. In the process of calibration, the GCPs can reference for low resolution DEM, also can be obtained through the filed measurement. Because the interferometric parameters calibration does not change the absolute phase, which avoids the local DEM restriction by low resolution DEM. In addition, the block based on the coherence map avoids DEM inversion error due to overall absolute phase deviation. Gaofen-3 InSAR data of Ningbo area are used to verify the effectiveness of the proposed method."}, {"label": 0, "content": "Conventionally, the signal component frequencies are estimated by spectral peak search process, and suffered with the common signal mismatch problem (SMP). MVDR and CCA are typical nonparametric methods in magnitude squared coherence (MSC) spectral estimation. In this paper, a scalar cost function is developed based on the CCA MSC spectrum, where local peak indicates the estimation of signal frequency. Then, a gradient-based adaptive-step algorithm is presented to find the local peaks. In simulations, the proposed algorithm improves the frequency estimation accuracy, and SMP is avoided."}, {"label": 0, "content": "In this paper cubic splines are designed as tone correction functions applied to the achromatic component of a spherical color model. Compared with the commonly used color models HSV and HSL, an advantage of the spherical color model is that color changes more perceptually smoothly along its coordinates. However, the spherical color model contains more color points than what can be displayed with the RGB color model, so a tone correction function might cause a gamut issue. The paper demonstrates the gamut issue can be avoided when tone correction functions are well designed and the general tone correction techniques still work well in the spherical color model. A particular type of cubic splines is designed to serve the purpose, and these splines can be adopted by the general tone correction techniques including those for low-key, middle-key and high-key images with correspondingly selected parameters. Experimental results demonstrate that these cubic splines work well for tone correction under the spherical color model."}, {"label": 0, "content": "Radar Cross Section (RCS) is a measurement of scattering performance of an object. RCS plays an important role in the design of stealth weapon system. However, the method of calculating RCS of every angle of azimuth and elevation is complex, and the efficiency is low. Evidently, it is of great significance for predicting the unknown RCS given a set of RCS data of an object. Towards this aim, we propose a novel approach using multilayers Long Short Term Memory (LSTM) networks based on unsupervised learning - the mechanism is similar to that of autoencoder. The encoder LSTM maps the input RCS into a fixed length representation. The decoder LSTM decodes the representation to predict RCS. For the purpose of this work, we create an 3D object and collect its RCS for a dataset with the help of electromagnetic simulation software, FEKO. The proposed networks have been applied on the test dataset and yield satisfactory results."}, {"label": 0, "content": "Camera model identification has been attracting a lot of attention lately, as a powerful forensic method. With the promising breakthroughs in the artificial intelligence applications, such systems were revisited to increase the expected accuracy or to solve the still persisting deadlocks. One of the most still-to-be-solved dilemmas is the image manipulations effect on the overall accuracy of the identification systems. A huge degradation in the performance is noticed, when images are post-processed using commonly used methods as compression, scaling and contrast enhancement. Using the state of the art Convolutional Neural Network (CNN) architecture proposed by Bayar et al to estimate the manipulation parameters, and dedicated feature extractor models to estimate the source camera. Multiplexers are used to shift the input image between the dedicated models through the output of the CNNs. Our proposed methods significantly outperform state of the art methods in the literature, especially in case of heavy compression and down sampling. The images used for testing were extracted from 10 different cameras, including different models from the same manufacturer. Different devices were used to investigate the methodology robustness. Moreover, such generic approach could revolutionary change the whole design methodology for camera model identification systems."}, {"label": 0, "content": "This paper presents a fast depth selection algorithm for CTU (frame coding units) based on machine learning. In view of the fast depth selection algorithm for CTU based on machine learning, due to the lack of the depth discrimination in the initial division of coding units and the inefficiencies of the coding efficiency caused by the input feature selection of the classifier, The paper firstly design the initial division depth prediction strategy based on the texture complexity and quantization parameters to skip some nonessential sizes of coding unit by analyzing the relationship between the texture complexity of the coding unit, the quantization parameters of encoder and the depth selection of the coding unit, and by combining the texture complexity and the quantization parameters to predict the initial dividing depth of the current coding unit. Secondly, by exploring the relationship between the bit-rate, distortion and the depth selection of the coding unit, the input characteristics of the classifier are determined and the selection strategy of the coding unit termination depth based on the bit rate and distortion is designed. Finally, the partition problem of the coding unit is modeled as the problem of the two-element classification and the nearest neighbor classifier is used. By skipping the calculation process of the time-consuming rate distortion cost, the ending dividing depth of the current coding unit can be judged in advance and accelerate the process of the inter-frame coding. Experimental results show that the proposed algorithm can decrease the 34.56% of the frame encoding time, while maintaining the accuracy of the coding unit compared with HM-15.0."}, {"label": 0, "content": "Traffic congestion and occlusions are major problems nowadays in metropolitan cities which leads to an ever growing traffic accidents. Therefore, the need of traffic flux management in order to avoid these congestions, unnecessary time wastage and tragic accidents is very important. Traffic regulation by optimizing timing of traffic control signals is one of the solutions for this purpose. This paper presents a low cost camera based algorithm in order to control traffic flow on a road. The algorithm is based on mainly three steps: vehicle detection, counting and tracking. Background subtraction is used to isolate vehicles from their background, Kalman filter is used to track the vehicles and Hungarian algorithm is exploited for association of labels to the tracked vehicles. This algorithm is implemented on both daytime and night time videos acquiered from CCTV camera and IR camera. Experimental results show the efficacy of the algorithm."}, {"label": 0, "content": "Diseases such as positional obstructive sleep apnea and pressure sores are closely related to body postures on a bed. To estimate body postures reliably and comfortably, we proposed a novel classification system using unconstrained measurements of ballistocardiogram (BCG) signals. A flexiblepiezo-electric polymer film sensor was embedded in a mattress to collect BCG data. The amplitude features based on the morphology of BCG were applied to Bayesian classifier with piecewisesmoothing correction. Twelve healthy subjects participated in the experiment. The final average prediction accuracies of four common body postures (supine, left lateral, prone and right lateral) on the bed all exceeded 97%, and a list of kappa coefficients calculated from classification results of subjects demonstrated almost perfect agreement. Overall, the developed system represents a brand new thinking in building an unobtrusive solution for body posture monitoring in our daily lives."}, {"label": 0, "content": "This paper presents a novel Viterbi method based on one-bit differential detection for Gaussian minimum shift keying (GMSK) used in the satellite based automatic identification system (AIS), which is easy to achieve in hardware. Because of the inherent intersymbol interference (ISI) of the GMSK, the bit error rate (BER) of hard decision based demodulation methods is always seriously exacerbated by Gaussian white noise especially under a small BT value. Correlation based traditional Viterbi method for GMSK will consume a mass of multiplier and adder resources and is not suitable for the hardware achievement. A phase rotation based differential Viterbi method is proposed in this paper, which just consumes few calculation resources and has better BER performance comparing with the hard decision method. The corresponding comparable result of the BER is also shown."}, {"label": 0, "content": "Fundamental matrix estimation based on RANSAC will encounter the problems of computational inefficiency and low accuracy when outlier ratio is high. In this paper, an optimized method via modification of the RANSAC algorithm is proposed to solve these problems. First, an isolation forest-based algorithm is performed to detect outliers from putative SIFT correspondences according to distribution consistency of features in location, scale and orientation. Then, a number of obvious outliers are eliminated from putative correspondences, which will enhance the inlier ratio efficiently. Finally, fundamental matrix is estimated with the optimized set. Repeated experiments indicate that the proposed method has testified result in speed and accuracy."}, {"label": 0, "content": "In order to solve the problem of underwater object images classification under the condition of insufficient training data, a novel underwater object images classification method based on Convolutional Neural Network(CNN) is proposed. Firstly, an advanced method of Markov random field-Grabcut algorithm is adopted to segment images into two regions: shadow and sea-bottom. Then, considering the character of the dataset, a CNN is constructed referring to Alexnet structure, consisting of two parts with different functions: convolutional part and classification part. At last, the CNN is trained to classify three different shapes of underwater objects(cylinder, truncated cone and sphere) utilizing the transfer learning approach. The method is applied to synthetic aperture sonar(SAS) datasets for validation. Comparing with Support Vector Machine(SVM) and CNN which only use trial dataset, the proposed method can achieve a better accuracy."}, {"label": 0, "content": "Based on instrumental sensors, a method of cascade estimation of the near-field signal source using the symmetric uniform linear array is proposed. The method realizes the separation, decoupling and estimation of parameters by reconstructing the virtual array and steering vector transformation. The method has no loss of aperture in reconstructing the virtual array, avoids the multi-dimensional spectrum search and is more practical. Compared with the traditional methods, the method is less computationally intensive, the gain and phase error calibration accuracy is higher, and the parameter estimation accuracy is higher. Simultaneously, cascading estimation enables real-time estimation of the azimuth, range, and error parameters, and simulation experiments show the performance of the proposed algorithm in this paper."}, {"label": 0, "content": "The minimum mean square error-residual inter-symbol interference cancellation(MMSE-RISIC) equalization algorithm for single carrier frequency domain equalization(SC-FDE) has estimated and removed the residual inter-symbol interference(RISI) of the minimum mean square error(MMSE) equalization, but the noise interference is still present in the decision data, what's more, the estimation deviation of RISI will bring additional disturbances, which reduced the accuracy of equalization. So an improved MMSE-RISIC equalization algorithm is presented and will be extended to the unique word(UW) based system of space-time block coded-single carrier frequency domain equalization(STBC-SC-FDE). This algorithm utilizes the correlation between the estimated noise in UW and the estimated noise in date, the noise in data can be predicted by the estimated noise in UW and will be removed before data decision, the estimation accuracy of RISI will also be improved. Simulation results have confirmed the significant performance gain the improved MMSE-RISIC equalization could achieve compared with the MMSE-RISIC equalization."}, {"label": 0, "content": "This paper introduces a method of abnormal human activity recognition in surveillance video. The method uses Bayes Classifier and Convolutional Neural Network to detect four activities, including walking, running, punching and tripping. KTH dataset is used as the input of Bayes Classifier and Convolutional Neural Network. Moving human target in each frame is detected by Kalman Filter and three features of the target image are extracted. The features include length-width ratio, entropy, and Hu invariant moment. Meanwhile, convolutional neural network of abnormal human activity recognition is built and trained. Experiments show that recognition accuracy of Bayes Classifier reaches 88%, 92%, 92% and 100% for each activity, and Convolutional Neural Network reaches 92%, 96%, 100% and 100% for each activity."}, {"label": 0, "content": "A Physically Unclonable Functions (PUFs) extracts the manufacturing variations of integrated circuits for key generation and authentication. It can be used to address the security issue in traditional non-volatile memory (NVM)-based key generation and authentication system. However, the powerful modeling attack based on machine learning has become a new threat of Strong PUFs-based authentication scheme. In this paper, we proposed a novel reconfigurable XOR Arbiter Physical Unclonable Functions (R-XOR APUFs) to resist this modeling attack. In this paper, the R-XOR APUF consist of multiplexers and inverters. The structure of generating two responses is configured according to challenges. The response of R-XOR APUFs is generated by XORing the two response. Therefore, R-XOR APUFs does not have a uniform model and effectively resists machine learning-based modeling attacks. The experiment results reveal that the uniqueness of R-XOR APUFs is 42.15% (the idea value is 50%) and the prediction rate of R-XOR APUFs reduces from 95% to 55% (the idea value is 50%) compared to the traditional APUFs."}, {"label": 0, "content": "Motivated by the remarkable performance achieved using deep learning strategies in solving action recognition tasks, an effective, yet simple method is proposed for encoding the spatiotemporal information of skeleton sequences into color texture images, referred to as Skeletal Optical Flows (SOFs). SOFs collectively represent the kinetic energy, predefined angles and pair-wise displacements between joints over consecutive frames of skeleton data, as color variations to capture meaningful temporal information and make them highly interpretable. A novel Convolutional Neural Network with Correctness-Vigilant Regularizer (CVR-CNN) is then employed to exploit the discriminative features of SOFs for human action recognition. Empirical results show that the efficiency of the proposed method is superior in terms of the generalizability of the generated model, the training convergence speed, and the resulting classification accuracy on commonly used action recognition datasets, such as MHAD, HDM05 and NTU RGB+D."}, {"label": 0, "content": "In recent years, the research on the rapid estimation of the parameters of maneuvering targets has received extensive attention. However, many existing parameter estimation algorithms have the problem of conflicting accuracy and computational complexity. In addition, when multiple maneuvering target parameters are estimated at the same time, the traditional time-frequency class algorithm will have cross-term interference. According to the above problem, we noticed that the auto item is constant and independent of the adjacent time delay while the cross term is a function of the adjacent time delay in the Higher-order Adjacent Cross Correlation Function (HACCF) expansion of radar echo signal. Based on that, a fast estimation algorithm for estimating multi - maneuvering target parameters is proposed. The algorithm firstly takes the mean extraction of the signal's HACFF to extract the auto items, and inhibits the cross term. Then we can estimate the frequency of auto items further and get accurate estimation of maneuvering target acceleration. Numerical simulations show that the calculation of the algorithm proposed is small and it can quickly estimate the maneuvering target parameters. This algorithm can estimate the parameters of multiple maneuvering targets simultaneously with high accuracy."}, {"label": 0, "content": "Various neuroimaging studies had demonstrated that multiple brain regions would activate during execute cognitive task. Meanwhile, Real-time functional magnetic resonance imaging neurofeedback (rtfMRI-NF) can assist subject self-regulation brain activity. However, the neural mechanisms about rtfMRI-NF were unclear. To investigate this problem, we combined graph theory with resting state fMRI to explore the topological properties of functional brain networks. In our study, subjects were provided with ongoing functional connectivity information which was related to emotion regulation. Our results showed that rtfMRI-NF training could alter the small-world properties and nodal degree in the temporal lobe, frontal lobe, limbic system. Together, our results suggested that rtfMRI-NF training was associated with alters in the topological properties of functional brain networks."}, {"label": 0, "content": "We discuss a two-frequency subtraction technique to reduce the energy leakage in a Fourier spectrum. In our method, the two strongest frequency components are determined by finding the periodogram over an interval such that the two frequencies will not interfere with each other. Such a method allows the subtraction of the two main frequency components more accurately from the original signal. The energy leakage from the main components is minimized to allow identification and more accurate determination of weaker components. Statistical error from the subtraction technique can be several times smaller than the FFT method. We show that the subtraction method is relatively robust for signals with varying amplitude or frequency."}, {"label": 0, "content": "When we estimate the DOA of LFM signal based on the traditional algorithm, it occurs many problems such as large sampled data and low estimation accuracy under low SNR. This paper proposes a new DOA estimation method based on compressed sensing theory, we use simulation experiments to verify the validity of its estimation of direction of arrival of LFM signals."}, {"label": 0, "content": "Based on the related theories and methods of complex networks, this paper proposes a method for evaluating the reliability of complex software based on weighted network model for large-scale complex software systems. This model is proposed to overcome the shortcomings of existing software networks in describing the dependencies of real software systems. First, the complex software system is abstracted into a complex software network, and the internal topological structure of a large-scale complex software system is viewed in the form of the network. Secondly, a weighted network model between nodes is established on the basis of this topology, and the actual dependencies of each structure within the software system are studied and analyzed. Finally, according to the weighted network model, the reliability analysis of a real software system is carried out, and the comparison with other similar technologies proves the validity of the method."}, {"label": 0, "content": "Sparse iterative covariance-based estimation (SPICE) method is a computational efficient sparse method for direction of arrival (DOA) estimation but has a poor performance in resolution and noise immunity. The high-order cumulant can extend the array aperture and reduce the Gaussian noise. Therefore, this paper proposed an improved SPICE based on fourth-order cumulant, which shares the same features of SPICE but has higher resolution and outperforms in low SNR case. Moreover, its computational cost is comparatively low by distilling the un-redundant data of uniform linear array. Simulations were conducted to validate and evaluate the proposed method."}, {"label": 0, "content": "A new multi-frame image super resolution (SR) algorithm via Bayesian modeling with natural image prior modeled by fields of experts (FoE) is proposed. Multi-frame SR can be used to obtain a high resolution (HR) image from a set of degraded low resolution (LR) images without changing any hardware device. However, SR is well known to be an ill-posed problem. So state-of-the-art solutions usually formulate the problem with Bayesian modeling techniques, which infer the HR image based on not only the LR input images but also on prior information about the HR image. Current Bayesian SR approaches typically use simple prior models such as L1 norm, TV prior and Laplacian prior, which cannot exploit the statistics of natural scenes well. In this paper, a Bayesian multi-frame image SR approach using a FOE model as the prior for natural images is presented. The Maximum a Posteriori (MAP) framework is used for estimating the HR image. The proposed method cannot only capture the statistics of natural images well, but also require less computational power than the other Bayesian modelling methods such as Sampling methods and Approximate inference. The proposed method shows superior or comparable results to the state-of-art multi-frame SR methods."}, {"label": 0, "content": "A novel quadrature phase shift keying (QPSK) carrier tracking method used in coherent demodulation is proposed for satellite communications. Frequency ambiguity is always ignored by the traditional ones which results in that the maximum pull-in range of the carrier tracking loop is limited to one eighth of the symbol rate. A novel low complexity QPSK frequency/phase discriminator and a frequency estimation and compensation (FEC) module are presented in this paper to enlarge the frequency pull-in range, which can resolve the phase jump and frequency ambiguity problem in carrier tracking process and also has the few calculation complexity comparing with the traditional QPSK detector. A parallel FLL-assisted-PLL is adopted to simplify the design structure through the module reuse. Comparing with the state-of-the-arts, this method has lower complexity, smaller variance and larger pull-in range."}, {"label": 0, "content": "This paper introduces an object-based method based on a new statistical distance for SAR image change detection. Firstly, multi-temporal segmentation is carried out to segment two temporal SAR images simultaneously. It considers the homogeneity in two temporal images, and could generate homogeneous objects in spectral, spatial and temporal. In addition, through setting different segmentation parameters, the multi-temporal images can be segmented in a set of scales. This process exploits the advantages of OBIA that could effectively reduce spurious changes, and considers the scale of change detection task. Secondly, a multiplicative noise model called Nakagami-Rayleigh distribution is employed to describe SAR data, and then applied to Bayesian formulation. Thus, a new statistical distance that is insensitive to speckles is derived to measure the distances between pairs of parcels. Then, cluster ensemble algorithm is utilized to improve accuracy of individual result in each scale to obtain the final change detection map. Finally, multi-temporal Radarsat-2 images are employed to verify the effectiveness of the proposed method compared with other four methods."}, {"label": 0, "content": "The aim of this project is to develop an efficient event-driven wireless sensor nodes based network for an effective and power efficient monitoring of plants health and larva population in a remote crop field. In this framework, an event-driven approach is proposed to detect larva and measure other system parameters like Acoustic Complexity Index (ACI), temperature, humidity and soil moisture. The sensors' data is collected by the front end event-driven sensing node, developed with a STM32F407VG board, via a serial port. The STM32F407VG board is based on the ARM processor. In contrast to the clock driven classical sensing nodes, the devised sensing nodes only acquire and transmits the sensors data in the case of a significant change. It significantly reduces the power consuming data acquisition and transmission activity as compared to the classical solutions. It improves the proposed solution power efficiency and autonomy as compared to the counter classical ones. The data from the node is transmitted to a base station by using a wireless ZigBee interface. The base station collects data from a group of event-driven sensing nodes. This data is transmitted to the Central Processing Unit (CPU) via the USB liaison between the base station and the CPU. On CPU this data is analyzed via the MATLAB based specifically developed application. The findings are displayed and logged on the CPU. It allows the terminal user to access this and to achieve a timely interaction and cure of the intended crop field. The system parameters are adjusted in order to achieve the effective modules integration and performance. The proposed system operation is validated with an experimental setup. Results have shown a promising system realization."}, {"label": 0, "content": "The learning methods have recently achieved great success for single image super-resolution. Due to the robust network, convolutional neural networks exhibit the state-of-the-art performance. In this paper, we propose a Dynamic Multi-mapping Convolutional Network (DMCN) that improve the SR performance. Instead of fixed kernels like Bicubic interpolation, we choose several dynamic filters to resize the LR input as the pre-trained module. Based on an end-to-end manner, more accurate and effective features from middle layers can be learned. Furthermore, multi-mapping module can deliver extra information and high-frequency details, which generates sharper high-resolution images from the network. Extensive quantitative and qualitative evaluations have been demonstrated that our algorithm improves effectively the resolution of the image."}, {"label": 0, "content": "This project employs the IoT with an intelligent event-driven system in order to realize an efficient quasi real-time attendance tracker. The idea is to keep the whole system in the standby mode except for the low power motion sensor. On the detection of an event, when a person enters and originates a motion, the front-end embedded processor is alarmed. Afterwards, it activates the remaining system modules like webcam, communication block, etc. The event-driven feature improves the system performance in terms of resources utilization and power consumption compared to the counter classical ones. A first system implementation is realized and successfully tested. It is based on a raspberry pi 3 board, which is integrated with two Passive Infrared (PIR) sensors and two webcams. On the occurrence of an event the webcam is activated and it captures an image. The image is recorded via the Raspberry Pi webcam server and is shared with other system modules via the Porta Space application, which acts as a hub between the Raspberry Pi and the cloud. Simultaneously the attendance status is updated via the IFTTT on the cloud-based log. Moreover, the concerned authorities are notified via an email. The process is repeated every time when a person enters or leaves the concerned place. The attendance log remains globally available via the cloud and can be accessed anytime. The system design flow is described. The devised system functionality is tested with an experimental setup. Results have confirmed a proper system operation."}, {"label": 0, "content": "The prevalence of sleep apnea hypopnea syndrome (SAHS) is increasing year by year and up to 14% in 2016. The disease poses a threat to human sleep safe. To detect the disease effectively and pre-alert, a method for the diagnosis of SAHS using respiratory signals was proposed. According to the characteristics of respiratory signal, nonlinear characteristics fractal dimension and sample entropy were introduced based on time-domain characteristics variance and clinical zero passing numbers as well as frequency-domain characteristics wavelet coefficients and wavelet energy. Then a 6-dimension feature vector was structured and input into the support vector machine (SVM), back propagation neural network (BPNN) and improved back propagation neural network (IBPNN) based on morphology. The method was verified by the 8 sets data including 3840 samples of the Physionet Apnea Database. Experimental results showed that classification accuracy of SVM and BPNN was 71.2% and 84.5% respectively. The IBPNN based on morphology improved the 85% of classification results and increased accuracy by 7.3%. It is very effective to detect the SAHS using the feature vector and IBPNN based on morphology proposed in this paper and provide an efficient and convenient method for the diagnosis of diseases."}, {"label": 0, "content": "In this paper, a novel algorithm based on convolutional neural network (CNN) and support vector machine (SVM) for fire detection in infrared (IR) video surveillance is proposed. To improve the performance of IR fire detection, we develop a 9-layer convolutional neural network named IRCNN instead of traditional empirically handcrafted methods to extract IR image features. Then, a linear support vector machine is trained with extracted features to achieve fire detection. Our network adopts data augmentation technique and Adam optimization to deal with problems caused by the insufficient dataset, and accelerate the training process. Experimental results show that our method achieved both high precision (98.82%) and high recall (98.58%) on our IR flame dataset and real-time detection on the ordinary infrared surveillance cameras."}, {"label": 0, "content": "When taking pictures in a low-light scene with artificial lighting, we often encounter the following problem: we can use short exposure setting which yields a dim, noise image but with a sharp outline, or we can use a longer exposure setting which yields a bright, saturated image but with blurred areas. In many cases, none of those images is good enough. Good brightness and color information are retained in longer-exposure images, whereas sharp outlines are retained in shorter ones. In this paper, we present a patch-based method to combine such image pair into a better one. In our method, we firstly perform a coarse-to-fine strategy to detect inconsistent pixels caused by moving objects, then we draw information from the two exposures based on a novel patch-based technique. Experimental results show that the proposed method effectively preserves sharp edges of the short-exposure image, and maintains the color, brightness, and details of the long-exposure image."}, {"label": 0, "content": "In order to improve the quality and visual effects of blurred images, we proposes an image deblurring algorithm based on dictionary learning. Firstly, we divides the blurred image into image block structure groups, then we recovers the image block through the K-SVD dictionary and the PCA dictionary, and finally the morphological operations is applied to the difference image to obtain restored image. Experimental results show that the proposed algorithm is better than others in peak structural similarity and visual effect."}, {"label": 0, "content": "In this paper we propose a novel method for detecting adversarial examples by training a binary classifier with both origin data and saliency data. In the case of image classification model, saliency simply explain how the model make decisions by identifying significant pixels for prediction. A model shows wrong classification output always learns wrong features and shows wrong saliency as well. Our approach shows good performance on detecting adversarial perturbations. We quantitatively evaluate generalization ability of the detector, showing that detectors trained with strong adversaries perform well on weak adversaries."}, {"label": 0, "content": "The output frequency response for most DACs rolls off according to the sin(x)/x frequency-response envelope [1]. This paper derives a FIR filter which is designed in the minimax sense with the ripple constraints as the design criteria for the frequency response compensation of DACs. The filter order estimation function under the criterion gives a detailed filter design configuration method, which can effectively shorten the design time and provide an accurate resource configuration reference for the top-level system design [2]. The simulation example verifies the advantages of the compensation filter in minimax sense and shows the performance of the compensation filter and the accuracy of the order estimation function."}, {"label": 0, "content": "In this paper, we investigate the outage performance of a general dual-hop multiple-input multiple-output (MIMO) amplify-and-forward (AF) relay network, where the source, relay and destination are all equipped with multiple antennas. By considering maximal-ratio-transmission (MRT) and maximal-ratio combining (MRC) for the transmitter and receiver, respectively, we first obtain the output signal-to-interference-plus-noise ratio (SINR) of the dual-hop AF relay system with multiple co-channel interferences (CCIs) and noise at the relay. Then, we derive closed-form expressions of the outage probability (OP) for both the fixed-gain and variable-gain multi-antenna relaying systems. Finally, computer simulations are carried out to validate the performance analysis. Our new analytical expressions not only provide a fast and efficient method to evaluate the outage performance of the system, but also enable us to gain valuable insights into the effects of key parameters on the performance of the dual-hop AF relaying system benefit from implementing multiple antennas at each of the three nodes in the relaying network."}, {"label": 0, "content": "Spacecraft detection is one of essential issues on aerospace information processing and control, and can provide reliable dynamic state of target, so as to support decisions made on target recognition, classification, catalogue, et al. Although numerous spacecraft detection methods exist, most of them cannot achieve real-time detection, and are still lack of better accuracy and fault-tolerance for different scenes. Recently, deep learning algorithms have achieved fantastic detection performance in computer vision community, especially the regression-based convolutional neural network YOLOv2, which has good accuracy and speed, and outperforming other state-of-the-art detection methods. This paper for the first time applies CNN to the detection of spacecraft and sets up a dataset for target detection in space. Our method starts with image annotation and data augmentation, and then uses our improved regression-based convolutional neural network YOLOv2 to detect spacecraft in an image. The experimental results have shown that our algorithm achieves 97.8% detection rate in the test set, and the average detection time of each image is about 0.018s, which has lower time overhead and better robustness to rotation and illumination changes of spacecraft."}, {"label": 0, "content": "Depression is one of the causes of suicide in the world next to other health issues that makes up an alarming point of mortality living in this lifetime. Melancholy that in the field of computer vision and signal processing has been tackled in various ways. Thus, this paper presents the classification model of detecting depression based on local binary pattern (LBP) texture features an image processing approach for pattern recognition on images. The study used the video recording from the SEMAINE database. The face image is cropped from a video and extracting Uniformed LBP features in every single frame. Part of the classification is to implement PCA eigenvalues from the original features to see the effects. The result of the accuracy was 81% of the SVM using RBF kernel classifier when detecting Depressed to Not Depressed Behavior on a captured motion picture."}, {"label": 0, "content": "In this paper, we proposed an effective method of facial expression recognition based on a G-2DPCA feature and Sparse Representation-based Classification (SRC). Gabor filters with five scales and eight directions are first employed for feature extraction. To address the high dimension of Gabor features, we select one out of forty Gabor filters with an optimal parameter pair of scales and directions to filter facial images. Two-dimensional principal component analysis (2DPCA) is utilized for image representation and data dimension reduction. It retains the 2D geometric structure of an image, and the image matrix does not need to transform into a vector, which reducing the computation time greatly. Finally, Gabor plus 2DPCA (G-2PCA) features are regarded as the atoms of dictionary in SRC. Experimental results demonstrate that the proposed method has better performance than the existing algorithms."}, {"label": 0, "content": "Emergency evacuation simulation is an important measure to effectively avoid personnel deaths and injuries. In view of the safe evacuation in underground tunnels, an emergency evacuation simulation system for personnel in three-dimensional tunnel is put forward. According to the different conditions encountered by the Agent in the tunnel, the behavior of the Agent is set and visualized. Through the design of three-dimensional model, evacuation path generation, visualization simulation, evaluation and analysis, the experiment of the underground tunnel evacuation is carried out. The results show that the system can effectively simulate the evacuation of people in the tunnel, which can be used for emergency evacuation guidance during disasters and evacuation simulation drills before disaster."}, {"label": 0, "content": "With the emergence of big data and the development of mobile devices, mobile data mining has received more and more attention. It shows its unique advantages, but it also exposes its inability to handle large datasets efficiently. Based on the traditional mobile data mining project, we combined cloud computing and proposed and implemented the MobileWeka2 model based on cloud computing. In order to prove the feasibility of the model, we conducted different data mining experiments on multiple data sets. Experimental results show that this model can efficiently process large data sets and solve the problems of traditional mobile data mining."}, {"label": 0, "content": "This paper presents a learning-based solution to tackle the real-time gesture recognition of bimanual (two hands) gestures which is not well studied from the literature. To overcome the critical issue of hand-hand self-occlusion problem common in bimanual gestures, multiple cameras from diversified views are used. A tailored multi-camera system is constructed to acquire multi-views bimanual gesture data, and data from each view is then fed into a separate classifier for learning. Thus, to ensemble results from these classifiers, we proposed a weighted sum fusion scheme of results from different classifiers. The weightings are optimized according to how well the recognition performed of the particular view. Our experiments show multiple-view results outperform single-view results."}, {"label": 0, "content": "An adaptive block-based compressive sensing (BCS) video reconstruction algorithm based on temporal-spatial domain characteristics is proposed. Firstly, assigned weight function is introduced, and the adaptive sampling scheme of joint wavelet coefficients and variance are developed. On the basic, the global measurement matrix is constructed by assigned weight matrix to realize global reconstruction. Secondly, combined with the block-based multi-hypothesis (MH) model and the minimum total variation (TV) model, the predictive-residual reconstruction model of joint temporal-spatial domain characteristics is constructed, and the prediction frame of the current frame is obtained by iteration. Thirdly, the residuals are calculated by global reconstruction and combined with the predictive frames of the current frame to reconstruct a new frame, To validate the effectiveness of proposed video reconstruction algorithm, the results are compared with other BCS algorithms which proposed from the aspects of video reconstruction in recent years. The experiment results show that the proposed algorithm can effectively improve the quality of video reconstruction and further reduce the computational complexity compared with other algorithms."}, {"label": 0, "content": "In order to discriminate the real targets, the clutter and the dense multi-false targets, we propose a factorized convolutional neural network-based algorithm for radar targets discrimination. We establish the factorized convolutional neural network model with depthwise separable convolution. To reduce the parameters of the model, we establish the simplified factorized convolutional neural network by reducing the numbers of both convolutional filters and connection nodes of fully connected layers. The result of the measured data demonstrates that, as compared with the existing model, the simplified factorized convolutional neural network has higher discrimination rate for the real targets, the clutter and the dense multi-false targets, and its parameters are less than ten percent counterpart of a recent proposed model."}, {"label": 0, "content": "Lip reading, the ability to recognize text information from the movement of a speaker's mouth, is a difficult and challenging task. Recently, the end-to-end model that maps a variable-length sequence of video frames to text performs poorly in real life situation where people unintentionally move the lips instead of speaking. The goal of this work is to improve the performance of lip reading task in real life. The model proposed in this article consists of two networks that are visual to audio feature network and audio feature to text network. Our experiments showed that the model proposed in this article can achieve 92.76% accuracy in lip reading task on the dataset that the unintentional lips movement was added."}, {"label": 0, "content": "Flower plays an extremely important role in our life, which has high research value and application value. The traditional methods of flower classification is mainly based on shape, color or texture features, and this methods needs people to select features for flower classification lead to the accuracy of classification is not very high. This paper aims to develop an effective flower classification approach using convolution neural network and transfer learning. In this paper, based on VGG-16, VGG-19, Inception-v3 and ResNet50 models were used to compare the network initialization model with the transfer learning model. The results show that transfer learning can effectively avoid deep convolution networks are prone to local optimal problems and over-fitting problems. Compared with the traditional methods, the accuracy of flower recognition on Oxford flowers dataset is obviously improved, and has better robustness and generalization ability."}, {"label": 0, "content": "The holomorphic embedding method (HEM) applied to the power flow problem has more robust performance than the Newton-Raphson method. But as the saddle-node bifurcation point (SNBP) is approached, more terms must be included in the Maclaurin series representation of the voltage to achieve a converged solution, leading to matrices that are ill-conditioned. This ill-conditioning may prevent HEM from converging or may interfere with the ability to predict the SNBP using the so-called roots method. In this paper, we look at the effect of the robust Pad\u00e9 approximation (RPA) method on both of these issues."}, {"label": 0, "content": "Voltage optimization on distribution networks is of uttermost importance to Distribution System Operators (DSO). The performance of the entire distribution network depends on the voltage profile of the system. With the current increase in the penetration of Distributed Energy Resources (DERs) on the distribution network, the challenges of optimized voltage profiles become aggravated. For PV connected systems, smart inverters can be used to partake in the voltage regulation and optimization process using their capability of reactive power injection and absorption. This is often referred to as Volt-VAR Optimization (VVO). Choosing the optimal Volt-VAR Curve (VVC) for the smart inverters often becomes challenging. This paper formulates 4 objective functions, and also proposes the use of Genetic Algorithm (GA) for VVC selection with the integration of a high-level penetration of PV smart inverter. The algorithm was tested on the standard IEEE 13 node distribution test feeder without the use of traditional voltage regulating devices such as Voltage regulators and capacitor banks. The result showed that the overall system active power losses can be minimized by carefully selecting the optimal VVC for the scenarios under study. Also, the results show the different dependencies of the minimization of the active power losses in the network on the VVC reactive power absorption and injection axis."}, {"label": 0, "content": "The sale of electric vehicles (EVs) is rapidly increasing around the world due to EVs' efficiency and energy security. Charging systems play a vital role in electric vehicle. Charging systems can be categorized into three levels according to Society of Automatic Engineers (SAE). This paper presents the topologies of three types of charging systems. The charging systems are simulated in RT-Lab real-time simulator. The input ac for Level 1 and Level 2 charging systems is single-phase. The charging system consists of two diode bridge rectifiers, a power factor correction (PFC) boost circuit, a DC/AC converter, an LLC resonant converter, and a high frequency transformer. Constant current/constant voltage (CC/CV) control is employed for the DC/AC converter. The Level 3 charging system uses a three-phase source as input and its bi-directional converter is equipped with reactive power and DC bus voltage control. Three testbeds are setup to simulate the three types of charging systems with different charging power levels. A 10 kWh-battery will be charged. Simulation results demonstrate the expected charging performance of the charging systems."}, {"label": 0, "content": "This research proposes a significant reduction in the processing time to solve the concurrent AC multistage transmission network expansion and reactive power planning problem with security constraints, by an innovative search space reduction strategy. Initially, the concurrent planning problem is modeled as a mixed-integer linear programming (MILP) problem, using an AC branch flow formulation to represent the steady-state operation of the transmission network. The innovative strategy consists of obtaining a stage-by-stage solution pool of the MILP model as a static problem, to identify the significant candidate lines. Therefore, those lines considered insignificant would not be considered as candidates in the multistage problem. Then, with the updated database, it is possible to solve the multistage MILP problem with a reduced search space. The evaluation of the proposed methodology is done using the IEEE 24- and 118-bus test systems, showing the performance of the proposed methodology."}, {"label": 0, "content": "Understanding the dynamics of a power system requires that information be presented in a meaningful way. Large-scale modal results are presented for a large interconnected power system using visualization methods to reveal the underlying oscillations in the system. Visualization tools are used to capture the quality of mode estimation among several bus signals, identify different modal interactions existing in the system and visualize modal power flows for tracking sources of grid oscillations. The use of wide-area visualization in a synthetic large interconnected power grid is used to uncover critical information about the dynamic state of the system, which would have otherwise, not been captured from a graphical plot of the time-varying signals."}, {"label": 0, "content": "Forecasting of consumer electricity usages plays an important role to make total smart grid system more reliable. As the activities of individual residential consumers has many uncertain variables, it is hard to accurately forecast the residential load levels. For planning of the electrical resources and to balance demand and supply, accurate forecasting tasks are critical. This paper presents Deep Neural Network (DNN) based short term load forecasting for Residential consumers. In this work, we compare the Mean Absolute Percentage Error (MAPE) value for residential electricity dataset using different types recurrent neural network (RNN). Our preliminary results indicate that Long short-term memory (LSTM) based RNN performed better compared with simple RNN and gated recurrent unit (GRU) RNN for a single user with 1-minute resolution based on one year of historical data sets."}, {"label": 0, "content": "The problem of nontechnical losses (NTL) detection using pattern recognition methods has been studied by many research groups. However, a comparison between the methods proposed by those authors is hardly ever possible because a database for comparison of NTL detection methods has not been made available to the research community. In this paper, we propose four variations of a database based on the IEEE 123 Bus-Test Feeder for testing NTL detection methods. The models and hypotheses used to synthesize the system are explained. An application of the Optimum-Path Forest classifier for NTL detection using the aforementioned database is also presented. The database is in Matlab platform and is available at http://www.power.ufl.edu."}, {"label": 0, "content": "Unit commitment (UC) problems are the most fundamental problems that system operators solve every day in both day-ahead and real-time markets to guarantee secure and economic operation. UC problems are usually formulated as a mixed-integer linear programming (MILP) or a mixed-integer quadratic programming (MIQP) model with binary variables representing ON/OFF statuses of generators, and is solved via Branch and bound (B&B) type algorithms. With the prosperity of applying semidefinite programing (SDP) in the power system field, researchers attempt to solve UC under the SDP framework by relaxing integrality requirements, seeking an optimal solution or a better lower bound than the traditional linear programming (LP) relaxation. This paper uses 2-order moment relaxation technique to reformulate UC problems as an SDP model, which can be solved by an interior point method. Considering significant computational burden by 2-order moment relaxation, a variable reduction strategy and two refined moment relaxation based UC models are further applied. The relationship among the proposed models in terms of their tightness is studied. In addition, a sufficient condition under which a solution is exact is stated. Numerical studies illustrate effect of the proposed models and potential applicability of the proposed models is also discussed."}, {"label": 0, "content": "In recent years, active distribution networks have gained in importance due to increased emphasis on renewable energy sources. The integration of these sources into the power system is intrinsically related to various types of uncertainties. In such cases, evaluating line loading and line active power is of immense importance in providing comprehensive details for use in power system planning. This paper presents two key (numerical and analytical) approaches for comparing computational times and accuracy for line loading and active power in an active distribution network. Monte Carlo and quasi-Monte Carlo simulations are categorized as numerical methods and combined cumulants and Gram-Charlier expansion is an analytical approach. The IEEE 13-bus test system was employed to conduct the required analysis. It was determined that the combined cumulants and Gram-Charlier expansion method is quite accurate and computationally efficient when compared to Monte Carlo and quasi-Monte Carlo methods."}, {"label": 0, "content": "An accurate estimation of a power system's state is a major requirement in the modern-day power system. An interconnected and highly nonlinear system requires a reliable and efficient algorithm for monitoring of the system's status in order to have a secure operation. The presence of wrong measurements has made the estimation process a challenging one. An efficient and reliable state estimator should have the ability to detect and eliminate the effects of bad-data during the estimation process. Least Measurement Rejected (LMR) estimator is one of such robust estimators with higher computational efficiency and better reliability. The performance of LMR estimator mainly depends upon the tolerance value of loaded measurements and tolerance is a constant value assigned to each of the measurement. This paper presents an efficient method of tolerance value selection for LMR estimator. Such selection of tolerance value will ensure the robustness of the estimator in terms of estimation accuracy and will provide better computational efficiency. The estimation accuracy and computational time of the proposed approach has been compared with Weighted Least Square (WLS) and Weighted Least Absolute Value (WLAV) estimator. The IEEE 30-bus system has been used to demonstrate the performance of the proposed estimator under different sets of bad measurement (single and multiple) scenarios."}, {"label": 0, "content": "This paper is aimed at evaluating the reliability indices of a distribution network (DN) considering the rerouting of the interrupted customers during outages. The DN is represented by its circuit graph that is used to analyze the effect of the failure mode (FMEA method) and compute the reliability of the system. To have a more accurate estimation, the capacity of the feeder and the voltage deviation when the rerouting occurs, are also considered. After the occurrence of an outage, the problem is finding the optimum tree by interchanging the status of normally open (NO) and normally closed (NC) switches to restore as many as interrupted customers subject to defined constraints. The proposed algorithm presents a systematic method to find the optimum solution of the restoration problem, and by decreasing the search space is suitable for analyzing the big size DN. To obtain the impact of each NC switches in the reliability improvement, a ranking method is also introduced."}, {"label": 0, "content": "Phasor measurement units (PMUs) can make state estimation more accurate by providing synchronized voltage phasor and current phasor measurements. Optimal PMU placement (OPP) minimizes the number of PMUs required for the system to be completely observable. This paper presents a DC state estimation model using mixed integer semidefinite programming (MISDP) approach for the OPP problem. A comparison between MISDP and mixed integer linear programming (MILP) is conducted. Power flow measurements, injection measurements, limited communication facility, and single PMU failure are studied for each approach. A formulation for MISDP-based PMU placement considering a single PMU failure is proposed. The advantages and disadvantages of each formulation are discussed."}, {"label": 0, "content": "In the current practice, locational marginal prices (LMPs) are computed every few minutes by solving an optimization problem with continuous variables only. Generator on/off statuses are decided by the day-ahead unit commitment scheduling procedure. Thus, unit commitment and locational marginal price computing are two separate steps. This paper presents a model that substitutes the traditional two-step method. It is based on bilevel programming. The proposed problem determines generator on/off status and locational marginal prices simultaneously. The efficiency of the model is demonstrated on the 5- and 30-bus systems."}, {"label": 0, "content": "This paper investigate the accuracies of short-term forecast for MISO\u2019s locational marginal pricing (LMP) data sets. A collection of methods such as rolling average, autoregressive integrated moving average (ARIMA), and long short-term memory (LSTM) were applied to a three-year data sets and compared against MISO\u2019s forecasting approach. Our preliminary findings indicate that the use of recurrent neural networks (RNN) gains an additional 25% increase in forecasting accuracy compared to MISO\u2019s forecasting approach."}, {"label": 0, "content": "Installation of flexible alternating current transmission system (FACTS) devices is a widespread approach for reducing ohmic losses in power networks. However, the location and control parameters of these devices should be determined through an optimization procedure. An interphase power controller (IPC) is one of the FACTS devices that can change the active power flow in network branches. However, the placement of this device has not been discussed yet. Therefore, an efficient optimization method will be introduced in this paper based on a genetic algorithm (GA) combined with optimal power flow (OPF) for IPC placement. Firstly, a simple novel method for entering the steady state model of IPC into the power flow equations is proposed. Secondly, the optimal values of decision variables such as IPC location and its control parameters will be determined by the proposed optimization method. The simulation results on IEEE 30-bus and 118-bus test systems show that the GA-based optimization process is able to obtain optimal solutions for the mixed integer placement problem, which would result in a more energy-efficient transmission system."}, {"label": 0, "content": "The paper presents the results of the research work funded by Salt River Project Agricultural Improvement and Power District (SRP) on maximizing the economic benefits to customers installing residential rooftop PV systems in SRP territory. The optimized discharge of the battery power which would help in the reduction of Demand Charge paid by the customer was the primary goal. Machine Learning algorithms were utilized as a better load forecasting technique to the ones already in place. The improved battery discharge algorithm would also reduce the battery charge-discharge cycles (cycling aging) thus, improving the battery life. The tests were performed in the state of Arizona, on a residential rooftop grid-tied PV with storage system installed at the Tempe campus of the Arizona State University."}, {"label": 0, "content": "This paper proposes an artificial neural network based approach to implement electric demand response (DR) programs. In order to maintain the continuous electricity supply, the total generated power must meet the load demand with spinning reserves for unforeseen contingencies. If a power system fails to satisfy all or part of its loads, then load curtailments cannot be avoided. However, load curtailments can be reduced or even avoided by implementing DR programs. The effectiveness of DR programs to reduce/avoid load curtailments depends on efficient and accurate identification of appropriate hourly loads for DR programs. In this work, a feed-forward artificial neural network approach is used to classify system hourly loads based on customers' potential participation in DR programs as well as to identify the effective periods of participation. In addition, a mathematical model based on the demand-price elasticity, incentives, and penalties is developed to calculate interruptible/curtailable loads for DR programs. The proposed method is demonstrated on the IEEE reliability test system. The results of the case study show that the proposed method is effective in identifying appropriate hourly loads to implement DR programs."}, {"label": 0, "content": "This paper introduces the design and implementation of a Python-based software package for cyber-physical power system research called Andes. Andes is developed in an attempt to bridge the gap between the traditional power system analysis and the fast-growing needs for cybersecurity studies. First, the architecture design is proposed to accommodate for power grid prototyping, communication network set up, and the interactions between the two. Design considerations are discussed from the research and development perspective. Examples are shown using Andes for modeling, monitoring, and visualization of cyber-physical power system studies."}, {"label": 0, "content": "To meet the challenges and needs of the evolving power grid with Phasor Measurement Units (PMUs) deployment, this paper applies the cloud computing technology as the new information infrastructure for utilities and Independent System Operators (ISOs). An architecture design of cloud-based synchrophasor analytical applications as a service for power system operation is proposed. The implementation proves the feasibility and effectiveness of using Software as a Service (SaaS) service model for power system operations at grid control centers."}, {"label": 0, "content": "Power system state estimation is a key component of real-time monitoring, enabling extensive analysis and decision making for grid security and efficiency. One challenge that has seen recent interest involves the monitoring and mitigation of geomagnetically induced currents (GICs). These quasi-dc currents are the result of solar activity and can cause additional reactive power losses in transformers. The subsequent loss of reactive power support may result in voltage deviation at many buses. In a traditional state estimator, these voltage deviations may be masked by or attributed to incorrect estimations of generator reactive power output. Alternatively, the voltage state estimate may accumulate additional error, due to trying to match measurements to equations that do not represent the actual physical system and condition. This paper presents a case study that shows the need for state estimation models that consider GIC effects and analyzes the required increase in GIC-related measurements and models incurred therein."}, {"label": 0, "content": "This paper describes a geometric approach to parameter identifiability analysis in models of power systems dynamics. When a model of a power system is to be compared with measurements taken at discrete times, it can be interpreted as a mapping from parameter space into a data or prediction space. Generically, model mappings can be interpreted as manifolds with dimensionality equal to the number of structurally identifiable parameters. Empirically it is observed that model mappings often correspond to bounded manifolds. We propose a new definition of practical identifiability based the topological definition of a manifold with boundary. In many ways, our proposed definition extends the properties of structural identifiability. We construct numerical approximations to geodesics on the model manifold and use the results, combined with insights derived from the mathematical form of the equations, to identify combinations of practically identifiable and unidentifiable parameters. We give several examples of application to dynamic power systems models."}, {"label": 0, "content": "The interpretation of power flows from a probabilistic perspective is an important topic in the development algorithms of complex network theory for power system analysis, as it is conceived as the theoretical foundation for many new techniques such as machine learning and artificial intelligence. In this paper, we first show that power flow can be mathematically represented as the equilibrium of probabilistic energy movements on a graph. We demonstrate that such an equilibrium can be reached if the energy movements follow the commonly known random process of Markov chains. This work advances the current state of the art by showing a way that connects the power flow models and the random walk models of complex networks. Moreover, several new insights on the electrical betweenness centrality measures used in power grid vulnerability analysis are presented from this new probabilistic perspective."}, {"label": 0, "content": "In this paper, we present a scheduling scheme for household Electric vehicles based on deep neural network based demand forecast. A novel clustering based Short Term Load Forecasting (STLF) using deep neural network (DNN) is presented in this paper to forecast the household and EV demand. The forecasting is performed on electricity demand profiles for 200 households from the Midwest region of the United States. Tensor-flow based deep learning platform was used to develop deep learning structure. The households are clustered according to demand profiles and the grouped consumers are used as the forecasting parameters. The scheduling model uses the forecasted household and EV demand values to develop a linear programming based optimization model to minimize the electricity cost for consumers. Household and cluster constraints are considered in the optimization model to limit the sudden surge in power demand during low-price time periods."}, {"label": 0, "content": "This paper presents a Mixed Integer Linear Programming (MILP) model for optimal allocation of Automatic Switching Devices (ASDs) and Distributed Generation (DG) in distribution networks. The model's formulation considers the application of ASDs for protection and post-fault restoration purposes, as well as the role of DG units in the restoration process. From the reliability perspective, System Average Interruption Frequency Index (SAIFI) is used as a metric index for obtaining an optimal placement of ASDs and DG units. Decoupling the optimal allocation problems of ASDs and DG units does not provide an optimal solution, since both are inherently interdependent with system reliability. Hence, the simultaneous allocation of ASDs and DG units is considered in this work. A Goal Programming approach is used to establish the optimal trade-off between the placement of ASDs and DG units, and the improvement on SAIFI. In other words, the developed model aims to i) minimize the number of interrupted consumers due to protection operation; ii) maximize the number of consumers restored by automatic network reconfiguration, and iii) minimize costs associated with the allocation of ASDs and DG units. Restoration is ensured by a set of linear power flow equations. The model's solutions are obtained from a Branch and Bound-based technique, and results are presented using the IEEE 123-bus system."}, {"label": 0, "content": "This paper proposes the Eigensystem Realization (ER) to estimate dynamic phasors. Synchrophasor estimates such as amplitude, phase, frequency and rate of change of amplitude are provided in one-cycle. Thus, a new ER-based phasor estimator has been proposed and assessed under steady-state and dynamic conditions in theoretical and actual signals coming from a commercial PMU. This paper compares the performance of ER-based phasor estimates method with that of the well-known Discrete Fourier Transform. Finally, the results exhibit that the proposed phasor estimator attains reliable estimates even though under polluted conditions by high harmonic content, being able to tracking the changes in amplitude, phase and frequency."}, {"label": 0, "content": "Demand response and active participation of end-users (prosumers) are expected to play a critical role in the future power grids. Market based transactive exchanges between prosumers are triggered by the increased deployments of renewable generations and microgrid architectures. Transactive Energy Systems (TES) employ economic and control mechanisms to dynamically balance the demand and supply across the electrical grid. Effective transactive mechanisms leverage on a large number of distributed edge-computing and a communication architecture. Given the prolific usage of digital devices, the assets within a transactive environment are vulnerable to various threats. This paper utilizes a machine learning technique to detect possible anomalies within a transactive energy framework. An ensemble based methodology is used to detect anomalies in the market and physical system measurements. The proposed technique is validated for satisfactory performance to detect anomalies and trigger further investigation for root cause analysis and mitigation."}, {"label": 0, "content": "Classic DC power flow and Generalized Generation Distribution Factors (GGDF) are used for modeling the transmission network constraints in a DC optimal power flow (OPF). The first method is known for its simplicity, accuracy, and robustness, and the second for its ability to express transmission power flows as a function of the power generation with less equality constraints. This paper compares the two formulations by testing them on a PJM 5-bus system and an IEEE 57-bus system including transmission losses, using different commercial optimization solvers."}, {"label": 0, "content": "A distributed energy management system for an interconnected multi-microgrid system is developed and tested. The distributed energy management system is formulated using the alternating direction method of multipliers (ADMM) and is implemented using the CVX platform of the MATLAB environment. In this work, microgrids are interconnected and communicate with each other in order to minimize the operational cost of the system and to derive maximum profits via energy exchanges with the main grid. Simulation results demonstrate the effectiveness of the proposed method."}, {"label": 0, "content": "A class of nonlinear equation solvers known as the holomorphically embedding method, when applied to the power-flow problem is theoretically guaranteed to find an operable solution to the power-flow problem, if one exists, provided rather mild conditions are satisfied. To date, all of the published approaches use a Gauss-Seidel-based fixed-point form as the starting point for the embedding. We show that a fixed-point form based on a Newton-Raphson scheme may also be use and has some advantageous properties when attempting to find the saddle-node bifurcation point (SNBP)."}, {"label": 0, "content": "The recent past has seen an influx of new generator interconnection to the power grid. Some of these new generator interconnections occur at places where there isn't enough transmission capability and hence get curtailed or penalized during the real time operations. In this paper a procedure is proposed to calculate the maximum possible MW injections with the help of Power Transfer Distribution Factors at each of the buses in the power grid without violating transmission limits. This in turn identifies areas on the grid with abundant transmission capability. The above calculation is computationally intensive, and GPUs were used to accelerate the computation and a speed up gain of up is a factor 186 for 9241 bus system."}, {"label": 0, "content": "In a deregulated electricity market, power system operator should systematically identify the optimal schedule of renewable distributed generation (DG) units to not only optimize the market profits but also improve the network conditions. This paper proposes a parallel computation-based methodology using fuzzy logic designed in the structure of a genetic algorithm (GA). Due to the efficient communication among the processors during the optimization, the proposed fuzzy-based parallel computation GA (FPCGA) addresses the shortcoming of the classic GA in convergence speed and quality of results. The proposed optimization algorithm is utilized in this paper to identify the optimal daily schedule for the system operator including the energy purchased from 1) the power grid, 2) each wind turbine DG, and 3) each photovoltaic DG. The efficiency of the proposed method is verified by its implementation on a 136-bus distribution system and its effectiveness is compared with similar methods."}, {"label": 0, "content": "The residential rooftop solar penetration in the U.S. has increased rapidly over the past few years. This increase, if not properly accounted for, can lead to operational and reliability challenges for the electric power industry in the form of under-utilization of available energy, increase in costs, and reduction in environmental benefits, as demonstrated by the California Independent System Operator (CAISO) Duck Curve. The authors of this paper had previously developed a bottom-up approach for computing season-wise household-level residential energy consumption profiles using a synthetic population resource. In this paper, that model is enhanced to account for the effects that increasing percentages of rooftop solar penetration can have on the residential energy demand profiles of different regions. This information will be very useful to electric power utilities because it will help them efficiently manage the increasing numbers of residential rooftop solar installations in their supply areas."}, {"label": 0, "content": "Power distribution systems require continuous monitoring as the integration of renewable energy resources is increasing and the load demand is growing. The implementation of synchrophasors in distribution systems enhances the situational awareness of the system and provides a unique opportunity for developing new monitoring algorithms. This paper proposes a voltage monitoring algorithm based on the synchrophasor-based linear state estimation method. Particularly, the voltage monitoring algorithm combines a set of early warning indicators and the BDS independence test which can detect the voltage instability in a timely manner while avoiding false alarms when the system is still away from the stability boundary. Numerical study has been conducted in the Quebec test feeder to show the effectiveness of the method."}, {"label": 0, "content": "Deployment of distributed energy resources (DERs) introduces more dynamics to the distribution system, and brings system operators more challenges in monitoring and controlling the distribution system. These challenges can be addressed by developing methods for real time extraction of the operating state and model of the system, that is advanced state estimators for distribution systems with high penetration of DERs. This paper presents an object-oriented Distributed Quasi-Dynamic State Estimator (DQDSE) that employs three-phase detailed models and enables data from sensors to be streamed and used by the DQDSE. Based on three-phase detailed quasi-dynamic models that track slow dynamics (e.g., controls of power electronics, electromechanical transients of motors, etc.), DQDSE forms network measurement model by using network-wise measurements, performs quasi-dynamic state estimation, and provides the best estimate of the distribution system states. The proposed DQDSE has the following advantages: (1) detailed modeling approach ensures accurate results even when accommodating unbalanced and asymmetric systems; (2) the measurement set of DQDSE contains measurements from sensors as well as other measurement types to increase redundancy; (3) the distributed architecture of DQDSE enables fast data processing. The paper presents the method via an illustrative example that substantiates the effectiveness of DQDSE."}, {"label": 0, "content": "The introduction of distributed generators (DG) and other emerging technologies such as electric vehicle charging (EV) to distribution networks have influenced the philosophy of operating the distribution networks. With the presence of these technologies, the nature of the distribution networks are changing from being passive networks to active networks and in order to accommodate these changes, the way of controlling and operating distribution systems must be reconfigured and distribution system state estimation based real-time model is needed for a secure control and protection in distribution systems. The objective of this paper is to present a comparison between the branch-current based distribution system state estimation in polar and rectangular coordinates. Moreover, the inclusion of the synchronized measurements, obtained from Micro-PMU is discussed. The methods are conducted on the IEEE-13 bus distribution test feeder and results are discussed."}, {"label": 0, "content": "The spread of microgrids is one of the most promising recent trends in the field of power systems, as they can help integrate distributed power sources and other cutting-edge technological advancements into power systems. In order to facilitate their expansion, more research on microgrid protection is needed, as they pose serious challenges to or even totally invalidate traditional protection schemes. In this paper, a comprehensive protection scheme for microgrids is proposed. Our scheme is based on an already extensively researched framework based on dynamic state estimation (DSE) and aims to utilize it for microgrid protection. The individual protection zones of a microgrid are monitored by settingless relays which continuously receive measurements and perform DSE in the time domain to detect abnormalities. However, power faults are not the only root cause of abnormal measurements. Relays can also receive erroneous measurements due to hidden failures in the system or due to malicious actors that try to inject false measurements. For this reason, we add a centralized layer to our scheme. This layer receives measurements from all the settingless relays of the microgrid and uses DSE in the quasi-dynamic domain to determine whether a settingless relay detects an abnormality due to a fault or due to a reason that does not warrant tripping action, which allows us to block erroneous tripping actions. Therefore, our layered approach increases the security and dependability of microgrid protection compared to traditional protection schemes."}, {"label": 0, "content": "A statistical model for predicting the output power and energy of Solar Photovoltaics (PV) has been developed. The multiple input single output (MISO) system is based on Jackknife regression and generates PV power in kilo-watts in response to inputs that include irradiance, precipitation, ozone, ambient temperature and atmospheric aerosol components. The model is trained and tested on data from National Renewable Energy Laboratory and residual statistical tests are applied to validate the estimation results. An absolute error of less than 1 kW is observed for 90.6 % of the predicted values that corresponds to a percentage error of less than 8.33 % for the 12 kW system under study."}, {"label": 0, "content": "This paper discusses an optimal joint placement problem of phasor measurement units (PMUs) and flow measurement devices for ensuring topological observability of power systems under N-2 transmission contingencies. Previous relevant studies focus on topological observability with the least-cost deployment of PMUs and flow measurement devices. In comparison, besides minimizing the total investment cost of PMUs and flow measurements, the proposed optimal location-based joint placement model also optimizes their locations for avoiding PMUs on radial nodes and encouraging flow measurements incident to nodes that are adjacent to PMU installed buses. The proposed model can be equivalently formulated as a mixed-integer linear programming (MILP) problem and solved via a decomposition based algorithm. Effectiveness of the proposed model is verified via a 14-bus IEEE power system. Numerical results show that, compared to traditional optimal joint placement models, optimal locations obtained by the proposed approach lead to noticeable improvements in state estimation accuracy when time skew errors are considered."}, {"label": 0, "content": "Distributed energy resources (DER) systems introduce uncertainties in the electrical grid that cannot be addressed by classical deterministic methods. Power system analytic tools, such as Load Flow (LF), should be revisited to address such uncertainties and dependencies. Probabilistic Load Flow (PLF) provides a solution to this problem by handling these uncertainties as random variables, which addresses the rising need for fast and accurate sampling methods. Among the existing methods, the Unscented Transform (UT) has provided reliable and fast estimations. In this paper, three variants of PLF based on the UT method, with the effects of their weighting and scaling parameters are described, analyzed and compared in the IEEE 30 test case."}, {"label": 0, "content": "Cardiovascular diseases are the primary cause of deaths in the world. Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. Due to its high prevalence and associated risks, early detection of AF is an important objective for healthcare systems worldwide. The growing demand for medical assistance implies increased expenses, which could be limited by implementing ambulatory monitoring techniques based on wearable devices, thus, reducing the number of people requiring observation in hospitals. One of the main challenges in this context is related to the large amount of data from patients to be analyzed, which points to the suitability of using computational intelligence techniques for it. The selection of the features to be extracted from data plays a key role in order for any classifier of heart rhythm to provide good results in this regard. This paper demonstrates that it is possible to achieve an accurate detection of AF using a very low number of relatively simple features extracted from photoplethysmographic signals, enabling the use of affordable wearable devices (with scarce processing and data storage resources) with this purpose over long periods of time. This fact has been validated in experiments using data from real patients under medical supervision."}, {"label": 0, "content": "In this paper, a deep reinforcement learning-based robust control strategy for quadrotor helicopters is proposed. The quadrotor is controlled by a learned neural network which directly maps the system states to control commands in an end-to-end style. The learning algorithm is developed based on the deterministic policy gradient algorithm. By introducing an integral compensator to the actor-critic structure, the tracking accuracy and robustness have been greatly enhanced. Moreover, a two-phase learning protocol which includes both offline and online learning phase is proposed for practical implementation. An offline policy is first learned based on a simplified quadrotor model. Then, the policy is online optimized in actual flight. The proposed approach is evaluated in the flight simulator. The results demonstrate that the offline learned policy is highly robust to model errors and external disturbances. It also shows that the online learning could significantly improve the control performance."}, {"label": 0, "content": "Despite the increasing popularity of deep neural networks (DNNs), they cannot be trained efficiently on existing platforms, and efforts have thus been devoted to designing dedicated hardware for DNNs. In our recent work, we have provided direct support for the stochastic gradient descent (SGD) training algorithm by constructing the basic element of neural networks, the synapse, using emerging technologies, namely memristors. Due to the limited performance of SGD, optimization algorithms are commonly employed in DNN training. Therefore, DNN accelerators that only support SGD might not meet DNN training requirements. In this paper, we present a memristorbased synapse that supports the commonly used momentum algorithm. Momentum significantly improves the convergence of SGD and facilitates the DNN training stage. We propose two design approaches to support momentum: 1) a hardware friendly modification of the momentum algorithm using memory external to the synapse structure, and 2) updating each synapse with a built-in memory. Our simulations show that the proposed DNN training solutions are as accurate as training on a GPU platform while speeding up the performance by 886\u00d7 and decreasing energy consumption by 7\u00d7, on average."}, {"label": 0, "content": "Optical combustion measurement and analysis systems using multiple sensors have received considerable attention. In particular, the image-based flame 3D reconstruction approaches using computerized tomography have been widely applied for the flame 3D reconstruction from a set of views by constructing the optimized linear combinations of the 3D scene and projected images. Previous techniques were easily computed but were weak against noise and blurring due to the underlying least square-based loss function. This paper presents a 3D density flame reconstruction method, captured from the sparse multi-view images, as a constrained optimization problem between the flame and its projected images. For effective estimation of the flame with a complicated structure in an arbitrary viewpoint, we extract the 3D candidate region of the flame and, then, estimate the density field using the compressive sensing. The objective function is a linear combination of the photo consistency cost and sparsity regularization terms, which avoids blurring in the reconstruction. The proposed approach is a powerful matrix factorization method with each voxel represented as a linear combination of a small number of basis vectors. The approach also effectively simplifies the reconstruction process and provides the whole 3D density field in one step. The experimental results verify that the proposed 3D density estimation performs favorably from the few flame images."}, {"label": 0, "content": "In real-time applications, a fast and robust visual tracker should generally have the following important properties: 1) feature representation of an object that is not only efficient but also has a good discriminative capability and 2) appearance modeling which can quickly adapt to the variations of foreground and backgrounds. However, most of the existing tracking algorithms cannot achieve satisfactory performance in both of the two aspects. To address this issue, in this paper, we advocate a novel and efficient visual tracker by exploiting the excellent feature learning and classification capabilities of an emerging learning technique, that is, extreme learning machine (ELM). The contributions of the proposed work are as follows: 1) motivated by the simplicity and learning ability of the ELM autoencoder (ELM-AE), an ELM-AE-based feature extraction model is presented, and this model can provide a compact and discriminative representation of the inputs efficiently and 2) due to the fast learning speed of an ELM classifier, an ELM-based appearance model is developed for feature classification, and is able to rapidly distinguish the object of interest from its surroundings. In addition, in order to cope with the visual changes of the target and its backgrounds, the online sequential ELM is used to incrementally update the appearance model. Plenty of experiments on challenging image sequences demonstrate the effectiveness and robustness of the proposed tracker."}, {"label": 0, "content": "Reliability classification of gear safety has long been a challenging issue in transmission industry because of complicated calculations and great classification errors of coupled parameters with insufficient data. This paper proposes a model based on generative adversarial network (GAN) as pretreatment to improve the accuracy of reliability classification. First, we present bounded-GAN to generate gear data within required boundaries without massive computations. In bounded-GAN, three bounded layers are designed to bound generated data in terms of different data characteristics; smooth targets are developed to enhance the ability of generating high-quality instances by the generator; Adam optimizer is used to train both generator and discriminator to avoid nonconvergence. Second, to overcome unlabeling defect of bounded-GAN, a mean-covariance labeling scheme is introduced to label the data according to the nearest classes of gear reliability within specific ranges. Finally, original and qualified data are combined to train classifiers. Simulations on gear data from industry show that our proposed model outperforms other methods on operational metrics."}, {"label": 0, "content": "Effective hyperspectral unmixing (HU) is essential to the estimation of the underlying materials' signatures (endmember signatures) and their spatial distributions (abundance maps) from a given image (data) of a hyperspectral scene. Recently, investigating HU under the non-negligible endmember variability (EV) and outlier effects (OE) has drawn extensive attention. Some state-of-the-art works either consider EV or consider OE, but none of them considers both EV and OE simultaneously. In this paper, we propose a novel HU algorithm, referred to as the variability/outlier-insensitive multi-convex unmixing (VOIMU) algorithm, which is robust against both EV and OE. Considering two suitable regularizers, a nonconvex minimization problem is formulated for which the perturbed linear mixing model proposed by Thouvenin et al., is used for modeling EV, while OE is implicitly handled by applying a p quasi-norm to the data fitting with 0 <; p <; 1 . Then, we reformulate it into a multi-convex problem which is then solved by the block coordinate descent method, with convergence guarantee by casting it into the block successive upper bound minimization framework. The proposed VOIMU algorithm can yield a stationary-point solution with convergence guarantee, together with some intriguing information of potential outlier pixels though outliers are neither physically modeled in the above problem nor detected in the algorithm operation. Finally, we provide some simulation results and experimental results using real data to demonstrate the efficacy and practical applicability of the proposed VOIMU algorithm."}, {"label": 0, "content": "Energy imbalance market (EIM) provides an opportunity that allows larger shares of variable renewable energy sources in the grid. Under highly volatile weather conditions, an accurate forecasting of photovoltaic (PV) power is necessary for grid stability and market operation. Most of existing forecasting methods strongly rely on the accuracy of measurements, and the adaptability of these methods to complex weather conditions is rarely discussed. In this paper, a weather classification multivariate adaptive regression spline (MARS) forecasting model is introduced for complex weather conditions in all seasons. It can be updated incrementally and its high computational efficiency satisfies EIM operations. A data set that consists of the historical power and meteorological parameters produced by a small-scale PV platform is classified and used to train MARS models with forecast horizons ranging from 15 min to 24 h in different seasons. The tests and analyses results indicate higher accuracy, adaptability, and efficiency of the novel model."}, {"label": 0, "content": "Both road users and administrators are keen to know the traffic volume at the arbitrary point on the road network. In China, charging systems have been fully established in closed large-regional freeway networks. They have accumulated massive amounts of toll collection data and provided a possible method to forecast unknown traffic volume at any designated cross-section located on a freeway. A systematic method is proposed to derive the traffic volume step-by-step. First, the average traveling speed is obtained for each vehicle on its shortest path. Then, the traveling time is estimated in each road segment. Finally, the historical traffic volume is derived at the designated cross-section. To make the obtained traffic volume data more practical, a deep learning-based autoencoder is used for forecasting the traffic volume and evaluating its prediction accuracy. All these proposed methods are evaluated with a collection of toll data for one month covering more than 5000 km of freeway under a centralized regional charging system. One location is randomly selected as the designated cross section at 2 km from the upstream toll gate on a road segment of the Xi\u2019an ring. The experimental results show the effectiveness and satisfactory accuracy of predicting the traffic volume in the designated cross-section compared with the data captured by the traffic video detection equipment. Rapid and successful prediction from available toll collection data may provide a practical method for deriving the traffic information without installing any additional regularly maintained detectors and equipment on the freeway."}, {"label": 0, "content": "This paper aims at the fault-tolerant adaptive control for the nonlinearly parameterized systems with mismatched disturbances. A novel adaptive control scheme is developed in this paper by adding a power integrator technique. The mismatched disturbances are estimated by a designed disturbance observer. A new fault-tolerant control method is proposed to deal with the uncertain actuator faults including actuator stuck. The proposed adaptive controller ensures that the closed-loop system is input-to-state stable. Simulation results for a robotic arm system are given to present the effectiveness of the developed control technique."}, {"label": 0, "content": "The Internet of Things (IoT) provides a new paradigm for the development of heterogeneous and distributed systems, and it has increasingly become a ubiquitous computing service platform. However, due to the lack of sufficient computing and storage resources dedicated to the processing and storage of huge volumes of the IoT data, it tends to adopt a cloud-based architecture to address the issues of resource constraints. Hence, a series of challenging security and trust concerns have arisen in the cloud-based IoT context. To this end, a novel trust assessment framework for the security and reputation of cloud services is proposed. This framework enables the trust evaluation of cloud services in order to ensure the security of the cloud-based IoT context via integrating security- and reputation-based trust assessment methods. The security-based trust assessment method employs the cloud-specific security metrics to evaluate the security of a cloud service. Furthermore, the feedback ratings on the quality of cloud service are exploited in the reputation-based trust assessment method in order to evaluate the reputation of a cloud service. The experiments conducted using a synthesized dataset of security metrics and a real-world web service dataset show that our proposed trust assessment framework can efficiently and effectively assess the trustworthiness of a cloud service while outperforming other trust assessment methods."}, {"label": 0, "content": "Existing IoT services are based on data communications technologies that do not involve the public switched telephone network (PSTN). Since the telephone numbers have been assigned to machine-type devices, PSTN switches can play a role in IoT service routing. In this article we deploy a PSTN-based IoT mechanism where the interaction between the users and the IoT devices is achieved through PSTN switches. To our knowledge, this is the first PSTN-based IoT solution in the world. With this mechanism, all PSTN customer premises equipment (CPE; fixed-line and mobile phones) can access IoT services without installing any software (mobile apps). By reusing the existing PSTN infrastructure, PSTN-based IoT offers telecom-grade service, security, and network management for IoT, which are very expensive to build in non-PSTN-based IoT. Our approach conveniently enables the existing CPE to access IoT applications, which will significantly promote the IoT service industry."}, {"label": 0, "content": "The 13 articles in this special section focus on security and privacy in wireless Internet of Things (IoT). IoT is a paradigm that involves networked physical objects with embedded technologies to collect, communicate, sense, and interact with the external environment through wireless or wired connections. With rapid advancements in IoT technology, the number of IoT devices is expected to surpass 50 billion by 2020, which has also drawn the attention of attackers who seek to exploit the merits of this new technology for their own benefits. There are many potential security and privacy threats to IoT, such as attacks against IoT systems and unauthorized access to private information of end users. As IoT starts to penetrate virtually all sectors of society, such as retail, transportation, healthcare, energy supply, and smart cities, security breaches may be catastrophic to the actual users and the physical world. To tackle the security challenges in the design of future wireless IoT systems, we have organized this Special Issue focusing on the security, privacy, and performance of future wireless IoT. "}, {"label": 0, "content": "IoT is leading a digital revolution in both academia and industry. It brings convenience to people's daily lives; however, the issues of security and privacy of IoT become challenges. Blockchain, a decentralized database based on cryptographic techniques, is promising for IoT security, which may influence a variety of areas including manufacture, finance, and trading. The blockchain framework in an IoT system is an intriguing alternative to the traditional centralized model, which is struggling to meet some specified demands in IoT. In this article, we investigate typical security and privacy issues in IoT and develop a framework to integrate blockchain with IoT, which can provide great assurance for IoT data and various functionalities and desirable scalability including authentication, decentralized payment, and so on. We also suggest some possible solutions to these security and privacy issues in IoT based on blockchain and Ethereum to show how blockchain contributes to IoT."}, {"label": 0, "content": "The Internet of Things (IoT) is becoming truly ubiquitous in our everyday lives, but it also faces unique security challenges. Intrusion detection is critical for the security and safety of a wireless IoT network. This article discusses the human-in-theloop active learning approach for wireless intrusion detection. We first present the fundamental challenges against the design of a successful intrusion detection system for a wireless IoT network. We then briefly review the rudimentary concepts of active learning and propose its employment in the diverse applications of wireless intrusion detection. An experimental example is also presented to show the significant performance improvement of the active learning method over the traditional supervised learning approach. While machine learning (ML) techniques have been widely employed for intrusion detection, the application of human-in-the-loop ML that leverages both machine and human intelligence to intrusion detection of IoT is still in its infancy. We hope this article can assist readers in understanding the key concepts of active learning and spur further research in this area."}, {"label": 0, "content": "An increasing number of wireless intelligent equipment is applied to ICS networks. However, it is virtually impossible to use regular encryption methods and security patches to enhance the security level of legacy equipment in ICS networks due to weak computing and storage capabilities of the equipment. To address these concerns, a hybrid-augmented device fingerprinting approach is developed to enhance traditional intrusion detection mechanisms in the ICS network. Taking the advantage of the simplicity of the program process and stability of hardware configurations, we first measure inter-layer data response processing time, and then analyze network traffic to filter abnormal packets to achieve the intrusion classification and detection in ICS networks. The device fingerprinting- based intrusion classification and detection approach is evaluated using the data collected from a lab-level micro-grid, and forgery attacks and intrusions are launched against the proposed method to investigate its robustness and effectiveness."}, {"label": 0, "content": "The ubiquity of 802.11 WiFi and the miniaturization as a result of Moore's law has recently enabled the success of IoT. From smart lightbulbs to smart toasters, many home appliances are now becoming both Internet-enabled and interconnected through WiFi. Soon, these futuristic smart homes will be able to run themselves, allowing the human operators to be fully in control of their homes - or will they? Despite the physical advancements made since the '90s, the same cannot be said of the vulnerabilities of these smart devices. We analyze a set of common smart home appliances - a lightbulb, power switch, motion sensor, security camera, and home assistant - putting their vulnerabilities to the test to see what a 21st century home intruder could discover."}, {"label": 0, "content": "Various security threats have prompted covert timing channels to become an important alternative for the transmission of confidential information in an untrusted Internet of Things (IoT). This article aims to demonstrate the susceptibility of IoT to covert timing channels over mobile networks. It presents the system model of a covert timing channel for IoT and then analyzes whether the traditional covert timing channels based on inter-packet delays apply to IoT over 4G/5G networks. Given that there are so many covert timing channels proposed for computer networks, we investigate different kinds of construction approaches of covert timing channels to illustrate the feasibility of building covert timing channels for IoT, including packet-reordering-based, rateswitching- based, packet-loss-based, retransmission- based, and scheduling-based covert timing channels. Furthermore, this article also discusses several detection methods of revealing and preventing covert timing channels for IoT."}, {"label": 0, "content": "Wireless channels are increasingly being used to transmit highly sensitive information in the Internet of Things. However, traditional cryptography methods cannot solve all security problems in IoT systems. Covert wireless communication can prevent an adversary from knowing the existence of a user's transmission; thus, it can provide stronger security protection for IoT devices. In this article, we consider the covert communication in a noisy wireless network, and find that the uncertainty of the aggregated interference experienced by the adversary is beneficial to the potential transmitters. From the network perspective, wireless communications can be hidden in the interference of the noisy wireless network, which the adversary observes as a \"shadow\" network to a certain extent. Furthermore, we discuss some new results on the effect of active eavesdropper. Square root law becomes invalid, and even jammer-assisted schemes have little effect on the covertness in the presence of active eavesdropper. Finally, we provide a vision for future research."}, {"label": 0, "content": "High-dimensional and sparse (HiDS) matrices generated by recommender systems contain rich knowledge regarding various desired patterns like users\u2019 potential preferences and community tendency. Latent factor (LF) analysis proves to be highly efficient in extracting such knowledge from an HiDS matrix efficiently. Stochastic gradient descent (SGD) is a highly efficient algorithm for building an LF model. However, current LF models mostly adopt a standard SGD algorithm. Can SGD be extended from various aspects in order to improve the resultant models\u2019 convergence rate and prediction accuracy for missing data? Are such SGD extensions compatible with an LF model? To answer them, this paper carefully investigates eight extended SGD algorithms to propose eight novel LF models. Experimental results on two HiDS matrices generated by real recommender systems show that compared with an LF model with a standard SGD algorithm, an LF model with extended ones can achieve: 1) higher prediction accuracy for missing data; 2) faster convergence rate; and 3) model diversity."}, {"label": 0, "content": "This paper proposes a unified framework to design sliding-mode control for stabilization of delayed memristive neural networks (DMNNs) with external disturbances. Under the presented framework, finite-time stabilization, and fixed-time stabilization of the controlled DMNNs can be, respectively, obtained by choosing different values for a specific control parameter. It is proved that the system responses can be made reaching the designed sliding-mode surface in finite and fixed time, and then stay on it. Moreover, it also illustrates that the inevitable external disturbances can be rejected by the designed sliding-mode control. Finally, the efficiency and superiority of the obtained main results are verified by comparisons with related works and numerical simulations."}, {"label": 0, "content": "The consumer Internet of Things (IoT) platforms are gaining high popularity. However, due to the open nature of wireless communications, smart home platforms are facing many new challenges, especially in the aspect of security and privacy. In this article, we first introduce the architecture of current popular smart home platforms and elaborate the functions of each component. Then we discuss the security and privacy challenges arising from these platforms and review the state of the art of the proposed countermeasures. We give a comprehensive survey on several new attacks on the voice interface of smart home platforms, which aim to gain unauthorized access and execute over-privileged behaviors to compromise the user's privacy. To thwart these attacks, we propose a novel voice liveness detection system, which analyzes the wireless signals generated by IoT devices and the received voice samples to perform user authentication. We implement a real-world testbed on Samsung's SmartThings platform to evaluate the performance of the proposed system, and demonstrate its effectiveness."}, {"label": 0, "content": "Wireless IoT is a promising area in which large quantities of data and information are collected, exchanged, and stored constantly. While these data and information are utilized for good purposes in wireless IoT, great risks arise in that attackers are seeking to exploit the merits of this new technology for their own benefits. In such a resource-constrained environment, devices are easily exposed to various attacks. Confidential private information is as valuable as it is vulnerable. It is difficult for end users to trust wireless IoT and to adopt related applications due to the lack of satisfactory privacy-preserving mechanisms, which has become the major obstacle for the adoption and popularization of wireless IoT. In this article, we first introduce the classical application scenarios of wireless IoT, along with related security and privacy attack models. We then present a brief overview of privacy-preserving schemes of wireless IoT. Based on the classification of application scenarios, we further present recent advances in privacy-preserving mechanisms in wireless IoT. Finally, we discuss open issues and future research directions for these application scenarios in wireless IoT."}, {"label": 0, "content": "Recently, the Internet of Things (IoT) has penetrated many aspects of the physical world to realize different applications. Through IoT, these applications generate, exchange, aggregate, and analyze a vast amount of security-critical and privacy- sensitive data, which makes them attractive targets of attacks. Therefore, it is rather necessary for IoT systems to be equipped with the ability to resist security and privacy risks when fulfilling the desired functional requirements and services. To achieve these goals, there are many new challenges for IoT to implement privacy preserving data manipulation. First, data analysts need to process privacy-sensitive data to extract the expected information without privacy disclosure. In addition, many privacy related factors, including privacy valuation and risk assessment, affect sensitive and private data trading between data owners and requesters. Moreover, the data owners' security behavior also plays an important role in privacy protection in IoT applications. Concerning these issues, this article introduces and surveys privacy preserving techniques in the processes of data aggregation, trading, and analysis: the balance between data analysis and privacy preservation from the data analysts' perspective, secure data trading from the perspective of data owners and requesters, and secure private data aggregation from the data owners' perspective."}, {"label": 0, "content": "With the diversification of location-based services in vehicle networks, users can obtain such services through submitting searching locations and points of interest. However, users may worry that their real locations and other privacy information will leak out when they get such services, so appropriate location privacy protection measures are necessary. Traditional location privacy protection, such as K-anonymous, cannot be carried out directly because of the characteristics of vehicle networks, such as high mobility. In order to solve such problems, this article proposes a strategy combining cache strategy with K-anonymous that can not only satisfy users' demand on obtaining required services with lowest cost, but also protect the location privacy of users. Specifically, on one hand, cache strategy that deploys part of services on roadside units in advance is used to maximize the satisfaction of users' service requests. On the other hand, each user is set with a K value to meet the need of K-anonymous. The trade-off of these two aspects guarantees users' services and the location privacy. The experiments show that the strategy proposed in this article is better than other strategies."}, {"label": 0, "content": "While enjoying the convenience brought by the Internet of Things (IoT), people also encounter many problems with wireless sensor networks (WSNs), the foundation of IoT. Security problems are especially of concern. In this article, we focus on location privacy, which is a major security issue in WSNs, and propose a k-means cluster-based location privacy (KCLP) protection scheme for IoT. To protect the source location, fake source nodes are used to simulate the function of the real sources. Then, to protect the sink location privacy, fake sink nodes and a specific transmission pattern are utilized. In order to improve safety time, a k-means cluster is applied to create clusters and fake packets that must pass through the area. Compared to contrasting algorithms, the KCLP scheme can increase the safety time and reduce delay at minor expense in energy consumption."}, {"label": 0, "content": "The proliferation of wireless devices and appliances is facilitating the rapid development of the Internet of Things (IoT). Numerous state-of-the-art applications are being used in, for example, smart cities, autonomous vehicles, and biocomputing. With the popularization of IoT, new challenges are emerging with respect to privacy issues. In this article, we first summarize privacy constraints and primary attacks based on new features of IoT. Then we present three case studies to demonstrate principal vulnerabilities and classify existing protection schemes. Built on this analysis, we identify three key challenges: a lack of theoretical foundation, the trade-off optimization between privacy and data utility, and system isomerism over-complexity and high scalability. Finally, we illustrate possible promising future directions and potential solutions to the emerging challenges facing wireless IoT scenarios. We aim to assist interested readers in investigating the unexplored parts of this promising domain."}, {"label": 0, "content": "With the increasing popularity of cloud/fog computing and because of the limited computing capability of wireless Internet of Things (IoT) terminals, big data have been sent to clouds/ fogs for analysis and processing in wireless IoT. However, how to carry out tensor analysis and processing without compromising security and privacy is a challenge in cloud/fog-based wireless IoT applications. Tensors have emerged as powerful tools for multi-dimensional data analysis and processing in wireless IoT applications. In this article, we propose novel privacy-preserving tensor analysis and processing models in cloud/ fog computing for wireless IoT applications. More specifically, we present a privacy-preserving cyber-physical-social big data processing model in cloud, privacy-preserving tensor analysis, a processing model based on tensor train networks in cloud-fog computing, and an optimization model for privacy-preserving tensor analysis and processing. We introduce a social recommendation system in smartphones as an example demonstrating the security and effectiveness of the proposed models for wireless IoT."}, {"label": 0, "content": "We study a communication scheduling and remote estimation problem within a worst-case scenario that involves a strategic adversary. Specially, a remote sensing system consisting of a sensor, an encoder and a decoder is configured to observe, transmit, and recover a discrete time stochastic process. At each time step, the sensor makes an observation on the state variable of the stochastic process. The sensor is constrained by the number of transmissions over the time horizon, and thus it needs to decide whether to transmit its observation or not after making each measurement. If the sensor decides to transmit, it sends the observation to the encoder, who then encodes and transmits the observation to the decoder. Otherwise, the sensor and the encoder maintain silence. The decoder is required to generate a real-time estimate on the state variable. The sensor, the encoder, and the decoder collaborate to minimize the sum of the communication cost for the sensor, the encoding cost for the encoder, and the estimation error for the decoder. There is also a jammer interfering with the communication between the encoder and the decoder, by injecting an additive channel noise to the communication channel. The jammer is charged for the jamming power and is rewarded for the estimation error generated by the decoder, and it aims to minimize its net cost. We consider a feedback Stackelberg game with the sensor, the encoder, and the decoder as the composite leader, and the jammer as the follower. Under some technical assumptions, we obtain a feedback Stackelberg solution, which is threshold based for the scheduler, and piecewise affine for the encoder and the decoder. We also generate numerical results to demonstrate the performance of the remote sensing system under the feedback Stackelberg solution."}, {"label": 0, "content": "We consider the problem of minimizing a block separable convex function (possibly nondifferentiable, and including constraints) plus Laplacian regularization, a problem that arises in applications including model fitting, regularizing stratified models, and multi-period portfolio optimization. We develop a distributed majorization-minimization method for this general problem, and derive a complete, self-contained, general, and simple proof of convergence. Our method is able to scale to very large problems, and we illustrate our approach on two applications, demonstrating its scalability and accuracy."}, {"label": 0, "content": "Passive acoustic monitoring is emerging as a promising solution to the urgent, global need for new biodiversity assessment methods. The ecological relevance of the soundscape is increasingly recognised, and the affordability of robust hardware for remote audio recording is stimulating international interest in the potential for acoustic methods for biodiversity monitoring. The scale of the data involved requires automated methods, however, the development of acoustic sensor networks capable of sampling the soundscape across time and space and relaying the data to an accessible storage location remains a significant technical challenge, with power management at its core. Recording and transmitting large quantities of audio data is power intensive, hampering long-term deployment in remote, off-grid locations of key ecological interest. Rather than transmitting heavy audio data, in this paper, we propose a low-cost and energy efficient wireless acoustic sensor network integrated with edge computing structure for remote acoustic monitoring and in situ analysis. Recording and computation of acoustic indices are carried out directly on edge devices built from low noise primo condenser microphones and Teensy microcontrollers, using internal FFT hardware support. Resultant indices are transmitted over a ZigBee-based wireless mesh network to a destination server. Benchmark tests of audio quality, indices computation and power consumption demonstrate acoustic equivalence and significant power savings over current solutions."}, {"label": 0, "content": "This paper investigates the differentially private problem of the average consensus for a class of discrete-time multi-agent network systems (MANSs). Based on the MANSs, a new distributed differentially private consensus algorithm (DPCA) is developed. To avoid continuous communication between neighboring agents, a kind of intermittent communication strategy depending on an event-triggered function is established in our DPCA. Based on our algorithm, we carry out the detailed analysis including its convergence, its accuracy, its privacy and the trade-off between the accuracy and the privacy level, respectively. It is found that our algorithm preserves the privacy of initial states of all agents in the whole process of consensus computation. The trade-off motivates us to find the best achievable accuracy of our algorithm under the free parameters and the fixed privacy level. Finally, numerical experiment results testify the validity of our theoretical analysis."}, {"label": 0, "content": "While gait recognition is the mapping of a gait sequence to an identity known to the system, gait authentication refers to the problem of identifying whether a given gait sequence belongs to the claimed identity. A typical gait authentication system starts with a feature representation such as a gait template, then proceeds to extract its features, and a transformation is ultimately applied to obtain a discriminant feature set. Almost every authentication approach in literature favours the use of Euclidean distance as a threshold to mark the boundary between a legitimate subject and an impostor. This article proposes a method that uses the posterior probability of a Bayes' classifier in place of the Euclidean distance. The proposed framework is applied to template-based gait feature representations and is evaluated using the standard CASIA-B gait database. Our study experimentally demonstrates that the Bayesian posterior probability performs significantly better than the de facto Euclidean distance approach and the cosine distance which is established in research to be the current state of the art."}, {"label": 0, "content": "A Flexible Machine Vision (FMV) Inspection System has been developed that requires minimal retuning in hardware and software as applications are changed up. The flexibility of the system was evaluated by applying it to an inspection problem with three different types of small parts: plastic gears, plastic connectors and metallic coins, with minimal retuning when moving from one application to the others. The system was required to differentiate between 4 different known styles of each part plus one unknown style, for a total of 5 classes. In previous work, a hybrid Support Vector Machine (SVM) classifier was developed for the connector application. When applied to the coin application, the hybrid SVM could not achieve the target performance of 95% accuracy. A new hybrid that method that combines SVM and an Artificial Neural Network (ANN) or ANN-SVM classifier was subsequently developed to overcome this problem and the results are presented in this paper. The image library used in this study is available at http://my.me.queensu.ca/People/Surgenor/Laboratory/Database.html."}, {"label": 0, "content": "Device Fingerprinting (DFP) is the identification of a device without using its network or other assigned identities including IP address, Medium Access Control (MAC) address, or International Mobile Equipment Identity (IMEI) number. DFP identifies a device using information from the packets which the device uses to communicate over the network. Packets are received at a router and processed to extract the information. In this paper, we worked on the DFP using Inter Arrival Time (IAT). IAT is the time interval between the two consecutive packets received. This has been observed that the IAT is unique for a device because of different hardware and the software used for the device. The existing work on the DFP uses the statistical techniques to analyze the IAT and to further generate the information using which a device can be identified uniquely. This work presents a novel idea of DFP by plotting graphs of IAT for packets with each graph plotting 100 IATs and subsequently processing the resulting graphs for the identification of the device. This approach improves the efficiency to identify a device DFP due to achieved benchmark of the deep learning libraries in the image processing. We configured Raspberry Pi to work as a router and installed our packet sniffer application on the Raspberry Pi. The packet sniffer application captured the packet information from the connected devices in a log file. We connected two Apple devices iPad4 and iPhone 7 Plus to the router and created IAT graphs for these two devices. We used Convolution Neural Network (CNN) to identify the devices and observed the accuracy of 86.7%."}, {"label": 0, "content": "Recent years, EMG has attracted much attention as a tool of human interface. In hand motion recognition and personal authentication using wrist EMG, we have obtained good results. However, there has been no way to establish them at the same time. Therefore, in this paper we measure EMG by attaching dry type sensors to wrist, and carry out hand motion recognition and personal authentication. The conventional method used EMG of movement Japanese Janken. We use a multi-input and multi-output model of a Convolutional Neural Network (CNN). The average accuracy of hand motion recognition is 94.5%. The average accuracy of personal authentication is 94.6%. In the conventional method, personal authentication was classified into two classes. However, we carry out multi-class classification in the proposed method. In feature extraction, we obtain 128\u00d78 input data from the measuring unit. Then, a filter size of the convolution layers is 3 \u00d7 3. CNN does not contain pooling layers in this paper. In the proposed method, the average accuracy of hand motion recognition is 94.6%. The average accuracy of personal authentication is 95.0%."}, {"label": 0, "content": "The handling of non-rigid objects, such as cables, with industrial robots is characterized by nonlinear, timedependent and location-dependent equations for the object's behavior. To manipulate them in a desired way, real-time capable simulations for control that are able to run in cycle-time are required as well as highly accurate and detailed simulations for path planning. We introduce a common simulation environment that is wrapped in a container with the advantages of including all dependencies and being easy to set up and orchestrate. Automated code generation is used to set up a standardized communication and virtualization in a Docker container such that only the simulation itself has to be provided by a user. The advantage of containerization, compared to the simulation running natively, is finally demonstrated with a sample robot simulation under heavy CPU load."}, {"label": 0, "content": "Farming of soft shell crab has been practiced in south-east Asian countries such as Indonesia. In the crab farming, a poor water quality increases the mortality rate of the crab in the pond. Then, in this paper, we propose a design and implementation of a water quality monitoring system for crab farming using IoT technology to give awareness to a farmer for maintaining acceptable levels of water quality in the pond. Hence, it contributes to increase the survival rate of crab and achieve higher yield of soft shell crab. Our proposed system uses a LoRa-based wireless sensor network and a lightweight Message Queuing Telemetry Transport (MQTT) protocol for exchanging messages between small embedded devices, mobile devices, and sensors. The system mainly consists of sensor node as publishers, and Raspberry pi MQTT broker, and mobile client devices as subscribers. The sensor nodes are built with small embedded devices, LoRa wireless interface, and water quality sensors, i.e. water temperature sensor, pH sensor, and salinity sensor. We also setup a web-based monitoring application using node-red dashboard for accessing water quality levels remotely."}, {"label": 0, "content": "A soft ring-shaped actuator is inspired by the human stomach. This actuator can contract inward, mimicking the movements of the human stomach. It can benefit the research field of medical science and food engineering. In this paper, we investigate the deformation of such actuator employing finite-element simulation and motion tracking. Simulations are carried out to examine the influence of different pressure on the performance of the actuator. Motion tracking system is applied to track the mid-points on the deformed surface when the actuator is under pressurisation. The results show that the actuator can achieve axisymmetric contraction when inflated. The principal movements of mid-points are on the horizontal plane, whereas the change in the axial direction is negligible. This investigation can benefit the understanding of the profile of the deformed membrane and the construction of the mathematical modelling."}, {"label": 0, "content": "The environmental problems and associated issues had been a source of worry for the world. Emerge of IoT (Internet of things) and the step towards the smart approach such as smart cities, smart buildings, and smart grid have really posed the successful implementation of IoT. The success is only possible in the real sense when the problematic issues can be addressed. This paper proposes an Internet of Things Technology based protection and monitoring of environment of a poultry house. The proposed software based hardware is capable of monitoring the environment related parameters such as air temperature, air humidity, O2, CO2 level of concentration and NH3 concentration. The wireless sensor is responsible for the effective data collection of the described parameters and also these are source coordination and control. The hardware is implemented successfully at different sites within the poultry shed. The experimental setup was found very effective and accurate. This scheme will earn a safe environment and profit to the poultry industry."}, {"label": 0, "content": "We present NavREn-RL, an approach to NAVigate an unmanned aerial vehicle in an indoor Real ENvironment via end-to-end reinforcement learning (RL). A suitable reward function is designed keeping in mind the cost and weight constraints for micro drone with minimum number of sensing modalities. Collection of small number of expert data and knowledge based data aggregation is integrated into the RL process to aid convergence. Experimentation is carried out on a Parrot AR drone in different indoor arenas and the results are compared with other baseline technologies. We demonstrate how the drone successfully avoids obstacles and navigates across different arenas. Video of the drone navigating using the proposed approach can be seen at https://youtu.be/yOTkTHUPNVY."}, {"label": 0, "content": "The main purpose of this paper is to build an embedded platform based on TMS320VC5509A processor, and finally realizes the recognition of snore and controls pillow change the height to relieve snoring symptoms. The whole embedded hardware platform collects the sounds through the microphone module, and completes data storage, data processing and data interaction through other peripherals. After a series of pre-processing operations, short-time energy and short-time zero crossing rate dual threshold detection is selected as endpoint recognition algorithm, MFCC(Mel Frequency Cepstral Coefficient) is selected as feature extraction and KNN(k-nearest neighbors algorithm) is selected as recognition algorithm. The experimental results show that this hardware system can run well and the design is reasonable. At the same time, it can also achieve a good accuracy in snore analysis and recognition."}, {"label": 0, "content": "Livestock is an essential commodity, especially in Indonesia, an agricultural country located in the Asia Pacific. Indonesia has vaster Greenland area than any other ASEAN countries. One way to increase livestock production is by implementing Smart Livestock Monitoring System which relies on Internet of Things technology such as LoRa. LoRa and other Low Power Wide Area (LPWA) technologies offer great advantages such as extended transmission range, extended node's battery life, and a massive number of nodes per gateway support. However, LoRa utilizes sub 1 GHz unlicensed spectrum which has been very crowded since the last decades and will be even more crowded soon following IoT trends. We propose to use mobile LoRa gateway to mitigate this issue. This paper simulates and compares deployments of LoRa using one mobile gateway with one and multiple static gateways. LoRa is simulated using one mobile gateway and compared with one and multiple static gateway deployments. Simulation result shows that one static gateway is better for narrow livestock area because it gives sufficient Data Extraction Rate (DER) value required for data transmission and lowest Network Energy Consumption (NEC) value. Otherwise, for vast livestock area, one mobile gateway is better because of smaller deployment cost and more than sufficient DER value."}, {"label": 0, "content": "Non-rigid registration is a crucial step for many applications such as motion tracking, model retrieval, and object recognition. The accuracy of these applications is highly dependent on the initial position used in registration step. In this paper we propose a novel Convex Hull Aided Coarse Registration refined by two algorithms applied on projected points. Firstly, the proposed approach uses a statistical method to find the best plane that represents each point cloud. Secondly, all the points of each cloud are projected onto the corresponding planes. Then, two convex hulls are extracted from the two projected point sets and then matched optimally. Next, the non-rigid transformation from the reference to the model is robustly estimated through minimizing the distance between the matched point's pairs of the two convex hulls. Finally, this transformation estimation is refined by two methods. The first one is the refinement of coarse registration by Iterative Closest Point (ICP). The second one consists of the refinement of coarse registration by the Normal Distribution Transform (NDT). An experimental study, carried out on several clouds, shows that the refinement of coarse registration with ICP gives, in the most cases, a better result than refinement with NDT."}, {"label": 0, "content": "In the engineering phase of modern manufacturing systems, simulation-based methods and tools have been established to face the increasing demands on time-efficiency and profitability. In the application of these simulation solutions, model-based digital twins are created, as multi-domain simulation models to describe the behavior of the manufacturing system. During the production process, a data-driven digital twin arises in the context of industry 4.0 based on an increasing networking and new cloud technologies. Recent developments in machine learning offer new possibilities in conjunction with the digital twin. These range from data-based learning of models to learning control logic of complex systems. This paper proposes a combined model-based and data-driven concept of a digital twin. It shows how to use machine learning in connection with these models, in order to archive faster development times of manufacturing systems."}, {"label": 0, "content": "As information technology has advanced in recent years, services which include personal authentication systems such as ATM are increasing. Current main personal authentication systems include IC cards, passwords, and biometrics authentication such as fingerprint authentication. However, there are several problems in these systems. Therefore, better systems are needed. As such systems, we propose a method to write numerals in the air using the Leap motion and to carry out personal authentication from such aerial handwriting data. We try to authenticate numerals 0 to 9 which are written by three subjects. After applying some pre-processing to inputs, learning and identification are carried out using CNN which is a method of machine learning. As a result, average identification accuracy was 90.3%. From this result, it is suggested that input numerals in the air can be authenticated and there is a possibility to construct a new personal authentication system."}, {"label": 0, "content": "An ultra low power acoustic wake-up detector based on high frequency signal analysis is presented in this paper. Focused on environmental or military Internet of Things (IoT) applications, it aims at detecting in real time the presence of specific animal species or drones for generating alerts and for triggering power consuming tasks such as high frequency signal recording only when needed. This wake-up detector continuously monitors the presence of specific frequencies in an analog acoustic signal, with a good frequency selectivity and a high frequency detection capability. It is based on an ultra-low analog frequency to voltage converter using a current-mirror, analog timers and comparators. Dedicated to long term stealth environmental or military surveys, a strong emphasis has been put on power consumption reduction in order to limit size and weight of the system. This power consumption has been reduced to 34\u03bcW, leading to a full year of autonomy including the microphone when powered by 3 coin cell CR2032 batteries."}, {"label": 0, "content": "Applications such as autonomous driving and virtual reality (VR) require low-latency transfer of high definition (HD) video. The proposed ultra-low-latency video coding method, which adopts line-based processing, has 0.44\u03bcs latency at minimum for Full-HD video. With multiple line-based image-prediction methods, image-adaptive quantization, and optimized entropy coding, the proposed method achieves compression to 39.0% data size and image quality of 45.4dB. The proposed basic algorithm and the optional 1D-DCT mode achieve compression to 33% and 20%, respectively, without significant visual degradation. These results are comparable to those for H.264 Intra despite one-thousandth ultra-low-latency of the proposed method. With the proposed video coding, the autonomous vehicles and VR devices can transfer HD video using 20% of the bandwidth of the source video without significant latency or visual degradation."}, {"label": 0, "content": "Presently, Biometric features are often used to identify suspects in law enforcement processes. One of these biometric features is Speaker Recognition. Speaker recognition is used to discriminate people by their voice. In this study, the problem that can be solved is how to classify audio sample that exist on the evidence with the voice of the suspect.In this final project is made a application's prototype that can be used to classify and in that case will be done speaker recognition technique (Speaker Recognition) to be able to classify the speaker's voice in the evidence and the voice of the suspect. The stages used to compare the sound is by extracting the sound features using the Mel-frequency Cepstral Coefficients (MFCC) method and using the Learning Vector Quantization Neural Network (JST-LVQ) method as the classification method of the voice extraction result.By using LVQ, the accuracy in recognition the speaker's voice is pretty good. The use of LVQ method produces best accuracy at 73,33% to recognize the speaker that with the same sentence, and 46,67% for different sentence. So the results obtained in accordance with the expected."}, {"label": 0, "content": "Blockchain technologies, such as smart contracts, present a unique interface for machine-to-machine communication that provides a secure, append-only record that can be shared without trust and without a central administrator. We study the possibilities and limitations of using smart contracts for machine-to-machine communication by designing, implementing, and evaluating AGasP, an application for automated gasoline purchases. We find that using smart contracts allows us to directly address the challenges of transparency, longevity, and trust in IoT applications. However, real-world applications using smart contracts must address their important trade-offs, such as performance, privacy, and the challenge of ensuring they are written correctly."}, {"label": 0, "content": "Skin cancer is the abnormal growth of skin cells that can not be controlled. Skin cancer appears when the DNA is damaged skin cells (mostly due to ultraviolet radiation from the sun) triggers mutations that skin cells grow rapidly, can not be controlled and start forming a tumor. Skin cancer can be overcome if it is detected earlier before spreading or doing metastasis. However, the tendency of people who are indifferent and reluctant to check or consult with doctors make his condition worse without realizing it. Therefore, designed an application for Skin Cancer Detection with Image Processing and Expert System using Forward Chaining and Certainty Factor method. The end result of image processing and expert system in this application is the assessment of High Risk, Low Risk, or Medium Risk of nevus conditions in patients. With the design of this system is expected to help raise awareness to detect early skin cancer. The results in this study show that the application has an accuracy rate of 100%. It shows that this system produces the same results as an expert."}, {"label": 0, "content": "Today, numerical controls (CNC) are the standard for the control of machine tools and industrial robots in production and enable highly flexible and efficient production, especially for frequently changing production tasks. A numerical control has discrete inputs and outputs. Within the NC channel, however, it is necessary to analytically describe curves for the calculation of the position setpoints and the jerk limitation. The resulting change between discrete and continuous description forms and the considerable restrictions in the parallelisation of the interpolation of continuous curves within the NC channel lead to a performance overhead that limits the performance of the NC channel with regard to the calculation of new position setpoints. This can lead to a drop in production speed and thus to longer production times. To solve this problem, we propose a new approach in this paper. This is based on the use of deep generative models and allows the direct generation of interpolated toolpaths without calculation of continuous curves and subsequent discretization. The generative models are being trained to create curves of certain types such as linear and parabolic curves or splines directly as discrete point sequences. This approach is very well feasible with regard to its parallelization and reduces the computing effort within the NC channel. First results with straight lines and parabolic curves show the feasibility of this new approach for the generation of CNC toolpaths."}, {"label": 0, "content": "Dual- (or multiple) rear cameras on hand-held smartphones are believed to be the future of mobile photography. Recently, many of such new has been released (mainly with dual-rear cameras: one wide-angle and one telephoto). Some of the notable ones are Apple iPhone 7 and 8 Plus, iPhone X, Samsung Galaxy S9, LG V30, Huawei Mate 10. With built-in dual-camera systems, these devices are capable of not only producing better quality picture but also acquiring 3D stereo photos (with depth information collected). Thus, they are capable of capturing the moment in life with depth just like our two eye system. Thanks to this current trend, these phones are now getting cheaper while becoming more power complete. In this paper, we describe a system that makes use of the commercial dual rear-camera phones such as the iPhone X, to provide aids for people who are visually impaired. We propose a design to place the phone on the chest centre of the user who has one or two Bluetooth headphone(s) plugged into the ears to listen to the phone audio outputs. Our system is consist of three modules: (1) the scene context recognition to audio, (2) the 3D stereo reconstruction to audio, and (3) the interactive audio/voice controls. In slightly more detail, the wide-angle camera captures live photos to be investigated by a GPS guided Deep Learning process to describe the scene in front of him/herself (module 1). The telephoto camera captures the more narrow-angle and thus to be stereo reconstructed with the aids of the wide angle's one to form a depth map (densed area-based distance map). The map helps determine the distance to all visible object(s) to notify the user with critical ones (module 2). This module also makes the phone vibrate when an object(s) located close enough to the user, e.g. within hand reach distance. The user can also query the system by asking various questions to get automatic voice answering (module 3). In addition, a manual rescue module (module 4) is also added when other things have gone wrong. An example of the vision to audio could be \u201dOverall, likely a corridor, one medium object is 0.5 m away - central left\u201d, or \u201dOverall, city pathway, front cleared\u201d. Audio command input may be \u201dread texts\u201d, and the phone will detect and read all texts on closest object. More details on the design and implementation are further described in this paper."}, {"label": 0, "content": "Nutrient elements of NPK are macro nutrients that play an important role in the growth and development of plants, therefore it is necessary to measure NPK nutrient content to measure how well soil fertility condition before the land planting period, but NPK measurement through laboratory tests takes a relatively long time. This research develops a prototype of NPK nutrient measurement system based on a mobile application by using soil image for determining the textural characteristic, the textural characteristics are processed with local binary pattern and back-propagation neural network to accelerate the measurement process.Sample data in this research was taken on rice field land in the province of Yogyakarta Special Region by varying the distance at 30 cm to 110 cm with interval 20 cm and angle image capture at -30\u00b0 to 30\u00b0 with interval 10\u00b0. Datasets were being pre-processed to improve image quality and adjust image format. Preprocessed results are extracted using local binary pattern uniform to obtain texture features. The texture features were being inputted of the neural network model, that being trained with a back-propagation algorithm by varying parameters of the neural network model.The model tested to determine the effect of distance and angle of image capture, system processing speed, and effect of artificial neural network parameters. The best model is implemented on a smartphone application. The results obtained an average of computation time 0.65s, and the optimal result is obtained at distance capture of 50 cm and angle capture of 0\u00b0 with the measurement accuracy at each soil nutrient level of nitrogen 91.80%, while phosphorus 83.49%, and potassium 82.54%, therefore the average is 84.16%."}, {"label": 0, "content": "As an alternative to voice, sign language and artificial larynx can be used. However, there are disadvantages where they require a long-term training and are expensive. Therefore, researches on detection of utterance by electromyography (EMG) analysis around the lips have been conducted. On the one hand, it is necessary to construct a personal authentication system to identify speakers. The electrode used in this paper is 2 electrodes sensor, which is small in size and a dry type. Three sensors are attached in the orbicularis muscle, the zygomatic major muscle, and the depressor angle oris muscle which can acquire myoelectric information necessary for identification in Japanese vowel utterance. EMG signals are measured using P-EMG plus. In order to eliminate noises, signal cutting is carried out before and after the central point of the acquired raw data. Furthermore, EMG data are divided to increase the number of data while overlapping. These are named \u201cDATA 1\u201d. A Hamming window is then applied for them, and the amplitude values of the power spectra are calculated by fast Fourier transform. Automatic verification and elimination of noise parts by quartile method were carried out. In order to reconstruct signals after noise elimination, the inverse Fourier transform is carried out and then a inverse Hamming window is applied. These are named \u201cDATA 2\u201d. Learning identification is carried out using a convolutional neural network. A large difference was found in accuracy depending on the data set created separately by measurement date. Therefore, it was found that intra-individual variation by each subject was large. In the future, it is necessary to further improve the data and to reduce individual variation within each subject."}, {"label": 0, "content": "IoT (Internet of Things) is the most important technical applications of engineering advancement in the world today, through Industrial Revolution - Industry 4.0. Since people uses phone for more than just daily communication devices, but as wireless smart devices in accessing / processing / sending information through fast telecommunication networks with enhancements embedded in it like cameras, GPS and OTT apps. Various development capabilities on other devices that enable a person to do, social media, video conferencing, video streaming, tracking, navigation, drone, remote, forecast, monitoring, payment and all other things that may be computationally proceed by sensor and actuator devices. The development of cutting-edge technology makes this smart capability to be applied to any device, as well as the ability of devices to interact with each other's through internet network. IoT as a vital aspect in Industry 4.0, is broadly embodied in smart city (policy driven), smart industry (business driven), and smart life (experience driven) solution. Utilizing the capabilities of Industrial IoT rightly can lead Nusantara appropriate development as a large archipelago and agrarian area that rich of natural resources. This research investigates the concept and IoT's use cases against variety of socioeconomic and specific geographic challenges, then evaluate based on PESTLE strategic analysis for the external and internal. The result are Development Strategy and Technology for Developing Nations."}, {"label": 0, "content": "By applying low power wide area network (LPWAN) to communication between a machine and the cloud, it can be anticipated that communication costs and the amount of power consumed by the machine can both be reduced. However, considering the addition of functions to the machine, it is necessary to have a technology able to transfer a large amount of data, which cannot be transferred by LPWAN, to the machine from the cloud at low cost. In this paper, a novel large data download method by Wi-Fi from cloud to machine using the terminals of general users is proposed. In the proposed method, the cloud selects the delivery terminals satisfying the data arrival rate requirement by analyzing the correlation of the movement history between the terminals. It guarantees a data arrival rate with a higher degree of accuracy than the existing DTN and CC-DTN, and at the same time minimizes the number of delivery terminals. In addition, this paper shows an authentication procedure that prevents DoS attacks on LPWAN by a spoofing terminal, which cannot be prevented by existing network authentication technology. Also, we report the effectiveness of the proposed method, which is confirmed by numerical calculation."}, {"label": 0, "content": "Estimation of the 6-Dof pose of 3d objects has been a hot research field for a long time. When robots and cameras are integrated into a system, the pose of the object can be estimated through the camera, and then the robot can be used to manipulate the object accurately. Traditional object pose estimation methods include the template-matching based method and invariant feature-based method. The method based on invariant features requires the extraction of invariant features from images with rich texture, so it is not suitable for texture-less parts, which are common in industrial applications. The template-matching method is based on edge and contour information, so it is more suitable for part detection and pose estimation of industrial applications. LINEMOD proposed by Hinterstoisser is a successful template matching method, which accelerates the template matching process through a specially designed storage structure. However, the template-based matching method generally adopts sliding window method and is very time-consuming in computation, which makes it impractical for robotic application. In this paper, we propose a new method, which combines Fully Convolutional Network (FCN) with LINEMOD algorithm. With this method, the detection and location of the object in the image can be archived quickly. Then the local image, instead of the whole image, is used for LINEMOD template matching. Experimental results show that, compared with the standard LINEMOD method, the pose estimation speed can be increased and consistent matching results can be obtained."}, {"label": 0, "content": "Predicting the future states of things is an important performance form of intelligence and it is also of vital importance in real-time systems such as autonomous cars and robotics. This paper aims to tackle a video prediction task. Previous methods for future frame prediction are always subject to restrictions from environment, leading to poor accuracy and blurry prediction details. In this work, we present an unsupervised video prediction framework which iteratively anticipates the raw RGB pixel values in future video frames. Extensive experiments are implemented on advanced datasets \u2014 KTH and KITTI. The results demonstrate that our method achieves a good performance."}, {"label": 0, "content": "Systems based on the concept of `Internet of Things' (IoT) differ by multi-tiered architecture, a great number of used `things', the influence of new types of attacks, the incompleteness and ambiguity of their parameters. For these reasons, solving security management tasks in IoT networks, such as network traffic analysis, requires applying intelligent approaches and methods. The purpose of the paper consists in development and assessment of a new algorithm of the network traffic analysis in a real or near real time. The paper also considers various variants for implementation of intelligent agents intended for network traffic analysis in IoT networks in different cases: (1) high-performance computers, (2) embedded devices, and (3) systems-on-chip. The agents are based on the algorithm of pseudo-gradient anomaly detection and fuzzy logical inference. The suggested algorithm operates in real time. The experimental assessment of the approach shows that the gain can reach 50% in accuracy and 90% in speed."}, {"label": 0, "content": "Flood is one of the common types of natural disaster in Indonesia, we need a system that can predict the arrival of the flood is important for the Indonesian people, especially people who live a certain area of the river flow. Some parameters that can be used to predict the flood are water level and rainfall around the river. Modeling system to predict the flood must have the prediction results as accurate as possible in order to produce a good system in predicting floods. Therefore, in this study proposed method of artificial neural network to analyze flood prediction ability by using artificial neural network In this study case using artificial neural network Radial Basis Function. Radial Basis Function is a model of artificial neural network architecture consisting of three layers of which are the input layer, hidden layer, and output layer. The data used for the training and testing process are data of water level and rainfall data in 2015 in Dayeuhkolot. Prediction results in the training and testing process resulted in MAPE values are 0.047% and 1.05% for water level data and 4.97% and 29.1% for rainfall data with combination of hidden node = 35, learning rate = 0.2 and Spread constant = 1.1 with the target epoch maximum termination of 5000 epoch."}, {"label": 0, "content": "Internet users in Indonesia have increased in recent years. Many product service providers who provide internet access services in accordance with tariff options and their superiority. In this research, sentiment analysis on social media to some service data service operator to see the level of public satisfaction in using data service of telecommunication operator for internet access in Indonesia. In this research is sentiment analysis with several stages, namely the collection of sentiment data using API (Application Programming Interface) which is available on Twitter. The preprocessing stage is then processed to process raw initial data, then perform POS tagging and weighing the word with TF-IDF calculation and perform classification using the Naive Bayes Classifier (NBC) method. This study yields an average value of 94,5% precision rate, 93,3% Recall and 99,09% Accuracy."}, {"label": 0, "content": "This paper presents a flex-rigid soft robot for flipping locomotion. The proposed robot is made into a strip shape and consists of three rigid limbs connected by two active flexible hinges. Its flipping locomotion is achieved by active folding and developing of the hinges. To validate its locomotion ability, experiment is conducted on level ground. The results show that the proposed flex-rigid robot can perform flipping locomotion with average velocity of 60 mm/s."}, {"label": 0, "content": "Overall Equipment Efficiency (OEE) is applied to measure the actual production capacity of equipment, and Theory of Constraints (TOC) is adopted to improve the system production efficiency. In order to obtain the OEE improvement method based on TOC, bottleneck identification model and buffer model are established. Thus, a multi-attribute bottleneck identification model is constructed based on Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) and Entropy Method. In addition, a time buffer model based on the Drum-Buffer-Rope (DBR) theory is proposed. This OEE improvement method can significantly increase OEE of bottleneck equipment. Moreover, due to the optimization of bottlenecks, the system production efficiency is improved. The effectiveness of this method is also verified in a semiconductor package process."}, {"label": 0, "content": "Moving-object tracking (estimating position and velocity of moving objects) is a key technology for autonomous driving systems and driving assistance systems in mobile robotics and vehicle automation domains. To predict and avoid collisions, the tracking system has to recognize objects as accurately as possible. This paper presents a method for recognizing vehicles (cars and bicyclists) and pedestrians using multilayer lidar (3D lidar). Lidar data are clustered, and eight-dimensional features are extracted from each of clustered lidar data, such as distance from the lidar, velocity, object size, number of lidar-measurement points, and distribution of reflection intensities. A multiclass support vector machine is applied to classify cars, bicyclists, and pedestrians from these features. Experiments using \u201cThe Stanford Track Collection\u201d data set allow us to compare the proposed method with a method based on the random forest algorithm and a conventional 26-dimensional feature-based method. The comparison shows that the proposed method improves recognition accuracy and processing time over the other methods. Therefore, the proposed method can work well under low computational environments."}, {"label": 0, "content": "Data collection and analysis of product are causing more and more attention with the rapid development of intelligent manufacturing. Faced with the problem that data acquisition system cannot be used universally, researchers have a long way to go. This paper designed a configurable data acquisition system for automatic filling lines, and the system can be used in other working lines without changing the hardware. The system is based on ARM and consists of three parts: data acquisition, server and cloud platform. Data acquisition part is responsible for acquiring data and sending data to host computer. Server part is used for receiving data from host computer and saving it in the database. Cloud platform serves for users and provides data analysis. The hardware design and software design of the acquisition board are also analyzed, meanwhile the communication protocol is made to ensure the data transmission. The data acquisition has been realized in this system and the whole process is running smoothly without problem."}, {"label": 0, "content": "Social media today is something that cannot be separated from each person, lik Instagram, twitter, facebook, path, line and many more. Everyone has at least 2 to 5 social media accounts on his smartphone. From this phenomenon its makes social media as a source of data that can be used to seek public opinion instantly.In this paper, sentiment analysis about public satisfaction in using data service of telecommunication operator in Indonesia, either at official account of each cellular operator or using the related keywords with cellular operator. The method used by the author is Support Vector Machine with TF-IDF weighting and utilization of POS Tagging and Negative Handling as improvement of accuracy before classification.In this paper, a system of sentiment analysis classification on the level of user satisfaction of operator data service. That is classification using support vector machine method. SVM with RBF kernel (Radial Basis Function). After preprocessing, POS Tagging is then TF-IDF. The results in this study showed an average f1-score rate of 95,43%, precision 92,45%, recall 93,90% and accuracy 99,01%."}, {"label": 0, "content": "There have been many varieties of driving assistance, and one aspect of them is the scope of emergency braking. Several researches have been analyzing emergency braking and proposed approaches to detect them. A focused but significant case is mistaken pedal pressing during emergency braking, which occurs when accelerator pedal is pressed instead of brake pedal. This paper aims to evolve a classifier to recognize mistaken pedal pressing based on behavior shown during pressing the pedals by using evolutionary computation. A driving simulator is used to collect the data, and genetic programming was used to perform the evolution."}, {"label": 0, "content": "Internet of Things (IoT) development is a very challenging topic, and the debate about the actual implementation is still wide open. Various studies have been conducted in term of smart home system based on IoT technology. However, resources that concern on how to practically implement the particular energy-saving and resource efficient technology for smart home need to be improved. In this study, presented the field experiment results related to implementation of low-power node modules using sub 1 GHz LPWAN (low-power wide area network) connectivity. LPWAN technology has unique characteristics, such as a transmit power that in SRD class (short range device) but cellular like coverage range. LPWAN network support star topology by default, so it can overcome the problem of power usage inconsistency, network delay and the complexity of the routing management process found on conventional mesh-based sensor networks. Based on the field experiment result, the furthest distance under outdoor condition could reach up to 350 meters (RSSI -85 dBm) and 150 meters (RSSI -95 dBm) under indoor condition. Based on the power usage test, life-span estimation could reach up to 1 year or 17 years depend on the scenarios and battery types."}, {"label": 0, "content": "In the Philippines, efforts are being made to address inaccessibility of Internet access by 42% of the population. One such service installs Wi-Fi hotspots in public areas and government buildings. To make the service more effective, data on Wi-Fi access point deployment is vital. While there exist platforms for crowdsourced data submission such as WiGLE, and methods such as dedicated wardriving, these can be costly, infrequent, and can easily become outdated. Regular data collection on Wi-Fi access points is necessary for better accuracy. For regular, inexpensive, and frequent data gathering, opportunistic wardriving is proposed, in which neighborhood public utility vehicles, such as the tricycle, are utilized to serve as vehicles for gathering network data. Leveraging public utility vehicles is inexpensive, and ensures regularity of data gathering because of their usual trips. Comparing this method with crowdsourced data, it is found that opportunistic wardriving is able to find at least 60% of the access points in an area. Opportunistic wardriving also ensures wide coverage of an area, accessing 83% of the chosen location, Brgy. Teachers Village East, in forty trips. It is hoped that this study will aid in understanding an inexpensive method for gathering information on Wi-Fi deployment in various areas."}, {"label": 0, "content": "The Prevention of cardiovascular disease requires continuous monitoring of cross-clock ECG signals along with the activity status. Traditional ECG Holter has numerous electrodes connected to the chest, which is heavy, so it is very difficult to carry by the patient, so ECG monitoring usually requires the patient to stay in the hospital for a long time. This paper presented a small ECG Holter device that was developed to detect arrhythmias in real-time based on Android mobile application. The ECG signals are obtained directly through ECG's three-electrode sensor then transmitted through a Bluetooth module to Android smartphone. Prepossessing ECG signal algorithm is implemented on Arduino Device. Android mobile application analysis and classify patient's ECG data to detect abnormal signs. Data used in testing and training was 303 cases acquired from El-Monofia University, 162 cases were normal, and 141 cases were abnormally divided into 57 cases were Coronary Artery Disease, 36 cases were Old Anterior Myocardial Infarction, and 48 cases were Sinus tachycardia. The experimental results show that the presented system's performance has been improved in the accuracy of diagnosis of arrhythmias and the identification of the most widely recognized anomalies in various activities."}, {"label": 0, "content": "Increasing popularity of drones inspires some people and companies to start using them as end-to-end package delivery tools. Despite reducing delivery time, drones running on batteries typically have a high power consumption relative to their battery capacity to provide power for motors, flight controller, and communication systems. Most drones do not have communication systems that provide a long-range coverage while preserving the power consumption. Developing a long-range and energy-efficient communication system becomes a main concern of this research. In terms of wireless physical layer technology, LoRa becomes one of the possible options due to its power efficiency. LoRaWAN, a de-facto standard protocol for LoRa intended for wide area networking, can be used for drone delivery application. However, it is not suitable for real-time and control-heavy applications. In this paper, the limits of LoRaWAN as a secondary communication mode for drone delivery system are evaluated. The results show that LoRaWAN protocol can still be used for a semi-real-time telemetry purpose in which it can send 10-20 bytes payload regularly with minimum of 2-3 seconds interval. In terms of coverage, the system can achieve up to 8 km in an urban area as tested, using the lowest spreading factor, considering the imperfection factor from the hardware. The percentage of packet loss using this configuration is still tolerable, i.e., up to 5%."}, {"label": 0, "content": "Being used for synchronizing and triggering environmental and military Internet of Things (IoT) wireless networks, an ultra low power wake-up system based on frequency analysis is presented in this paper. With an average power consumption of 34\u03bcW, this wake-up detector is able to detect signals in different frequency bands, generating a separate output interrupt for each of them. Adding an additional frequency band detector only costs an additional 500nA. Applications to data retrieval in a military smart dust using a drone on a battlefield, and for activation and synchronization of a environmental wireless sensor network are presented. This later uses a multi-frequency light pulses burst propagation algorithm for triggering a whole wireless sensor network in harsh conditions such as a dense rain forest in 50ms, and for synchronizing it with a timing precision of less than 20\u03bcs in a large network."}, {"label": 0, "content": "As the internet protocol is the de-facto standard for communication on the internet even very constrained devices with low computing power, working memory and battery power capacity should be addressable via an IP-based network infrastructure. To fulfil this necessity communication with wireless devices could be realized by applying 6LoWPAN. Wireless sensor networks are a main topic nowadays, but sometimes a well structured, wire-based infrastructure is more suitable.Based on this assumption we transferred basic 6LoWPAN concepts, ideas, and two implementations from an IEEE 802.15.4-based wireless to a wire-based RS485 network. In this paper, we outline our main objectives and decision points of this transformation process from a technical point of view.We highlight our crucial adoption steps and main changes to bring IPv6 network capability to RS485. Contiki and GNU/Linux implementations for our IPv6 over RS485 approaches were realised to verify correct functionality.With this work we enable tiny devices to be integrated efficiently into IPv6 networks without the necessity of equipping them with Ethernet controller or other expensive hardware components, avoid translation and transformation steps in the gateways, and so speed up development."}, {"label": 0, "content": "The paper applies image classification techniques to protect users of the Internet from inappropriate information. It describes a modular hierarchical classification system and proposes an approach to integrate image classification modules without additional retraining. The experiments devoted to the integration of an image classification module into the current infrastructure of the Web classification system are outlined and analyzed."}, {"label": 0, "content": "Automatic disease detection using visible symptoms on leaves is becoming more and more important. Here we describe an algorithm, which uses machine learning to detect diseases in a wide variety of plants and diseases. High accuracy (>93%) was obtained with very noisy images, different backgrounds and different disease coverage. The algorithm is able to train itself, which means that the accuracy can increase with usage. It can run on a variety of platforms including smartphones and can thus aid non-expert farmers manage diseases effectively."}, {"label": 0, "content": "In this paper, a framework for collaborative localization of heterogenous systems is presented. Making advantage of the original MSCKF framework, we design a collaborative MSCKF filter that operates in two levels and allows a decentralized 3D collaborative localization without use of external computation systems. To achieve that, based on MSCKF localization, we first propose a range based collaboration that we optimize using the extracted environment constraints, an operation allowed by the use of a truncated unscented kalman filtering updates. The collaborative filtering is managed to not impact the original MSCKF odometry properties. The framework is applied to collaborative localization of aerial and ground robots; experimental results show the effectiveness of the proposed method."}, {"label": 0, "content": "Nowadays, cities all over the globe are transforming into smart cities. Smart cities initiatives need to address environmental concerns such as air pollution to provide clean air. A scalable and cost-effective air monitoring system is imperative to monitor and control air pollution for smart city development. Air pollution has notable effects on the well-being of the population a whole, global atmosphere, and worldwide economy. This paper presents a scalable smart air quality monitoring system with low-cost sensors and long-range communication protocol. The sensors collect four parameters, temperature, humidity, dust and carbon dioxide in the air. The proposed end-to-end system has been implemented and deployed in Yangon, the business capital of Myanmar, as a case study since Jun 2018. The system allows the users to log in to an online dashboard to monitor the real-time status. In addition, based the collected air quality parameters for the past two months, a machine learning model has been trained to make predictions of parameters such that proactive actions can be taken to alleviate the impacts from air pollution."}, {"label": 0, "content": "Autism spectrum disorder(ASD) is a kind of developmental disorder which attracted a lot of attention of researchers for its urgency and pervasiveness. The diagnosis and intervention of ASD is still complicated and hard to handle. The rapid development of technology has brought new methods to the auxiliary diagnosis of ASD, such as face detection, gaze estimation, action recognition, etc. The paper proposed a preliminary visual system for assistant diagnosis of ASD in a core clinical testing scenario-response to name. The eye center localization and gaze estimation were applied to measure the responses of the subject. The purpose of this paper is analyzing the feasibility of this system, and optimizing the sensing structure and the evaluation indicator."}, {"label": 0, "content": "The goal of industry 4.0 is to use all the information that can be extracted from a supply chain to continuously optimize all aspects of its operation. The data acquisition is still a big challenge and the first step of the fourth industrial revolution. Getting data from software is much easier than getting data out of hardware like manufacturing machines. Especially if the Programmable Logic Controller (PLC) data is either poorly documented or not designed for these requirements. Therefore, we created a methodology to track the most common movement of a machine, which is the linear motion. The solution is an IoT-device: a small, wireless, and low cost sensor, designed to provide data about linear motions within a machine in real-time. We designed a static generic model and a method for machine optimization with data acquisition results comparable to results in other approaches. By implementing the methodology to a real industrial scenario, the results enable us to prove our hypothesis. The IoT-device data was as good as the PLC data and even closer to real-time. Our methodology also shows a higher potential to automate the data analysis."}, {"label": 0, "content": "The increasing complexity of parts and the growing quality requirements pose new challenges for today's manufacturing industry. Multi-Stage Production Systems, which are known for complex links and sequences of many different process steps, must be adapted to these requirements. This means, being cost-effective and flexible while still meeting high quality standards. The idea of Zero-Defect Manufacturing aims to reduce scrap, rework and special operations by analyzing and optimizing multi-stage production systems through data-driven and learning-based approaches. A Knowledge Capturing Platform concept is introduced to extract a deeper understanding from collected data with inter-stage correlation methods, part variation approaches along the line and intelligent monitoring systems."}, {"label": 0, "content": "In this paper, we present a new method for automatic mass function estimation and focal elements selection as a fundamental step to apply the Dempster-Shafer Theory (DST). The idea is to use the centroids and membership distributions obtained by applying the Fuzzy-C-Means algorithm (FCM) to define the mass function. The proposed method allows finding composite focal elements that represent the highest uncertainty and ambiguity. Experiments were conduced on multi-spectral and multi-temporal images for the purpose of change detection by integrating the proposed method of mass function estimation in a process of post-classification by DST. The proposed system of change detection is characterised by a multi-level of imperfection handling where the ambiguity is modelled firstly by FCM, then the uncertainty and the imprecision are handled in the step of mass function estimation. The effectiveness of the proposed methodology is demonstrated by applying it to find transformed region within two landsat images where we obtained high rates of classifications."}, {"label": 0, "content": "This paper addresses for the first time the multilabel classification of High-Voltage (HV) discharges captured using the Electromagnetic Interference (EMI) method for HV machines. The approach involves feature extraction from EMI time signals, emitted during the discharge events, by means of 1D-Local Binary Pattern (LBP) and 1D-Histogram of Oriented Gradients (HOG) techniques. Their combination provides a feature vector that is implemented in a naive Bayes classifier designed to identify the labels of two or more discharge sources contained within a single signal. The performance of this novel approach is measured using various metrics including average precision, accuracy, specificity, hamming loss etc. Results demonstrate a successful performance that is in line with similar application to other fields such as biology and image processing. This first attempt of multi-label classification of EMI discharge sources opens a new research topic in HV condition monitoring."}, {"label": 0, "content": "In this paper, the deformation details of a soft actuator of a soft surface manipulator was investigated. The relation between the pressure of the inflation and the object displacement was established. Finite Element Analysis was used to investigate the working principle of single soft actuator. The simulation results show how the soft actuators effect the movement of the object on the surface. The relation between inflation pressure and object displacement was established by applying different pressure to the Finite Element model. The soft surface manipulator was considered as a servo mechanism acting on the object. The kinematic model of the object was established to facilitate the development of trajectory tracking algorithm in the future work."}, {"label": 0, "content": "The lie is very detrimental to the fraudulent acts of many people who were cheated. The lies are common in the general population. To be able to reveal a lie we can detect through some limbs that unconsciously will show a different reaction when someone is lying. Among them, through organs of our eyes can detect someone is lying or not.Lie detection discussed in this final Task is the eyes, namely with the object of eye tracking and eye pupil diameter changes by using method of Wavelet Transform to Gabor Image Processing process and afterwards perform the classification to determine the answer someone is lying or not by using a Decision Tree. The existence of this lie detector, is expected to be helpful for people who need to detect lies. With the final test results are accurate. This research has the precision value of 97%, 94%, and recall accuracy 95% of testing has been done."}, {"label": 0, "content": "In recent years, clustering has emerged as a promising approach to facilitate data routing and data aggregation in Wireless Sensor Network (WSN). Although clustering based routing approaches are appropriate for small-scale networks, they do not fit large scale WSNs as it is the case in LEACH [1]. Indeed, clustering suffers from the adverse effects of isolated nodes in the network and some coverage problems. To deal with these issues, we present LEATCH-L, a Low Energy Adaptive Tier Clustering Hierarchy for Large scale WSNs. The proposed approach makes the major functions of LEACH applicable to large-scale WSNs whose dimension is much larger than the largest transmission radius of the sensor nodes. The latter imposes a dynamic decomposable structure on the network topology which results in a set of smaller subnetworks. Such decompositions are implemented through a smart m-level hierarchical clustering process. Moreover, the proposed approach involves a two level data aggregation. Evaluation results show that the introduced approach is scalable with significantly much better performance than the state-of-the art approaches."}, {"label": 0, "content": "Research on data confidentiality, integrity and availability is gaining momentum in the ICT community, due to the intrinsically insecure nature of the Internet. While many distributed systems and services are now based on secure communication protocols to avoid eavesdropping and protect confidentiality' the techniques usually employed in distributed simulations do not consider these issues at all. This is probably due to the fact that many real-world simulators rely on monolithic, offline approaches and therefore the issues above do not apply. However, the complexity of the systems to be simulated, and the rise of distributed and cloud based simulation, now impose the adoption of secure simulation architectures. This paper presents a solution to ensure both anonymity and confidentiality in distributed simulations. A performance evaluation based on an anonymized distributed simulator is used for quantifying the performance penalty for being anonymous. The obtained results show that this is a viable solution."}, {"label": 0, "content": "On battery-operated devices, energy and power consumption are main concerns. With the recent advancement of technology, mobile devices can be integrated with traditional systems for running complex computations. In fact, mobile devices can easily become part of computational networks and share their computational and memory resources. Despite this, traditional simulation frameworks are not designed to perform well on heterogeneous networks. This is mainly due to the limited computational resources that are available on mobile devices. In this paper, we propose SEECSSim (SEECSSim is derived from School of Electrical Engineering and Computer Science (SEECS)) that is a simulation framework specifically designed for mobile devices. SEECSSim includes state-of-the-art distributed synchronization algorithms that are implemented to run on mobile or embedded devices. To benchmark the proposed framework, the well-known PHOLD model is used and performance results are reported in terms of execution time, CPU usage, memory and energy consumption."}, {"label": 0, "content": "This paper presents a data assimilation technique for social agent-based simulation to fit real world data automatically by a reinforcement learning method. We used the hidden Markov model in order to estimate the states of the system during the reinforcement learning. The proposed method can improve simulation models of the social agent-based simulation incrementally when new real data are available without total optimization. In order to show the feasibility, we applied the proposed method to a housing market problem with real Korean housing market data."}, {"label": 0, "content": "The emerging Fifth Generation (5G) mobile networks have been attracting enormous attention from various stakeholders around the world. In particular, in the research community, prototyping 5G infrastructures and deploying 5G services have gained gears recently towards realising market-oriented 5G trials. However, accessing to and programming on real-world 5G infrastructure is almost prohibitive for most 5G researchers especially in academia. Therefore, it is critical to build realistic yet cost-efficient 5G infrastructure emulators for 5G research labs to enable credible 5G research activities. This paper proposes such a 5G infrastructure emulator that is able to emulate a realistic 5G network in a lab setting based on a small number of commercial-off-the-shelf servers by leveraging virtualization and other technologies. Moreover, this emulator allows a service provider to automatically deploy 5G services from `empty' machines through advanced automation. The emulation platform is described in details with the 5G infrastructure and service deployment procedure highlighted. Empirical results are presented to show the performance of the proposed emulator."}, {"label": 0, "content": "When expressing concerns about the credibility of simulation studies, simulation data have been traditionally in the focus. However, what about another and, some might argue, even more central product of simulation studies, i.e., the simulation model itself? How can the credibility of a simulation model be assessed? Therefore, information about the process of generating a simulation model is needed. This provenance relates entities (or artifacts) and activities involved in the generating process. Based on simulation studies we will illuminate how the provenance of a simulation model relates the refinement, extension, composition, calibration and validation of simulation models to the diverse sources used in these processes. To exploit this information, unambiguously means for specifying entities play a central role. For example, a formal domain-specific language for modeling facilitates assessing and reusing simulation models. Similarly, a declarative domain-specific language for specifying simulation experiments, helps utilizing simulation experiments done with earlier models for future models. Thus, provenance, information about the past, does not only allow to understand the present, but also to design the future, in opening up new avenues for generating and analyzing simulation models."}, {"label": 0, "content": "In this paper, we derived the analytical expressions of the system performance (in term outage probability and throughput) of the power splitting half-duplex power beacon-assisted energy harvesting relay network in both amplify-and-forward and decode-and-forward modes. Moreover, the analytical results are also demonstrated and convinced by using Monte-Carlo simulation. The numerical results demonstrated and convinced the analytical and the simulation results are matched well with each other in connection with all possible system parameters."}, {"label": 0, "content": "Currently, there is a lack of tools for real validation of 5G scenarios. The increasing traffic demand of 5G networks is pushing network operators to find new cost-efficient solutions. The selected solution is a multi-tenancy approach that, together with user mobility will impose some architectural changes. This approach increases service dynamism making it necessary to have tools that provide these new capabilities to be able to validate each development. This work presents a novel experimentation framework for the emulation of 5G scenarios providing them with real-time user mobility and multi-tenancy. The functionality of this novel framework has been validated through different experiments."}, {"label": 0, "content": "Crowd simulation can play a crucial role when it comes to the design of Smart Environments. Crowd simulation can give insights on the flow of pedestrian in particular facilities and explore the interplay between ambient intelligence deployments and users. Most researchers develop crowd simulations using commercial game engines built with the editors they usually provide. This prevents a deeper experimentation with the problems of crowd simulation and enforces to stick to the development paradigm of the tool. As a consequence, it couples the scientific experimentation that produces the crowd model with the actual construction of the simulation tool. Besides, a crowd simulation may require more resources than those available to the scientist. A solution would be to conceive crowd simulation as a service that, on the one hand, it allowed scientists to experiment with the latest advances without the burden of installing elements or acquiring expensive computational resources; and, on the other hand, it enabled developers to evolve the tool in a scalable way. The contribution of the paper is a framework that enables the \u201csimulation as a service\u201d approach for crowd simulations when they are run with a 3D representation. As a proof of concept, the paper illustrates how crowd simulations can be used to generate datasets that allow studying the deployment of sensors in a large facility."}, {"label": 0, "content": "This paper describes a modelling and analysis experience concerning time synchronization in wireless sensor networks (WSN). A fully distributed algorithm is formally modelled and its properties assessed through statistical model checking. The described work is based on the Theatre framework which rests on actors and asynchronous message passing. Theatre can be reduced to the Uppaal Statistical Model Checker (SMC). The paper discusses the chosen time synchronization algorithm, outlines the Theatre modelling features and its mapping on to Uppaal Smc, and shows a Theatre model for the selected time synchronization algorithm enhanced with a new adaptation mechanism for energy saving. The model is then analyzed through simulations."}, {"label": 0, "content": "Distributed and Real-Time Simulation represents a solid and effective approach to manage the ever-increasing complexity of modern systems. The IEEE 1516-2010 for Modeling and Simulation (M&S) High Level Architecture (HLA) is an interoperability standard for Distributed Simulation (DS) used to support analysis, engineering and training in different research and industrial domains. Using HLA, simulation entities (called Federates) can interact (that is, to publish and/or subscribe ObjectClasses and InteractionClasses, to communicate data, and to synchronize actions) with other Federates in a common simulation environment (called Federation) through the services provided by the Run-Time Infrastructure (RTI) that abstract and hide the details of the computing infrastructures making them communicable. In today's HLA-DS systems, in which simulation entities are highly concurrent, distributed, and the interactions among them are asynchronous, great benefits can derive from the exploitation of reactive approaches, tools and techniques so as to reactively manage the Federates' communication flow instead of handling it through HLA Callbacks. In this context, the paper presents a solution for defining reactive HLA Federates along with the RxHLA software framework that aims at defining and building reactive, concurrent, and distributed time/event-driven simulation components (Federates) in a r.eactive fashion."}, {"label": 0, "content": "The conception of Cyber-Physical Systems is a complex task: the multiple components making up those systems might not be fully known by the system architect, and putting those components together generates a new source of complexity. Study and validation of those systems is often done through simulation. Moreover, the CPS simulation is often studied through distributed simulation, as the CPS might itself be distributed or too complex. We present a methodology to distribute a simulation model in order to take full advantage of multiple processing units. We ensure that said distribution does not impact the simulation of our modeled system."}, {"label": 0, "content": "Rise in use of mobile and wearable electronic devices increase the demand of solutions to power these devices conveniently and safely for humans and environment. Harvesting of human body energy is one of perspective solutions for this problem. This paper studies performance of harvesting of human body waste heat using thermoelectrical generators. Generated form of electricity is not suitable for direct use by electronic devices and special converters have to be used to transform and store electrical energy. Study analyzes three commercially available low voltage step-up converters in conjunction with series of 5 thermoelectric elements located on lower leg. An approach for defining efficiency of energy harvesting is presented for comparison of step-up converters and finding the best solution in required conditions."}, {"label": 0, "content": "Human activity measurement and classification has been hot research topic for several years. Most of the solutions are based on the mobile phones, however there are also some wearable device implementations that have very specific functionality. The aim of this paper is to propose a human activity recognition and fall detection solution that provides extra safety for people working in challenging conditions. The system is integrated with the monitoring solution that provides real-time information about all workers and raises automatically an alarm in case of accidents or abnormal conditions."}, {"label": 0, "content": "Cognitive radio (CR) systems have to be able to detect the presence of a primary user (PU) signal by sensing the spectrum area of interest. Due to radiowave propagation effects like fading and shadowing, spectrum sensing is often complicated, because the PU signal can be attenuated in a particular area. In this paper, we explore a distributed spectrum sensing approach that exploits the largest eigenvalue of correlation matrices (CMs) that are adaptively estimated, based on the combine and adapt least (CTA) type of diffusion method with no fusion center (FC). More specifically, CR nodes exchange also observations with a subset of neighbouring nodes and combine the neighbouring observations based on the locally estimated signal to noise ratio (SNR) values. We analyse the resulting detection performance and verify the theoretical findings through simulations."}, {"label": 0, "content": "The paper presents a method for representing the instruction set truth tables of microprocessors with High-Level Decision Diagrams (HLDD). A behavior level fault model is defined for the microprocessor control parts on the basis of instruction level truth tables (TT). Two methods are proposed for creating HLDDs from TTs with minimization of the edges on graphs: greedy algorithm, and branch and bound algorithm (B&B). A simple and fast computable lower bound is proposed to be used for pruning the search space of the B&B algorithm. Experimental data of using the fault model for several microprocessors and comparison data are provided to show the efficiency of the proposed high-level fault model over the gate-level Stuck-at-Fault (SAF) model."}, {"label": 0, "content": "Estimation of pulse rate from photoplethysmogram (PPG) signals has led to a breakthrough in a smart wristband technology. This study introduces a method for estimating heart rate recovery (HRR) using a custom-made wrist-worn device, capable of acquiring instantaneous pulse rate, as well as a consumer smart wristband, which provides pulse rate at intervals of 5 s or longer. The feasibility to estimate HRR parameters using the PPG-based devices was assessed by comparing to the synchronously acquired reference electrocardiogram. Three HRR parameters were studied on pulse rate data, obtained from 22 healthy participants, instructed to perform standardized stair climbing test. Study findings show that HRR parameters, estimated using the wrist-worn device, are associated with twice lower absolute error compared to the consumer smart wristband, emphasizing the importance of an instantaneous pulse rate to ensure a sufficiently accurate parameter estimation."}, {"label": 0, "content": "In this paper a performance of conventional adaptive finite impulse response filters and a feed forward multi-layer neural network is examined for predicting the future values of the slip angle of a vehicle. The obtained results depending on a number of inputs and a prediction horizon are compared in terms of prediction error and a required number of operations for executing the algorithms."}, {"label": 0, "content": "A Ground Control Station (GCS) is an essential element to supervise and control autonomous vehicles performing complex missions in real time. In the new era of Internet of Things, where systems are highly connected, these missions demand enormous amounts of computational power to correctly manage the coordination of all the vehicles involved. In this scope, the set of Unmanned Vehicles (UVs) included in the mission must achieve more difficult tasks everyday. As a consequence, the development of a robust, reusable and adaptable GCS framework to allow a single operator to monitor and control a team of heterogeneous agents raises a number of research and engineering challenges. In this paper we introduce an adaptive event-driven framework specially designed for GCSs involved in heterogeneous multi-agent missions that takes advantage of two features: 1) it allows the GCS to add or remove both actual or simulated agents in real time, changing the number or types of monitored agents, and 2) from a software design perspective, the graphical user interface dynamically changes its view in order to minimize operators fatigue and mental workload, facilitating the success of the mission in such complex environments. We also show one of the tests performed with the adaptive framework, where after observing how a real UV deployed in a water surface performs successfully a set of previously planned trajectories, we will see how a simulated UV joins the mission in order to fulfill a leader-follower maneuver."}, {"label": 0, "content": "Recent topics of interest such as smart cities and autonomous driving are currently in focus of many research activities. In this context, simulations are used to evaluate new algorithms, performance of current technologies, or the impact of upcoming products. In particular, they allow finding errors and optimizing parameter sets prospectively, prior to a real-world implementation. Simulation models of many traffic problems need to handle large-scale scenarios, connect entities from different domains, and run in feasible time. In order to meet these challenges, an extendable multi-level traffic simulation approach is proposed in this paper. We briefly introduce existing traffic simulation techniques, name upcoming problems, available solution approaches, and topics regarding the development of our framework. As a first step, we coupled two different resolution levels of traffic simulation by using High Level Architecture (HLA) and evaluated this approach in light of simulation results and simulation performance."}, {"label": 0, "content": "Despite importance higher education institution sustainable development paradigm and many publications in this area, there are at least two real problems that are restricted successful implementation of this paradigm: a) complexity the subject domain and b) lack of a methodological base to reuse some interesting already created models together with future models. This paper introduces an approach to reduce the subject domain complexity by substituting it with a set of abstract infrastructure formalized systems. Within this approach, the models' reusability problem can be reduced by using a simulation umbrella as a united methodological base of the paradigm implementation. To Illustrate how the proposed approach works the paper includes a simulation case-study associated with a USA young fast-growing higher education institution."}, {"label": 0, "content": "Despite having one of the most efficient transportation systems in the world, Singapore is still faced with congestion issues regularly, especially during peak hour periods, due to a number of reasons. We investigate some of the factors contributing to this issue and propose a simulator supplied with predictive travel times through congestion prediction, in order to evaluate and improve bus utilization through effective scheduling. We introduced a conceptual framework to integrate neural network models into simulation so as to improve real-time supply based on several possibilities of demands. This paper will delineate the steps taken to produce the simulator and discuss the evaluation of these models."}, {"label": 0, "content": "Virtual Machine (VM) live migration is one strategic approach that can be employed to reduce energy consumption and increase the utilisation of a single computer in large computing infrastructure. However, virtualisation in High Throughput Computing (HTC) has received limited attention in the literature. In this paper, we present an extension of an existing trace-driven simulation to incorporate virtualisation. Furthermore, we implement the pre-copy live migration algorithm to provide a test environment for job live migration in HTC system. Our simulation provides the total number of migrations and their overall time of migrations as well as calculates the energy consumption of migrations during its runtime. In this paper, we propose two methods to perform the live migration in the HTC system. We demonstrate that our responsive migration could save up to 75% of the system wasted energy."}, {"label": 0, "content": "A thorough performance evaluation of protocols and algorithms for (wireless) networks requires simulation and real-system experiments, as both of them provide individual benefits. Usually, this calls for two separate implementations: One tailored to a discrete-event simulator and a second designed to run on real hardware. Therefore, significant effort is required to implement the same mechanisms or protocols twice. To avoid this overhead, we propose a comprehensive framework based on DPDK and OMNeT ++, allowing to run simulations and real-system experiments from the very same codebase. Hence the best of both worlds is available: scalable scenarios and reproducibility when simulating, and realistic behavior and real-world performance metrics when running real-system experiments. Our evaluation of several representative real-world networking scenarios analyzes similarities between simulation and real-system results and discusses the framework qualitatively. Quantitative results indicate that the approach performs well, i.e., it allows even for productive deployment using the codebase later on, and results from both worlds are comparable."}, {"label": 0, "content": "Microscopic traffic simulation is associated with substantial runtimes, limiting the feasibility of large-scale evaluation of traffic scenarios. Even though today heterogeneous hardware comprised of CPUs, graphics processing units (GPUs) and fused CPU-GPU devices is inexpensive and widely available, common traffic simulators still rely purely on CPU-based execution, leaving substantial acceleration potentials untapped. A number of existing works have considered the execution of traffic simulations on accelerators, but have relied on simplified models of road networks and driver behaviour tailored to the given hardware platform. Thus, the existing approaches cannot directly benefit from the vast body of research on the validity of common traffic simulation models. In this paper, we explore the performance gains achievable through the use of heterogeneous hardware when relying on typical traffic simulation models used in CPU-based simulators. We propose a partial offloading approach that relies either on a dedicated GPU or a fused CPU-GPU device. Further, we present a traffic simulation running fully on a manycore GPU and discuss the challenges of this approach. Our results show that a CPU-based parallelisation closely approaches the results of partial offloading, while full offloading substantially outperforms the other approaches. We achieve a speedup of up to 28.7\u00d7 over the sequential execution on a CPU."}, {"label": 0, "content": "In distributed environments, where unknown entities cooperate to achieve complex goals, intelligent techniques for estimating agents' truthfulness are required. Distributed Reputation Management Systems (RMSs) allow to accomplish this task without the need for a central entity that may represent a bottleneck and a single point of failure. The design of a distributed RMS is a challenging task due to a multitude of factors that could impact on its performances. In order to support the researcher in evaluating the RMS robustness against security attacks since its beginning design phase, in this work we present a distributed simulation environment that allows to model both the agent's behaviors and the logic of the RMS itself. Moreover, in order to compare at simulation time the performance of the designed distributed RMS with a baseline obtained by an ideal RMS, we introduce an omniscient process called truth-holder which owns a global knowledge all involved entities. The effectiveness of our platform was proved by a set of experiments aimed at measuring the vulnerability of a RMS to a common set of security attacks."}, {"label": 0, "content": "High-speed permanent magnetic synchronous machine (PMSM) drive systems offer several advantages compared to low and medium speed drive systems, such as higher power density, capability of directly driving high speed loads. Due to reliability and rotor dynamic balance problem, sensorless drive is preferred in high-speed drive system. This paper proposed a model reference adaptive system based sensorless control method of a high-speed permanent magnetic machine with 1- F startup strategy. The proposed method can guarantee reliable startup ability and achieve smooth transition between the open-loop and closed-loop control scheme. Also, more accurate rotor position estimation is obtained with the compensation of voltage in high frequency."}, {"label": 0, "content": "A remote monitoring system of multifunctional direct drinking machine based on Internet of things (IoT) is designed in this paper. The local control system detected the status data of water purifiers according to the temperature sensor, the flowmeter and the water level gauge, then sent the collected parameters to the backstage management system in the cloud by General Packet Radio Service (GPRS) communication network or Wireless Fidelity (WIFI) in real time. Prototype design and applications show that the management system has friendly user interface and is easy to be operated. It may display the working states of the direct drinking machine in real time, manage big data, and support remote operation of the equipment. Radio frequency identification (RFID) technology and mobile payment technology are employed to realize automatic user identification, water purchase, information maintenance and other functions. The comprehensive monitoring and controlling system has realized the function of connecting users and multifunctional direct drinking machines in the Internet of things, which has good application potential."}, {"label": 0, "content": "Nowadays, human society has entered the digital and network information era. A unique feature of this era is the digitalization of personal identity. The worldwide demand for accurate and reliable identification and authentication is steadily growing, and biometric authentication technologies, such as fingerprint, face, finger vein, iris and DNA, are playing an increasingly important role in our society. Due to the rapid development of Internet and cloud computing technologies, cloud biometric authentication technology is becoming a vital development direction for biometric technology. Among these, iris recognition is an important biometric technology. This paper presents a method for cloud-based iris recognition to illustrate the relationship between cloud computing/storage and cloud recognition."}, {"label": 0, "content": "For an interior permanent magnet synchronous machine (IPMSM), parameters mismatching would affect the performance and reduce the output efficiency.In this paper, a variable parameter Maximum Torque Per Ampere(MTP A)control method based on online parameters identification was proposed. Firstly, an adaptive linear neural network algorithm is used to identify the motor parameters, and then the MTPA control is modified by combining the identified parameters. The results show that the proposed strategy has a nice adaptation to parameters mismatching, along with an improved output efficiency which is beneficial to magnetic recording motor."}, {"label": 0, "content": "The ever-increasing number of wireless network systems brought a problem of spectrum congestion leading to slow data communications. All of the radio spectrums are allocated to different users, services and applications. Hence studies have shown that some of those spectrum bands are underutilized while others are congested. Cognitive radio concept has evolved to solve the problem of spectrum congestion by allowing cognitive users to opportunistically utilize the underutilized spectrum while minimizing interference with other users. Byzantine attack is one of the security issues which threaten the successful deployment of this technology. Byzantine attack is compromised cognitive radios which relay falsified data about the availability of the spectrum to other legitimate cognitive radios in the network leading interference. In this paper we are proposing a security measure to thwart the effect caused by these attacks and compared it to Attack-Proof Cooperative Spectrum Sensing."}, {"label": 0, "content": "High computational requirements are commonly associated with the hydraulic simulation of large-scale water distribution. The convergence of the cumbersome iterative procedures involved has been a well-debated issue for the past decades. The large-scale and non-linear properties pose a great hindrance towards the development of online applications for water distribution network (WDN) analysis and pressure control thereof. Consequently, there has been a great interest in the deployment of model-free techniques to mimic the rather computationally expensive non-linear hydraulic simulations. As the hydraulic simulation based research is still being conducted, the advantages of model-free techniques make them more suitable alternatives. Artificial neural networks (ANN) is one of the most successful model-free methods for WDN analysis and management. In this paper, a literature synopsis of existing applications of model-free approaches in water distribution is presented. The technical advantages of applying such technique in a large-scale non-linear network are brought up in this paper."}, {"label": 0, "content": "Vehicular Ad-hoc Network (VANET) is a definite form of Mobile ad-hoc Network (MANET), which delivers data communication in Vehicular environment using wireless transmission. Its key goal is to increase the service quality of intelligent transportation systems (ITS), such as road safety, logistics, and environmental kindliness as well as information exchange. Nowadays transportation systems are facing serious issues in terms of performance and efficiency of VANET applications, nevertheless these depend typically on the method in which messages are conveyed between the nodes. Finding a better routing protocol for dynamic VANET systems is one of the main challenges. This paper presents an Ant Colony Hybrid Routing Protocol (ACOHRP) to improve the service quality of ITS, by increasing the efficiency and reliability of vehicle traffic information message transmission. ACOHRP delivers high efficiency through better beginning of packet delivery ratio to end-to-end delay. A comparative study on the proposed ACOHRP and existing Dynamic Source Routing (DSR) protocol is conducted in a realistic scenario with VANET architecture, to demonstrate the performance of the proposed method. Simulation based testing is performed using Matlab with ACOHRP performing better than DSR in a dynamic environment of VANETs."}, {"label": 0, "content": "Ad-hoc Mobile Cloud (AMC) came up as a result of the need to alleviate the inadvertent and incessant connectivity challenges that are experienced in Mobile Cloud computing (MCC). Thus, mobile devices teamed up in groups to share their resources among one another which are majorly Web Services, Storage and Computing resources. However, potential participants in AMC often feel a level of insecurity as they envisaged loss of control over various personal and confidential data in their mobile devices which can occur in the course of sharing of resources. Therefore, AMC as a service provisioning paradigm for mobile device users has a need for an implementation of a trust management system to serve as a protection and guarantee of confidence for intended AMC participants. This study proposed a Multi-Criteria Trust Management system (MCTM) architecture for AMC to protect the interest of various participating mobile devices. This system provides an avenue to identify trustworthy mobile devices to whom they can carry out resource sharing and as well alert or notify them of any malicious act that could have happened. We carried out a simple evaluation procedure with a number of incorrect service matches in the course of responding to a query as well as comparing the level of precision attained by the proposed architecture with already existing research. The proposed system proves to give a maximum confidence to mobile users to show high interest in participating in the AMC system."}, {"label": 0, "content": "Recommendation systems for mobile phones are of great importance for mobile operators to achieve their desired profit targets. In a client inferred market, the number of contract users and contract phones is especially significant for mobile service operators. The tremendous growth in the number of available mobile cellular telephone contracts necessitates the need for a recommender system to assist users discover suitable contracts based on their usage patterns. This study used a hybrid of both collaborative and content-based filtering. A prototype of a mobile recommender system was developed and evaluated using precision and recall. The developed recommender system was able to successfully recommend packages to subscribers. A precision-recall curve was produced, and it showed good performance of the system. This study successfully showed that a hybrid system was able to recommend products to the mobile subscribers."}, {"label": 0, "content": "In this paper, we introduce an alternative solution to the many existing IoT data acquisition and storage systems. We present a self-designed and developed prototype electronic circuit extension for Raspberry Pi development board used for collecting sensor data. There is also presented a Pi4Java API based Java application used for sensor data collection and storage. We set up an Apache Cassandra database cluster, to stores large amounts of sensor data on low-cost servers, providing high availability. In addition, a web application is also presented, that allows different data visualization operations to be performed on the stored data. The presented system is a full IoT data acquisition, storage and visualization solution."}, {"label": 0, "content": "In this manuscript a transmission technique, which can save energy thanks to a supportive transmission in the feedback channel is presented. It could be useful in future telemetry networks embedded into IoT or stand-alone WSNs or DSC. The basic round condition for its practical exploitation is that a central node collecting information has enough energy-much more than the supported node. It is suitable in a scenario in which the data collected by the central node and transmitted could be modelled as a Poisson distributed random variable. It is a modification of the technique presented in [1]. In contrast to [1] in this new method the central node does not need to have any prediction capabilities. Beyond this it allows us to make a tradeoff between the achieved gain in savings and delay in data delivery. A proof of concept is given by an example in which approximately 1 bit on average is needed to transmit one value of the Poisson distributed random variable."}, {"label": 0, "content": "While houses were simply connected to the electrical grid and contained a gas or oil heating system, for a long time energy flows were simple. With the integration of alternative energy systems such as photovoltaic (PV) systems, heat pumps, combined heat and power units (CHPs), home batteries and electrical cars into houses, energy flows become more complicated. This paper describes the implementation of a model to simulate the technical and economical outcome of a system with the previously mentioned components in single- and multi-family houses in Germany. As input for climate data and energy demand, literature data is used. The simulation is capable of economic and ecologic estimation and can be controlled by a web interface with the intention to provide a straightforward online tool for homeowners."}, {"label": 0, "content": "The idea of Non-Intrusive Load Monitoring (NILM) is to monitor a network of electronic consumers through a centralized capture of current and voltage. Based on the waveform, the devices can be distinguished by their specific turn-on and turn-off events. Most methods use the power signals active power P and reactive power Q (PQ). Various types of machine learning and classification algorithms have already been applied to a variety of different datasets. The power signals can already achieve good results for different classification algorithms on different datasets. However, the information content of the power signals is limited. When calculating the power from the current and voltage signal, information is lost through integration over a period. Many publications in the field are based on proprietary, unpublished datasets. Often, only particular scenarios are in focus. Thus, real comparability between the works is difficult. The Home Equipment Laboratory Dataset 1 (HELD1) dataset introduces the current-voltage based new waveform called Frequency Invariant Transformation of Periodic Signals (FIT-PS) for use with the well-known standard machine learning algorithms, k Nearest Neighbors Classifier (kNN), Support Vector Machine Classifier (SVM) and Naive Bayes Classifier (Bayes). With FIT-PS, more than 90% of events can be correctly assigned to the devices. This work allows an equitable comparison for already applied methods in the context of NILM with the effectiveness of this new waveform."}, {"label": 0, "content": "This paper deals with the processing of vehicle data in a stream using machine learning. In modern vehicles, a significant amount of sensor data are generated, which are only used temporarily before being discarded. The toolchain presented here aims to historize the data and evaluate it in near real time. Stream machine learning is used to process the data. Requirements for the toolchain are the platform-independent use of these and the free provision of the tools used. The result is a complete and innovative toolchain that maps everything required. From the reading of data on the vehicle to the use of stream machine learning and the evaluation of the data. An illustrative use case is presented and an outlook on extensions of the toolchain is given."}, {"label": 0, "content": "The aim of our research was to elaborate the current concept of the IoT based on the scientific papers up to now and to draw up the potential legal and security risks, which may affect the users or the state. We categorized the challenges of the IoT usage. The main problems in the aspect of data protection, are traceability and confidentiality issues."}, {"label": 0, "content": "Epilepsy is a chronic disorder that causes unprovoked, recurrent sudden abnormal reactions of the brain. Characterizing electroencephalogram (EEG) signals of the patient is an effective way for the early prediction of epileptic seizures. In this paper, a new method called the entropy of visibility heights of hierarchical neighbors (EVHHN) is proposed to detect seizures from the EEG signals. First, the visibility relationships of three nearest neighbors are determined by a visibility criterion. Then, we compute the visibility heights of three nearest neighbors for each data point. Next, the four different kinds of entropy associated with neighbor visibility states are calculated to characterize the EEG signals and finally these features are validated by LS-SVM classifiers. In the experiment, the normal and ictal EEG signals are classified with the accuracy of 99.6%, meanwhile, the interictal and ictal EEG signals are distinguished with the accuracy of 98.35%, which proves the effectiveness of our proposed method. Notably, the computational time of extracting features for each set is 1.751 s, which is largely reduced compared with other weighted visibility graph-based methods. In conclusion, the EVHHN can potentially be an effective method for characterizing complicated EEG signals and real-time detection of epileptic seizures."}, {"label": 0, "content": "Feature extraction is of great importance for running states monitoring and performance evaluation of mechanical electro-hydraulic systems (MEHS). However, due to the complexity of the multi-domain energy conversion property of MEHS, especially during the varying operation conditions, it is quite difficult to extract the desired features effectively. In addition, the conventional signals are difficult to be collected and analyzed, as different kinds of coupled information are mixed together. Therefore, based on a power distribution analysis of MEHS, it is found that the change rate of the kinetic energy (CRKE) can be considered as a suitable index for evaluating the performance, such as energy saving and output stationarity of the considered MEHS. In order to characterize the magnitude of CRKE, a cooperation analysis method is proposed by using internal and external features. In the proposed method, the kinetic energy stiffness (KES) is selected as the internal feature, while the instantaneous speed fluctuation (ISF) is chosen as the external feature. According to a Lissajous figure-based information fusion approach and the order tracking technology, a systematic method is developed to obtain the magnitudes of KES and ISF. Furthermore, based on the complementary advantages and mutual relationship of KES and ISF, the performance of MEHS is analyzed under varying operation conditions. The proposed method is verified through experiments with a real rig. The results show that the changes of KES and ISF can effectively reflect the change in the operation condition, and lower KES loss can improve the efficiency of the system and also restrain the ISF."}, {"label": 0, "content": "Automatic age estimation from facial images has attracted increasing attention due to its promising potential in real-life computer vision applications. However, due to uncontrollable environments, insufficient and incomplete training data, strong person-specific and large within- age span variations, age estimation has become a challenging problem. Among published age estimation, hierarchical age estimation methods have achieved comparable performance improvement than single level approaches. Most of the published hierarchical approaches have mainly used support vector machines to classify age groups followed by support vector regression for withina- age group age estimation. In this paper, we present a novel hierarchical Gaussian process framework for automatic age estimation. It consists of multi-class Gaussian process classifier to classify the input images into different age groups followed by a warped Gaussian process regression to model group specific aging patterns. In this paper, we separately tune the hyper-parameters for each age group at the regression stage. Compared with existing single level Gaussian process approaches for age estimation, our approach is computationally efficient at both the levels of hierarchy. Partitioning data into different age groups and learning group-wise hyper-parameters is computationally more efficient than learning complete training data. Misclassifications at the group boundaries are compensated at the regression stage by overlapping the neighboring age ranges. Finally, through extensive experiments on two popular aging datasets, the FG-NET and the Morph-II, we demonstrate the effectiveness of our algorithm in improving age estimation performance."}, {"label": 0, "content": "Just noticeable difference (JND) for stereoscopic 3D content reflects the maximum tolerable distortion; it corresponds to the visibility threshold of the asymmetric distortions in the left and right contents. The 3D-JND models can be used to improve the efficiency of the 3D compression or the 3D quality assessment. Compared to 2D-JND models, the 3D-JND models appeared recently and the related literature is rather limited. In this paper, we give a deep and comprehensive study of the pixel-based 3D-JND models. To our best knowledge, this is the first review on 3D-JND models. Each model is briefly described by giving its rationale and main components in addition to providing exhaustive information about the targeted application, the pros, and cons. Moreover, we present the characteristics of the human visual system presented in these models. In addition, we analyze and compare the 3D-JND models thoroughly using qualitative and quantitative performance evaluation based on Middlebury stereo datasets. Besides, we measure the JND thresholds of the asymmetric distortion based on psychophysical experiments and compare these experimental results to the estimates from the 3D-JND models in order to evaluate the accuracy of each model."}, {"label": 0, "content": "Advanced communications and networks greatly enhance the user experience and have a major impact on all aspects of people\u2019s lifestyles. Widely deployed sensor nodes provide support for these services. However, although energy harvesting and transfer technology provides a solution to allow the long-term survival of wireless sensor nodes for wireless sensor networks, the single collection scheme causes a lot of energy waste. Thus, efficient energy utilization and fast data collection are still serious challenges for energy harvesting wireless sensor networks. To overcome these challenges, an adaptive collection scheme based on matrix completion (ACMC) is proposed to reduce delay and to improve the energy utilization of the network. In the ACMC scheme, compared with traditional data collection schemes, the data collection schemes vary with the available energy, collecting large amounts of data when the available energy is sufficient to obtain high-quality data-based applications. Otherwise, adaptive selecting the collected data based on previously collected data, the amount of data collected can be effectively reduced based on the application requirements, thereby improving the energy utilization of the network. The ACMC scheme also proposes a method for reducing the delay by increasing the duty cycle of the nodes that are far from the CC. At the same time, the transmission reliability of these nodes increases due to the increase in the transmission frequency. Thus, the ACMC scheme can also further reduce the delay of the network. The experimental results of the ACMC scheme in planar networks show better performance than the traditional data collection schemes and can improve the energy utilization of the network by 4.26%\u20136.68% while reducing the maximum delay by 9.4%."}, {"label": 0, "content": "Planetary gear is an important part of the transmission system of large electromechanical equipment. Therefore, it is very important to monitor the degradation of the state of the planetary gear. A method for the degradation state recognition of planetary gear based on the features with multiple perspectives and linear local tangent space alignment (LLTSA) algorithm is presented. First, the time domain features of the original vibration signal are extracted, which have the statistical properties and global significance. Then, the detailed features which pay more attention to the detailed information of the vibration signal are extracted on the basis of improved complete ensemble empirical mode decomposition with adaptive noise, and all those features constitute high dimensional original features. In order to solve the problems of information redundancy and interference features, the original features are processed by LLTSA, and the extraction of low dimensional sensitive features can be achieved. Finally, the optimized support vector machine is studied to recognize the low dimensional sensitive features. The result shows that the proposed method can recognize different degradation states of planetary gear accurately and effectively."}, {"label": 0, "content": "It is crucial to implement an effective and accurate fault diagnosis of a gearbox for mechanical systems. However, being composed of many mechanical parts, a gearbox has a variety of failure modes resulting in the difficulty of accurate fault diagnosis. Moreover, it is easy to obtain raw vibration signals from real gearbox applications, but it requires significant costs to label them, especially for multi-fault modes. These issues challenge the traditional supervised learning methods of fault diagnosis. To solve these problems, we develop an active learning strategy based on uncertainty and complexity. Therefore, a new diagnostic method for a gearbox is proposed based on the present active learning, empirical mode decomposition-singular value decomposition (EMD-SVD) and random forests (RF). First, the EMD-SVD is used to obtain feature vectors from raw signals. Second, the proposed active learning scheme selects the most valuable unlabeled samples, which are then labeled and added to the training data set. Finally, the RF, trained by the new training data, is employed to recognize the fault modes of a gearbox. Two cases are studied based on experimental gearbox fault diagnostic data, and a supervised learning method, as well as other active learning methods, are compared. The results show that the proposed method outperforms the two common types of methods, thus validating its effectiveness and superiority."}, {"label": 0, "content": "The aim of this research is to develop an innovative low cost and affordable platform for smart home control and energy monitoring interfaced with augmented reality. This method will educate people about energy use at a time when fuel costs are rising and create novel methods of interaction for those with disabilities. In order to increase the awareness of energy consumption, we have developed an interactive system using Augmented Reality to show live energy usage of electrical components. This system allows the user to view his real time energy consumption and at the same time offers the possibility to interact with the device in Augmented Reality. The energy usage was captured and stored in a database which can be accessed for energy monitoring. We believe that the combinations of both, complex smart home applications and transparent interactive user interface will increase the awareness of energy consumption."}, {"label": 0, "content": "This work is a continuation of a larger research work which advocates that Distance Education (DE) through audio-only learning mode can be developed into a full fledge audio-MOOC. Audio MOOC framework is an innovative framework which enables learning through mere phone calls. It has been conceived to digitally include low literate population in the education process by opening up access to learning materials to the unreached and the have-nots usually hindered by barriers such as language, literacy, culture, connectivity and distance which existing MOOCs have failed to address. This work demonstrates how our proposed framework is used to connect to a remote island lost in the middle of the Indian Ocean with limited maritime and air access but which since some few years back can be connected via basic phones through voice calls. Agalega is an ideal test case scenario for our research since it characterizes remoteness, limited connectivity, semi-literate population with limited access to education which our research aims at addressing. A group of 50 Fishermen was identified from both the Agalega islands. The course was of 9 days duration from 15 to 23 September 2017. The system was conducted live over the telephony network making use of our GSM gateway. The specificity of the system was that our GSM gateway resided in Mauritius connected to a cloud server, while the course was delivered to people of Agalega 1100 Km far from Mauritius over the sea. Nevertheless, our system performed as expected and proved to be a success."}, {"label": 0, "content": "Modern tactical wireless network (TWN) communication technologies are not only capable of transmitting voice but also capable of transmitting data. Due to such capabilities, TWN have high security requirements as any security breach can lead to detrimental effects. Hence, securing such an environment is not only a requirement but also a virtual prerequisite to the network centric warfare operational (NCW) theory. One key to securing this environment is to promptly and accurately recognize information warfare attacks directed to the network and respond to them. This is achieved using intrusion detection systems (IDS). However, false detection of nodes in hostile environment remains a major problem that need to be addressed. Recently, machine learning methods and algorithms have shown applicability and are growing research area for cyber security and intrusion detection. Conversely, several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. The question then becomes, which one amongst these machine learning algorithms have the potential to enhance or address IDS issues in TWN. In this paper, seven machine learning classifiers are analyzed; Multi-Layer Perceptron, Bayesian Network, Support Vector Machine (SMO), Adaboost, Random Forest, Bootstrap Aggregation, and Decision Tree (J48). WEKA tool was used to implement and evaluate the classifiers. The results obtained indicate that ensemble-based learning methods outperformed single learning methods when we consider the detection accuracy metrics; AUC, TPR, and FPR. However, ensemble classifiers tend to be slower in in terms of build time and model test time."}, {"label": 0, "content": "We report on two sets of perceptual evaluations of our South African text-to-speech voices by language practitioners. In the first evaluation, we measure baseline quality in terms of how understandable and human-like the voices sound. We also determine baseline usability by asking a series of questions related to accessibility and mainstream application settings. In the second evaluation, we employ the same criteria to compare pronunciation improvements against the baseline. The results indicate success in many areas, but also illuminate room for improvement in others, especially in the cases of the African languages."}, {"label": 0, "content": "Orthogonal frequency division multiplexer (OFDM) is a recent modulation scheme used to transmit signals across power line communication (PLC) channel due to its robustness against some known PLC problems. However, this scheme is greatly affected by the impulsive noise (IN) and often causes corruption with the transmitted bits. Different impulsive noise error correcting methods have been introduced and used to remove impulsive noise in OFDM systems. However, these techniques suffer some limitations and require much signal to noise ratio (SNR) power to operate. In this paper, an approach of designing an effective impulsive-noise error-correcting technique was introduced using three-known artificial neural network techniques (Levenberg-Marquardt, Scaled conjugate gradient, and Bayesian regularization). Findings suggest that both Bayesian regularization and Levenberg-Marquardt ANN techniques can be used to effectively remove the impulsive noise present in an OFDM channel and using the least SNR power."}, {"label": 0, "content": "Due to the various advantages that the cloud can offer to robots, there has been the recent emergence of the cloud robotics paradigm. Cloud robotics permits robots to unload computing and storage related tasks into the cloud, and as such, robots can be built with smaller on-board computers. The use of cloud-robotics also allows robots to share knowledge within the community over a dedicated cloud space. In order to build-up robots that benefit from the cloud-robotics paradigm, different cloud-robotics platforms have been released during recent years. This paper critically reviews and compares existing cloud robotic platforms in order to provide recommendations on future use and gaps that still need to be addressed. To achieve this, 8 cloud robotic platforms were investigated. Key findings reveal varying underlying architectures and models adopted by these platforms, in addition to different features offered to end-users."}, {"label": 0, "content": "Cognitive Radio networks (CR) is a new technique that uses available unlicensed spectrum band this is due to the limited number of fixed licensed spectrum bands [1]. The main features of the CR are spectrum sensing, management, sharing, and mobility [2]. This paper focuses on spectrum mobility for military systems often referred to as spectrum handoff for military systems, which is a process when the CR user changes its frequency of operation due to spectrum occupancy by the licensed user. Spectrum mobility is important in military networks since mission success may depend on reliable communications. In this paper, we propose the use of Fuzzy logic to come up with a decision making handoff scheme. The proposed scheme avoids interruptions caused by the movement of a secondary user and minimizes handoff latency if the SU selects a correct channel. It was observed through the analysis of literature that even though most of the existing schemes can perform handoff successfully, these existing schemes result in slow handoff and are complicated to implement."}, {"label": 0, "content": "Passwords form part of our daily routine and even though there are alternative authentication mechanisms, such as biometrics, passwords stubbornly persist. Passwords have been around for the last few decades, but also the various problems associated with users trying to create passwords that are strong and secure. Users are faced with a cognitive burden in managing passwords which often leads to poor password practices or users recycling passwords across various accounts. While there is no anticipated end to the use of passwords, scholars have identified that passwords need to be better supported - one such method is using a password manager. There is a wealth of technical research relating to password managers, which has led to drastic improvements and the maturing of the technology. However, there is a little research on why people would choose to adopt password managers. To explore these factors, this research uses an adapted version of the Unified Theory of Acceptance and Use of Technology (UTAUT2) that includes trust as an additional construct. Using empirical data, the results of the study show that performance expectancy, habit, and trust are key factors in the intention to adopt a password manager."}, {"label": 0, "content": "Internet of Things has gained the attention of almost everybody due to its capability of monitoring and controlling the environment. IoT helps making decisions supported by real data collected using large number of ordinary day-to-day devices that have been augmented with intelligence through the installation of sensing, processing and communication capabilities. One of the main and important aspects of any IoT device is its communication capability for transferring and sharing data between other devices. IoT devices mainly use wireless communication for communicating with other devices. The industry and the research community have proposed many communication technologies for IoT systems. In this paper, the authors present the results of an in-depth study carried out on the benefits and limitations of these communication technologies."}, {"label": 0, "content": "This prototype system known as the Wearable Instantaneous Ball Speed Estimator (WIBASE) was designed to measure the bowling speed of a cricketer during training. When fast bowlers are training, coaches have to assess their ability to bowl consistently fast balls even when they are required to perform long bowling spells, hence the need for reliable, accessible and affordable equipment for measuring their bowling speed cannot be over emphasised. The WIBASE seeks to fill in this gap. It is made up of two hardware components; a computer and a wrist-worn electronic board that houses among other components, a 3-dimensional (3D) acceleration sensor. The system tracks the three-axis acceleration generated by the movement of the arm when delivering the ball and stores these values. The raw sensor data from three different sensors namely accelerometer, gyroscope and magnetometer is processed by a Digital Motion Processor (DMP) on the board in a process known as Sensor Fusion before it is sent via Bluetooth to the computer. The computer runs a Python script that receives the filtered acceleration which consists of both static acceleration and dynamic acceleration. The acceleration is numerically integrated over a minute period of time around the release point using the Trapezoidal method of integrating numerical data to derive the speed of the cricket bowler. The results obtained from the three sets of experiments that were conducted show that the WIBASE can track the 3D acceleration of the hand when bowling, derive the speed of the bowlers and display the speed on a computer while logging all the data into a file."}, {"label": 0, "content": "The most efficient solvers use composite procedures that adaptively rearrange computation algorithms to maximize simulation performance. A similar concept can be integrated into a process of electronic circuit analysis, where the combination of different algorithms allows scalability of simulation performance. In this paper, we propose a new adaptive internal solver based on Biconjugate gradient stabilized method for the iterative solution of nonsymmetric linear systems supplemented with incomplete LU factorization as an efficient replacement for the direct solver implemented in program Spice for solving large-scale circuits. We describe basic concepts of a simulation of electronic circuits with nonlinear time dependent devices and present implementation examples of the proposed methods. Optimal setting of the method and its application in program Spice is shown in comparison to other modern iterative solvers for nonsymmetric linear systems."}, {"label": 0, "content": "In the field of Cyber Security there has been a transition from the stage of Cyber Criminality to the stage of Cyber War over the last few years. According to the new challenges, the expert community has two main approaches: to adopt the philosophy and methods of Military Intelligence, and to use Artificial Intelligence methods for counteraction of Cyber Attacks. This paper describes some of the results obtained at Technical University of Sofia in the implementation of project related to the application of intelligent methods for increasing the security in computer networks. The analysis of the feasibility of various Artificial Intelligence methods has shown that a method that is equally effective for all stages of the Cyber Intelligence cannot be identified. While for Tactical Cyber Threats Intelligence has been selected and experimented a Multi-Agent System, the Recurrent Neural Networks are offered for the needs of Operational Cyber Threats Intelligence."}, {"label": 0, "content": "Data infrastructure and quality are vital organs influencing the health of any organizations and are essential for creating and delivering business insights. Due to this fact, stakeholders expect a flawless experience, real-time solutions and support. However, these expectations are normally too high for IT departments that are always immersed with various users' requests. The Big Data era and its analytics have put organizations in positions to predict and forecast users' needs to produce a great user experience. This is achieved through intuitive design, error-free coding and quality performance. There is increasing demand for better business analytics so as to enable organizations establish solid foundation by building out a data management ecosystem that delivers flexibility and performance required by cognitive solutions. The objective of this paper was to highlight how universities can leverage better analytics to process amount of continuously generated data from various sources and generate actionable insights needed to achieve academic business goals. This study followed an interpretive paradigm taking a case study of a South African university and employed semi-structured interviews. This study found that data value and the use thereof in the management of academic divisions of the university should be consistent with faculty business plans. In this way, business analytics is not haphazardly used but is strategically aligned with faculty plans and overall institutional strategic objectives and targets."}, {"label": 0, "content": "In this paper, we present a state-of-the-art motor imagery brain computer interface system (BCI) based on non-invasive approach in the form of electroencephalogram (EEG) with an objective of evaluating the performance of supervised machine-learning algorithms applied on features extracted from pre-processed EEG signals. Two categories of features were utilized namely a high dimensional feature set extracted from 22 EEG channels and a feature set extracted from two EEG channels (C3 and C4). Four signal classifiers namely KNN, Regression tree, NB and LDA are applied on wavelet-based EEG signal features for discrimination of four classes of motor imagery (MI) tasks (left hand, right hand, foot and tongue). Efficient discrimination of motor imagery tasks is significantly dependent on signal-to-noise ratio of EEG signals to enhance the performance of signal classifiers. A pre-processing technique is firstly applied on filtered EEG signals to remove contamination in the form of artifacts. Then, useful signal features are extracted from artifact free EEG signals, whereby relevant subsets with high predictive power are selected using feature selection technique. The best features subsets are fed into signal classifiers for classification purposes. A highest average classification accuracy of 73.06% and 72.95% was achieved using NB while classifying both features acquired from 2 and 22 EEG channels respectively."}, {"label": 0, "content": "Retinal vessel tortuosity is an early indicator of different retinopathies. Although various automated methods in determining retinal vessel tortuosity have been proposed in the literature, there are needs for further study. This study extracted three different features namely distance metric, normalized hybrid metric and non-normalized hybrid metric from the thinned vessels. The weights of vessel data samples were dynamically updated using the Adaboost with linear discriminant analysis (LDA) and the feature correlation was used to facilitate the selection of the best feature combination at each of the boosting iteration rather than a single feature that minimizes the weighted error at each of the iterations. Adaboost with LDA method is then used for the classification of the retinal vessels as either tortuous or normal using a majority voting method. The proposed method achieves the accuracy rate of 100% for the training sample sizes of 70%, 80% and 90%."}, {"label": 0, "content": "This paper presents a pre-processed faster region convolution neural network (faster RCNN) for the purpose of on-road vehicle detection. The system introduces a preprocessing pipeline on faster RCNN. The preprocessing method is for the improvement on training and detection speed of Faster RCNN. A preprocessing lane detection pipeline based on the Sobel edge operator and Hough Transform is used to detect lanes. A Rectangular region is then extracted from lane coordinates which is a reduced region of interest (ROI). Results show that the proposed method improves the training speed of faster RCNN when compared to faster RCNN without preprocessing."}, {"label": 0, "content": "Mauritius suffers from chronic water shortages that can severely impact its economy and the well-being of its population. Both surface and groundwater availability are determined by rainfall, which is in turn influenced by large-scale circulation patterns such as the El Ni\u00f1o Southern Oscillation (ENSO) and the Indian Ocean Dipole (IOD). Here we report on the influence of these two teleconnection patterns and present the result of a simple neural network for precipitation forecasting, based on the state of ENSO and IOD. Data from the Vacaos station, for the period 1961 to 2012 is used. We found statistically significant correlation between average winter rainfall and ENSO and IOD indices. The correlation for summer was negligible. The prediction of summer precipitation was less accurate than that of winter precipitation. The findings from this study can help in more efficient planning and management of water resources on the island."}, {"label": 0, "content": "The provision of Cloud services through the use of a Cloudlet technology, provides a number of advantages with regards to Quality of Experience (QoE) for its consumers. The QoE includes, but is not limited to free of cost, low-latency, and one-hop WiFi network consumption. Most of the advantages are achieved through the deployment of a comprehensive business model. A well-defined business model is one with well-defined value logic. Value creation is a fundamental element of a business model. It helps identify relevant customer segments, value proposition for customers and mechanism that will be used to provide the created value. However, poorly defined value logic into a business model can lead to business failure in addressing their customers' needs and issues. The literature reveals that researchers delved more into the evaluation of Cloud business models and its subsequent value creation, but none has attempted on either creation or deployment of a Cloudlet Business model. Therefore, this study introduces a Cloudlet Business Model (CBM) that can be used to support the easy deployment of a Cloudlet technology by SMEs such as coffee shops and shopping malls. In order for SMEs to meet their customers' needs and be able to sustain their businesses. This study undertakes the use of a Four Box model to bring out the value perspective of a CBM. This resulted in the identification of the value as small data, how it is generated, delivered and compensated for especially cost in ensuring the effectiveness of a CBM on the SMEs."}, {"label": 0, "content": "Rapid technological advancements are disrupting traditional job markets necessitating job seekers to develop new skillsets suitable for the digital economy. This phenomenon has a major impact on the economy, particularly in developing countries. Consequently, it is crucial for institutional offerings to be aligned to industry requirements in every discipline of higher education, in order to ultimately sustain and improve the economy. This paper presents a framework designed to determine the alignment of the digital skills that students acquire from higher education to the digital skills requirements of industry. This alignment will aid higher education institutions in improving the digital skills preparedness of their graduates, and ultimately sustaining the digital economy. Given that the digital economy requires its employees to possess a specific level of digital skills, which may vary in each sector, the proposed framework is therefore not discipline specific. Consequently, this framework may employed to establish an alignment between any discipline in higher education and the respective industry that its graduates feed into. The authors have systematically reviewed related articles to determine the factors influencing the digital skills preparedness of graduates for industry. Relevant studies were analyzed, thereby resulting in the development of a digital skills preparedness model."}, {"label": 0, "content": "The paper discusses multithreaded processing of images on graphic processing units for the purposes of feature detection and matching. The problem of feature detection and feature correspondence is applied for image stitching and panorama creation. Parallel GPU implementation based on nVidia CUDA is presented and experimentally evaluated and compared by parallel multithread CPU processing for shared memory parallel computational model."}, {"label": 0, "content": "In this paper, a Software Defined Network was created in Mininet using Python script. An external interface was added in the form of an OpenDaylight controller to enable communication with the network outside of Mininet. The OpenDaylight controller was hosted on the Amazon Web Services elastic computing node. This controller is used as a control plane device for the switch within Mininet. The OpenDaylight controller was able to create the flows to facilitate communication between the hosts in Mininet and the webserver in the real-life network. In order to test the network, a real life network in the form of a webserver hosted on the Emulated Virtual Environment - Next Generation (EVE-NG) software was connected to Mininet."}, {"label": 0, "content": "This article is depicting the Strengths and weaknesses of Artificial Intelligence related to the improvement of customer online and offline experience, and the possible methods in order to measure them. These methods include both researches non-based and based on interviews. The presence of AI in the retail industry is becoming a key component of the customer experience. Through a deep analysis of existing tools to extract information, we try to explain ways to interpret them, in order for companies to create a real usage out of them, either on online or offline retail experience. Hence, with this research, we also want to provide an insight on how this experience could be improved in the future, and how it will most likely be inherent to our daily customer experience."}, {"label": 0, "content": "Image retrieval is gaining prominence in the area of medical image processing especially in the domain of fundus images. This work aims to propose a proficient algorithm for features mining in Fundus images and thereby extract the information through Content Based Image Retrieval process. The automated extraction of important features such as exudates aids medical practitioners in effectively overcoming various diseases pertaining to the patient. Although multiple methods of extracting these features are available, they lack in retrieval aspect of the information or the accuracy of the feature extraction."}, {"label": 0, "content": "Classification is a commonly used modelling method for data mining. A classification model is a predictive model which is used to predict a categorical value, called a class. Ensemble classification modelling involves the creation of several base models and a combination algorithm for the base model predictions. The classification modelling process uses a set of instances called the training data. Each instance consists of values for the predictor variables and a categorical label called the class. A class is called a minority class if it has a much smaller number of training instances compared to the other classes. This results in a low level of correct classification compared to the classification performance for the majority classes. Positive-versus-negative (pVn) classification has been reported in the literature as an ensemble classification method which is applicable to classification modelling for multi-class prediction tasks. The purpose of this paper is to report on experimental results for the performance of a replication method for improving classification performance for minority classes, using pVn classification modelling. The experimental results demonstrate that the classification accuracy for minority classes can be improved through the use of pVn classification models."}, {"label": 0, "content": "The agriculture sector is important to the economic growth of agriculture-inclined countries like South Africa. Issues regarding agriculture may not only directly affect consumers, but cause the increase in food price. This may further affect the price control policy on food. This paper discusses soil manuring as one of the problems faced in optimum food productivity. Since leaves droppings could be decomposed to fertilize the soil, the need to understand the factors that may affect the process or outcome is important, hence our project. We designed a compact monitoring circuit using the Redboard and GSM/GPRS module. Three TCS3200 color sensors were used to detect leaves droppings, giving a large coverage area. The colors detected were categorized to numbers, for possible data analysis. Other sensors were used to collect parameters for possible factors that may affect decomposition. We measured soil moisture, soil temperature, ambient temperature, relative humidity, and dew point. Our Heroku-deployed developed cloud platform was synced to the monitoring circuit for remote monitoring. Our whole project embraced a continuous real-time monitoring from the site to the cloud platform."}, {"label": 0, "content": "A below average throughput of Information Technology students specializing in software development is a challenge that many Universities and Universities of Technology in South Africa face. Contributing factors to this phenomenon are varied at best, but one of the identified factors are that students in this field, especially first year students, find it difficult to conceptualize the associated information and manner of thinking required to become successful in their studies. This is especially true when considering object orientated programming concepts and paradigms that students are required to master as part of their studies. Literary evidence suggests that a high level of working memory, which is associated with abstract thinking ability, is required when learning and applying object orientated programming concepts. The problem becomes more evident and serious if we consider that the Information and Communication Technology sector of a country is largely dependent on the graduating student populous in terms of growing the sector sustainably. A specialized software instrument was developed and tested in an attempt to affect a change in the abstract thinking ability of students from a student sample at a University of Technology. The results of this study focusses on the effect that the instrument realized on the academic performance of first year students related to particularly to object orientated programming and their abstract thinking ability in general as gauged by, amongst other instruments, the General Scholastic Ability Test, or GSAT, rather than focusing on the instrument itself."}, {"label": 0, "content": "Advances in robotics and cloud computing have led to the emergence of cloud robotics where robots can benefit from remote processing, greater memory and computational power, and massive data storage. The integration of robotics and cloud computing has often been regarded as a complex aspect due to the various components involved in such systems. In order to address this issue, different studies have attempted to create cloud robotic architectures to simplify representation into different blocks or components. However, limited study has been undertaken to critically review and compare these architectures. As such, this paper investigates and performs a comparative analysis of existing cloud robotic architectures in order to identify key limitations and recommend on the future of cloud robotic architectures. As part of this study, 7 such architectures have been reviewed and compared and results showed limited evaluation of existing architectures in favour of security weaknesses."}, {"label": 0, "content": "Grinding in ball mills is a crucial technological and industrial process which is used for the reduction of the size of particles with variant physical, chemical and mechanical characteristics. The control performance of the ball mills' grinding process is of outmost importance as this will determine the profit, where the energy consumption, the product quality and time efficiency are commonly concerned. In this paper, nonlinear model predictive control for ball mill grinding process is implemented. Economic performance, time delays and the consumption of energy in the grinding process with the proposed control system are engaged using Discrete Element Method (DEM) software. The results from experimental tests indicate the proposed method to be effective."}, {"label": 0, "content": "This paper reviews the development and application architecture of an expert system to assist the Mauritian population with queries they may have about labor or employment law. The expert system makes use of Machine Learning, Speech Recognition/Synthesis and Natural Language Processing techniques to converse with users through a web interface. The expert system also takes advantage of a large knowledge base, that allows the system to teach itself employment law principles. The knowledge base is created from \"Understanding Employment Law and Remuneration Orders in Mauritius\", written by Ved Prakash Torul [1], which is a simplified version of the Employment Relations Act and the Employment Rights Act. The book explains employment law in common language, to help the public understand their constitutional rights. The expert system allows users to communicate and express their employment issues, so that they are aware of their next course of action, either they are an employer, employee, or a union. The paper also reviews the evaluation period, which consisted of a preliminary testing period. Through the evaluation, it was concluded that the expert system was able to respond to individual responses with a Precision of 66% and Recall of 85%. While the Expert System is able to converse with users on certain topics on Employment Law, further evaluation would need to be conducted. Additionally, the knowledge base will need to be updated over time."}, {"label": 0, "content": "Fog computing aims to bring cloud computing capabilities to the edge of the network, closer to the end user, enabling lower latencies, location awareness, and mobility support among other advantages. The combination of IoT and Fog encompasses a highly complex scenario with a huge amount of data and varying number of devices that must cooperate with each other. Fog computing networks may be designed as autonomous networks. In this case, there is a requirement for effective management and orchestration mechanisms to guarantee acceptable performance of applications and services, while still leveraging of cloud capabilities. Mechanisms typically applied to \"cloud-only\" implementations cannot naturally be migrated to the Fog given its particular characteristics. This calls for the design and development of new management and orchestration mechanisms for the Fog. In this paper we propose a design the use the finite state machine to enhance decision making in an autonomous Fog computing network. The proposed scheme is expected to optimise Fog computing networks autonomy and improve performance and cost."}, {"label": 0, "content": "Prohibition signs are commonly used for safety purposes in order to prevent and protect individuals from dangerous situations. These signs are placed in or around areas whereby they are clearly visible to the public. However, the visually impaired cannot visualize such signs. To help them, this paper proposes a system that combines Convolutional Neural Network (CNN) model and Computer Vision (CV) algorithms to detect and recognize prohibition signs in real scenes. The system uses pre-trained AlexNet model, fine-tuned using Prohibition Signage Boards (PSB) dataset and combined with Maximally Stable Extremal Regions (MSER) and Optical Character Recognition (OCR) techniques for text extraction and classification, to enhance the system performance. The experiments indicate that high recognition accuracies are achieved from a variety of prohibition images and prohibition texts."}, {"label": 0, "content": "The prospects of achieving a trillion connected internet of things (IoT) devices by 2020 has created the urgency for effective intrusion detection systems (IDS) for these devices. Although it has been argued that the most effective technique used in such systems is anomaly detection, there exist no mechanisms to determine their performance in real-life deployment. In this paper, we report the results of applying asymptotic analysis to evaluate the performance of an anomaly detection algorithm which is designed using logic reasoning through fuzzy logic methodologies. In order to achieve this, the IDS was included as part of intrusion detection software for ZigBee Wireless Sensor Networks (WSNs). In particular, the solution is targeted to address the ZigBee protocol's vulnerability to flood attacks during node discovery and association to the network. The intrusion detection software is hosted external to the WSNs in pursue of a light solution mindful of resource preservation in sensor nodes."}, {"label": 0, "content": "As we see the cyberspace evolve we also see a directly proportional growth of the people using the cyberspace for communication. As a result, the misuse of the cyberspace has given rise to negative issues such as cyberbullying, which is a form of harassing other people using information technology in a deliberate and continual manner. The detection and prevention of cyberbullying becomes critical for safe and health social media platforms. In this paper, a review of the cyberbullying content in Internet, the categories of cyberbullying, data sources containing cyberbullying data for research, and machine learning techniques for cyberbullying detection are overviewed. The main challenges of the cyberbullying detection are demonstrated, including the lack of multimedia content-based detection and availability of public accessible dataset. Suggestions are provided as the conclusion of the overview."}, {"label": 0, "content": "Hand detection is critical in gesture recognition for conveying information or control commands between persons and computers. The accuracy of hand detection from images plays an important role in these applications. Extraction of effective features is the main factor in this task. The features should be discriminative, robust to different variations and easy to compute. This paper presents the experimental comparison of features commonly used in object detection, such as Haar-like features, a histogram of oriented gradient (HOG), and local binary pattern (LBP), using hand detection as the test platform. The adaptive boost (AdaBoost) cascade classification method is employed to combine \"weak learners\" to a strong classifier. The classifier was trained using 300 positive images, which are images containing the hand (region of interest (ROI)) and 10000 negative images, which are images that do not contain a hand on them. Different parameter combinations of the classifier are considered for comparative experiments. The performance of the classifier using Haar, HOG and LBP features were evaluated with 320 static test images. The results show that parameter combinations have significant effects on the hand detection accuracy, which also differ when different features are used."}, {"label": 0, "content": "We categorized VPN's impact in to two different aspects; The affecting aspect, and the affected aspect. The affecting aspect of the impact encompasses factors such as security, algorithms, hardware, and software. Whereas the affected aspect is of the likes of network performance. While VPNs have managed to integrate security, one of the affecting aspect of the impact on one hand, on the other hand, VPNs should be regarded as a potential threat to network performance. In this study, for affordability purpose, we choose to use NS-2 simulated test-bed to shed light on the VPN's performance impact in a network. Considering the most common network performance metrics, throughput and delay; we assessed these performance metrics by means of average and percentage changes theories. The findings emphasize quantitative impact on the TCP/IP throughput than on its counterpart UDP/IP. We finally developed an analytical equation to model this VPN's performance impact."}, {"label": 0, "content": "Recommender systems are engines that recommend new items to users by analyzing their preferences. The web contains a large amount of information in the form of ratings, reviews, feedback on items and other unstructured data. These details are extracted to get meaningful information of users. Collaborative filtering and content-based filtering are two common approaches being used to make recommendations. The paper aims to introduce a hybrid recommendation technique for Big Data Systems. The approach combines collaborative and content-based filtering techniques to recommend items that a user would most likely prefer. It additionally uses items ranking and classification technique for recommending the items. Moreover, social media opinion mining is added as a top-up to derive user sentiments from user's posts and become knowledgeable about users' tastes hidden within social media. A prototype has been implemented and evaluated based on the recommendation techniques."}, {"label": 0, "content": "Sophisticated mobile applications may require more resources than are readily available on mobile devices. Mobile device resources such as processing power and storage can be extended with cloud-based mobile augmentation. However, some resources, specifically battery life and bandwidth, cannot be augmented. This research identifies that it is important to be able to estimate the energy consumption of both offloaded and local tasks when making offloading decisions. Due to the fact that the energy consumption profile of mobile devices with diverse capabilities are not the same, this aspect needs to be considered. This research proposes the Switch framework to conserve the limited battery life on mobile devices using a device specific energy consumption profile. The evaluation of the framework suggests that Switch can successfully be used to conserve battery life on mobile devices by making intelligent offloading decisions."}, {"label": 0, "content": "Association rule mining is an important data mining technique that help discover interesting attribute relationships that are useful for decision making. Most association rule mining methods use item-set manipulation approach, whereby data type must be categorical in nature. When a dataset contains numerical attributes, they will need to be discretized before rule mining. At the moment, most unsupervised data discretization methods do not account for data distributions, and users have to try different methods and discretization settings in order to improve rule mining results. In this paper, we propose using TwoStep clustering for data discretization. Unlike simple discretization methods, TwoStep automatically determines the discretization intervals by taking into account the unique data distribution property of each attribute. In our experiments, we evaluated the performance of Apriori algorithm based on four datasets, whereby each dataset was pre-processed using TwoStep and three other commonly used discretization methods. Our results show that TwoStep produced the greatest number of high-quality rules, as compared to common discretization methods."}, {"label": 0, "content": "The purpose of this article is to analyze the power of WeChat and question ourselves on the hypothetical expansion. We discovered WeChat by living in China. For us, it is an application which gather everything. It is Instagram, Facebook, Twitter in just one app. But since we are living in a world with so many differences and different laws, we want to know if this app can really encounter the same success in different countries. WeChat has a big place in the life of Chinese people and this is something which is very interesting. Everything is around this app. The payment method, the social life and the way of interacting with people in general. We decided to split our study in three parts in order to answer the problematic."}, {"label": 0, "content": "5G wireless together with optical backbone networks are expected to be the main pillars of the envisaged next /future generation networking (N/FGN) infrastructures. This is an impetus to practical realization of an IoT network that will support and ensure relatively higher bandwidth as well as enhanced quality of service (QoS) in both access and core network sections. The high-speed wireless links at the network peripherals will serve as a conducive platform for device-to-device (D2D) communication. D2D driven applications and services can only be effective as well as secure assuming the associated machine type communication devices (MTCDs) have been successfully verified and authenticated. Typically, D2D type services and applications involve the interaction of several MTCDs in a group. As such, secure and effective D2D group-based authentication and key agreement (AKA) protocols are necessary. They need to inherently achieve efficacy in maintaining the group key unlink-ability as well as generate minimal signalling overheads that otherwise may lead to network congestion. In this paper we detail a secure and efficient Group AKA (Gr-AKA) protocol for D2D communication. Its performance is compared to that of existing similar protocols and is found to comparably lower both computational as well as signalling overhead requirements. Overall the analysis shows that the Gr-AKA protocol improves performance in terms of fulfilling D2D communication's security requirements."}, {"label": 0, "content": "In our modern world, almost every aspect of our lives is directly affected by the revolution of digitalization. Everything we rely on nowadays has a computer chip and a software controlling it whether it is a mobile phone, a car, a computer, or any other electronic device. These individual life affecting technologies are composing of cyber physical systems communicating via the Internet of Things (IoT). This paper addresses the development of Intrusion Detection Schemes for IoT. The increasing risks in IoT infrastructure compromise mainly against IPv6 over Low Power Wireless Personal Area Networks (6LowPAN) are a concern. The data breaches and data manipulation are discussed by identifying the most recent hacker methodologies and tools and analyzing what the breach will affect inside an IoT network. After discussing the \"Problems\", the paper will discuss the existing intrusion detection schemes with their limitations and will propose a more effective solution."}, {"label": 0, "content": "Heterogeneous IoT enabled networks generally accommodate both jitter tolerant and intolerant traffic. Optical Burst Switched (OBS) backbone networks are handling the resultant volumes of such traffic by transmitting it in huge size chunks called bursts. Because of the lack or limited buffering capabilities within the core network, contentions as well as congestion may frequently occur and thus affecting overall supportable quality of service (QoS). Both contention and congestion will be characterized by frequent burst losses especially when traffic levels surge. The congestion is normally resolved by way of deflecting contending bursts to other less congested paths even though this may lead to differential delays incurred by bursts as they traverse the network. This will contribute to undesirable jitter that may ultimately compromise overall QoS. Noting that jitter is mostly caused by deflection routing which itself is a result of poor wavelength and routing assigning, in this paper we propose a controllable deflection routing (CDR) scheme that allows the deflection of bursts to alternate paths only after controller buffer preset thresholds are surpassed. In this way bursts intended for a common destination are always most likely to be routed on the same or least cost path end-to-end. We describe the scheme as well as compare its performance to other existing approaches. Both analytical and simulation results overall show that the proposed scheme does lower both congestion as well as jitter, thus also improving throughput as well as avoiding congestion on deflection paths."}, {"label": 0, "content": "This paper presents a method for realizing energy neutral operation on energy harvesting wireless sensor nodes (WSN) and its implementation, regarding that the available environmental energy is unpredictable and changes over time. The method utilizes adaptive duty cycling which provides energy-neutral operation according to the energy available in the environment and the instantaneous energy state of the node through an energy management circuit. The proposed method is implemented using a MicaZ mote as the WSN and two different vibration-based harvesters: piezoelectric and electromagnetic. The node incorporating a piezoelectric harvester, operates for only 130.5 s with a fixed duty-cycle of 0.21%, and requires an inactive time of 93.5 s for charging. On the other hand, with the proposed strategy, the node achieves energy-neutral operation by self-adjusting to 0.17% duty-cycle. Energy-neutral operation is also demonstrated by incorporating an electromagnetic energy harvester attached to the wrist of a runner: When no energy is available for harvesting, the proposed strategy shows about 64% increment in lifetime before going to sleep mode. These demonstrate that the proposed energy management policy proves to achieve energy-neutral operation in an efficient way."}, {"label": 0, "content": "Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness."}, {"label": 0, "content": "In this paper, the design of the control chart when the variable of interest follows the gamma distribution under the neutrosophic statistical interval method (NSIM) is proposed. The average run length, probability in-control, and probability of out-of-control will be derived using the NSIM. The neutrosophic control chart coefficient will be determined by the algorithm under the NSIM. The neutrosophic average run length for various shifts and specified parameters will be determined. The efficiency of the proposed control chart is discussed using the simulation study and a real example."}, {"label": 0, "content": "This paper presents an approach that combines conventional image processing with deep learning by fusing the features from the individual techniques. We hypothesize that the two techniques, with different error profiles, are synergistic. The conventional image processing arm uses three handcrafted biologically inspired image processing modules and one clinical information module. The image processing modules detect lesion features comparable to clinical dermoscopy information-atypical pigment network, color distribution, and blood vessels. The clinical module includes information submitted to the pathologist- patient age, gender, lesion location, size, and patient history. The deep learning arm utilizes knowledge transfer via a ResNet-50 network that is repurposed to predict the probability of melanoma classification. The classification scores of each individual module from both processing arms are then ensembled utilizing logistic regression to predict an overall melanoma probability. Using cross-validated results of melanoma classification measured by area under the receiver operator characteristic curve (AUC), classification accuracy of 0.94 was obtained for the fusion technique. In comparison, the ResNet-50 deep learning based classifier alone yields an AUC of 0.87 and conventional image processing based classifier yields an AUC of 0.90. Further study of fusion of conventional image processing techniques and deep learning is warranted."}, {"label": 0, "content": "With the development of the Internet of Things (IoT) technology, its application in the medical field becomes more and more extensive. However, with a dramatic increase in medical data obtained from the IoT-based health service system, labeling a large number of medical data requires high cost and relevant domain knowledge. Therefore, how to use a small number of labeled medical data reasonably to build an efficient and high-quality clinical decision support model in the IoT-based platform has been an urgent research topic. In this paper, we propose a novel semi-supervised learning approach in association with generative adversarial networks (GANs) for supporting clinical decision making in the IoT-based health service system. In our approach, GAN is adopted to not only increase the number of labeled data but also to compensate the imbalanced labeled classes with additional artificial data in order to improve the semi-supervised learning performance. Extensive evaluations on a collection of benchmarks and real-world medical datasets show that the proposed technique outperforms the others and provides a potential solution for practical applications."}, {"label": 0, "content": "Fog computing enables computation, storage, applications, and network services between the Internet of Things and the cloud servers by extending the Cloud Computing paradigm to the edge of the network. When protecting information security in Fog computing, advanced security with low latency, wide-spread geographical distribution support, and high flexibility should be taken in to considertion first, because of its huge number of nodes. In this paper, we propose a new cryptographic primitive, named CCA2 secure publicly-verifiable revocable large-universe multi-authority attribute-based encryption (CCA2-PV-R-LU-MA-ABE), to achieve flexible fine-grained access control in Fog computing. In this primitive, end nodes in fogs generate private keys from multiple authorities that might be differentiated by their geographical locations or functions, and their attributes can be denoted by any strings in the large universe, which meets diverse needs in practical Fog applications. In addition, the accessibility of nodes can be revoked efficiently even by resource-limited devices. To ensure the validity of ciphertext, this primitive supports public verification and only valid ciphertext can be stored or transmitted. Based on the primitive and the feature of Fog computing, we construct a concrete CCA2-PV-R-LU-MA-ABE scheme. We define the security model of this primitive, which is much more secure than the CPA-secure scheme. Finally, we compare the efficiency of the proposed concrete scheme with that of the existing CPA-secure scheme by both theoretical and experimental analysis, and the results show that the extra consumption of efficiency to improving CPA to CCA2 is considerably low. The proposed scheme is highly secure, flexible, and efficient enough to be deployed in practical Fog computing."}, {"label": 0, "content": "The development of high-resolution video mounts a serious challenge to the previous video coding standard. The appearance of the new generation standards greatly relieves the dilemma but increases the coding complexity dramatically. Motion estimation is considered as the module with a relatively high computational complexity. In this paper, a parallel motion estimation implementation is proposed, which includes pre-motion estimation, integer motion estimation, and fractional motion estimation. They are highly accelerated on GPU based on AVS2, which is one of the new generation standards. A rapid mapping table algorithm is introduced to improve the efficiency of data access. In addition, a quasi-integral-graph algorithm is designed to calculate SAD or SATD efficiently for blocks of different sizes. The two novel techniques can effectively improve the utilization and efficiency of threads and exploit the characteristics of GPU. The experimental results show that the proposed parallel method can effectively accelerate the motion estimation."}, {"label": 0, "content": "We propose a top-down approach for formation control of heterogeneous multiagent systems, based on the method of eigenstructure assignment. Given the problem of achieving scalable formations on the plane, our approach globally computes a state feedback control that assigns desired closed-loop eigenvalues/eigenvectors. We characterize the relation between the eigenvalues/eigenvectors and the resulting interagent communication topology, and design special (sparse) topologies such that the synthesized control may be implemented locally by the individual agents. Moreover, we present a hierarchical synthesis procedure that significantly improves computational efficiency. Finally, we extend the proposed approach to achieve fixed-size formation and circular motion, and illustrate these results by simulation examples."}, {"label": 0, "content": "The frequency-domain Kalman filter (FKF) has been utilized in many audio signal processing applications due to its fast convergence speed and robustness. However, the performance of the FKF in under-modeling situations has not been investigated. This letter presents an analysis of the steady-state behavior of the commonly used diagonalized FKF and reveals that it suffers from a biased solution in under-modeling scenarios. An effective improvement of the FKF is proposed, having the benefits of the guaranteed optimal steady-state behavior at the cost of a very limited increase of computational burden. The convergence behavior of the proposed algorithm is also analyzed. Computer simulations are conducted to validate the improved performance of the proposed method."}, {"label": 0, "content": "Compared to in-clinic balance training, in-home training is not as effective. This is, in part, due to the lack of feedback from physical therapists (PTs). In this paper, we analyze the feasibility of using trunk sway data and machine learning (ML) techniques to automatically evaluate balance, providing accurate assessments outside of the clinic. We recruited sixteen participants to perform standing balance exercises. For each exercise, we recorded trunk sway data and had a PT rate balance performance on a scale of 1-5. The rating scale was adapted from the Functional Independence Measure. From the trunk sway data, we extracted a 61-dimensional feature vector representing the performance of each exercise. Given these labeled data, we trained a multi-class support vector machine (SVM) to map trunk sway features to PT ratings. Evaluated in a leave-one-participant-out scheme, the model achieved a classification accuracy of 82%. Compared to participant self-assessment ratings, the SVM outputs were significantly closer to PT ratings. The results of this pilot study suggest that in the absence of PTs, ML techniques can provide accurate assessments during standing balance exercises. Such automated assessments could reduce PT consultation time and increase user compliance outside of the clinic."}, {"label": 0, "content": "In recent years, great success has been achieved in visual object detection, which is one of the fundamental tasks in the field of industrial intelligence. Most of existing methods have been proposed to deal with single well-captured still images, while in practical robotic applications, due to nuisances, such as tiny scale, partial view, or occlusion, one still image may not contain enough information for object detection. However, an intelligent robot has the capability to adjust its viewpoint to get better images for detection. Therefore, active object detection becomes a very important perception strategy for intelligent robots. In this paper, by formulating active object detection as a sequential action decision process, a deep reinforcement learning framework is established to resolve it. Furthermore, a novel deep Q-learning network (DQN) with a dueling architecture is proposed, the network has two separate output channels, one predicts action type and the other predicts action range. By combining the two output channels, the action space is explored more efficiently. Several methods are extensively validated and the results show that the proposed one obtains the best results and predicts action in real time."}, {"label": 0, "content": "Working on product lifetime data is of significant importance for evaluating safety and reliability, predicting remaining useful life and formulating maintenance strategy or replacement policy. In practical applications, observed datasets often consist of failure data and randomly censored data, which are referred as general censored data. Meanwhile, inverse Gaussian (IG) distribution has been widely adopted to depict lifetime data because it not only can possess flexible expression formats but also can explain the mechanism of first hitting time from a soft failure viewpoint. Motivated by these two regards, this paper develops a novel method on best linear unbiased estimations (BLUEs) for general censored data. A three-parameter IG distribution type is adopted. A novel method is established to optimize the skewness parameter. Then, BLUEs of mean and standard deviation can be obtained. The proposed method can construct closed-form parameter estimations in linear functions of order statistics. The computation process has been further simplified for flexible applications. The frequently utilized maximum likelihood estimation method is also introduced as a reference for a better understanding. Comparative results of both comprehensive simulation study and empirical application illustrate that the proposed method can significantly enhance the estimation accuracy and keep a stable performance, because more life information can be extracted and adopted from the censored datasets."}, {"label": 0, "content": "Suffering from speckle noise and complex scattering phenomena, classification results of SAR images are usually noisy and shattered, which makes them difficult to use in practical applications. Deep-learning-based semantic segmentation realizes segmentation and categorization at the same time, and thus can obtain smooth and fine-grained classification maps. However, this kind of methods require large data sets with pixel-wise categorical annotations, which are time consuming and tedious to retrieve. Compared with photographs and optical remote sensing images, manually annotating SAR data is even harder, which results in a delay of using relevant techniques in this field. In this letter, a new data set is proposed to support semantic segmentation for high-resolution PolSAR images. Limited by the aforementioned problems, the data set is only a small one with 50 image patches. Therefore, two transfer learning strategies are proposed, which adopt the fully convolutional network (FCN) and U-net architecture, respectively, and use distinct pretraining data sets to adapt to different situations. The experiments demonstrate the good performance of both methods and a promising applicability of using small training sets. Moreover, although trained with small patches, both networks can perfectly apply on large images. The new data set and methods are hopeful to support various PolSAR applications as baselines."}, {"label": 0, "content": "This paper investigates the possibility of improving the stability of radio-frequency transfer in telecommunication dense wavelength division multiplexing fiber-optic networks. As it has been identified, the dispersion compensation fibers (DCFs), frequently used in these networks, cause substantial differential delay, whose temperature-induced fluctuations have the most significant impact on the deterioration of the stability of the frequency transfer. The authors present a method that allows achieving significant improvement in the long-term stability of the frequency transfer. The developed method is based on modeling the impact of DCFs with the help of remotely accessible temperature sensors factory installed by the manufactures in DCF modules. The effectiveness of the proposed solution has been tested on three different long-haul routes (up to 1550 km), set up in the operational Polish National Research and Education Network."}, {"label": 0, "content": "With the increasing penetration of distributed photovoltaic generation and energy storage systems in the demand side of the power system, new demand side model structures are necessary in order to better describe the dynamic performance of the power system. In this paper, a composite demand side model structure with load, distributed photovoltaic generation, and energy storage system together with a model parameter identification method are proposed to improve the traditional load model identification. The structure of the demand side model is proposed first and is further simplified so as to be identified at a high voltage level bus. The model parameter identifiability analysis is conducted based on the sensitivity method. The ambient signal data and disturbance data based model parameter identification method is proposed for the new demand side model structure using the differential evolution optimization method. The case study results for the WSCC 9 bus system show the effectiveness of the proposed model structure. Then, the case study results in a simplified 500-kV network of the Guangdong Power Grid show the effectiveness of the parameter identification method."}, {"label": 0, "content": "Consensus is fundamental for distributed systems since it underpins key functionalities of such systems ranging from distributed information fusion, decision making, to decentralized control. In order to reach an agreement, existing consensus algorithms require each agent to exchange explicit state information with its neighbors. This leads to the disclosure of private state information, which is undesirable in cases where privacy is of concern. In this paper, we propose a novel approach for undirected networks, which can enable secure and privacy-preserving average consensus in a decentralized architecture in the absence of an aggregator or third party. By leveraging partial homomorphic cryptography to embed secrecy in pairwise interaction dynamics, our approach can guarantee convergence to the consensus value (subject to a quantization error) in a deterministic manner without disclosing a node's state to its neighbors. We provide a new privacy definition for dynamical systems, and give a new framework to rigorously prove that a node's privacy can be protected as long as it has at least one legitimate neighbor, which follows the consensus protocol faithfully without attempts to infer other nodes' states. In addition to enabling resilience to passive attackers aiming to steal state information, the approach also allows easy incorporation of defending mechanisms against active attackers who try to alter the content of exchanged messages. Furthermore, in contrast to existing noise-injection-based privacy-preserving mechanisms that have to reconfigure the entire network when the topology or number of nodes varies, our approach is applicable to dynamic environments with time-varying coupling topologies. This secure and privacy-preserving approach is also applicable to weighted average consensus as well as maximum/minimum consensus under a new update rule. Numerical simulations and comparison with existing approaches confirm the theoretical results. Experimental results on a Raspberry-Pi board based microcontroller network are also presented to verify the effectiveness and efficiency of the approach."}, {"label": 0, "content": "Stochastic dynamics is a research topic for railway vehicles involving a wide range of randomness or uncertainty. However, the modeling and calculation of stochastic dynamic systems are often high-cost and low-efficiency. Neural network is an effective machine learning tool driven by data; this paper devotes to bridge the gap between neural networks and stochastic dynamics and to attain proper uses of this technique in railway vehicles. The mapping capability of neural networks for various stochastic suspension dynamics is validated by the proposed random repetition scheme. And this powerful computational tool is applied to predict the dynamic performance of high-speed trains in service instead of dynamics calculations; a typical case is analyzed to emphasize the advantage of the dynamic performance evaluation considering the coupling of various factors that it can enhance the security and reliability by attaining prognostic and health management and condition-based maintenance."}, {"label": 0, "content": "We present a cofactor-based endmember extraction strategy for estimating green algae area in geostationary ocean color imager multispectral images. Our strategy improves the efficiency of the widely used N-FINDR endmember extraction method from two aspects. First, our strategy exploits the cofactor matrix for searching the largest simplex volume, which just computes matrix inverse and determinant for a small number of times (or even once). This is more efficient than the enumeration of determinants for all pixels in N-FINDR. Second, our strategy empirically obtains optimal endmembers through a few recursive iterations of cofactor matrix updates, contrasting a large number of repetitive volume maximizations with random initializations in N-FINDR. Experimental evaluation in terms of green algae area estimation validates that our strategy achieves the same accuracy as N-FINDR with much more efficiency."}, {"label": 0, "content": "The path towards wind power forecasting has yielded huge socio-economic benefits at a global scale. However, most of the previous studies tend to emphasize the improvement of deterministic forecasting, usually losing sight of the significance of probabilistic forecasting. In this paper, a novel forecasting system that can perform deterministic and probabilistic forecasting of wind power simultaneously, composed by the modules of feature selection, forecasting, system optimization, and system evaluation is presented to further supplement the existing studies in this field. Concretely, a hybrid feature selection strategy is proposed in the feature selection module to determine optimal system input; superior to traditional gradient descent algorithm, a dynamic reservoir theory-based recurrent neural network is developed in the forecasting module; an enhanced multi-objective optimization algorithm with the objectives of accuracy and stability is proposed in the system optimization module to provide an optimal scenario for system parameters; the effectiveness and feasibility of the proposed system is then validated in the evaluation module. Moreover, the comprehensive performance analysis of the proposed system is investigated in depth. Finally, the experimental results demonstrate that the proposed system has a significant advantage over the benchmarks considered, further verifying its tremendous potential to be used in a practical wind power system."}, {"label": 0, "content": "A regularized optimization problem over a large unstructured graph is studied, where the regularization term is tied to the graph geometry. Typical regularization examples include the total variation and the Laplacian regularizations over the graph. When the graph is a simple path without loops, efficient off-the-shelf algorithms can be used. However, when the graph is large and unstructured, such algorithms cannot be used directly. In this paper, an algorithm, referred to as \u201cSnake,\u201d is proposed to solve such regularized problems over general graphs. The algorithm consists in properly selecting random simple paths in the graph and performing the proximal gradient algorithm over these simple paths. This algorithm is an instance of a new general stochastic proximal gradient algorithm, whose convergence is proven. Applications to trend filtering and graph inpainting are provided among others. Numerical experiments are conducted over large graphs."}, {"label": 0, "content": "Video summarization (VSUMM) has become a popular method in processing massive video data. The key point of VSUMM is to select the key frames to represent the effective contents of a video sequence. The existing methods can only extract the static images of videos as the content summarization, but they ignore the representation of motion information. To cope with these issues, a novel framework for an efficient video content summarization as well as video motion summarization is proposed. Initially, Capsules Net is trained as a spatiotemporal information extractor, and an inter-frames motion curve is generated based on those spatiotemporal features. Subsequently, a transition effects detection method is proposed to automatically segment the video streams into shots. Finally, a self-attention model is introduced to select key-frames sequences inside the shots; thus, key static images are selected as video content summarization, and optical flows can be calculated as video motion summarization. The ultimate experimental results demonstrate that our method is competitive on VSUMM, TvSum, SumMe, and RAI datasets about shot segmentation and video content summarization, and can also represent a good motion summarization result."}, {"label": 0, "content": "Air pollution has become a worldwide concerned issue and automatical estimation of air quality can provide a positive guidance to both individual and industrial behaviors. Given that the traditional instrument-based method requires high economic, labor costs on instrument purchase and maintenance, this paper proposes an effective, efficient, and cheap photo-based method for the air quality estimation in the case of particulate matter (PM2.5). The success of the proposed method lies in extracting two categories of features (including the gradient similarity and distribution shape of pixel values in the saturation map) by observing the photo appearances captured under different PM2.5 concentrations. Specifically, the gradient similarity is extracted to measure the structural information loss with the consideration that PM2.5 attenuates the light rays emitted from the objects and accordingly distorts the structures of the formed photo. Meanwhile, the saturation map is fit by the Weibull distribution to quantify the color information loss. By combining two features, a primary PM2.5 concentration estimator is obtained. Next, a nonlinear function is adopted to map the primary one to the real PM2.5 concentration. Sufficient experiments on real data captured by professional PM2.5 instrument demonstrate the effectiveness and efficiency of the proposed method. Specifically, it is highly consistent with real sensor's measures and requires low implementation time."}, {"label": 0, "content": "Due to the complexity of modeling deformable materials and infinite degrees of freedom, the rich background of rigid robot control has not been transferred to soft robots. Thus, most model-based control techniques developed for soft robots and soft haptic interfaces are specific to the particular device. In this letter, we develop a general method for stiffness control of soft robots suitable for arbitrary robot geometry and many types of actuation. Extending previous work that uses finite element modeling for position control, we determine the relationship between end-effector and actuator compliance, including the inherent device compliance, and use this to determine the appropriate controlled actuator stiffness for a desired stiffness of the end-effector. Such stiffness control, as the first component of impedance control, can be used to compensate for the natural stiffness of the deformable device and to control the robot's interaction with the environment or a user. We validate the stiffness projection on a deformable robot and include this stiffness projection in a haptic control loop to render a virtual fixture."}, {"label": 0, "content": "In this paper, we deal with a double control task for a group of interacting agents that have second-order dynamics. Adopting the leader-follower paradigm, the given multiagent system is required to maintain a desired formation and to collectively track a velocity reference provided by an external source only to a single agent at time, called the \u201cleader.\u201d We prove that it is possible to optimize the group performance by persistently selecting online the leader among the agents. To do this, we first define a suitable error metric that is able to capture the tracking performance of the multiagent group while maintaining a desired formation through a (even time-varying) communication-graph topology. Then, we show that this depends on the algebraic connectivity and on the maximum eigenvalue of the Laplacian matrix of a special directed graph depending on the selected leader. By exploiting these theoretical results, we finally design a fully distributed adaptive procedure that is able to periodically select online the optimum leader among the neighbors of the current one. The effectiveness of the proposed solution against other possible strategies is confirmed by numerical simulations."}, {"label": 0, "content": "In this letter, a novel 2-D square-root-based memory polynomial behavioral model is proposed. A new set of square-root-based basic functions is adopted to describe the characteristic of the predistorter. Since the proposed model has only two nested summations, the number of coefficients of the proposed model is greatly reduced compared with models which have three nested summations. The experimental results show that the proposed model can reduce the coefficients by more than 66.7% compared with the 2-D digital predistortion (2D-DPD) model. Moreover, the proposed model can achieve better adjacent channel power ratio (ACPR) performance. Compared with the 2D-DPD model and the simple online coefficient update model, the proposed model can improve the normalized mean-square error by 11 dB. Compared with the 2-D modified memory polynomial model, the proposed model can obtain similar ACPR performance with a shorter running time."}, {"label": 0, "content": "Many recent variational optical flow methods are not robust for illumination variance, and they only consider local image relation in terms of illumination. In this paper, we propose a new efficient illumination-invariance total variation optical flow method called the weighted regularization transform, which uses and optimizes the Weber's Law. Our method exploits unequal probability as the weight that has non-local information to estimate stable optical flow despite illumination changes. The proposed method uses a coarse-to-fine pyramid model to reduce the influence on the data term from illumination. Then, an energy optimization procedure is introduced to constrain the minimization of the data term with the non-local regularization. Experimentation with the proposed method has been performed on three optical flow datasets and a face liveness detection database, which have challenging illumination variations, and the results demonstrate that the proposed method is quite robust with respect to variations in illumination."}, {"label": 0, "content": "In this paper, the parameters of the derivative of blood pressure signals for the waveform classification of the wrist pulse have been analyzed. The method used here shows the relationships between pulse waveforms and the amplitudes of the characteristic points of the first derivative of blood pressure signals. The algorithm considered in the paper can be used for the computerization of pulse diagnostics."}, {"label": 0, "content": "In this paper we review the registration of a binary element in a discrete channel with erasure using a nonlinear scale constructed on the basis of a fuzzy membership function, the concept of fuzzy sets theory. The source of information loss of a binary element is shown when it is recorded in a traditional way. The mechanism of compensation of information losses on the basis of a nonlinear scale is given."}, {"label": 0, "content": "We describe a procedure of compact behavioral models design of the communication channel through a borehole pipe for measurement while drilling (MWD) microwave systems based on results of measurements of radio pulse signals during testing of the channel. Characterization of the microwave channel for the decline-directed borehole and a transmitter on the Gunn diode is described as an example of the offered process. The procedure is based on measurement of signal samples of the transmitter with the subsequent accounting of electrodynamical parameters, attenuation in the media, and on the pipe walls."}, {"label": 0, "content": "According to rules of interaction between the subject of the Electricity Market and JSC ATS, subjects of the Electricity Market are obliged to implement the daily hourly forecast in the mode \"for a days ahead\". To ensure of high-quality prediction of the electricity loads, subjects of electricity market need to prepare the regulatory base, to develop a technique of creation of the forecast of the electricity loads, and also to count the risks connected to the accuracy of the used models. On the one hand, the complexity of the problem being solved is characterized by the availability of data on the supply points, since not always the subject of the electricity market has the opportunity to collect data on the consumption of individual power facilities in the hourly mode. From the other hand, the introduction of commercial accounting systems can solve this problem with the investment of a large investment in the installation automatic system for commercial measurement of the electricity loads (ascme), but as a rule subject of electricity market goes for such long-term payback costs. The work can be useful both to specialists of power sales companies who are engaged in building forecast models, as well as to specialists of the electricity market entities, who carry out forecasts for the electricity market for the day-ahead. The main aim of the study is applying methodology forecasting using neural network for building predictive models for LLC \"Omsk Energy Retail Company\". The methods used in the study: Holt-Winters model, the ARIMA, neural networks, temperature and wind index. The results. It considered methods of construction of predictive models, the path of their evolution since the launch of Electricity market. Method of constructing the forecast of \"Omsk Energy Retail Company\" was developed using neural network, taking into account the temperature and wind index and allocation of common types of days by electricity load."}, {"label": 0, "content": "This article suggests an algorithm of formation a training set for artificial neural network in case of image segmentation. The distinctive feature of this algorithm is that it using only one image for segmentation. The segmentation performs using three-layer perceptron. The main method of the segmentation is a method of region growing. Neural network is using for get a decision to include pixel into an area or not. Impulse noise is using for generation of a training set. Pixels damaged by noise are not related to the same region. Suggested method has been tested with help of computer experiment in automatic and interactive modes."}, {"label": 0, "content": "A model of neurons for biometric authentication, capable of efficient processing of highly dependent features, based on the agreement criteria (Gini, Cramer-von-Mises, Kolmogorov-Smirnov, the maximum of intersection areas of probability densities) is proposed. An experiment was performed on comparing the efficiency of neurons based on the proposed model and neurons on the basis of difference and hyperbolic Bayesian functionals capable of processing highly dependent biometric data. Variants of construction of hybrid neural networks, that can be trained on a small number of examples of a biometric pattern (about 20), are suggested. An experiment was conducted to collect dynamic biometric patterns, in the experiment 90 people entered handwritten and voice patterns during a month. Intermediate results on recognition of subjects based on hybrid neural networks were obtained. Number of errors in verification of a signature (handwritten password) was less than 2%, verification of a speaker by a fixed passphrase was less than 6%. The testing was carried out on biometric samples, obtained after some time period after the formation of training sample."}, {"label": 0, "content": "This article presents the imitation computer model of a synchronous-in-phase electric drive that takes into account the impulse type of the control system, which allows simulating transient processes with high accuracy while studying effective methods of synchronous-in-phase electric drive control and devices implementing these methods. The modelling accuracy when using the developed model is especially increased (in comparison with the model that doesn't consider the impulse type of the control system) when studying transient processes in the low-frequency range of the electric drive rotation. The developed model allows to carry out researches of the electric drive in a range of low frequencies, where there is a strong influence of signal discretization on the transient processes type."}, {"label": 0, "content": "We considered the issues of synthesis of the algorithms for adaptive nonlinear signal processing using feed-forward blocks of nonlinear transformation under the influence of non-Gaussian noise with unknown density of distribution of instantaneous values or its envelope. It is shown that to plot the adaptive feed-forward blocks of nonlinear transformation, the algorithms for estimating the parameters of linear model of probability density function of noise can be used. This model is presented in the form of a generalized polynomial of decomposition in a series of linearly independent functions, and, also, in the form of nonlinear models, such as generalized Gaussian distribution and abnormally cluttered distribution."}, {"label": 0, "content": "Estimation of machinery health, identification of defects and deviations of dynamic mechanical and technological equipment, formation and determination of parameters and criteria of technical condition are based on the measurement of parameters of oscillatory processes. As a rule, as a source of information vibroacoustic oscillatory processes are used. Such processes are characteristic both for vibroacoustic-based diagnostics, and for acoustic-emission control. Many state evaluation criteria are based on the measurement of process peak parameters. Given the random nature of the processes, estimating the measurement error and determining the peak values of signals becomes an urgent task. The task of the given work is presentation of methodical bases of estimation of measurement error and determination of peak values of vibroacoustic signals taking into account laws of distribution of their instantaneous values. In particular, the paper presents a solution to the problem of determining the dependence of the asymptotic error estimate of the sample quantile on the value of discrete values in the sample (time realization of the signal). This allows us to justify and verify the metrology of measuring instruments. Besides it allows to estimate reliability of measurements of peak values of the vibroacoustic signal."}, {"label": 0, "content": "The paper considers the method of forming a training sample for intelligent methods of predicting electricity loads based on artificial neural networks. The training sample is formed taking into account the criteria of informativeness and compactness. It is shown how much the accuracy of the forecast can be increased with the approach used."}, {"label": 0, "content": "An effective technique for the loss probability calculation in queueing systems with heavy tails is developed. The technique is used for analysis of queueing systems with power-law distributions which are widespread as models of network devices operating under fractal traffic. Dependence form of loss probability on a buffer capacity in the systems is analyzed. The effect of the channel number in queuing systems on the dependence is examined. In practice, the developed rapid technique and obtained results of its applying might be used for solving of engineering problems of analysis and design of modern computer networks."}, {"label": 0, "content": "The article addresses the well-known \"knapsack problem\" in the context of the distribution of virtual servers within the cloud infrastructure. At the same time, the main focus in solving this problem is the distribution of virtual machines within the cloud, which includes resources with different access capabilities. The mathematical model proposed in the article allows conducting optimal allocation of resources not only taking into account such parameters as random access memory and processor power, but also parameters of data storage systems related to memory capacity and access speed. The resulting model was tested on a real network infrastructure. From a practical perspective, the method of virtual machines distribution under question allows using the low-power devices available at the data center."}, {"label": 0, "content": "This paper aims at presenting the approach for the process of attributes verification in the Attribute-Based Encryption schemes. We considered ABE methods in the Internet of Things environment. The main idea is to allow users or device owners to verify attributes using standardized and well-known authorization protocols like Oauth2. More accurately, Attribute Authority will use Authentication as a Service approach for users and device controllers authentication. We described software service for Attribute-Based Encryption methods in the context of FIWARE platform. Components of this platform use Oauth2 for authentication and authorization mechanisms. More specifically, we developed a web application which allows devices to create orders for ABE secret keys with particular attributes. Further users should approve or deny these orders. We used FIWARE components for users and devices authentication. To the best of our knowledge, it is the first implementation of ABE algorithms in FIWARE ecosystem."}, {"label": 0, "content": "In this paper the general aspects of energy consumption and environmental effects of cryptocurrency mining technology are considered. For the data mining equipment, the main technical specifications defining its energy efficiency are analyzed. The aggregation of separate units within data mining pools or installation of respective farms were shown to have the most prominent energy saving effect. Eventually some design and operation issues related to power supply of data mining pools are outlined. The sufficient amount of standards prescribing the power supply design for this new type of energy consumers is still missing. Considering the facility located in Moscow power supply of a data mining pool is studied and respective power quality survey is conducted. Finally, the impact of data mining equipment on the power factor and grid voltage variations is demonstrated."}, {"label": 0, "content": "Cooperative navigation is a promising set of approaches for increasing the accuracy of navigating of vehicles as well as road safety in difficult environment such as urban canyons. DSRC (Dedicated Short-Range communication) is the radio-communication standard for vehicles. Usually cooperative navigation is based on sharing GNSS and other primary sensors measurements between nearby vehicles via DSRC (or other telecom systems such as 3G/4G). In addition to communication, the on-board IEEE 802.11p DSRC receiver allows to measure the angle between vehicle's building axes and direction of received signal (from nearby vehicle). DSRC signals could share GNSS measurements and mutual headings of other surrounding vehicles. On-board fusing of surrounding vehicles' coordinate and corresponding heading angles measurements leads to increasing of navigating accuracy. Here the possible effect of proposed approach is estimated."}, {"label": 0, "content": "This work considers the problem of diagnostics and control of technical state (TS) of various industrial equipment. The relevance of the problem is caused by a significant increase in the level of requirements for the reliability of industrial facilities, the aging of existing equipment and technological complexes and thus the complexity of systematizing of various diagnostic information for decision-making. Because of this there is a need to develop a universal automated information system based on the method of diagnostics of the TS of a variety of industrial objects. Due to the multitude of factors affecting the TS of the equipment, the technical state index (TSI) is used as a complex indicator, the calculation method of which is based on the mathematical model of fuzzy sets. The diagnostics of TS is carried out based on a system of indicators, the configuration of which is related to the structure of the equipment. This work presents the results of developing a decision support system for managing complex technical and software objects. The work also shows a block diagram of the algorithm of how this system functions, describes the functionality and interface of the developed software."}, {"label": 0, "content": "In operation, methods of estimating probability density distributions are considered, which are urgent in the solution of the filtering issues of the useful information on the background of external acoustic noise in the telecommunications systems. Parametric and non-parametric methods of estimating probability densities are discussed, methods for determining an empirical distribution function for the case of a limited sample volume. It is shown that the approximation of the probabilities empirical data can be performed by the method of nuclear evaluations. Within this method, the estimate may be represented by the convolution of the core and the empirical density. It derives from the fact that the nuclear score is a result of a histogram of the histogram evaluation. It has been shown that reconstruction of the distribution function as a polynomial in the system of functions is the question of finding coefficients, which is the task of linear regression, which is solved by minimisation of the quadratic function of the loss built on the basis of the use of the least-squares method and representing the discrepancy of the empirical data and the estimates obtained on their basis. The results of the experimental studies show the error of the reconstruction one-dimensional function of probability density for the case of audio signals and acoustic interferences, given different kinds and orders of polynomial approximation."}, {"label": 0, "content": "The issue of creating high-performance computing systems based on heterogeneous computer systems is relevant, since the volume of processed information, calculations and studies with large data sets is constantly increasing. The aim of the work is to develop a model for predicting the performance of heterogeneous computing systems and its experimental evaluation in the simulation of access to the memory and in modeling fundamental parallel algorithms. As a result, the use of the developed model allows making an adequate estimate of the time of the parallelized task execution using heterogeneous computer systems based on graphics processors."}, {"label": 0, "content": "definition of the function of instrumental contact establishing for short-range radio detection devices is introduced. Analytical expression for the function of distance change between object and the detection device is defined. Analytical dependences of the function of instrumental contact establishing in two-dimensional and three-dimensional coordinate system are obtained provided that object (or objects) and the device of detection move in space on trajectories of complex shapes. It is shown that the accumulating probability of detection of object is calculated on the basis of function of the function of instrumental contact establishing."}, {"label": 0, "content": "An analysis of existing methods for solving the problems of risk assessment showed that they are based on the lack of computational capabilities and the lack of necessary information about the conditions of the problem. Therefore, in such cases it is advisable to use fuzzy mathematical methods. In this paper, we consider approaches to solving the problem of risk assessment with fuzzy source information."}, {"label": 0, "content": "Anonymous Voice over IP (VoIP) communication is an important tool to provide freedom of speech. For achieving a good trade-off between quality of service (QoS) and anonymity, connecting mixes by padded links is a promising approach. However, constructing suitable overlay topologies for establishing padded links is not easy: existing approaches are either not scalable with regard to the number of mixes because of using full meshes or impractical due to computational complexity of the algorithms used to create optimal reduced topologies. This prevents them from being deployed as volunteer-based networks like Tor. In this paper, we propose and study different heuristic strategies for reduced topology construction and path selection with low computational complexity. We evaluate our approach using latency and bandwidth estimations from the Tor network and demonstrate that we can achieve appropriate performance and anonymity metrics for VoIP. Especially, the achieved properties are similar to those of a recently proposed approach that calculates optimal topologies for small networks."}, {"label": 0, "content": "Discrete messages transmitted over the radio channels are distorted under the influence of various kinds of additive and multiplicative interferences, this being the reason for errors on the receiver-side of the radio line. Error probability in the received message characterizes the communication channel quality at a definite period of time required for the radio line adaptation to the communication conditions. An immediate estimation of the error probability requires a lot of time, which in many cases exceeds the communication channel stationary interval and makes it impossible to provide operational adaptation of the radio line to the continuously changing communication conditions. The error probability and the value of telegraph (end) distortions in the received discrete message are known to be determined by the ratio of the received signal power and noise, which cannot be estimated directly, since both of these components are found in the communication channel simultaneously. Estimation of the degree of telegraph distortions takes less time compared to the time of error probability estimation. Therefore it is practical to assess error probability in the communication channel indirectly, estimating the degree of telegraph distortions at a limited interval of time in the sliding window, which is less than the interval of stationary state of the communication channel. The paper describes a specific version of the technical (software) implementation of the device for measuring the degree of telegraph distortions. The findings of the study can be used in the design of devices for radio line adaptation to communication conditions."}, {"label": 0, "content": "Effectiveness increase of information system design is a relevant objective. To solve the problem network model of information system organization in the conditions of uncertainty has been developed, operation numerical characteristics and parameters of network graph have been counted, estimate of probability of completion of work complex to a fixed term has been determined. As methods of study, methods of network planning and management in the conditions of uncertainty have been used. The results of the study allow increasing effectiveness of information system design."}, {"label": 0, "content": "At present, as the residential electricity power consumption growth rate is far higher than that of industrial and agricultural electricity power consumption, optimize the residential electricity power consumption behavior is of great significance to reduce the power grid peak-valley gap and comprehensive energy loss. To study the non-intrusive load identification technology, analyze residential electricity usage details and provide electricity usage suggestion to residents is of paramount importance. In this paper, the non-invasive load identification technology is deeply studied and the layered classification algorithm based on multidimensional load characteristic matching is proposed. On the basis of State Grid single-phase carrier intelligent electricity meter, the intelligent power meter with residential load identification function is developed. In order to verify the load identification function of smart electricity meters, the test environment was set up in the laboratory. Test results displayed that the identification accuracy of the electric appliance type can be 100%. In addition to the extremely complicated working conditions, the power consumption identification accuracy of smart electricity meters is more than 90%."}, {"label": 0, "content": "Load Monitoring (LM) is a fundamental step to implement effective energy management schemes. LM includes Intrusive LM (ILM) and Non-Intrusive LM (NILM). Compared with intrusive approaches, non-intrusive approaches enjoy low cost, easy installation, and promising scalable commercialization potentials. This paper provides a survey of effective NILM system framework and advanced load disaggregation algorithms, reviews load signature models, presents existing datasets and performance metrics, summarizes commercial applications such as demand response, highlights the challenges, and points out future research directions."}, {"label": 0, "content": "Partial Discharge (PD) pattern recognition is one of the most important steps of PD based condition monitoring of high voltage cables, which is challenging as some types of the PD induced by cable defects are with high similarity. In recently years, deep learning based pattern recognition methods have achieved impressive pattern recognition accuracy on speech recognition and image recognition, which is one of the most potential techniques applicable for PD pattern recognition. The Stacked Denoising Autoencoder (SDAE) based deep learning method for PD pattern recognition of different insulation defects of high voltage cables is presented in the paper. Firstly, five types of artificial insulation defects of ethylene-propylene-rubber cables are manufactured in the laboratory, based on which PD testing in the high voltage lab is carried out to produce 5 types of PD signals, 500 samples for each defect types. PD feature extraction is carried out to generate 34 kinds of PD features, which are the input parameters of the PD pattern recognition methods. Secondly, the principle and network architecture of SDAE method and the flowchart of SDAE based PD pattern recognition are presented in details. Thirdly, the SDAE method is evaluated with the experimental data, 5 different types of PD signals, which achieves a recognition accuracy of 92.19%. Finally, the proposed method is compared with the traditional pattern recognition methods, Support Vector Machine (SVM) and Back Propagation Neural Network (BPNN). The results show that the pattern recognition accuracy of the proposed method is improved by 5.33% and 6.09% compared with the SVM method and the BPNN method respectively, which is applicable for pattern recognition of PD signals with high similarity."}, {"label": 0, "content": "Clinics with limited resources rely on paper patient records because they are easy to use, reliable, and can be supported by the clinics' financial and technical resources. Electronic health record (EHR) systems provide benefits in patient information management and reporting; however, they often require financial and technical resources that exceed those available to the clinics. This paper hypothesizes that limited-resource clinics could successfully install and sustain a patient-record automation system if it did not require resources beyond the reach of those clinics-if such a system was available. Because no system was found, the piClinic Console was developed to test this hypothesis. The piClinic Console is a Raspberry-Pi-based, patient-record automation system that provides essential patient-record automation functions and runs on hardware that costs less than $300 USD per clinic. This paper describes the features that provide the most benefit to the clinics and that run on a low-cost system as determined through end-user observation, participatory design, and iterative user testing. Preliminary testing shows that the piClinic Console can provide immediate benefits to clinic information processing and can prepare the clinic for a smoother transition to more complete EHR system when the resources to sustain one become available. The piClinic Console system is in its early stages of field testing and this paper describes the design and development process, the results of performance and user testing, and the plans for future research and development."}, {"label": 0, "content": "With the development of science and technology, wind speed prediction plays an important role in smart grid. In order to improve the efficiency of wind power and ensure the safety of the new smart grid, the accurate prediction of wind power is very important. Because the variation of wind speed is nonlinear, and wind speed is influenced by many factors, Scholars all over the world have proposed a variety of forecasting methods to achieve a relatively accurate prediction of wind speed.In this paper, a novel method of wind speed prediction based on long short-term memory network with peephole (peephole LSTM) and wavelet decomposition is proposed. Firstly, the wind speed data are processed by wavelet dynamic decomposition, using the peephole LSTM structure to analyze the data sequence, after that a new model of peephole network is established. Through the forward training and the back-propagation algorithm update the weights iteratively, as a result get the optimal parameters.Selecting one month's data and simulating with MATLAB. High prediction accuracy is obtained by simulation. Predict the next 5 days through the first 25 days of training, and the MAPE is used to analyze prediction data. The feasibility of this method is proved by simulation results."}, {"label": 0, "content": "The reliability and lifetime determine the levelized cost of energy (LOCE) of photovoltaic (PV) modules and effectiveness of PV system. Although this theme has attracted researchers attention in recent years, there still lack an effective method to model PV modules power degradation. Therefore, this paper put forward to adopt gamma process to establish the relationship between PV modules power degradation, and temperature, relative humidity (RH). And then PV modules service lifetime is predicted under accelerated damp-heat conditions. Firstly, accelerated damp-heat tests are carried out on three different temperature and RH levels. Based on Peck model, a data transformed method is proposed to obtain more power degradation data under other seven damp-heat conditions. Secondly, gamma process with an exponential transformation is applied to model PV modules power degradation under accelerated damp-heat conditions. The relationship between power degradation and temperature, RH is established by theoretical derivation and validated by experimental data. Then Expectation Maximum (EM) algorithm is proposed to estimate model's parameters. Finally, PV modules lifetime under several different damp-heat conditions is predicted. It is found that PV modules lifetime is approximate 20 to 25 years under (50\u00b0/45% RH) condition. But we also conclude that PV modules lifetime sharply decreases as the increment of temperature and RH. Hence, more factors or other test types are recommended to be considered in later accelerated tests."}, {"label": 0, "content": "With the ever-increasing number of diagnosed cases of Parkinson's Disease in the Philippines, there is a need for Ambient Assisted Living systems that will help improve the quality of life and independent living of patients with Parkinson's Disease. Currently, there are a lot of existing Ambient Assisted Living systems, such as the RAReFall Detection system, which incorporates various sensors, such as wearables, external sensors, and smartphone sensors to detect and recognize human activities. However, these existing systems are not easily accessible due to the costly and complex nature of the equipment being used. To address this problem, this project aims to create a cost-efficient, state of the art, accessible and user-friendly smartphone based Ambient Assisted Living system which incorporates the use of embedded smartphone accelerometer and gyroscope sensors in order to detect and categorize the daily activities and falls of patients with Parkinson's Disease, and at the same time employing new techniques to be able to provide a means for immediate response and to give appropriate advice in order to prolong the independent and active participation of patients in their communities."}, {"label": 0, "content": "PV power generation has such shortcomings as volatility, intermittency and so on. When it connects to the power grid, it will adversely affect the power system. So it is important to predict the power of PV power generation system accurately. The more similar the weather condition is, the more similar the generation of photovoltaic power generation system is. This paper proposes a new PV Generation Power Prediction model Based on GA-BP Neural Network with Artificial Classification of History Day. Firstly, we classify historical weather data artificially. Then, we establish different PV Generation Power Prediction model for each weather type using BP neural network and genetic algorithm (GA-BP neural network). At last, the power of the forecast day is predicted by the GA-BP neural network which is for the weather type of the forecast day."}, {"label": 0, "content": "Since the traditional correlation analysis based on complex power flow calculation cannot meet the requirements of performance evaluation of current distribution network planning, the BP neural network (Back Propagation Neural Network, the BPNN) based correlation mining is proposed in this paper. With the reconstruction measures and performance indexes as the training sample sets, the corresponding correlation model through the offline learning of sample data can be obtained. As a result, when given reconstruction measures in practical application, the neural network training can give the results of performance indexes quickly and accurately. In addition, in order to improve the generalization mapping capability of BPNN, the genetic algorithm is used to optimize the weights and thresholds of the BPNN. Experimental result based on the IEEE 33 node network shows the accuracy and effectiveness of the presented methodology."}, {"label": 0, "content": "To estimate the level of 10kV distribution network line losses more integrally and precisely, an evaluation method based on BP neural network (BPNN) improved by particle swarm optimization (PSO) has been proposed. Making full use of existing data resources in the State Grid Corporation of China, the relevant information in the process of10 kV distribution network line and line loss has been collected and integrated, to formulate the power grid equipment data and operation data in the process of power distribution and utilization. Firstly, the electrical characteristic indices have been selected and established to reflect the structure and operation state of 10kV distribution network. Secondly, the inertia weight and the acceleration coefficient of PSO have been dynamically adjusted so that the weights and biases of BPNN are searched more effectively. Then nonlinear relation between electrical characteristic indexes and line losses is fitted through the learning of training sample sets so as to predict the line losses of test sample sets. Finally, the PSO-BPNN is proved to be effective and proper through an actual 10kV distribution network sample data."}, {"label": 0, "content": "To model the load area and simplify the electrical network, a multi-port area load equivalent modelling method (ALEEM) based on the extended generalized ZIP load model (EGZIP) is proposed. Different from the traditional ZIP load, the EGZIP load model incorporates the voltage magnitudes and voltage phase angles of all boundary buses, which can equivalently model the area load with multiple boundary buses more accurately. The load flow calculation considering the EGZIP is derived and analyzed. Also, based on the hierarchical and partitioning characteristics of the power grid, a multi-port equivalent strategy is proposed to reduce the number of parameters to be identified in the equivalent model. For parameter identification, the currents measured at varying operation conditions on the boundary bus are used to construct the least square estimation (LSE) problem. The interior point method is used to identify the model parameters. The simulation test conducted on the 87 bus system proves the equivalent model derived from the proposed ALEEM based on EGZIP has higher accuracy and the multi-port equivalence strategy can reduce both the number of parameters to be identified and the time consuming on equivalent process."}, {"label": 0, "content": "Due to the excessive number of databases, unbalanced development and behindhand sensing infrastructures, distributed network data suffers from inconsistency, data missing, large measurement error and other data quality problems, which hinder the development of smart distribution network. In order to discover more complex deep-seated rules and provide more effective decision support for power system decision-making, it is necessary to study data mining and analysis methods that are suitable for massive data under current situation. This paper studies on the method of identifying bad data for multi-temporal and multi-spatial data in distribution networks and propose a method to identify bad data using likelihood-ratio test for 3D spatio-temporal data. In order to speed up the data processing rate, a 3D-LRT method based on multi-threading and Hadoop parallelization methods is proposed."}, {"label": 0, "content": "Urban gun violence in cities across the world is a serious issue for public safety agencies and disaster management organizations. This led us to the development of the EDNA drone, an aerial robotics solution designed to equip first responders in high-risk settings with lifesaving-edge tools for situational awareness and non-lethal conflict resolution. The EDNA is an unmanned aerial vehicle (UAV) that delivers the patent-pending \u201cPredictive Probable Cause\u201d technology. The EDNA drone is designed to provide automated real-time analysis to assist teams entering high-risk situations where gun violence may occur. By leveraging machine learning, biometric sensors, and advanced materials in the field and routing feedback to an intuitive augmented-reality interface, the EDNA will provide autonomous threat detection and bullet-stopping capabilities wherever those features are needed--to groups such as Police and Sheriff's Departments, Fire Departments, and EMT and emergency rescue teams. Data from the EDNA drone's sensors is fed to machine learning algorithms running on the drone in real-time. Through a neural network trained on past data, the EDNA is able to detect the presence and location of firearms and explosives, even through walls or other obstacles. Through the use of advanced metal foams and composite materials, the armored drone can even stop bullets-functionality which has obvious benefits for humanitarian deployment."}, {"label": 0, "content": "Energy security is vital to the national welfare and the livelihood of the masses. As the most important link in the energy transfer chain, the security of power system operation is of great significance. For a long time, how to improve the knowledge of power system--one of the most complicated artificial system--has always been the goal and motivation of electronic engineers. This paper firstly summarizes the cognitive method of general physical system and power system, teases out their inner link and mapping relationship. With the population of WAMS in power grid, a new idea about taking advantages of WAMS data for stability detection, data-driven research method is purposed. This paper reviews on the classification, development and status quo of all kinds of data-driven methods. In view of the ambiguity in the internal of data-driven methods, this paper summarizes all kinds of technique routes and a clear frame of data-driven method is sorted out."}, {"label": 0, "content": "In general, distributed renewable energy, energy storage and DC load are connected to the traditional AC distribution network through multistage converters, which leads to low energy efficiency of the system. AC/DC power distribution technology can effectively reduce the intermediate link of AC/DC transformation, and improve the economy, reliability and operation flexibility of power distribution. This paper is aimed at the economic energy problems of efficient access for high-capacity distributed renewable energy and DC load, on the basis of research before, several aspects such as system structure design, key equipment support technology and operation control technology are explored, in order to provide some ideas and references for further research and application of AC/DC hybrid system."}, {"label": 0, "content": "A total transfer capability (TTC) calculation model based on stacked denoising autoencoder (SDAE) is proposed in this paper, considering static security, static voltage stability and transient stability constraints. The TTC calculation model consists of feature pre-screening, SDAE and the regression layer. Fast correlation-based filter (FCBF) is used to eliminate irrelevant and redundant features to improve the training efficiency of SDAE. SDAE takes advantage of the deep structure to extract high-order features relevant to TTC from original features. The regression layer is utilized to create the mapping between high-order features and the TTC value. Experiment results of a real power system demonstrate that the proposed TTC calculation model has higher computational accuracy than shallow machine learning models and feature pre-screening decreases the training time of the TTC calculation model obviously."}, {"label": 0, "content": "At present, the injection of false data (FDI) in power system network brings a direct challenge to state estimation and reduces the reliability of the system. The data collected by WAMS system is of high frequency and accuracy, which can effectively prevent FDI attacks. This paper focuses on the problem of PMU optimization configuration considering false data injection. On the premise of ensuring the overall observability of the system and taking the zero injection node into account, the effect of false data injection on the power grid is effectively reduced through the optimized configuration of PMU, and the accuracy of state estimation data is improved to the maximum extent. Taking IEEE14 IEEE30 and IEEE57 standard nodes as examples, the data accuracy was improved to the maximum extent with the minimum number of PMU, and the feasibility of the method was verified."}, {"label": 0, "content": "This paper proposes a simplified simulation and modelling tech for microgrid. This method can save simulation time and computational memory, compared with the simulation of the detail model. Firstly, the grid-tied inverter, the integrated load, and the nonlinear load in microgrid are analyzed and modeled. Next, their models are simplified as Th\u00e9venin-Norton equivalent models. Moreover, the consistency between hardware equivalent model and control equivalent model of power electronics devices is proved. Finally, the correctness of the simplified simulation and modelling tech is confirmed during voltage sag in the improved IEEE 13 Node Test Feeder system in MATLAB/SIMULINK. The results show that the outputs characteristics of the simplified models are consistent with those of their detailed models."}, {"label": 0, "content": "This paper presents a novel equivalent modeling method for distributed photovoltaic (PV) power station clusters. Deep Learning (DL) is proposed to PV power station clusters modeling and the training algorithm is Deep Belief Network (DBN) made up of multiple layers of restricted Boltzmann machines (RBM). A DBN algorithm is applied to dynamic equivalent modeling of PV clusters after first-step clustering using the improved K-means algorithm. The input variables of the neural network are irradiance variation, voltage fluctuation, reactive power reference of the dual-loop controller, the output active and reactive power of PV clusters. The output variables are the output active and reactive power of PV clusters. The datasets are obtained through 6560 experiments. Then a layer-by-layer unsupervised learning method is used to pre-train the network followed by fine-tuning the parameters using a supervised back-propagation (BP) method. Finally, the equivalent model of PV clusters based on DBN is built and applied to a typical distribution network in Anhui Province. The validity and accuracy of the proposed model are verified in three different disturbance cases. At the same time, the computational complexity and the simulation time are reduced using the proposed model significantly."}, {"label": 0, "content": "The Unified Power Flow Controller (UPFC) is the most complex and powerful FACTS device, which can provide full or independent control of line transmission parameters, such as voltage, line impedance and phase angle. In this paper, steady-state model of unified power flow controller (UPFC) and power flow algorithm of power system with UPFC were studied. The parallel and series sides of UPFC were equivalent to separate power injections, and an alternating iteration algorithm of power flow for power system with UPFC was presented. In order to improve the convergence of the algorithm, this paper analyzes the impact of UPFC's various control strategies on the convergence of power flow calculations, and proroses a control strategy for converting constant power control to constant variable control during the iterative process. Based on the actual project of Nanjing Western UPFC, the correctness and effectiveness of the model and algorithm are verified."}, {"label": 0, "content": "With the improvement of industrialization, people's requirements for power supply quality also increase year by year. Therefore, how to ensure the normal supply of electricity has become a very concerned issue for power maintenance personnel. In this paper, we use the image captured by the cable tunnel inspection system to locate the cable connector in the image and map it to the infrared image based on the convolution neural network method. Based on the Analysis of the temperature of the cable connector in time, the monitoring system is able to make an alarm on the abnormal situation which can maintain the safety of power supply and extend the service life of the cable. Aiming at the small number of sample images collected, a transfer learning method is adopted, which reduces the training intensity and ensures a better positioning and recognition effect."}, {"label": 0, "content": "Studying the propagation mechanism of cascading failure and identifying the vulnerable lines in the power grid is of great significance for the prevention control of cascading failure. The existing works pay more attention to vulnerable lines and high-risk path, which are hard to explain the overall propagation characteristic of cascading failure. This paper applies sequential pattern mining technology to cascading failure analysis. The concept of cascading failure pattern (CFP) is defined firstly, and a cascading failure pattern mining algorithm (CFPMA) based on PrefixSpan is proposed. The relevance of lines is then calculated based on the result of mining algorithm. Test result on the IEEE 39-bus system shows that the proposed method can find out the CFP which can clearly show the overall propagation mechanism of cascading failure. Furthermore, test results under different power flow snapshots indicate that the CFP doesn't change with system status."}, {"label": 0, "content": "In order to realize the online evaluation and control of transient stability after power system fault, a decision tree method and emergency control scheme based on Fisher linear discriminant were proposed. This method firstly processed the offline simulation data based on Fisher liner discrimination (FLD), reduced the data dimension and obtained the distance from the data point to the classification hyperplane, and obtained a more adaptive classification decision tree by training the distance data. Fit the distance to obtain a stability index that can guide emergency control. On this basis, the optimal load shedding and generation shedding scheme were calculated according to the 0-1 linear programming, and the loss was reduced to the minimum under the premise of ensuring the stability of the power system. Finally, the simulation analysis was carried out in the WECC system and New England system. The results showed that the decision tree based on FLD is classified accurately and interpretatively. The online emergency control strategy based on stability margin can effectively deal with the transient instability after fault, and restore the system to stable operation."}, {"label": 0, "content": "Wind power curve is a key tool to characterize wind power output feature, and is also the basis of wind power planning and operation research. The wind power curve is a high dimension matrix data with local property. So it's a vital task to find an effective method to reduce dimension of the curve. In this paper, the latest techniques of artificial intelligence and deep learning are introduced to probe a new method for reducing the dimension of wind power curve. The convolutional autoencoder of typical deep learning framework is redesigned, and it learns feature representation from massive history data. The experiment result shows that the proposed autoencoder is better fit the wind power curve dimensionality reduction study."}, {"label": 0, "content": "The extension of Transmission network is highly important because of the power transfer capability impingement of transmission lines on power market business but at present moment the new extension of the transmission network is confined because of the substantial contemplation, financial issues and potential health trappings of the electromagnetic field. This consistently growing need for electric power has formed it necessary to employ the accessible transmission network assets. The implementation of flexible AC transmission system (FACTS) might be fruitful choice to enhance power flow capacity (PFC) in existing transmission network. FACTS devices diverse the tracks of exchanged power over transmission lines through changing bus voltage angle, the reactance of transmission lines. In this work, a miniature of Unified Power Flow Controller is modified with Fuzzy Logic (FL) based shunt and serial controllers to increase the power network stability. Fuzzy control is employed in the control scheme of UPFC and Pulse width modulation. A critical case of 3 phase fault occurrence in a power system has been studied. The results are compared with the presence and absence of UPFC. Simulations have been accomplished on MATLAB/Simulink software. To present the effectiveness of purposed controller, the obtained outcomes have been also compared with PID controller. It reveals that the Fuzzy Logic based UPFC has the best performance over PID Controller."}, {"label": 0, "content": "The role of technology as a key driver of sustainable development has long been recognized. Technology leverages total-factor productivity and contributes to the growth of welfare. The lack of adequate financial and human capital hampers the adoption of technology in many parts of Africa. On September 2015, the General Assembly of United Nations adopted as a consensus resolution the 2030 Development Agenda for Sustainable Development with seventeen goals (SDGs). Creating a knowledge-based economy is a key enabling factor for achieving the SDGs. If successful, this will result in a reduction of inequalities and enhance the quality of life in those countries. It is our belief that education plays a key role in effective and sustainable development. While there are many ways for doing this, we believe that strengthening budding academic institutions is a sustainable long-term strategy. Such approach needs to take the local situation into account and requires flexible frameworks with the ability to respond to local needs. We have embarked in an international effort to implement a solid proof-of-concept of our approach. Specifically, it focuses on two SDGs, \u201cgood health and well-being\u201d together with \u201cquality education\u201d. If successful, we expect a positive impact on a third goal, \u201cdecent work and economic growth\u201d. The Canary Islands are a Spanish region and a European outermost region near West Africa. Thanks to the unique combination of geolocation and level of development, the islands play an increasing role as a logistics, trade hub, and launch base for many countries in Western Africa. Many aid organizations, including UN World Food Program, Red Cross/Crescent and USAid, use the Canary Islands as logistics base. In this paper, we report our current experience within the European funded INTERREG MACBioIDi project. One of our main aims is to create a learning community in the fields of medicine and engineering. The participants live in different countries. They speak different languages and come from a variety of cultures with different social and economic boundary conditions. African participants in the project come from Cape Verde, Mauritania, Senegal, and Mozambique. Technology partners come from the outermost European regions of Canary Islands, Azores and Madeira. The main technological platform used in this project is called 3D Slicer. 3D Slicer is an extensible, free software for visualization and processing of biomedical images and was created by a community of scientists and engineers led by researchers from Harvard. Training and mentoring is provided to African physicians, biomedical engineers and researchers in the use 3DSlicer. In order to support these activities, a training center has been created at the University of Las Palmas de Gran Canaria (ULPGC). We first describe the overall strategy of 3D Slicer training, designed in response to the needs of each African country. We highlight the major hurdles faced (e.g. internet connectivity, maintenance of medical equipment, differences in linguistic and social context). Then we focus on the implementation of the training program. Training sessions were held in Africa and at ULPGC, and based on a blended learning program with \u201cface to face\u201d lessons, coaching, collaboration, multimedia, web based learning and support resources. Finally, we analyze the first results obtained in the program training and the measures adopted to readjust the action plan in the coming months."}, {"label": 0, "content": "mHealth4Afrika is a collaborative research and innovation project, focused on supporting Horizon 2020 Societal challenges and Sustainable Development Goal 3. It is researching and evaluating the potential impact of co-designing and developing an open source, multilingual enabled mHealth platform to support quality community-based primary maternal healthcare delivery at semi-urban, rural and deep rural clinics, based on end-user requirements in Southern Africa (Malawi, South Africa), East Africa (Kenya) & Horn of Africa (Ethiopia). This paper aims to share the co-design process applied to develop and validate Beta platform v1, and the implications this had on the design of subsequent iterations of the beta platform. The Beta platform v1 validation was undertaken with 36 participants from 11 healthcare clinics across Northern Ethiopia, Western Kenya, Southern Malawi and Eastern Cape, South Africa during November - December 2017, using a mix of ethnographic observation and semi-structured interviews. These findings have informed the co-design of the subsequent iterations of the mHealth4Afrika beta platform, which is being used in the participating clinics on a phased basis during Q3 - Q4 2018. This platform integrates Electronic Medical Records, Electronic Health Records, medical sensors, visualisation and automatically generate monthly health indicators, thus supporting better clinical care and decision making, and freeing up time for clinical care and continuous professional medical education, thus reducing mortality rates. The expected outcome is a multi-region proof of concept that can make a significant contribution in accelerating exploitation of mHealth across Africa to improve health outcomes and stregthen health systems."}, {"label": 0, "content": "As the global population soars from today's 7.3 billion to an estimated 10 billion by 2050, the demand for Food, Energy and Water (FEW) is expected to more than double. Such an increase in population and consequently, in the demand for FEW resources will undoubtedly be a great challenge for humankind. A challenge that will be exacerbated by the need for humankind to meet the greater demand for resources with a smaller ecological footprint. This paper is proposing a system developed to optimize the use of water, energy, fertilizers for agricultural crops as a solution to this great challenge. It is an automated smart irrigation system that uses real time data from wireless sensor networks to schedule an irrigation. The test-bed consists of a wireless network monitoring soil moisture, temperature, solar radiation, humidity, and fertilizer sensors embedded in the root area of the crops and around the test-bed. Wireless sensor data transmission and acquisition is managed by an Access Point (AP) using ZigBee protocol. An algorithm was established based on threshold values of temperature and soil moisture automated into a programmable micro-controller to control irrigation time. The system's energy demand is completely supplied by a solar Photo-voltaic (PV) panel supplemented with an energy storage unit. The experimental data obtained from this prototype will be modeled and optimized to investigate food production profile as a function of energy and water consumption. It will also attempt to understand the effect of extreme weather conditions on food production. This holistic approach will explore the nexus between water and energy resources, and crop yield for several essential crops in an attempt to design a more sustainable method to meet the forecasted surge in demand."}, {"label": 0, "content": "Aimed at the problem that voltage at the end of feeder lines is always too low to start up pump loads in low-voltage distribution network (LVDN) mainly composed of agriculture loads of oxygen pumps and water pumps, a distributed reactive power compensation optimal allocation model of LVDN including pump loads characteristics is proposed. In this model, the objective function includes both the investment cost of the compensation device and the economic benefits of reducing power loss of LVDN, and various constraints of LVDN in the condition of normal operation and startup of pump loads are included in the constraints. A sigmoid function is used to approximate the discontinuous sign function in the objective function, and a nonlinear penalty function which has relative large curvature near the discrete value is used to address the discrete variables characteristics of reactive power compensation capacity, then the primal-dual interior-point algorithm is used to solve the optimization model. Numerical results of an actual LVDN show that the obtained optimal allocation scheme of the method can improve the voltage quality of distribution network, ensure the normal startup of pump loads, and decrease the power loss of LVDN effectively."}, {"label": 0, "content": "Dynamic security assessment (DSA) is widely used in dispatching operation systems, and calculation speed is one of its most important performance indices. In this paper, a deep learning model called Siamese neural network is proposed aiming to predict the transient stability indicators of power system, for example critical clearing time (CCT). The method is much faster than the simulation and suitable for online analysis. Firstly, a simulation sample database is constructed based on historical online data; then a Siamese model is trained, which uses static state quantities as its inputs like active power of generators. While a new online power flow needs to be evaluated, the high level features of Siamese model are obtained and a k-NN is implemented to find the most familiar samples in the database using the chosen features; the final result will be determined comprehensively by the familiar samples. The validity of proposed method is verified by the simulation using online data of State Grid Corp of China (SGCC) and different key faults. It is proved that the method meets the requirements for speed and accuracy of online analysis system, especially for small sample set."}, {"label": 0, "content": "A new concept of approximate power flow (APF) is proposed in this paper, aiming to help deal with the non-convergence problem of power flow calculation. In the approximate power flow model, active and reactive power decoupling strategy is adopted, and a branch model with virtual midpoint is the key foundation of the whole research. Based on the branch model, the approximate power flow equation is constructed and its iterative solving method with good characteristics of convergence is also introduced. Active and reactive power automatic adjustment measures are also used to improve the algorithm robustness. The effectiveness and feasibility of this approximate power flow model is proved by the error and robustness analysis for practical examples. The APF program has been developed based on the proposed method, and can be applied to the actual large-scale power grid."}, {"label": 0, "content": "The power load of residential community has the characteristics of wild fluctuations, complex influence factors and difficult forecasting. To deal with that, a short-term load forecasting (STLF) method for residential community based on gated recurrent unit (GRU) neural network was proposed. The least absolute shrinkage and selection operator (Lasso) and partial correlation analysis are used to analyze the influence of temperature, humidity, rainfall and wind speed on the load. It shows that the average temperature affects the change of the load most among various factors, thus the average temperature is added as an input variable to the load forecasting model based on GRU network. The simulation results show that the proposed method is faster within the similar forecasting accuracy, compared with the long short-term memory (LSTM) network and traditional recurrent neural network (RNN). It's proven to be a more effective residential community short-term load forecasting method."}, {"label": 0, "content": "This paper starts from the weight and sensitivity of the parameters of the identified elements, A method for parameter identification of synchronous generator based on rough set theory and particle swarm difference algorithm is proposed. Firstly, the output of synchronous generator is constructed by rough set theory, The attribute reduction of the decision table is made to obtain the attribute parameters which have great influence on the simulation results. Then, according to the dependence of the calculated parameters, the pruning dimension of the identification parameters was realized. Finally, A generator model is built on the Matlab/Simulink platform, The model parameters are identified by particle swarm difference algorithm and model input and output data. The simulation results show that the proposed method can effectively improve the efficiency and precision of parameters identification."}, {"label": 0, "content": "This paper presents a method to identify T-equivalent parameters of transformer using its port data. In order to use the port information of the transformer to determine the internal fault of the transformer, according to the model reference adaptive principle, the recognition model and the adjustment model of the transformer are constructed. The fitness function is composed of the response output of the two models, and PSO algorithm is used to identify its parameters of equivalent circuit. Firstly, the simulation platform is built in MATLAB/Simulink to iterate two times. The excitation parameters identified by the first iteration are given as the known quantity, and the second iteration can identify the leakage impedance parameters of the high and low voltage side of the equivalent circuit. Then, the feasibility of the algorithm is verified on the experimental platform."}, {"label": 0, "content": "Considering the actual operation conditions and the general conditions of simulation calculations, this paper presents a simplified critical AC-DC system voltage interaction factor (SCADVIF) index and a method to fast evaluate the commutation failure risk of multi-infeed HVDC systems. The method still does not require modeling power system components in detail for dynamic simulation. By calculating and comparing all the ADVIFs and SCADVIFs of an AC/DC system, the receiving-end AC buses at which faults are applied may cause commutation failure of multi-infeed HVDC system can be found quickly: if ADVIFjm \u2265 SCADVIFjm, a fault occurring at receiving-end system AC bus m would result in commutation failure at HVDC j. In this way, it is possible to quickly identify all AC buses where a three phase fault would cause single or multiple commutation failures. The validity and accuracy of the proposed approach are demonstrated by comparing with simulations results using an actual planning large power grid. The proposed method instead of time-domain simulation analysis tool greatly reduces the computing time of researchers, and improves their work efficiency. The proposed index and method can be widely used in the planning, design and operation of AC/DC power grids."}, {"label": 0, "content": "This paper presents an iterative approach to jointly estimate the states in combined heat and power systems (CHPS). The node method is used to address the temperature quasi-dynamics in the district heating system (DHS), resulting in a dynamic state estimation (DSE) model. An alternating estimation strategy is employed to effectively handle the complicated time-delay constraints of temperature in the computation of DSE. Case studies are conducted on two CHPS test systems to verify the effectiveness of the DSE and the alternating approach. Simulation results show that the DSE in CHPS outperforms the static state estimation and the separate state estimation in individual energy systems in terms of accuracy."}, {"label": 0, "content": "A novel penetration capability of distribution generation methodology to voltage constraint is developed. The design goal of the methodology is to decide the maximum DGs' capacity and the best integrated location without voltage violation. Instead of dealing with the combinatorial nature of the proposed problem, the proposed methodology employs a two-stage methodology to decide the solutions. The first stage is a linear method to estimate the capabilities under all the possible DGs' locations. The second stage is a detailed calculation stage which employs a nonlinear optimization method to calculate the exact maximum penetration capability under the best location determined in stage 1. The maximum penetration capability and the corresponding location can be easily calculated by the proposed methodology. Finally, several examples under single and multiple DGs in IEEE 33-bus power system are used to verify the proposed methodology. The numerical studies show that the effectiveness of the proposed methodology in calculating the maximum penetration capability and determining the best location for DGs in distribution network systems."}, {"label": 0, "content": "There are many fuzzy problems in the power system, such as fault diagnosis, condition assessment, etc. Despite the rapid development of science and technology, uncertain and incomplete information still exists to varying degrees. These problems of ten involve multiple factors. It is very important to choose a suitable method to deal with such problems. In this paper, a new uncertainty algorithm is proposed based on fuzzy theory, rough set theory and evidence theory to solve the problem of multi-index uncertainty in power system. This algorithm can process inaccurate data and then analyze it. Taking the transformer state evaluation as an example, the application of this algorithm is illustrated."}, {"label": 0, "content": "We present our research on understanding innovation for education in two Rwandan secondary schools. Our innovation for education project focused on developing spatial thinking skills via Geographic Information and Communication Technologies (GeoICT)-based training. Specific GeoICT used focused on 2D, vector-based maps used on Android tablets and commercial desktop Geographic Information Systems (GIS) software. Trainings were conducted in the context of a research program that sought to develop new approaches for Rwandan education innovation. We discuss qualitative results from teacher and student reflections gathered from a Web-based survey about what it was like to be part of the innovation for education process, broader opportunities spatial thinking provides, and innovation for education process feedback. We also conducted extensive group interviews with teachers at the two schools based on data collected from Web surveys. The interviews and surveys allowed us to assess four ways our innovation for education approach impacted teachers and students. First, teachers and students identified broader societal benefits and individual opportunities the innovation for education process is creating. Second, Rwandan teachers identified education and societal benefits for problem solving and reasoning stemming from increased thinking ability, GeoICT training, and space-time thinking ability. Third, teachers found new roles and identities for themselves through incorporation of spatial thinking-oriented curriculum and GeoICT training. Fourth, the importance of certificates and recognition artifacts as tools for students and teachers to establish their new competencies. Our focus on innovation for education, spatial thinking and GeoICT inform the literature onbroader technology-enhanced quality education delivery research on the value of spatial thinking and GeoICTs."}, {"label": 0, "content": "The model of reactive power optimization including interval uncertainty (RPOIU) is used to make a voltage control strategy for ensuring the interval state variables (including load voltages and reactive power generation) of the power grid within their safe operating limits, under interval active power generation and power load demand input data. To solve the RPOIU model, this paper defined security limits, and then switches the RPOIU model to two deterministic reactive power optimization models, whose constraints bound limits are the security limits. By solving the deterministic models through the interior point method, a voltage control strategy is obtained as the solution of the RPOIU model. The results obtained by the proposed method are compared with the linear approximation method, which is a previously proposed effective method for solving the RPOIU model, and simulation results and analysis demonstrate the advantages, effectiveness, and good applicability of the proposed method."}, {"label": 0, "content": "We propose a hybrid probabilistic interval prediction method for short-term load forecasting. The method combines K-means clustering based feature selection approaches and online Gaussian processes regression(OGPR) to generate better prediction results. The K-means clustering algorithm based feature selection are used to select the most relevant features during a dynamical process to better capture the load characters along with time. OGRP, includes dynamically updating the hyper-parameters and training sample sets as two key features, is served as a forecasting engine to carry out load probability interval prediction. The load data from Queensland market, Australia is used to validate the model proposed. The comparative results show that the proposed approach can obtain higher quality prediction interval."}, {"label": 0, "content": "This paper proposes a distributed demand response algorithm that considers the uncertainty of resident behavior. This algorithm based on the alternating directions method of multipliers (ADMM) allows for distributing the optimization process across several servers/cores, which conserves users' privacy and reduces the computational complexity of demand response. At the same time, the robust optimization method is applied to deal with the uncertainty of the response process, reduce the impact of resident behavior uncertainties. Finally, the effectiveness of the algorithm has been verified through the simulation."}, {"label": 0, "content": "Power system static stability situation assessment is the core of power system security prevention and control. Most of traditional static stability assessment methods are focused on physical models and high-intensity simulations. These methods have a large amount of calculation and high dimensionality, and its online engineering applicability cannot be guaranteed. In order to better ensure the safe operation of power system, this paper proposes a static stability assessment method based on scale-invariant feature transformation(SIFT). This method directly extracts the association of holographic data under various static operating conditions of power system. Based on the operating feature it is improved for the generalized elasticity index and it is achieved for an accurate assessment of static stability situation. The simulation result of the New England 10-machine 39-bus system indicates that the improved grid generalized elasticity index has a higher slope and better engineering application value."}, {"label": 0, "content": "Unbalanced structure of distribution network brings difficulties into its analysis and computation. With increasing integration of renewable distributed generation (DG) in distribution network, more accurate and efficient sensitivity analysis method is required to facilitate following control and optimization procedures. In this paper, a sensitivity analysis method for unbalanced distribution network is developed. The proposed method utilizes existing measurement data to construct linearized power flow model and sensitivity data is obtained directly via solving linear equations. The numerical tests show that the proposed method achieves higher accuracy than traditional methods."}, {"label": 0, "content": "Gas turbine distributed energy supply system (DESS) is a kind of important black-start unit in the future power system. This paper proposes a method of using Support Vector Machine (SVM) model for fast amplitude determination of transmission line switching overvoltage in the black-start plans based on Gas turbine distributed energy supply system. Black-start is the last line of defense for ensuring the reliability of power system. Hence black-start plays an important role both in the process of system recovery to ensure system security. During the process of making black-start plans of power system, it is necessary to verify the rationality of some technical issues by repeated modeling and simulation of different black-start plans, thus costing a lot of manpower and time. In recent years, distributed integrated energy supply system is greatly supported by government because of high efficiency and less pollution. Especially, gas turbine integrated energy supply system has excellent self-start and flexible adjustment ability, which can be considered as suitable black start unit. In this paper, firstly, the black-start scenarios are classified by the function and the type of the black-start units. Secondly, transmission line switching overvoltage involved in the process of black-start are modeled through PSCAD/EMTDC simulation software and analyzed by a large number of simulations. Thirdly, a support vector machine (SVM) model is established for fast amplitude determination of overvoltage in a black-start scenario. In this model, the selection of characteristic inputs in SVM method is analyzed in detail under the influence of important technical problems and the features of Gas turbine distributed energy supply system, and then the characteristic inputs are selected by orthogonal decomposition method. In the study case, artificial neural network (ANN) and support vector machine method are used for comparison, 200 samples are used in training set and more than 1400 samples are used in testing set, the error analysis shows that the support vector machine method is more effective than the artificial neural network method in the case of small training sample size. At last, an actual example analysis which considered the Guangzhou Higher Education Mega Center distributed energy station as black-start unit shows that the fast amplitude determination of switching overvoltage model can effectively reduce manpower and time."}, {"label": 0, "content": "WTCM (Wind Turbine Condition Monitoring) system is important for wind farm operators to realize condition-based O &M (operation & maintenance), in the purpose of reducing O &M cost and improving wind turbine reliability. A WTCM method using only SCADA data based on data mining algorithm is proposed in this paper. Firstly, ARD (Automatic Relevance Determination) algorithm is adopted to determine the effective variables that are relevant to wind turbine condition. Feature vector is then extracted using the effective variables to represent the operation condition of wind turbine. Finally, the condition of a wind turbine is determined using outlier detection algorithm based on the extracted feature vector. Real-world dataset is used to validate the efficiency of the proposed method. Experiment results show that the proposed method can provide advanced failure alarm for wind turbines many days before failure happens. O &M cost can be reduced by condition-based O &M strategy using the result of our proposed WTCM method."}, {"label": 0, "content": "The line loss in power distribution network is an important index that affects the economic benefit of power supply enterprise. In order to ensure the accuracy and stability of the line loss calculation based on large amount of power measurement data, the distributed parallel processing method is applied to the line loss computing service, and the line loss calculation model in power distribution network was obtained by the fitting of BP neural network. Furthermore, many examples are given to test the algorithm proposed in this paper, the results show that the method can guarantee the stability and the accuracy of calculation results in line loss calculation."}, {"label": 0, "content": "Non-contact assessment of breathing is a valuable tool to assist in the health screening of infants and neonates, very old frail patients, or patients with infectious diseases, where it is not practical or advisable to use any device that attaches to the patient. Low-cost mobile phone-based methods to assess breathing are of particular interest in global health applications where alternative instrumentation is not available or affordable, and also for home use by consumers in wealthier areas. In this context, we have developed a smart phone mobile application which uses a thermal camera module to automatically measure respiration rate (RR) and respiration rate variability (RRV). The mobile application makes use of machine vision algorithms to detect a human face and then find the nostril points under the nose which are used for measurement of temperature. Temperature fluctuations in the nostrils are then used to approximate air flow rates and calculate timing parameters. Our device was tested with eleven human subjects in a clinic and validated against a gold-standard impedance pneumography belt using three intensities of breathing (normal breathing, shallow breathing, and deep breathing) as well as two different positions of the camera (camera at 0 degrees and -20 degrees angle respectively). With the thermal camera located at a height slightly lower than the face (20-degree angle), the results from the mobile phone tool had good agreement with the clinical device, with the mean standard error (MSE) of 0.8, 1.4, and 1.8 bpm, for normal breathing, shallow breathing, and deep breathing respectively."}, {"label": 0, "content": "Pulmonary and respiratory diseases comprise a large proportion of the global disease burden, responsible for both mortality and disability. This burden is especially concentrated in the developing world, where air pollution levels are generally high and resources for diagnosing these diseases are very limited. Health workers and many general practitioner doctors in developing countries are not trained to diagnose pulmonary diseases, leading to high rates of misdiagnosis and underdiagnosis. Motivated by this need, we have developed a mobile toolkit that can be used for screening and diagnostic guidance for three of the most common pulmonary and respiratory diseases (Asthma, Chronic Obstructive Pulmonary Disease (COPD) and Allergic Rhinitis (AR)). The toolkit consists of a mobile phone app, known as Pulmonary Screener, which is used in conjunction with a low-cost (<;US$10) peak flow meter. Machine vision software enables the phone camera to automatically track and capture the reading from the peak flow meter without the need for any electronics, Bluetooth radio, or batteries. Using the peak flow meter reading as well as an integrated clinical questionnaire, a machine learning model is then used to calculate the individual probabilities of a patient having a specific pulmonary disease (Asthma, COPD, AR, other) or comorbidities (Asthma + AR, COPD + AR). The machine learning models used in the application were trained using diagnostic data from 325 patients collected at a pulmonary clinic over the past 3 years. Based on 50 iterations of a held-out test set, the Pulmonary Screener achieves accuracy (AUC) values above 0.90 for all diseases and combinations, with the exception of Asthma with AUC = 0.84. Sensitivity and specificity values were for all diseases was also greater than 0.90. To our knowledge, this is the first clinically validated mobile tool that is capable of diagnosing multiple pulmonary diseases in a single app. Future versions of the Pulmonary Screener will make use of ongoing data collection to expand support for infectious diseases as well, including pneumonia and tuberculosis, and include features extracted from auscultation and cough sounds."}, {"label": 0, "content": "The operation and maintenance management of the distribution network (DN) mainly includes fault analysis, active early-warning and differentiated operation and maintenance. In the context of multi-time-scale and multi-spatial-temporal data in DN, this paper deals with the application of data mining for distribution network operation and maintenance management. In the paper, the one-dimensional fault feature is extracted from fault information by K-means clustering algorithm. Then, we employed Apriori algorithm to mine association rules of different failure modes and establish key performance matrix. The spatial-temporal characteristics are analyzed based on high-dimensional random matrix theory (RMT). Afterwards, one-dimensional and multi-dimensional fault features are combined based on D-S evidence theory so that the fault diagnosis criteria of DN is obtained. At the same time, comprehensively considering the DN operating state and the variation for power users, health index and importance index of equipment are established, which could help to significantly reduce the decision-making risk of DN operation and maintenance. The result of simulation proves the effectiveness of the proposed method."}, {"label": 0, "content": "With distributed energy resources widely integreted to an distribution network (DN) in multiple microgrid (MG) manner, the distributed dynamic optimal power flow (D-DOPF) becomes a major concern because DN and MGs may belong to different owners. This paper proposes a fully D-DOPF algorithm based on the cutting plane consensus (CPC) method and Ward equivalent. First, DN and MGs are decoupled by means of Ward equivalent, i.e., at optimizing each MG, other MGs and DN are replaced by their Ward equivalent circuits respectively, while at optimizing DN, MGs are replaced by their Ward equivalent circuits respectively. Then, high-precision linearization of nonlinear power flow equations is used to express node voltages approximately by a linear function of node injection powers respectively for each MG and DN. Hence, the D-DOPF model is built as a distributed quadratic programming (D-QP) problem, taking DN and MGs as independent agents. Finally, CPC is applied to solve D-DOPF. For each agent, a master problem is constructed to approximate the original problem, and only the cutting plane constraints are transferred between agents. This method does not need an upper level coordinator and also guarantees that each agent has good confidentiality."}, {"label": 0, "content": "With the development of intelligent substation, substations rely more on network communication to transmit substation information. High substation automations with reliable communication avoid the traditional complicated operation which needs to be done manually, and improve the efficiency of substation operation. But higher requirements for the reliability of information integrity are put forward. The protection setting of intelligent substation directly affects the correctness of the protection devices (Intelligent Electronic Device) and further affects the normal operation of the entire power system. Because the protection setting of protective devices is affected by power system operation mode, season climate and many other factors, the protection setting often needs to be adjusted. When the number of protection devices in substation is increasing rapidly, the frequent change of the protection setting can easily cause the missing or omission of protection setting for certain protection devices, resulting in the potential danger of the power system. On view of the above problems, combined with the development trend of IEC61850, this paper focuses on the on-line management of protection devices. By studying the data attribute of the online management of protection device settings and combining the layered network structure of the intelligent substation, the online management system for protection device settings is constructed, with the aim to eliminate the hidden danger caused by the missing or omission of protection setting."}, {"label": 0, "content": "It is an effective way to identify substation switch state using deep learning directly based on massive large image samples, which requires high-performance servers for off-line model training and high-quality industrial personal computer (IPC) for running models efficiently. The processing cost and delay will considerably increase by this means and the identify speed of robots for potential defects in the field reduces accordingly. Therefore, an image recognition method using only regular computers and IPC is proposed in this paper. Through target detection based on HSV (i.e. Hue, Saturation and Value) color space, this method firstly prefetch and preliminary screen the potential identifiers from large image samples, and subsequently trains a classification model with artificial neural network utilizing smaller samples labeled. Finally, target identifier can be located and identified through target detection, and then be used for recognizing switch state according to their relative positions. The experimental results show that with limit hardware resources, this method can process image samples efficiently and accurately based on robot vision. It is demonstrated to be a lightweight solution for precisely recognizing substation switch state."}, {"label": 0, "content": "The PT low frequency nonlinear oscillation is easily excited by external disturbances in the distribution network with ungrounded neutral point, which can cause the PT fuse fuse, burning loss and even the explosion accident frequently. The mechanism of PT low frequency oscillation is analyzed theoretically, and the phenomenon and key factors are simulated based on the electromagnetic transient simulation program PSCAD. A new low frequency oscillation suppression method for PT is proposed, that is, the PT neutral point is grounded by low frequency surge suppressor. The effectiveness of the suppression method is verified by simulation, and the low frequency inrush current can be controlled within the safe range. The theoretical research and the new suppression method proposed in this paper can provide guidance for PT fault analysis and the formulation of countermeasures in distribution network."}, {"label": 0, "content": "Due to the continuous development of distributed energy and energy storage systems, the VSC based DC and AC-DC hybrid distribution networks have been developed rapidly in recent years. Compared with the traditional AC distribution network, DC distribution network can remove the power conversion links in power supply, storage, and end loads, reducing the complexity and cost of the access system, improving the network efficiency and power quality. This paper introduces several VSC based DC distribution network projects to be implemented in China. The key technologies proposed, including AC/DC conversion, control and protection, fault isolation and restore, and DC transformer, are analyzed together with their technical challenges. This paper also explains the development trends of several DC distribution network related subjects such as DC voltage level, key equipment development, rapid protection & isolating fault, and regional coordinated control etc."}, {"label": 0, "content": "Recently, the rapid development of Artificial Intelligence (AI) has attracted more and more attention. It has been over two decades since AI techniques emerged in power systems as effective tools to solve many complex problems. The new generation AI technologies will promote energy transition and support future power system with no doubt. This paper tried to grasp the research hotspots, frontiers and mainstream trends of AI research in power systems through a literature data collected from the Web of Science (WOS) database between 2010 and 2018, by using a widely used tool in knowledge mapping-CiteSpace. The collaboration networks were analyzed among different countries/regions and institutions contributing to the publications. A detailed discussion was given based on the general statistical data and the visualized knowledge maps. The pivotal and mushrooming articles were identified and reviewed by introducing the betweenness centrality and citation bursts as indicators."}, {"label": 0, "content": "To meet network security requirements, all components of network are designed to be able to withstand contingencies, which results in significant cost to network development. Hence, network pricing should take network security into consideration. In the original long-run incremental cost(LRIC) pricing method, security factor is defined to charge for network security. It assumes that security factor is constant before and after the nodal injection. This paper proposes an improved LRIC pricing method, considering the fact that security factor of each component will be changed with nodal injections. This paper analyzes the factors that affect the security factor and illustrates the improved LRIC method with dynamic security factor. To demonstrate the difference between the improved LRIC pricing method and the original LRIC pricing method, simulation studies are conducted on a three-nodes system and the IEEE 14-nodes system, respectively; and the results show that the improved LRIC method can more accurately reflect the impact on network costs from users and ensure more reasonable allocation of network costs."}, {"label": 0, "content": "Wind power has been promoted to mitigate the energy crisis worldwide. It has beneficial impacts on economic and environment. However, owing to the chaotic nature of atmospheric movement, wind power generation always exhibits nonlinear and non-stationary uncertainties, which brings great challenges for power system operation and planning. To meet the challenges, a novel deep learning based combined approach is proposed for uncertainties. In this approach, a start-or-art point forecast approach is proposed based on wavelet transform and RNN-RBM deep learning. Raw wind power is decomposed into different frequencies. The nonlinear patterns are used to improve the forecast accuracy by this approach. Consequently, the probabilistic distribution of wind power data can be statistically formulated via the non-parametric approach. The experimental results demonstrate that this combined approach reduces the forecast error, which has a better performance than the other two comparison forecast approaches. The average accuracy of MAPE has increased by about 3.2%."}, {"label": 0, "content": "Critical infrastructure is vulnerable to a broad range of hazards. Timely and effective recovery of critical infrastructure after extreme events is crucial. However, critical infrastructure disaster recovery planning is complicated and involves both domain-and user-centered characteristics and complexities. Recovery planning currently uses few quantitative computer-based tools and instead largely relies on expert judgment. Simulation modeling can simplify domain-centered complexities but not the human factors. Conversely, human-centered design places end-users at the center of design. We discuss the benefits of combining simulation modeling with human-centered design and refer it as human-centered simulation modeling. Human-centered simulation modeling has the capability to make recovery planning simpler and more understandable for critical infrastructure and emergency management experts and other recovery planning decision-makers. We qualitatively analyzed several resilience planning initiatives, post-disaster recovery assessments, and relevant journal articles to understand experts and decision-makers' perspectives. We propose a conceptual design framework for creating human-centered simulation models for critical infrastructure disaster recovery planning. This framework consists of three constructs: 1) user interaction with design features that end-users interact with, including model parameters assignment, decision-making support, task queries, and usability; 2) system representation that refers to system components, system interactions, and system state variables; and 3) computation core that represents computational methods required to perform processes."}, {"label": 0, "content": "The experimental research on the security and stability control system (referred to as SSCS) of UHVDC transmission project mainly involves the functional verification on information interaction between SSCS and UHVDC control and protection system, the fault criterion of DC converters, the calculation of DC power loss, and the coordinated control strategies such as generators/loads tripping or DC transmission power modulation after failures in DC system or N-2 faults in AC system, and other aspects. This paper analyzed the existing test methods for SSCS, then proposed a modular modeling method. This modeling method transformed the external system of the AC/DC hybrid network into REI nodes, and performed coherency-based dynamic equivalence on the sending-end generator groups. The modular design reduces the granularity of simulation calculations, improves the speedup ratio of parallel computing, and improves the efficiency of processors usage to meet demands for large-scale closed-loop testing on UHVDC system-level protection technologies including DC co-control and precisely machines tripping. Based on this modelling design, the function of DC control and protection system was simulated, the interface between RTDS and SSCS was realized, and the simulation and test platform for UHVDC SSCS was built on RTDS. The stability control strategies and the system function verification of Zhalute-Qingzhou \u00b1800 kV UHVDC SSCS were carried out on the platform. The simulation and test results verified the effectiveness of the typical test scheme.."}, {"label": 0, "content": "We are developing a new natural language processing (NLP) method to facilitate analysis of text corpora that describe long-term recovery. The aim of the method is to allow users to measure the degree that user-specified propositions about potential issues are embodied within the corpora, serving as a proxy for the disaster recovery process. The presented method employs a statistical syntax-based semantic matching model and was trained on a standard, publicly available training dataset. We applied the NLP method to a news story corpus that describes the recovery of Christchurch, New Zealand after the 2010-2011 Canterbury earthquake sequence. We used the model to compute semantic measurements of multiple potential recovery issues as expressed in the Christchurch news corpus that span 2011 to 2016. We evaluated method outputs through a user study involving twenty professional emergency managers. User study results show that the model can be effective when applied to a disaster-related news corpus. 85% of study participants were interested in a way to measure recovery issue propositions in news or other corpora. We are encouraged by the potential for future applications of our NLP method for after-action learning, recovery decision making, and disaster research."}, {"label": 0, "content": "Disasters whether natural or man-made have great impact on countries and civilians. Proper information across the main disaster phases need to be delivered on time and to the right people to minimize the impact and provide needed resources. Social media and Twitter in particular, is an important mean of information sharing in real-time as part of a complete cyber-physical emergency management system during a disaster. Twitter can be used in any place in the world through smartphones or other mediums with an internet access connection. The vast and varied number of tweets produced during a disaster will benefit from the cloud scalable storage and processing resources. As a centralized processing system is more vulnerable when a disaster strikes, there is a need for a more resilient distributed system architecture that allows for the distribution of both processing and storage resources. The goal of our study is to develop and evaluate a prototype of a microservice architecture for twitter data analytics during a disaster that meets the requirements of disaster management. In this paper, we design a cloud-based microservices twitter analytics framework for disaster management and implement a basic prototype system. Our prototype system demonstrates that the microservices approach allows for a distributed, dynamic, reliable and scalable system architecture on cloud platform that goes in hand with disaster domain requirements."}, {"label": 0, "content": "Agriculture has been identified as one of the pathways to achieve the Zero hunger goal of the United Nation's Sustainable Development Goals. [1]. Pests are one of the biggest factors affecting agricultural yield. Besides causing loss of yield, using the wrong pesticide could lead to a loss in investment for farmers. Hence accurately identifying pests and treating them correctly will increase yield and reduce wastage. Deep learning is a subset of Artificial Intelligence, which has become popular in recent years to perform tasks like image classification, speech recognition, etc. Of all the deep learning architectures, Convolutional Neural Networks (CNNs) are the most widely used architectures to classify images. Accurate classification of images can have wide applications, for example identifying agricultural pests. In this paper, we analyze the effects of dataset size on the accuracy of a Convolutional Neural Network when used for agriculture. We trained VGG16, ResNet and Inception CNNs with a small custom image dataset and CIFAR10. Our results show that larger training data leads to higher classification accuracy. We plan to test using a larger agricultural data set and deploy for farmers in rural communities in India."}, {"label": 0, "content": "Classroom observations are a key component of professional development programs for teachers. While there are many classroom observation systems, these systems are costly to implement and may suffer from biased feedback and Hawthorne effect. Automation of classroom observation processes can potentially help obviate these challenges. This paper presents the design and implementation of an automated classroom observation system based on audio data collected during a class session using an App on the teacher's smart phone. The App automatically labels classroom activities into Stallings-type class observation categories like lecture, classwork, classroom management, practice, question/answer, and reading aloud. Based on the teacher's use of different teaching activities and student performance, the app can provide teachers with intelligent recommendations on how to best allocate class time to various activities. The App used machine learning techniques and was trained on classroom observation data collected from semi-rural primary schools in Pakistan. A variety of machine learning algorithms were evaluated, and using 10-fold cross-validation, the Random Forest algorithm yielded the best accuracy of about 69%. The results show that this approach is a viable and a much cheaper limited alternative to physical classroom observations especially in low-resource contexts of the developing world."}, {"label": 0, "content": "Demand for food is increasing with the growing world population. Cultivable land however is decreasing by the day due to rapid urbanization, and farm-yield needs to increase to meet ever-increasing food-security needs. Rapid strides in Internet of Things (IoT) have made it possible to carry out a comprehensive monitoring of the farm at various levels to help achieve this while optimally utilizing essential resources. To have a highly granular view of the farm, we have deployed an IoT based precision farming system consisting of a cluster of devices measuring over 14 ambient parameters below the soil, at the crop level, and the ambient environment. The system has been integrated with our digital farming platform on the cloud that provides a flexible interface to gather sensor data from disparate sources and issues contextual advisory to the farm supervisor in order to carry out specific actions on the field. The setup has been used so far to monitor two horticultural crops, cabbage and capsicum that were suited to the agro-climatic zone of the deployment region, for the Rabi (Winter) season of 2017. We present our experiences with the work and learnings from the deployment over the season which led to a reduction in agri-input cost by 20% and improvement in yields by more than 10%."}, {"label": 0, "content": "Making decisions during a disaster can be challenging when human lives and infrastructures are exposed. An important factor to consider when allocating resources, in these situations, is the critical infrastructures' interdependencies. i2Sim, the Infrastructures' Interdependencies Simulator, is a tool build for that purpose. As a layered architecture, i2Sim includes a dedicated decision-making layer. The use of Reinforcement Learning (RL), a machine learning approach based on an agent learning from experience, has been successfully tested with some dimensionality constraints. This paper introduces two improvements to our previous tests aiming at increasing speed and allowing larger dimensionality problems. The first addition is an improved reward scheme for speeding up convergence while guaranteeing it. The correct application of shaping rewards requires a deep understanding of the problem and extensive convergence tests. The second improvement added is the implementation of a scheduler programmed to trigger multiple instances of the same model using different parameters. This scheduler partitions the state space, enabling the agent's training to be done in parallel via a distributed RL algorithm. With this idea, the state/action matrix representing knowledge is partitioned for training, assigned to computing nodes, populated with knowledge (trained) and collected/reconstructed for use. This work has tested on an IBM cluster with 24 computing nodes. The test model is an aggregated model of the City of Vancouver configured for a disaster with numerous consequences over the different critical infrastructures. Based on the model's configuration, 24 scenarios were identified, created and solved simultaneously. The scheduler automates the training by setting up model and learning parameters, looping execution of instances and gathering results from all nodes. The results verify a proof of concept and enable applicability to new models with highly increased dimensionalities."}, {"label": 0, "content": "Safe and affordable surgery is not accessible for five billion people when they need it. Multiple surgical capacity studies have shown that hospitals in low-and-middle income countries do not have complete coverage of basic surgical equipment such as, theatre lights, anesthesia machines and electro surgical units. Currently, almost all equipment is designed and manufactured with a main focus on the context in high income countries. The context in low-and-middle income countries in which surgical equipment is used, differs from high income countries, especially in terms of financial resources and access to maintenance, spare parts and consumables. The aim of this study is to present a roadmap for design of surgical equipment for worldwide use. The roadmap consists of four phases: before the start of a design project a clear need for certain surgical equipment should be identified (Phase 0). During Phase 1 the context should be researched thoroughly by determining the barriers encountered by patients to surgical care, the structure of the health care system and if the aspects required for safe surgery are in place. In Phase 2 the implementation strategy and design requirements should be determined and in phase 3 prototyping starts in close interaction with local end-users. We believe that designers should strive for design that is of the same quality and complies with the same safety regulations as equipment designed for HICs. In this way user and patient safety can be assured in any setting worldwide. And we advocate for surgical equipment that fits the context optimally and that will be applicable in comparable settings globally."}, {"label": 0, "content": "In this paper the authors would discuss about the enhancement in the existing work done on the Internet of Things (IoT) with reference to the Indian farmers. Here the major discussion would be related to the soil testing and its constituents required for the growth of crops. The major achievement of the paper would be the action/process to be followed post soil testing of the region. Soil testing would give the data and parameters about the soil which usually comprise of soil nutrients like phosphorous, potassium and nitrogen. The work done in this regard is that after the collection of soil data, these data would be entered in the developed software application which would tell the user which crop to grow in that soil. These data about the soil would already be preloaded in the software application and further in addition to current existing temperature and weather conditions (which would be continuously monitored through the sensors) it would recommend the suitable crop to be grown in that specific soil. The authors would discuss about the Application Program Interface (API) systems software which has been developed with reference to this application. The developed software is supported through mobile app for easy access. This app is in use and is a free source for the researchers who are working in soil testing areas or IoT for agriculture application. The authors in paper are also trying to use the above solution to help Indian farmers industrialize the individual farming using the Internet of Things (IoT) and connecting directly with market with the help of one centralized mobile app."}, {"label": 0, "content": "Component sizing for renewable generation and battery storage is a key economic driver for small off-grid electrical systems (minigrids). Stochastic, time-series simulation is the go-to tool for sizing studies. For computational efficiency, these simulations typically avoid electrical system models and utilize power-balance methods that capture net energy flows without modeling current and voltage - parameters often utilized for common supervisory control actions. As a result, power-balance models often overestimate performance or fail to simulate critical supervisory control states necessary for control system development. In this work we demonstrate a modeling method that retains computational efficiency while calculating required currents and voltages. Since microgrid components frequently utilize battery voltage as a key control input, the method emphasizes an efficient representation of battery terminal voltage as a function of state-of-charge and battery current. We demonstrate that critical battery parameters can be extracted from simple pulsed discharge tests which could be conducted in field conditions. Application to typical microgrid illustrates that the method will identify control instabilities which would be missed by power-balance methods, while executing at acceptable speeds."}, {"label": 0, "content": "The reliable operation of the relay protection equipment and its secondary circuit is an important factor to ensure the safety and stability of power grid. It is of great significance to improve the operation and maintenance efficiency of the relay protection equipment and its secondary circuit. In this paper, the design of relay protection intelligent mobile operation and maintenance management system based on power wireless virtual private network is elaborated, including the architecture of system and the deployment mode of communication network. In addition, information synchronization methods between the system and relay protection statistical analysis and operation management module, condition-based maintenance assistant decision-making module and some other system data modules are illustrated in detail. Through collection, collation, interaction and sharing of intelligent information in substation, the system realizes the purpose that equipment information are mobilely, paperlessly and standardizedly managed and controlled, which can improve the convenience and efficiency of on-site operation and maintenance work and its management level, and as a result is quite significant to ensure the safety and stable operation of power grid. The system has been successfully applied in substation and achieved good results."}, {"label": 0, "content": "With the operation of substantial HVDC lines, the risk of cascading failures is increased greatly. The propagation mechanism of cascading failures in AC/DC hybrid systems is analyzed, and a complete cascading failures chains search model for AC/DC systems in large-scale power grids is established. A method for rapid evaluation of AC faults to DC is proposed, which is divided into two steps. At first, the short circuit ratio of the system after the occurrence of the failure chain is calculated, and the extreme failure chain is identified. Time domain simulation is performed to whether it will trigger DC block. For the non-extreme accident chain, the magnitude of the fault is quantified by the area where the voltage after the fault exceeds the normal voltage value. The Leven berg-Marquardt neural network (LMNN) is used to quickly calculate the voltage dips degree, and the time domain simulation check is performed on the severe fault. The line failure probability index and the line failure risk index are defined. The pruning search is carried out according to the line outage risk after the occurrence of the accident chain at each level, which reduces the search space ensuring the search accuracy and greatly improves the search efficiency. Finally, the effectiveness of the method proposed to search high-risk cascading failures chains in AC-DC Hybrid Power System is verified by the simulations concerning the Shandong power grid of China."}, {"label": 0, "content": "The incremental distribution network operating income will be the focus of attention of the company that has the power of incremental distribution network operation under the electricity reform. Based on this, this passage establishes the dynamic economic dispatch model of incremental distribution network considering P2G. It also proposes a catastrophe genetic algorithm based on double iterative optimization genetic algorithm to solve the time coupling problem under the constraints of control means, such as energy storage and demand-side response. Taking the improvement IEEE33 node model as an example, the influence of various regulatory methods on the operating income of incremental distribution network is analyzed and discussed. It is verified that the consideration of 2PG and demand-side response is essential to improve the operating income of incremental distribution network."}, {"label": 0, "content": "Text analytics has been widely used in many different domains to discover valuable knowledge hidden inside a specific text. In terms of power dispatching, a manual always contains a large amount of unstructured data, which makes it a tough job for dispatchers to remember and understand that information. This paper addresses the above problems by adopting text analytics. Based on the idea of Natural Language Processing, a series of key technologies are adopted to do the text analyzing job such as data structure transformation, efficient word segmentation tools for Chinese and Word2Vec calculation, which are helpful for dispatchers to deal with the dispatching manual."}, {"label": 0, "content": "With the development of smart substation, information sharing and its redundant transmission of analog quantities in secondary system provide new research ideas for state estimation of power system. This paper proposes secondary system state estimation method based on information redundant of secondary system and gives its structure and content and builds a linear static model of state estimation used redundant relationship of analog quantities. Furthermore, a numerical example is designed to simulate and verify the proposed method. The experiment results show that the method can effectively reduce absolute error of analog quantities and improve uploading analog quantities reliability. The research method in secondary system state estimation has a broad range of applications and research space."}, {"label": 0, "content": "In order to realize the transient and accident analysis of reactor system and verification of system control scheme, the latest computer technology is used to develop advanced graphical modeling tool for TRANTH code, the human-machine interaction interface, and integration research between software and digital reactor design and verification platform, so as to the design and simulation system of digital reactor fluid instruction control system is built up. The nuclear power plant (ACP1000) is chosen as a research object some typical operating conditions are selected and executed. Results show that the design and simulation system can implement the modeling, calculation, interactive manipulation and display of reactor fluid instruction control system, which brings a multi-professional joint simulation of reactor fluid instruction control system to fruition, and provides technology support for rapid generation, evaluation, verification and optimization of design scheme about reactor fluid instruction control system."}, {"label": 0, "content": "This paper analyzes the external meteorological factors that influencing galloping, a BP neural network learning algorithm is established by taking wind, inducted angle of wind direction and line, relative humidity, and ambient temperature as input vectors. The galloping probability is predicted by judging whether the prone-galloping weather conditions are satisfied utilizing the proposed method, and its prediction performance is assessed through several test indexes with the purpose of improvement. A case study is presented by adopting historical galloping data of Henan power grid, and the result shows that the proposed method is effective and practical, which can provide support for power system operation staffs to make reasonable decisions as well as ensure the power grid securely tiding over the peak-load during winter."}, {"label": 0, "content": "Based on the background of DC normal education project in the Jinji Lake core area of Suzhou Industrial Park, this paper discusses the key technologies in the practice of multi-terminal DC power distribution technology engineering. Firstly, the basic situation of Suzhou four-terminal DC project was outlined, and the key electrical equipment and its selection principle were analyzed, including the converter valves, bridge arm reactors, link transformers, and DC monitoring and protection systems. Secondly, based on the RTLAB real-time simulation platform, the simulation model of Suzhou four-terminal DC distribution system was built. Finally, based on the RTLAB simulation model, single-ended converter startup, four-terminal stable operation, active power step and reactive power step were simulated. The simulation results show that the four-terminal DC grid model has good steady state performance and active power and reactive power step response performance. It can provide reference for the future DC distribution network construction."}, {"label": 0, "content": "Consensus algorithms are widely applied in distributed control of microgrids. With distributed control, the performance of a microgrid is significantly relevant to the time delays of the communication network. This paper mainly focuses on the time delays in the consensus-based control in microgrids. The influence of time delays is analyzed and the stability margin of the time delays is given. It is proved that the stability of the microgrid depends only on the local processing time delays of DGs, while the communication time delays between different DGs would only influence the convergence speed. Moreover, an AC microgrid with four DGs and some loads are implemented and the stability margin of time delays is verified by MATLAB/Simulink simulation results."}, {"label": 0, "content": "Based on the analysis of the fault characteristics of the inverter interfaced distributed generation under PQ control and the influence of the distributed generation access on the traditional protection technology, an active distribution network protection scheme considering the capability of regional measurement is proposed. The protection scheme based on the direction of zone current is proposed for the region with enough capability of measurement. The distribution network is divided into zones, and the fault zone is judged by collecting the fault component of current at breakers. As for the region with poor capability of measurement, an reclosing scheme coordinated with DG re-grid timing is proposed. The proposed scheme effectively solves the influence of distributed generation access on traditional protection and reclosing of distribution network. The simulation results of the examples in PSCAD/EMTDC prove the effectiveness of the proposed scheme."}, {"label": 0, "content": "Radio frequency(RF) communication technology has been applied widely in data acquisition system especially in low-voltage electric area. As a property of RF communication, co-channel interference could not be ignored in improving communication performance. This paper proposes an RF channel selection method based on adaptive sensing to increase acquisition efficiency. The key steps as channel evaluation, adaptive sensing, channel negotiation and power control are given. Both simulation and experiments in advanced metering infrastructure(AMI) are conducted to validate the proposed method."}, {"label": 0, "content": "This paper proposes a new generalized power load modeling method, which contains multiple RBF neural network model structures. Firstly, based on the RBF neural network, a sub-model is built to describe typical load characteristics. It breaks through the limitations of load components, and adapts to the diverse development of modern power system. Then, based on Bayesian estimation theory, multiple typical load models are merged into one integrated load model. It solves the problem of insufficient generalization ability of traditional load model, and adapts to the time variation of power load. Finally, the proposed method was applied in IEEE 14-bus test system. The results obtained proved its validity."}, {"label": 0, "content": "State of charge (SOC) estimation is a core technology for battery management system (BMS), which plays an important role to make electric vehicles (EVs) operate safely, reliably and economically. In this paper, a new approach based on the Spherical Simplex-Radial Cubature Kalman Filter (SSRCKF) algorithm is presented to improve the accuracy of SOC estimation. The superiority of the proposed approach has been proved through the Worldwide harmonized Light Vehicles Test Procedure, which came into effect last year in the European Union. In addition, noise are added to the measured data of current and voltage to verify the its anti-interference ability. By comparing with the Unscented Kalman Filter (UKF) and the Cubature Kalman Filter (CKF), the experimental results show that the SSRCKF algorithm estimated the SOC more accurately than the UKF and CKF."}, {"label": 0, "content": "At the substation job site, the use of intelligent video surveillance technology can greatly reduce the supervision burden on safety inspectors for irregular operations of operators. However, in the outdoor complex working environment, the identification accuracy of recognition algorithm based on the traditional radial basis function neural Network (RBFNN) is not high enough and the missing alarm rate is high. In order to solve this problem, this paper proposes the RBFNN robust algorithm for dress recognition based on classifier output sensitivity. The algorithm firstly extracts the shape and color feature vector of the helmet, the top and the bottom of the operator image. Then, the Monte Carlo method is used to randomly sample the points in the neighborhood of training samples to expand the number of samples and reduce the volatility of the classifier output. And then, the loss function that considers the sensitivity of the sample neighborhood is established. Finally, the weights from the hidden layer to the output layer are solved by Gauss-Newton method. The RBFNN classifier based on Gaussian function is established. The simulation results show that the recognition algorithm based on sensitivity RBFNN (S-RBFNN) can effectively reduce the missing alarm rate, which is more robust in practical applications."}, {"label": 0, "content": "In terms of the accumulation of historical data of the power system, China's power grids have accumulated a great number of dispatching and operating data which contains different voltage levels. Online safety and stability analysis conduct a comprehensive system assessment every 15 minutes, including power flow, short circuit, static safety analysis, voltage stability, transient stability, small disturbance stability, limit transmission power, and scheduling assistance decision-making, and the calculation data and result data are about 1G. Based on the six-month online historical data accumulated by a provincial power grid, this paper studies the characteristics of the actual power grid security and stability, and proposes a system stability analysis method based on the online security analysis of historical resources. At the same time, Considering the time-varying characteristics of the power system, an operation rule extracting method was proposed. Firstly, based on the correlation analysis methods, the corresponding features are selected for each type on the hierarchical grid. Then, a power grid security risk assessment system is established based on load ratio, and line limitation. Finally, this article automatically discovers the characteristics and rules of grid dispatching operations from numerous online data, provides a theoretical basis for operational mode and scheduling decisions, further improves the ability of large-scale grid dispatching and online safety analysis, and enhances the ability of power grids to resist the influence of external factors on safe and stable operations ability and has important practical application value."}, {"label": 0, "content": "Power System Stabilizers(PSSs) are well-designed devices to measure and enforce improvements in synchronous generators\u2019 system-stability, which offer overwhelmingly superior cost performance compared to other optimal reconstruction or enhancement of power systems. The techniques of PSSs have been focused by power industry and academic circles in many years. The paper presents a performance comparison of several advanced techniques based on Adaptive Fuzzy Control, Artificial Neural Network (ANN), Genetic Algorithm(GA) and Hybrid Artificial Intelligent(HAI), Fuzzy Logic and Particle Swarm Optimization(FLPSO) techniques. With their merits on dealing with PSSs\u2019 implemental structures, models with unknown or variable parameters, we study the main indices to compare the performance of the referred intelligent techniques including simplicity of prototype, robustness and response speed, complexity of algorithm, flexibility in implementation and applicability to hybrid AVRs so on. The comparison results show that intelligent techniques improve PSSs comprehensive performance of being more effective and vigorous in damping out low frequency oscillations by overcoming inherent limitations in conventional control methodologies. Intelligent techniques could be especially considered in application of smart grid with large-scale grid-connected renewable energy power and random high power loads."}, {"label": 0, "content": "In two-settlement electricity markets, virtual bidding may significantly influence Locational Marginal Price (LMP) in day-ahead market. An excessive amount of virtual load may cause congestion on certain transmission lines and LMP difference between certain nodes, which may be manipulated by market participants to arbitrage in other derivative markets, such as the Financial Transmission Rights (FTRs) market. For Independent System Operators (ISOs) and regulators, it is vital to calculate LMP difference caused by virtual bidding in advance to monitor market behaviors and to improve market efficiency. However, the conventional enumeration-based method that calculates LMP differences is time-consuming and inaccurate. In this paper, a fast algorithm is proposed to calculate LMP differences using the parametric programming method. Instead of calculating repetitively, the proposed algorithm finds the turning points where LMP differences change. The case study based on a modified IEEE-9 bus system shows that the proposed algorithm outperforms both in efficiency and accuracy."}, {"label": 0, "content": "Dynamic voltage restorers have been examined to compensate voltage sags in distribution networks to avoid production losses, especially in the Premium Power Park(PPP). If the compensation strategy can be optimized to extend the compensation time as long as possible, then the application of DVR in power system can be more popular, and the intelligent level of the power grid can also be effectively improved. As a result, this paper proposed a time-maximized compensation strategy to achieve a maximum compensation time based on the energy-minimized compensation strategy. According to the formula of dc-link discharge, the calculating formula of optimum phase angle jump of load voltage is deduced, and the principle and characteristic of the compensation strategy are analyzed respectively for single-phase and three-phase system. Simulation and experimental results are presented to confirm the effectiveness of the proposed compensation strategy."}, {"label": 0, "content": "Electric vehicles (EVs) as distributed storage devices have the potential to provide frequency regulation services due to the fast adjustment of charging/discharging power. Along with the policy incentives, it is practical for EVs to take part in the regulation market through the aggregator. An optimal control strategy based on reinforcement learning (RL) for electric vehicles (EVs) in distributed networks is proposed in this paper. The overall goal is to follow the regulation signals sent by the system operator in the real time regulation market by controlling the EVs in the parking lot. To achieve this, the reinforcement learning algorithm is employed to optimize the charge and discharge strategy of the EVs, so that the aggregator optimally allocates the regulation power and the baseline charging power to EVs to respond to the regulation signals for the best regulation performance. Comprehensive simulation studies have been carried out based on the data of PJM electricity market and the results show that the regulation performance based on the control strategy is excellent in both cases of traditional and dynamic regulation signals."}, {"label": 0, "content": "With the wide application of power electronic technologies in power system, such as HVDC, new energy power stations and FACTS, the dynamic characteristics of the interconnected power grid have changed dramatically, and the traditional quasi steady model based electromechanical transient program cannot simulate the fast switching process of the power electronic components and its non-fundamental frequency or asymmetry characteristic, which is important to the accuracy of HVDC and FACTS models. Because of the limitation of calculation load and computing speed, traditional electromagnetic transient program cannot replace electromechanical transient program in large-scale power system stability analysis. FEMTP should be redesigned for transient stability analysis of large-scale electronic power system. Then the generator and transmission line's electromagnetic models are discussed in this paper, and the discussion and simulation shows the adequation of their electromechanical transient model in FEMTP. Afterwards, the implementation of FEMTP was discussed in detail, and the example of China East power grid shows the appropriation of simplified electromechanical models and proposed technologies."}, {"label": 0, "content": "The huge AC power girds of China have been all connected by dozens of HVDC links now. To understand the behavior of the complicated network and mechanism of interaction between AC and DC, there is a demand for a more precise simulation technology. Traditional power system analysis tools, such as electromechanical transient program and off-line electromagnetic transient program, the models of DC control and protections are simplified to some degree. Sometimes the simplification may not reflect the same act that grid may do as in the field. In order to get a detailed, panoramic view, it is believed that simulating the large-scale network model by electromagnetic transient software in real-time with actual DC control and protection devices linked will give a better performance. In this way, the simulation time step should be small enough so that the physical DC controllers can be connected to realize hybrid simulation. Normally, microsecond level is applied. However, running a tremendous AC/DC grid model of thousands of buses with a time step of microseconds is the precondition of digital-analog hybrid simulation, and also a great challenge to all experts in the field of power system real-time simulation of the world. Based on task mapping and parallel computing technology, power system real-time simulator HYPERSIM and supercomputer SGI UV300 are used in this paper to achieve the goal. Considering of the processor's computational capability, on account of naturally decoupling by long distributed parameter lines, this paper optimizes the method of large-scale grid decoupling, splits the sophisticated region grid model into distributed tasks reasonably. Exploring the automatic task mapping function in HYPERSIM, a synthetic method about multiple auto-assigned parameters optimization is proposed. Eventually, an electromagnetic transient network model of 17746 single phase buses can be operated in real-time, which is a breakthrough on scale expanding of real-time simulation. The precision of AC/DC grid simulation makes a step further. The realization of massive grid model real-time operation gives a new technical means for characteristic cognition, system planning, operating, decision-making and failure reproduction of Chinese future power system."}, {"label": 0, "content": "The safe operation of transmission overhead lines is often threatened by the fast growing and high growth trees under their line corridor. When the safe distance between the line and the tree is insufficient due to the limited height of transmission network, it is easy to occur tree related fault and tripping. Therefore, in order to effectively prevent the damage to overhead lines caused by the growth of extra high trees, it is necessary to know the growth rule of extra high trees and predict their growth height. In this paper, the deep learning algorithm is used to study the growth rule of extra high trees under overhead transmission lines. Different deep learning and artificial neural network algorithms such as Deep Belief Network, Auto-Encoder and Long-Short-Term-Memory Algorithm are used to predict the tree height, and the validity of these algorithms is verified. Furthermore, these algorithms are combined to verify that the combined algorithm has higher prediction accuracy than the single multilayer perceptron and mathematical statistical model."}, {"label": 0, "content": "As a widely used power system analytical toolset in the world, Power System Simulator for Engineering (PSS/E) is also characterizing with its powerful user-defined function. This paper illustrates a study of user-defined modeling function in PSS/E through modeling a VSC-HVDC transmission system. The user-defined modeling (UDM) function of PSS/E is introduced in detail, then a modeling method for VSC-HVDC is proposed. Through comparing the simulation curves of user-defined VSC-HVDC in PSS/E with that simulated by PSCAD/EMTDC, the correctness of the VSC-HVDC user-defined model, the feasibility as well as the practicability of PSS/E's user-defined function are validated. The modeling method proposed in this paper can provide guidance and a basis for the modeling of other complex components in PSS/E for simulations."}, {"label": 0, "content": "This paper proposes a method of identifying major power quality disturbance sources based on monitoring data correlation analysis among multiple grid nodes in a regional power grid. First, using correlation calculation between the voltage quality index in each node and the current quality index in each branch, strong-correlated branches which significantly affect voltage distortion in problematic nodes are extracted. Second, the contribution of each strong-correlated current branch to node voltage distortion is quantified through partial correlation analysis. Finally, the major disturbance sources are identified by means of contribution estimation. The proposed method is validated through a case study conducted on a regional power grid which includes multiple types of disturbance sources. The results evidenced that the method can identify correctly the major disturbance sources responsible for node voltage distortion, thus providing effective guidance for disturbance source governance."}, {"label": 0, "content": "On the premise of improving the traditional line detection method based on transient zero sequence current (TZSC) waveform comparison, a fault line detection method based on variational mode decomposition (VMD) and phase space reconstruction was proposed in this paper. First, the first modal component was extracted by VMD of the TZSC of each line running up to the first feature extraction and noise elimination. Then the phase space reconstruction of the extracted modal component was carried out by using coordinate delay method. The C-C algorithm was used to estimate the optimal phase space reconstruction dimension and time delay. The mean dimension and time delay were taken respectively for creating similar phase spaces to achieve the second feature extraction. Finally, the phase space similarity matrix was established by comparing the similarity degree of every two zero sequence current phase spaces. Calculate the comprehensive correlation coefficient, and the line with the minimum comprehensive correlation coefficient was selected as the fault line. The simulation results verified the applicability of the line selection method, high sensitivity and strong anti noise."}, {"label": 0, "content": "High penetration of volatile renewable energy produces uncertainties in power system and poses severe challenges to transmission network planning (TNP). Usually, only one type of mathematical model of different uncertainties was considered in every single TNP method. However, in the process of TNP, different uncertainties may show different mathematical features and need to be represented by various types of mathematical models. Aiming at this problem, a TNP model taking into account interval uncertainty model of renewable energy generation and fuzzy uncertainty model of predicted load is proposed based on the expanded fuzzy chance constrained programming. In accordance with the features of the constructed planning model, the model is transferred to a robust TNP model considering interval uncertainty model of renewable energy generation and predicted load. Thus, the calculation burden for solving the planning model decreases. The analyses on the modified IEEE RTS 24-bus system and a 231-bus system verify the effectiveness and adaptability."}, {"label": 0, "content": "With large-scale distributed generators (DGs) penetrated into the active distribution network (ADN), conventional load flow convergence failure is incurred by heavy power transmission. The Holomorphic Embedding Load Flow Method (HELM) has proven to be more robust than the Newton-Raphson method under heavy power transmission and is not sensitive to the initial points. At present, HELM is mainly designed for balanced transmission networks. In this study, we developed a three-phase HELM model to accommodate DGs, delta connection loads, and ZIP loads for ADN. The effectiveness and better performance of the proposed method under heavy load situations were validated using modified unbalanced IEEE 13, 34, 37, and 123 test feeders."}, {"label": 0, "content": "With the most HVDC projects in operation and largest renewable energy generation capacity, State Grid has become one of the most complex power grids worldwide, which makes it difficult to simulate, analyze and control. To improve the characteristic cognition level of State Grid, more detailed simulation method such as hybrid simulation of TS and EMT is widely applied in routine operation, and meanwhile more scenarios and contingencies are considered. However, these enormously increase the computing loads, and the conventional simulation tools are too slow to support the actual daily operation. In this paper, a novel digital parallel simulation system of State Grid is proposed, which adopts supercomputing technology to deal with massive simulation cases in parallel, and then provides simulation services to remote users. The system architecture and subsystems are introduced in detail. Firstly, two levels of parallel computing are carried out to speed up the simulation, case parallelism and sub-grid parallelism. Secondly, relying on cloud simulation technology, remote access is realized to the national dispatching center and dozens of regional/provincial dispatching centers. Moreover, several key technologies of the system are explained, including TS-EMT hybrid simulation, multi-layer sub-grid parallel simulation, automatic initialization of EMT simulation, etc., which promote the system performance greatly. Since the system has been applied in State Grid to support routine operation simulation and analysis, several actual applications are introduced at the last, which demonstrates the great enhancement of power grid simulation efficiency."}, {"label": 0, "content": "Insulators are important components of power transmission and transformation equipment in power systems. Insulator identification is a basis work for evaluation of insulation status of insulators in computer vision. With the recent development of big data and cloud computing technologies, terminal-to-terminal picture recognition was accomplished based on deep learning algorithms, making it possible to apply insulator image recognition in power systems. This paper firstly introduced the research background of deep learning: the recent YOLO (You Only Look Once) convolutional neural network algorithm, established insulator image databases for train and test, and preprocessed the images of training insulator images with the TensorFlow platform. With the YOLO algorithm applied, the training of the image database for 5 days was completed, and a good recognition result was achieved. Then the results were compared between the Fast R-CNN algorithm and the YOLO algorithm in identify speed and accuracy. Based on relevant paper, the accuracy of insulator image recognition is defined, and the factors that affect the accuracy of insulator image recognition are discussed: It is concluded that the accuracy of identification increases with the increase in the number of training insulators, which is the next step in the identification of insulators."}, {"label": 0, "content": "An electromechanical-electromagnetic hybrid simulation algorithm based on boundary nodes grouping and decoupling is proposed in this paper. The UHV $AC/DC$ backbone network is described as electromagnetic transient models and the others are described as electromechanical transient models and power grids at low voltage side are decoupled at their boundary. A new partitioning method of backbone network based on heuristic rules is proposed to support electromagnetic transient parallel computing and a hybrid parallel simulation algorithm based on subnet groups is designed. The theoretical analysis and derivation of the algorithm shows that the algorithm can improve the efficiency of hybrid simulation with a large number of boundary nodes."}, {"label": 0, "content": "The fundamental theory of energy networks in different energy forms is established following an in-depth analysis of the nature of energy for comprehensive energy utilization. Based on the physical theories of analytical mechanics, thermodynamics, heat transfer, fluid mechanics (fluid network), electromagnetic field theory (electric network), the generalized equations of energy transfer and transformation are established. Then the generalized lumped element models are derived, which include the generalized resistance, capacitor and inductance. To establish the equations of energy networks, the Kirchhoff's Law in electric networks is extended to energy networks, which is called the Generalized Kirchhoff's Law. The equations are finally unified into a complete energy network equation system. A numerical example is given to testify the effectiveness of energy network theory."}, {"label": 0, "content": "This paper investigates the distributed security constrained economic dispatch (SCED) problem in an active distribution network, pursuing the minimization of generation cost of the entire system considering network security and generation limit constraints. A feedback strategy derived from the duality-based method is proposed to achieve the distributed solution of SCED. In the proposed strategy, the distribution management system (DMS) and microgenerators (MGs) are regarded as intelligent agents in charge of fast calculation and measurement. The variation of system demand results in fluctuations on branch flows, which can be measured by DMS and then transmitted to MGs. Each MG can adjust itself active power output independently so as to respond to system demand variation, and meanwhile ensure network security. The proposed method only requires partial dual variable information transfer from the DMS agent to MG agents, and has plug-and-play features. Simulations of the IEEE 33-bus system are provided to illustrate the performance and robustness of the proposed algorithm."}, {"label": 0, "content": "Poor efficiency and long running time of existent optimization algorithms in dealing with multi-objective multi-variable community microgrid optimization have always been a concern. To address this issue, a novel layered optimization algorithm based on NSGA-II is proposed. The proposed algorithm integrates the structural feature of community microgrid and the concept of multi-agent system into the optimization process and decomposes the complex microgrid optimization into several household optimizations of smaller scale and one central microgrid optimization. The household operation is optimized first and the central microgrid optimization is solved subsequently based on the Pareto solution set of household operation problems to obtain the optimal operation mode. Simulation results demonstrate that the proposed strategy is effective in improving optimization efficiency."}, {"label": 0, "content": "The automated system inspection with defect detection in large-scale photovoltaic (PV) farms is an urgent problem to be addressed. This paper presents a transfer learning based solution for visible module defects diagnosis in these conditions. The solution mainly includes three parts: a normative, time-updated, sundry dataset; a pre-trained deep learning model; a transfer learning strategy. Image augmentation technology is applied in the dataset. The proposed transfer learning based solution is able to extract the defects' features from local representation to global one and low level to high one with more robustness in comparison with the enhanced CNN based method. The proposed solution is evaluated through extensive experiments and the numerical results clearly demonstrate its effectiveness."}, {"label": 0, "content": "An adaptive sub-synchronous resonance (SSR) damping controller using Kalman filters are proposed for AC/DC hybrid system with high penetration wind in this paper. First, the supplementary damping coordinated controller of DFIG and HVDC are designed using multiple input multiple output LMIs. Then the adaptive method using Kalman online estimations for tracking the operating points caused by the variable wind power output. The AC/DC hybrid system integration with wind power generation is used as test system. The characteristic of variable SSR modes with wind power outputs is investigated, and the simulation results demonstrated that the adaptive SSR damping control method could not only track the operating condition, but also could give robust and effective control in the case of large fluctuation of wind power output."}, {"label": 0, "content": "In recent years, cyber-attacks have become an imminent danger threatening the stable power supply. Data-driven approaches based on machine learning techniques are frequently used to detect cyber-attacks. To get satisfactory detection performances, these approaches have to establish an in-depth understanding of power system operating behaviors, not only under normal conditions but also under contingencies like short-circuit faults. Contingency data appears far more rarely than normal data in the historical measurement database. If normal and contingency data are uniformly sampled to generate training datasets, the included contingency examples may be too few for machine learning models to recognize the operating patterns thoroughly. In this paper, a data preparation method combining Random Matrix Theory (RMT) and Adaptive Synthetic Sampling (ADASYN) algorithm is proposed. RMT is applied to extract rare contingency data from the historical operating database and ADASYN is used to synthetically generate new contingency examples according to the existing ones. Case studies show that this method can facilitate the learning of the intrinsic statistical characteristics of power system operating data, and then enhance the detection of cyber-attacks."}, {"label": 0, "content": "This paper develops a data fusion technology based modeling framework for classifying the underlying cause of power quality (PQ) disturbances. First, the moving-window technique is used to cluster disturbance period with the consideration of the temporal propagation of disturbance energy. Secondly, the PQ disturbance measurements, equipment switching action data and alarm events are integrated by utilizing entity matching method. Then, the distributed mining of association rules is designed to obtain strong association rules within integrated data for describing the relationship between PQ features and event causes. The analysis results have good generalization performance. Finally, the real grid data were taken as an example to verify the effectiveness and practicability of the proposed method. The test results show that the proposed method can analyze the relationship between the typical PQ disturbance features and event causes effectively. This relationship is meaningful for power quality improvement."}, {"label": 0, "content": "The infrared images recorded by the inspection robot can accurately locate the heating defects of the equipment and have an important role in the fault diagnosis of the equipment. The infrared thermal image is completed by the robot in the shooting stage, while the processing and analysis of the later period still need artificial progress. In this paper, the infrared thermal image of the transformer bushing is processed by the method of image recognition and pattern recognition, which is concerned with the over-reliance on the artificial characteristics of the infrared thermal image processing and analysis process. Firstly, The Normalized Cross Correlation (NCC) template matching method and Otsu threshold segmentation method are used to get the region of the interests (ROI) of the bushing. Then the maximum temperature rise, temperature mean value, temperature variance, temperature gradient and other characteristics of the ROI are extracted. Finally, support vector machine is used to identify the status of bushing. The results show that the proposed model can reduce the manual interference of infrared thermal image and has high accuracy, which is suitable for engineering application."}, {"label": 0, "content": "With the rapid development of ultra-high voltage ac and dc projects (UHVAC/DC), a large-scale UHVAC/DC hybrid network has formed in China, increasing the control complexity for power grid operation. To maintain such a complex power grid operate in the stable and safe states, the operation characteristics analysis of it is highly necessary and significant. However, until now, all the existing power grid simulation tools cannot complete this challenging mission suitably because of the requirements for high accuracy modeling, large-scale electro-magnetic transient simulation, and massive off-line calculation, etc. In order to solve the above-mentioned problems, the new generation UHVAC/DC power grid simulation platform (NGSP) architecture is proposed in this paper, and the detailed design schemes of NGSP is also presented. With the help of the proposed designs, the computing scale of NGSP is larger than 6000 nodes in digital-analog hybrid simulation, and the speed-up ratio is more than 3000 in digital simulation, which represents the most powerful power grid digital simulation capacity all over the world. And the comprehensively comparative results between simulation and engineering recorded data are presented to verify the correctness and effectiveness of NGSP. Moreover, the application effects of NGSP are also described in this paper to verify the practicability of NGSP."}, {"label": 0, "content": "Modular superconducting magnetic energy storage (SMES) is designed to achieve a modular layout of SMES through advanced control technology to solve the problem of SMES capacity limitation. In this paper, self-adaptive state of charge (SOC) feedback control is used to distribute the power of the modular SMES, the economy of the device and power efficiency and other factors are restrained as constraints. The multi-objective evolutionary algorithm is chosen as the solution algorithm to optimize the capacity of the modular SMES."}, {"label": 0, "content": "Partial Discharge (PD) pattern recognition is one of the most important steps of PD based condition monitoring and diagnosis of High Voltage (HV) cables. Although different types of pattern recognition methods, e.g. Support Vector Machine (SVM), Back-propagation Neural Network (BPNN) and Deep Learning, have been developed and applied to PD pattern recognition, limited training samples is one of the most important factors which restricts the PD pattern recognition accuracy. To overcome the challenge two PD data augmentation methods, based on the Variable Noise Superposition (VNS) and Generative Adversarial Network (GAN), are presented in the paper, which are evaluated with 1500 sets of experimental PD data and three pattern recognition methods, Support Vector Machine (SVM), Logical Regression (LR) and Random Forest (RF). The results show that PD pattern recognition accuracy, based on the VNS data augmentation method, is improved by 0.99%, evaluated with SVM, and is improved by 0.96%, evaluated with RF; PD pattern recognition accuracy, based on the GAN data augmentation method, is improved by 0.52%, evaluated with SVM, and is improved by 1.72%, evaluated with LR. Both two methods, VNS and GAN, are effective for PD data augmentation, which are applicable for PD pattern recognition of HV cables and other HV apparatuses, especially for pattern recognition methods which requires large volume of training samples."}, {"label": 0, "content": "An effective Physical Protection System (PPS) requires methodologies and tools to automatically evaluate the PPS effectiveness, provide the suggestions or strategies for the further redesign or optimizations, and assist NPPs to conduct training for their staffs. A virtual reality platform for evaluation, design and training of PPS (IPAD) was proposed in one of the authors' previous works. This paper supplements a training module using IPAD platform for the virtual training of PPS. The virtual training will help detection staffs and response forces grasp the operational procedure of PPS when intrusion or emergency events occurred."}, {"label": 0, "content": "Electromagnetic transient (EMT) simulation programs are often used to obtain the optimal values of circuit and control parameters. In this paper, an EMT-simulation-aided optimization technique using the Kriging method is proposed. Then, it is applied to the design of the automatic reactive power regulator (AQR) controller of a static synchronous compensator (STATCOM). It is shown that the optimal solution of the AQR controller can be obtained efficiently by the proposed technique."}, {"label": 0, "content": "In this paper, a graph computation based power flow algorithm is introduced to solve large-scale AC/DC hybrid systems with multi LCC (Line Commuter Converter) based DC grids. The proposed approach is to improve the computational efficiency of constructing related matrices and getting power flow results without changing the conventional sequential iteration method. The hybrid system is modeled as a graph of vertices and edges with info of topology and parameters, thus local computation could be done independently for like formulating mismatch vectors of \u0394P, \u0394Q, \u0394\u03b8, \u0394V and matrices of B', B\", only with parameters on its linked edges and adjacent vertices. Then by taking advantages of hierarchical parallel computing, the FDPF (fast decoupled power flow) of AC system, calculation of DC grids and mismatch comparing for each iteration of the sequential method could be done parallelly. This method is implemented on a graph database platform, and tested on IEEE 300-Bus, modified South Carolina 500-Bus system and a Chinese system to verify the accuracy and time-saving performance."}, {"label": 0, "content": "With the addition of new distributed energy sources, a new and more accurate load model need to be established. Based on MATLAB/Simulink, this paper researches on the modeling and simulating of photovoltaic power generation system and battery energy storage and their equivalent model. First, a detailed model of the photovoltaic system is built. Boost circuit and MPPT control are selected on the DC side of photovoltaic power generation system. The double closed loop control method of current internal loop and external voltage loop is used for grid inverter control. The model is improved considering the capability of LVRT (low voltage ride through). Based on the above precise models, the model is equaled. The equivalent transfer function is calculated by the grid connected control loop, so the model is simplified. The battery energy storage module establishes a general mathematical model based on the charging and discharging principle. Charge and discharge are controlled through the DC/DC converter. And it is connected to the power grid after inverting and filtering. The common PQ control are selected in the inverter control mode. According to the same principle, the energy storage model is equaled. Finally, based on the traditional load model, the above load model is added to set up a new generalized integrated load model. And the actual model is replaced to build up an equivalent model. The equivalent model is compared with the actual model to verify the equivalent description ability."}, {"label": 0, "content": "In order to predict insulator contamination degree on the transmission lines, this paper proposed an insulator contamination prediction model based on back-propagation (BP) neural network optimized by genetic algorithm. In view of the inadequacies of the slowness of convergence rate and the weakness of global optimization ability, the genetic algorithm was used to improve the performance of prediction result. This paper not only consider meteorological factor which includes temperature, wind, precipitation and relative humidity as input layer but also consider air quality index (AQI) which contains PM2.5, PM10 as input layer. Moreover, the equivalent salt deposit density (ESDD) and non-soluble deposit density (NSDD) were set as output variables. The results show that the optimized prediction model has better ability in searching optimization. Furthermore, the prediction result of proposed model is more accurate than that of the BP neural network."}, {"label": 0, "content": "In order to determine the accurate allowed penetration level of small hydropower (SHP), the random characteristic of the loads and SHP output are considered in this paper, and the corresponding stochastic models are established. While weather condition for different regions differences, the concept of regional synchronization feature of SHP generation is proposed, and non-parametric kernel density estimation method is used for hydropower output modeling with the consideration of randomness. The expressions for calculating capacity of SHP are presented based with the node voltage constraint. The differences between the two stochastic models are taken into consideration, and the correction coefficient is introduced in the expression. Finally, the effectiveness of the method proposed in this paper is validated by a case study based on the load data of a real distribution network in Southern China Power Grid."}, {"label": 0, "content": "With more and more disturbance sources such as high-speed railway and renewable energy generation, the power quality problem has become increasingly complex, which seriously affects the reliable operation of the power grid. Identifying the types of disturbance sources that cause power quality events based on power quality monitoring data will support for targeted control disturbance sources, also provide evidences for determining contribution between customers and operators. This paper proposes a method for identification of disturbance sources based on random forests. Firstly, it chooses analysis indices and extracts both temporal and statistical characteristics of selected indicators from historical data. After balancing datasets, the data are used as eigenvectors of training random forests. Secondly, combining with OOB(out-of-bag), one of evaluation indicators, adjustment of random forest parameters in a closed-loop is used to construct cost-optimized random forest classifier. Thirdly, the type of disturbances is on-line identified using the classifer. Based on data from a power quality monitoring system in a regional grid of China, it is verified that the method has a high accuracy for identifying disturbance sources such as electric railways, converter stations, wind power, photovoltaics, and smelters."}, {"label": 0, "content": "A custom designed distribution network communication systems can maintain the fast and reliable information transmission and lead to improved operation and management of a distribution automation system. In this paper, the distribution automation transformation scheme of a regional power grid is analyzed. Then a simulation model of three types units, including the intelligent distributed protection unit, the three-remote unit and the master station unit, is constructed based on the Optimized network engineering tool (OPNET) Modeler platform, which is the main novelty of this paper. Finally, an intelligent distributed protection system is constructed and simulated using the models. The simulation results show that the models can act an effective tool to analyze the network performance, which can provide a quantitative basis for the design of the distribution network communication systems."}, {"label": 0, "content": "Recently, the user-side data of power grids gradually exhibit massive data and high complexity features. The traditional power anomaly detection model has been difficult to meet the existing requirements. In recent years, the neural networks and machine learning methods that are widely used in anomaly detection, but all of these methods have a high demand for training samples and cannot be well applied when missing sample tags on the power data set. This paper designs an unsupervised power data anomaly detection model mainly based on the isolated forest algorithm. The model includes modules for feature extraction, feature reduction, and isolated forest computing. The research results show that using this model to detect abnormal power usage data can process large amounts of data in a short time, but also can accommodate the lack of training samples and can better meet the practical needs of the power sector."}, {"label": 0, "content": "On the basis of the principle of multi-frequency ultrasound, genetic algorithm GA and back propagation neural network BPNN, this paper proposed a prediction study of density of transformer oil. Taking 110 sets of transformer oil belonged to China southern power grid as an example, a prediction model of density of transformer oil was established based on BPNN, with the 242 dimensional multi-frequency ultrasonic data of oil sample as the input and density as the output. By adjusting the number of hidden layer neurons, the network was trained. Moreover, the genetic algorithm GA was introduced to optimize the network parameters. All results show that compared with the traditional standard BPNN model, the output value of density of transformer oil with the GA-BPNN model is much close to the real value with small errors, which lays a solid foundation to test transformer oil other parameters with tell multi-frequency ultrasonic technology."}, {"label": 0, "content": "In recent years, micro-phasor measurement unit $(\\mu \\mathrm {P}\\mathrm {M}\\mathrm {U})$ and advanced metering infrastructure (AMI) have been gradually applied to distribution network, providing a large amount of distribution network measurement data, which makes it possible for parameter estimation of distribution network containing T-connection lines. This paper proposes a parameter estimation method for T-connection line in distribution network based on $\\mu \\mathrm {P}\\mathrm {M}\\mathrm {U}$ and AMI. Firstly, the T-connection line model considering the actual installation location of the $\\mu \\mathrm {P}\\mathrm {M}\\mathrm {U}$ and AMI in distribution network is constructed, and the virtual measurement is determined with real-time measurement data of the $\\mu \\mathrm {P}\\mathrm {M}\\mathrm {U}$ and AMI. Secondly, the measurement function equation and Jacobi matrix based on augmented state estimation method are listed. Thirdly, a weighted least square model with voltage phasor and T-connected line parameters as its augmented state variables is established based on multi-interval $\\mu \\mathrm {P}\\mathrm {M}\\mathrm {U}$ and AMI measurement data. Finally, the effectiveness of the proposed algorithm is validated by the four T-connection line zones of the NEV test feeder, and the accuracy of estimation parameters of different lines is analyzed. The method overcomes the difficulty of estimating the parameters of T-connection lines under the condition of insufficient measurement data in traditional distribution network, laying the foundation for the real-time optimization operation of the distribution network."}, {"label": 0, "content": "Multi energy flow coupled network computing provides the constraints of network balance and system energy security for optimal operation of regional integrated energy system, and it is also an important basis for the system security and stability analysis. Considering the dynamic response characteristics of multi energy coupling system with multiple time scales, the network model for combined calculation of electric, gas, thermal / cold multi energy flow is constructed from two aspects of steady state and dynamic in this article. Furthermore, in the process of dynamic model solving, for improving the computational efficiency without affecting the simulation accuracy, the hybrid step time domain simulation method with electromechanical transient simulation for electric power system and medium-long term transient simulation for non-electrical system is proposed, and this method can effectively support the network computation of the multi-energy coupling system. Finally, the model and algorithm are verified by building an example of an electro-thermal coupling network and carrying the simulating."}, {"label": 0, "content": "This paper focuses on the application of the protective clothing detection system by using S-HOG+C operator for substation workers in order to meet the practical needs in a complex environment. An image is firstly divided into three cells including a helmet, upper and lower part of a body according to the characteristics of objects. The HOG and HOC features of the three cells are extracted individually to train a classifier. A linear Support Vector Machine is used in our study. The S-HOG+C operator is used to improve the detection accuracy on the protective clothing of substation workers. Three evaluation indicators are used to analyze the performance of the S-HOG+C operator. The experimental results suggest that the S-HOG+C operator perform better than the HOG+C operator in the protective clothing detection for substation workers. This work is supported by Science and Technology Project of Guangdong Power Grid Co., Ltd. (GDKJXM20162351)."}, {"label": 0, "content": "This paper proposes a method to identify the equipment anomalies based on convolution neural network, aiming at the weak-light situation inside the cable tunnel. This paper has proposed a method to identify the equipment anomalies with weak light situation inside the cable tunnel, based on the convolution neural network. On the basis of the gray image, this method adds the Sobel operator to enhance edge-preprocessing effect and start training through Convolution Neural Network (CNN). The convergence criteria is the Loss Function related with the Weight Parameter W and Bias Parameter b. The convergence method is the one named Backpropagation, which updates the parameters each time to reduce the loss. The fast operating speed of full connection layer can help getting the direct classification result of equipment status in the image. Based on the experimental analysis of the internal images of the Zhuhai tunnel, it can be seen that this method is suitable for the dark and chaotic environment of the tunnel. Additionally, it has a high recognition rate for the image segmentation of the lighting equipment and a high accuracy for the classification of the abnormal situation of the image."}, {"label": 0, "content": "According to the existing problems of substation hard platen state recognition, this paper studies the intelligent recognition system based on machine learning without modifying the secondary device. A manual patrol inspection car is designed to facilitate image acquisition. Using the hard platen state recognition algorithm based on machine learning, a hard platen intelligent patrolling application is developed. The current state of the hard platens can be quickly obtained by simply input the collected hard platen image. In addition, the patrol database is set up, and the hard platen table is kept, and the patrol and maintenance tasks can be managed. The experiment of this system was carried out at a substation in Guizhou. The results show that the system can recognize the state of the hard platens quickly and accurately, and truly realize the intelligence of the hard platen inspection work."}, {"label": 0, "content": "Usually, MMS packets are used to encapsulate the information of protection settings and TCP is the suite of protocols used to transmit the MMS packets in smart substations. Since TCP's own mechanism, such as vulnerability to Head-of-Line (HoL) blocking attacks, the protection settings of TCP transmission often fail to meet the high reliability requirements of important power information. A new method of SCTP protection setting and transmission based on IEC61850 intelligent substation information structure is proposed in this paper. To realize seamless handover of double MMS network during failure time, the SCTP's multi host function is optimized. To avoid HoL blocking attacks, the multi stream mechanism is used. This paper further studies the realization method and implementation process of SCTP transmission with protection setting in detail."}, {"label": 0, "content": "The distributed generator and non-linear loads will lead to deterioration of power quality at the point of common coupling between microgrid and low voltage distribution network. Tracking and compensating for the power quality problem of the converter is a means to ensure the safe and stable operation of microgrid. At present, the research on power quality problems basically adopts the strategy of pre-emptive and post-processing, which can not guarantee the continuous and stable operation. Based on this, an advanced converter with situation awareness and orientation function is proposed. It is mainly carried out in three aspects: awareness, decision-making and execution. The situation awareness of the elements related to power quality is realized through the establishment of the information management system and the optimization of the prediction algorithm. The decision model of grid-connected converter and the optimal control are used to realize the active decision of the problem. The situation orientation of power quality problem is realized by establishing a smooth switching model of adaptive control strategy. This paper provides a new idea for more effective management of power quality problems in microgrid."}, {"label": 0, "content": "A novel model parameter identification based bus-bar protection principle is proposed in this paper. An inductance model can be developed when an internal fault occurs on bus. By taking the inductance and the resistance of the model as the unknown parameters to be identified, the equivalent instantaneous impedance and the dispersion of the parameter can be calculated. Utilizing their difference, the external fault and the internal fault with different current transformer (CT) saturation extent can be distinguished correctly. Based on this, a new criterion with self-adaptive restraint characteristic for bus-bar protection is put forward. As the new principle is suitable for non-periodic component, fundamental component and harmonic component, it can operate more rapidly comparing with the traditional phasor based bus-bar protection principles. Moreover, the proposed principle is inherently immune to the impact of fault current flowing out when a fault occurs in the protection zone of the breaker-and-a-half bus-bar and is insensitive to fault resistance. Simulation results show that the presented principle has high sensitivity and reliability."}, {"label": 0, "content": "Recently, Dispatching methods for DC distribution networks have been the focus of researches. However, there is not an evaluating index, which considers the intense coupling property of power and voltage in DC distribution networks, to judge the effectiveness of these methods. Aiming at DC distribution networks with complicated converters' control modes, this paper proposes an evaluating index utilizing Thevenin theorem. Then, an auto regulation of droop control is presented based on the analysis of droop characteristic. Besides, according to the aforementioned evaluating index, this paper designs an automatic generation control strategy for DC distribution networks with converters' multi-control, whose function is to optimize bus power, bus voltage and network losses with the modified genetic algorithm (GA). The auto regulation of droop control and automatic generation control strategy compose the two-stage dispatching method. Finally, a testing network is employed to demonstrate the practicality and applicability of the evaluating index and the two-stage dispatching method. Results show that the evaluating index can accurately reflect the operating conditions of DC distribution networks with different forms of converter control and the operating conditions can be effectively optimized with two-stage dispatching method."}, {"label": 0, "content": "A mixed integer linear programming model, which can be used in microgrid (MG) intra-day scheduling, is put forward in order to conciliate further the desired attributes of accuracy and computational performance of solving existing model. The proposed model is the minimization of MG economic operation cost considering reactive power capability of distributed generation (DG). Compared with the literature of microgrid economical dispatching, the linearized constraint model, such as power flow with voltage amplitude limitation, DG reactive power characteristic and branch capability based on analytic geometry, exchange power and power factor of point of common couple, and et al, is included in the paper. The performance of the proposed model and accuracy of the results are verified by using an experimental microgrid for engineering practical purposes. Finally, the effects of different segments on the feasible region coverage of DG operating point are discussed."}, {"label": 0, "content": "The switchgear is the key equipment in the power system. The overheating of the switchgear contacts is the main cause of the failure of the switchgear. Since the switchgear is relatively closed so that the internal contacts cannot be directly measured, which need to push out the contact status through a portion that is easy to measure. Therefore, this paper uses the collected temperature of the infrared window to push out the contacts temperature based on the BP neural network to determine whether the switchgear contacts are faulty. Firstly, the ANSYS Workbench finite element software is used to simulate the switchgear and obtain the contacts temperature and infrared window measurement temperature as the BP neural network test data. Then, the BP neural network model is trained to determine the model parameters. Finally, the trained BP model is used to test and determine the accuracy of the fault diagnosis of the switchgear contacts based on the BP model. The experimental results show that the maximum error between the expected data and the predicted data based on the BP neural network model does not exceed 1. 5 \u00b0C, which is within the normal range. Therefore, based on this model, the operation and maintenance staff can acquire the contacts state of the switchgear by measuring the infrared window temperature. It is of great significance to the safe operation of the switchgear."}, {"label": 0, "content": "In the simulations of voltage source converter (VSC) based DC grids, fast protection schemes for overhead lines, such as traveling wave protection unavoidably require a small time step when simulating high voltage direct current (HVDC) circuit breakers. In order to present protection processes accurately and realize hardware-in-the-loop (HIL) simulation, this paper proposes a modeling method of HVDC circuit breakers for small time-step real-time simulation. After using the transmission line modeling method(TLM) to solve the arrester and constant impedance model of the switch, the admittance matrix of the HVDC breaker will keep constant, which reduces computing time greatly. A simplified HVDC breaker is used to interpret the modeling method. And a test circuit is implemented on a field programmable gate array (FPGA) board, on which efficient and accurate simulation results are obtained."}, {"label": 0, "content": "The infrared thermal image is an importance in condition monitoring of electrical equipment. However, the infrared image edge is coarse, fuzzy, and with serious noise and brings great inconvenience to the infrared image processing. An adaptive optimal threshold edge extraction algorithm based on improved Sobel operator is proposed in this paper. In the algorithm, by eight-direction Sobel operators the infrared image edge is extracted in the high temperature area in noisy environment. At the same time, the wavelet coefficients in sub-band are described by general Gaussian distribution and the variance is estimated from the local neighborhood information of sub-band wavelet coefficients so that the adaptive optimal denoising threshold can be obtained. Finally, the edge extraction infrared image is denoised by combining with improved Sobel operator and the optimal threshold. Matlab simulation results show that the algorithm can effectively detect the edge of infrared image and greatly improve its anti-noise ability."}, {"label": 0, "content": "DC voltage control for modular multilevel converter (MMC) not only needs to control the total DC voltage, but also must control the capacitor voltage balancing of each sub-module (SM). The sorting-based traditional capacitor voltage balancing control algorithm is simple and easy to implement, but it needs to sort the capacitor voltages of all SMs of the valve arm and has low calculation efficiency. In this paper, a novel capacitor voltage balancing control algorithm based on the dynamic tiered sorting is proposed. By dynamically layering all sub-modules (SMs) on the valve arm, the number of SMs participating in the sorting is greatly reduced. Finally, the proposed algorithm is applied to the electromagnetic transient program of ADPSS (Advanced Digital Power System Simulator, ADPSS), and the correctness and the effectiveness of the proposed algorithm are verified by the back-to-back MMC-HVDC simulation case. The results show that the proposed algorithm not only can achieve the same control effect as the traditional algorithm, maintain the stability of all SMs capacitor voltage on the valve arm, but also can greatly improve computational efficiency."}, {"label": 0, "content": "To address the limited speed and scale of the simulation of modular multilevel converter (MMC) in electromagnetic transient simulation program (EMTP), this paper put forward a MMC fast electromagnetic transient model based on rotation transformation. The crucial advantage of the rotation transformation is to reduce the fundamental frequency of the original signal via the rotation transformation of coordinate system. With proposed method implemented in the EMTP of MMC, the simulation step size can be relatively increased, and the fast electromagnetic transient simulation can be achieved. In this paper, the equivalent model of MMC switch function model expressed by the state equation is derived. Based on this, the novel MMC model based on rotation transformation is further derived combined with the concept of rotation transformation. Then, the state equation of proposed novel MMC model is calculated in Matlab, and the simulation results and simulation time-consuming are compared with which of 51-level topological MMC electromagnetic transient model established in PSCAD/EMTDC. The results of comparative analysis show that the simulation duration of the MMC model based on rotation transformation is far less than the traditional MMC electromagnetic transient model without sacrificing accuracy."}, {"label": 0, "content": "Regarding the stability analysis and protection control of line-commutated converter-based high voltage direct current (LCC-HVDC) sending-end power grid, the assessment and extraction of dynamic response features is of great importance. This paper proposes a two-stage approach incorporating local and global analyses to extract the spatiotemporal distribution features of dynamic frequency and voltage responses of large sending-end power grid under LCC-HVDC blocking. From the perspective of time, the local quantitative indices are constructed in the first stage to measure the response time delay, deviation magnitude, rate of change and cumulative effects of dynamic trajectories. Then in the second stage, the temporal features of different observation sites are compared globally, in which the observation space and dissimilarity matrices are created to depict the spatial disparities of the whole observation region. Based on the extracted spatiotemporal features, a typical dynamic response pattern of sending-end power grid is captured to detect the LCC-HVDC blocking. Sichuan power grid in China is selected as a case study to verify the proposed approach and demonstrate the obtained features and pattern."}, {"label": 0, "content": "This paper presents a dynamic identification method for abnormal state detection in low signal-to-noise ratio (SNR) environment based on spiked population model which inspires from random matrix theory for developing the theory and method of grid situation awareness based on big data technology. It firstly constructs a data source matrix and obtains a moving split-window matrix and its standard matrix, then acquiring the sample covariance matrix. The global SNR estimation is performed using the classical spectral estimation method corrected by the Kaiser window function, from which a corresponding dynamic threshold is derived. In this way, the situation awareness and early warning for interconnected power systems could be achieved by calculating maximum eigenvalue of the sample covariance matrix and the dynamic threshold to check violation. Utilizing MATLAB\u00ae software, the case studies have been carried on an IEEE 50-machine system, involving two main working conditions such as abnormal load mutation and short circuit fault. The results show that the proposed methodology has the advantage of higher noise resistance in comparison with the traditional mean spectral radius based method and preliminarily verifies that it would be robust under incomplete information."}, {"label": 0, "content": "This paper investigates the controller design problem of networked control systems subject to cyber attacks. A hybrid-triggering communication strategy is employed to save the limited communication resources. State measurements are transmitted over a communication network and may be corrupted by cyber attacks. The aim of this paper is to design a controller for a new closed-loop system model with consideration of randomly occurring cyber attacks and the hybrid-triggering scheme. A stability criterion is obtained for the system stabilization by employing Lyapunov stability theory and stochastic analysis techniques. Moreover, the desired controller gain is derived by resorting to some matrix inequalities. Finally, a numerical example is exploited to demonstrate the usefulness of the proposed scheme."}, {"label": 0, "content": "The radio frequency spectrum crunch has triggered the harnessing of other sources of bandwidth, for which visible light is a promising candidate. Even though visible light communication (VLC) ensures high capacity, coverage is limited. This necessitates the integration of VLC and device-to-device (D2D) technologies into heterogeneous networks. In particular, mobile users which are accessible by the VLC transmitters can relay data to mobile users which are not, by means of D2D communication. However, due to the distributed behaviors of mobile users, determining optimal data transmission routes from VLC transmitters to end mobile devices is a major challenge. In this paper, we propose a reinforcement learning (RL)-based approach to determine multi-hop data transmission routes in an indoor VLC-D2D heterogeneous network. We obtain the rewards for the RL-based method dynamically, by formulating the interactions between the mobile users relaying the data as an equilibrium problem with equilibrium constraints and using alternating direction method of multipliers to solve it. The proposed technique can achieve optimal data transmission routes in a distributed manner. The simulation results demonstrate the effectiveness of the proposed approach, showing that transmission routes with low delays and high capacities can be achieved through the learning algorithm."}, {"label": 0, "content": "The papers in this special section focus on cloud computing, fog computing, the Internet of Things, and Big Data analytics for the future healthcare industry, or Healthcare 4.0. Healthcare Industry 4.0 allows increasing flexibility in production, speeding up both manufacturing and market processes, increasing both the product quality and productivity, and changing business models modifying the interaction with value chain, competitors, and clients. Healthcare Industry 4.0 requires investments and mind-set change for cross-industry collaboration, agreements on data ownership, security, legal issue solving, product registration standards, new machine-to-machine communication protocols, and employment/skills development. Furthermore,Healthcare Industry 4.0 is revolutionizing the market of health service provisioning to patients and clinical operators. "}, {"label": 0, "content": "The paper considers technologies of increasing energy efficiency of technological modes at robotized productions. The project of trajectories optimization module of industrial manipulator is put forward. The module is integrated into an informative space of robotized section and analyzes operating programs of CAM - system. It defines the volume of associated energy costs of technological process, puts forward corrective actions, which are aimed at increasing energy efficient of technological process while changing the program code without going beyond the limitations of the technology. The base of the computing part includes intellectual algorithms of identifying non-linear dependencies and output decisions. Results of the research are proved by experiment that was made at robotized machining section with industrial robot KUKA KR - 60."}, {"label": 0, "content": "A fast and accurate analytical inverter-fed induction machine iron loss calculation model is proposed in this paper. The proposed model takes account of the influence of the output voltage harmonics from the inverter on the iron loss of the motor based on the piecewise variable coefficients method. Additionally, the proposed method incorporates slot harmonic component's influence on the iron loss. The validity of the proposed model is verified by comparing its calculated core loss values with measured ones of a 5.5- and a 55-kW inverter-fed induction motors under different speeds and load conditions. Compared with the classical iron loss model and piecewise variable coefficient iron loss model based on the finite-element method, the proposed model can reduce the computational burden significantly with desirable accuracy."}, {"label": 0, "content": "In this paper, an analytical framework is provided to analyze the uplink performance of device-to-device (D2D)-enabled millimeter-wave (mm-wave) cellular networks with clustered D2D user equipments (UEs). The locations of cellular UEs are modeled as a Poisson point process, while the locations of potential D2D UEs are modeled as a Poisson cluster process. Signal-to-interference-plus-noise ratio outage probabilities are derived for both cellular and D2D links using tools from stochastic geometry. The distinguishing features of mm-wave communications such as directional beamforming and having different path loss laws for the line-of-sight and non-line-of-sight links are incorporated into the outage analysis by employing a flexible mode selection scheme and Nakagami fading. Also, the effect of beamforming alignment errors on the outage probability is investigated to get insight into the performance in practical scenarios. Moreover, area spectral efficiency of the cellular and D2D networks is determined for both underlay and overlay types of sharing. Optimal spectrum partition factor is determined for overlay sharing by considering the optimal weighted proportional fair spectrum partition."}, {"label": 0, "content": "Tomographic synthetic aperture radar (TomoSAR) inversion of urban areas is an inherently sparse reconstruction problem and, hence, can be solved using compressive sensing (CS) algorithms. This paper proposes solutions for two notorious problems in this field. First, TomoSAR requires a high number of data sets, which makes the technique expensive. However, it can be shown that the number of acquisitions and the signal-to-noise ratio (SNR) can be traded off against each other, because it is asymptotically only the product of the number of acquisitions and SNR that determines the reconstruction quality. We propose to increase SNR by integrating nonlocal (NL) estimation into the inversion and show that a reasonable reconstruction of buildings from only seven interferograms is feasible. Second, CS-based inversion is computationally expensive and therefore, barely suitable for large-scale applications. We introduce a new fast and accurate algorithm for solving the NL L1-L2-minimization problem, central to CS-based reconstruction algorithms. The applicability of the algorithm is demonstrated using simulated data and TerraSAR-X high-resolution spotlight images over an area in Munich, Germany."}, {"label": 0, "content": "The majority of existing wireless communication systems relies on training-based signaling, i.e., schemes that rely on accurate channel estimates for detecting the transmitted symbols. Training requires its own hardware and algorithms and consumes a significant portion of the channel coherence interval, which is typically short in vehicular communications. The drawbacks of training can be alleviated by using noncoherent signaling schemes, which dispense with the training phase altogether. We will focus on a particular class of such schemes, i.e., Grassmannian signaling. This scheme is optimal for high signal-to-noise ratio (SNR) communication over noncoherent block-fading channels, which are likely to arise in many future networks."}, {"label": 0, "content": "Multi-hop broadcast communication is one of the main schemes for supporting message dissemination in vehicular ad hoc networks. In this paper, we propose an efficient receiver-oriented broadcast scheme in which the receiving vehicles' probability of forwarding is modeled by the symmetric volunteer's dilemma game. Based on this game, the vehicles that receive the broadcast message are players. At least one of the players should pay a cost and be a volunteer to rebroadcast the message, and then all will benefit from this volunteering. Utilizing fuzzy logic techniques and considering information from the network layer about local density and probability of transmission, the contention window size at the MAC layer will be adjusted. We develop ns-3 simulations to evaluate the performance of the proposed volunteer's dilemma inspired broadcast (VDIB) scheme in terms of reachability, number of rebroadcasts per covered vehicle, number of bytes sent per covered vehicle, and per-hop delay. VDIB performance is compared with that of the distance-to-mean broadcast method, broadcast component of contention-based forwarding, distribution-adaptive distance with channel quality, statistical location assisted broadcast, fuzzy logic-based broadcast, bandwidth efficient fuzzy logic assisted broadcast, and intelligent hybrid adaptive broadcast protocols in both highway and urban environments."}, {"label": 0, "content": "In this paper, we propose a fast and efficient multitemporal despeckling method. The key idea of the proposed approach is the use of the ratio image, provided by the ratio between an image and the temporal mean of the stack. This ratio image is easier to denoise than a single image thanks to its improved stationarity. Besides, temporally stable thin structures are well preserved thanks to the multitemporal mean. The proposed approach can be divided into three steps: 1) estimation of a \u201csuperimage\u201d by temporal averaging and possibly spatial denoising; 2) denoising of the ratio between the noisy image of interest and the \u201csuperimage\u201d; and 3) computation of the denoised image by remultiplying the denoised ratio by the \u201csuperimage.\u201d Because of the improved spatial stationarity of the ratio images, denoising these ratio images with a speckle-reduction method is more effective than denoising images from the original multitemporal stack. The amount of data that is jointly processed is also reduced compared to other methods through the use of the \u201csuperimage\u201d that sums up the temporal stack. The comparison with several state-of-the-art reference methods shows better results numerically (peak signal-noise-ratio and structure similarity index) as well as visually on simulated and synthetic aperture radar (SAR) time series. The proposed ratio-based denoising framework successfully extends single-image SAR denoising methods to time series by exploiting the persistence of many geometrical structures."}, {"label": 0, "content": "Mission-critical wireless sensor networks are attractive for information gathering in various complex environments and support many mission-critical applications, such as industrial automation and security surveillance. However, in order to fully exploit these networks for such applications, agile and scalable network management for data transfer and computing task implementation are essential. Thus, in this paper, we propose a software-defined mission-critical wireless sensor network (MC-SDWSN) which can solve the existing challenging issues in tradition WSNs, such as resource utilization, data processing, system compatibility, and strict latency requirement. The architecture is based on the idea of SDN architecture, combining the hierarchical cloud and edge computing technologies. Based on the MC-SDWSN architecture, a novel centralized computation offload strategy in sensor network application is proposed to show the feasibility. The simulation results in confirm the MC-SDWSN architecture, and the edge offloading strategy could support the critical missions effectively."}, {"label": 0, "content": "Most of the available graph-based semisupervised hyperspectral image classification methods adopt the cluster assumption to construct a Laplacian regularizer. However, they sometimes fail due to the existence of mixed pixels whose recorded spectra are a combination of several materials. In this paper, we propose a geometric low-rank Laplacian regularized semisupervised classifier, by exploring both the global spectral geometric structure and local spatial geometric structure of hyperspectral data. A new geometric regularized Laplacian low-rank representation (GLapLRR)-based graph is developed to evaluate spectral-spatial affinity of mixed pixels. By revealing the global low-rank and local spatial structure of images via GLapLRR, the constructed graph has the characteristics of spatial-spectral geometry description, robustness, and low sparsity, from which a more accurate classification of mixed pixels can be achieved. The proposed method is experimentally evaluated on three real hyperspectral datasets, and the results show that the proposed method outperforms its counterparts, when only a small number of labeled instances are available."}, {"label": 0, "content": "Recently, numerous remote sensing applications highly depend on the hyperspectral image (HSI). HSI classification, as a fundamental issue, has attracted increasing attention and become a hot topic in the remote sensing community. We implemented a regularized convolutional neural network (CNN), which adopted dropout and regularization strategies to address the overfitting problem of limited training samples. Although many kinds of the literature have confirmed that it is an effective way for HSI classification to integrate spectrum with spatial context, the scaling issue is not fully exploited. In this paper, we propose a high efficient deep feature extraction and the classification method for the spectral-spatial HSI, which can make full use of multiscale spatial feature obtained by guided filter. The proposed approach is the first attempt to lean a CNN for spectral and multiscale spatial features. Compared to its counterparts, experimental results show that the proposed method can achieve 3% improvement in accuracy, according to various datasets such as Indian Pines, Pavia University, and Salinas."}, {"label": 0, "content": "We focus on two problems related to rumor detection. First, stance classification with respect to a rumor and, second, rumor veracity prediction. In the stance classification task, we aim to identify the users' stance toward the underlying rumor in a Twitter conversational thread, whereas, in the second problem, i.e., veracity prediction, we aim to verify the authenticity of a rumorous tweet (i.e., source tweet) in a conversational thread. We propose an MLP-based feature-driven model for veracity prediction and a hierarchical LSTM-based approach for detecting stances toward a rumor in a conversation thread. Evaluations show that our proposed system attained better performance in comparison with the various state-of-the-art systems on both the tasks."}, {"label": 0, "content": "Diagnostics and monitoring are resource-intensive, but necessary processes for providing reliable services by cellular operators. The high cost of existing measuring systems and the need for constant participation in the measurement of highly qualified specialists imposes its limitations. Thus, the purpose of developing an alternative solution for diagnosis and monitoring tasks is relevant. The proposed solution is distributed automated information and measuring system based on a smartphone, using the passive method of monitoring. The proposed solution makes it possible to use the capabilities of modern smartphones and differs relatively simplicity of development, implementation and operation, as well as a high degree of flexibility and mobility."}, {"label": 0, "content": "The paper describes the solution of the problem of the porting content from the old website to the new platform with added interactive functionality. Made the rationale for the selection of the content management system Drupal-based on the analysis of the comparative characteristics of the other CMS. The paper provides detailed practical implementation of the desired functionality. A brief overview of the classes of functional requirements for information security is given. On the example of one of the classes the adaptation of the ranking method to assess the probability of unauthorized access taking into account the factors formulated. For each factor are valid values. The analysis of the password length depending on the size of the original alphabet of characters. Probabilistic estimates of hacking are calculated, recommendations on the optimal combination are given."}, {"label": 0, "content": "The aspects of developing three-dimensional displacement control system with application of Case-based reasoning technology are offered in the paper. Fuzzy set theory is implemented for increasing accuracy of the system while determining three-dimensional location of the studied object. Fuzzy logic mechanism is used for processing signals, a linguistic variable is entered for fuzzificating input data, a term-set of seven values is formed, a mechanism of searching membership function is described and a rule base for defuzzificating output data is formed as well."}, {"label": 0, "content": "Ground loop is a serious problem in a lab or an industry. It conflicts with low-level signals instrument and sometimes endangers for human being. However, detecting the ground loop within a huge instrument is difficult task and time consuming matter. To solve this problem, a novel technique is applied for detecting ground loop in a complex situation using internet of thing (IoT). This approach consists of an exciter module with IoT device that generates a 100 kHz ground loop current and a detector to determine affected cable by receiving the test current. Multiple detectors used to give a virtual cable id in a complex area to identify fault cable. After detecting ground loop the affected cable id sent to the server for taking an action to prevent the ground loop."}, {"label": 0, "content": "Despite the diversity of automated management systems and the wide range of their functional features, there is a lack of informational and decision support of production management at the operational level. Researches of both Russian and foreign authors confirm that informational aspect of management is the foundation of its efficiency. The design and the usage of software which collect, modify, interpret information and forecast processes development on its basis are the key to enterprises efficiency when transferring to the sixth techno-economic paradigm. At the operational level of assembling management, one of the major processes is defect management. The paper proposes the way of increasing efficiency of engineering technologist work related to the processing of requests for registering defects revealed during aircraft assembling. Commonly, informational support of defect management processes is provided by paper or electronic registers, catalogs and manuals along with the data of different corporate information systems. Considering increasing intensification of production processes and therefore workers' informational workload, to increase speed and improve quality of processing information required for making decisions related to defect processing, it is proposed to use an expert system which allows accumulating data about typical solutions and provides simple decision support resources. The first results of the expert system implementation, which was developed by the authors and allowed improving engineering technologist work related to the processing of defect registration requests, confirm the relevance of further researches of expert systems as a tool of informational and decision support at the operational level of production management."}, {"label": 0, "content": "The acoustic emission phenomenon arises due to local changes of dynamic structure of solid body. Main sources of emission are micro and macro cracks, friction and shifts. Active and passive acoustic emission methods are widely used to study the strength of materials, the landscape stability and various stages of seismic process. Geoacoustic emission is an acoustic emission of the sound range and it describes the interaction of micro and macro dislocations. Anomalies of geoacoustic signals may be earthquake precursors and they are of great interest to researchers. A typical geoacoustic signal consists of sequence of specific shape short pulses. Authors propose an additive model of geoacoustic signal. According to the model the signal decomposes into a sum of components described by the modulated Berlage and Gauss functions. The use of the matching pursuit method was offered to determine model coefficients. Unfortunately, this method has cubic computational complexity depending on number of functions on which the signal is decomposed. This article is devoted to ways to improve accuracy of the model offered by authors. Various numerical schemes allowed to increase the adaptive property of matching pursuit method with respect to geoacoustic emission signals are considered and compared."}, {"label": 0, "content": "The errors in the means of measuring technology have a significant impact on the technological processes of creating permanent joints. This is also true for the processes of creating permanent joints based on induction heating. In this technological process take place an induction heating of materials with different values of emissivity, which introduces significant errors in the indications of measuring equipment. Non-contact pyrometric sensors are used to take readings of the induction soldering process, which also causes the occurrence of a large number of factors that introduce errors into the readings of these sensors. To ensure high quality control of the induction soldering process, it is necessary to carry out identification of errors. This article suggests the use of intelligent methods for identifying errors in measuring instruments. As an algorithmic support, it is proposed to use the apparatus of artificial neural networks. The software offers the use of the programming language \u201cPython\u201d and the set of libraries \u201cAnaconda\u201d. In the article is described the process of effective neural network structure and parameters search. The results of experimental study confirm that the usage of intelligent methods for identifying errors allows to improve the quality of induction soldering process control as well as other methods of permanent joints creation."}, {"label": 0, "content": "Message passing model, represented by MPI (Message Passing Interface), is the main tool for parallel programming for distributed computer systems. The most of MPI-programs contain collective communications, which involve all the processes of a parallel program. Effectiveness of collective communications substantially effects on total time of program execution. In this work, we consider the problem of design of the algorithm of barrier synchronization, which refers to one of the most common types of collective communications. We developed adaptive algorithm of barrier synchronization, which suboptimally selects barrier synchronization scheme in parallel MPI-programs among such algorithms as Central Counter, Combining Tree and Dissemination Barrier. The adaptive algorithm chooses the barrier algorithm with the minimal evaluation of execution time in the model Lo g P. Model LogP considers performance of computational resources and interconnect for point-to-point communications. Proposed algorithm has been implemented for MPI. We give the results of experiments on cluster systems, analyze dependency of algorithm selection on LogP parameters values. In particular, for the number of processes less than 20 adaptive algorithm selects Combining Tree, while for a larger number of processes adaptive algorithm selects Dissemination Barrier. Developed algorithm minimizes average time of barrier synchronization by 4%, in comparison with the most common determined barrier algorithms."}, {"label": 0, "content": "We created a mathematical model of a sodium high-pressure lamp. This model is used in production before sending lamps to the consumer. To develop the model, we used a mathematical model. An analytical method was used to describe the operation of a sodium lamp based on differential equations. We also used the singular value decomposition algorithm to find the coefficients of the ARMA model. Also, the transfer function of the ARMA model was obtained. Then we tested the models to control the quality of sodium lamps in production. The obtained results of the simulation coincide with the experimental results. A graphical dependence is obtained in the case when the standard deviation is 1. Using a series of tests based on the singular value decomposition method, we confirmed the adequacy of elaborated model by Kolmogorov-Smirnov criterion."}, {"label": 0, "content": "The paper presents a new methodological approach to the automated construction of software for solving the design problems of heating systems. The methodological approach is based on the Model-Driven Engineering paradigm. The essence of this paradigm is that the software is generated on the basis of formal description represented by models. The knowledge about heating systems, applied problems, and the applied software is formalized in the form of ontologies. The automated construction of the software system is performed on the basis of a computer model of a heating system, the ontologies and modern metaprogramming technologies. The proposed approach allows us to successfully solve the problem of separation of methods for solving applied problems and models of heating system elements. To this end, the methods are implemented in the form of software components that are not related to properties and models of specific equipment. And the models of heating system elements are automatically compiled into software components. In the process of software system construction, software components that implement models and methods are integrated dynamically. As a result, the software system oriented to solving a specific applied problem is created in an automated mode. The developed approach has been used for the implementation of the SOSNA software. The software is applied to design urban heating systems."}, {"label": 0, "content": "A novel spectral-domain singularity subtraction technique for accelerating the convergence of Sommerfeld integral tails is proposed for planar stratified media that include a perfect electrically conducting layer. Numerical results show that the extension avoids catastrophic cancellation in the spatial domain between the analytically computed and the numerically integrated terms, yields a rapidly decaying spectral tail, and enables accurate calculation of the Green's functions."}, {"label": 0, "content": "The article examines the effectiveness of the scheduling algorithm for solving the problem of increasing the efficiency of the operation of corporate information and computer networks based on the use of priority information processing models. The possibility of increasing the efficiency is caused by the analysis of the features of properties and characteristics of the considered network in various modes of operation, which are obtained at the modeling stage. The order of servicing of information flows of different types, determined by the introduction of priorities, allows to reduce the average waiting time for system operation and loss due to waiting. The task of analyzing of the efficiency of priority systems for the transmission and processing of information in complex information and computing systems is the perspective application-oriented task due to use of an effective dispatch algorithm."}, {"label": 0, "content": "In this paper, the problem of inter cell interference suppression which under the multi cell heterogeneous network system model is studied. Game theory is used to build the model of frequency and timeslot selection. The analysis shows that the proposed resource reuse model is a potential game, which can improve the throughput of the network."}, {"label": 0, "content": "A computational benchmark suite is presented for quantifying the performance of modern RCS simulations. The suite contains a set of scattering problems that are organized along six dimensions and range from basic to challenging. It also includes reference solutions, performance metrics, and recommended studies that can be used to reveal the strengths and deficiencies of different simulation methods."}, {"label": 0, "content": "A parameter estimation algorithm for multiple frequency-hopping (FH) signals based on maximum energy difference is proposed in this paper. First, the time-frequency (TF) matrix is obtained by TF analysis such as short-time Fourier transform (STFT) and smoothed pseudo wigner-ville distribution (SPWVD). Then, the carrier frequency and the TF data with valid frequency are determined according to the distribution of energy. Next, the number of signal segments and window length in each nonzero row of the TF data is obtained. Finally, hopping time and hop cycle are estimated based on the maximum energy difference. Simulation results indicate that the proposed algorithm has better anti-noise performance than the TF pattern modification method and the STFT-SPWVD method. The new method is suitable for asynchronous and synchronous network."}, {"label": 0, "content": "The proposed control can be applied at different enterprises of industry in the building of systems of remote diagnostics of technical condition and determining of the correct functioning of multivariable objects aimed at reducing the number of emergency operating modes of electric equipment, reduce the load measuring channels and improve the speed and reliability of solving the problems of identification of technical condition of Electromechanical systems. This is achieved through the implementation of the principle as maximum approximation processing of the measuring information to the places of its occurrence and solution of problems of identification directly on the remote object."}, {"label": 0, "content": "In this paper, we implemented the architecture of DV700 which is deep learning based image recognition accelerator in edge computing applications, and measured performance in FPGA environment. As a result, it operates at 12.9 fps in GoogleNet and 15.6 fps in SSD algorithms with 79MHz operating frequency environment."}, {"label": 0, "content": "The problem of events detecting and classifying in the continuous video stream of information and telecommunication security systems is solved. As a basic algorithm, it is proposed to use the ensemble of deep neural networks - convolutional networks and feedback networks, in particular, for classification and annotation, which makes it possible to recognize abnormal emergency situations. A training set of abnormal situations was collected, various architectures of deep neural networks were trained and tested. It is shown that the use of the indRNN layers achieves up to 70% when recognitions multi-class events in the video stream. The new is strengthening of classification estimation of a video segment by extracting the keywords from the automatic annotation. The developed software package can be implemented in the integrated security system of abnormal situations recognition in real time."}, {"label": 0, "content": "A waveform design procedure for improving the estimation of Doppler frequencies in active remote sensing applications is presented. The bound on frequency estimation is analyzed in terms of a continuous waveform, and the optimal waveform is inferred. Several waveform designs are analyzed, showing that a near-optimal dual-pulse waveform can achieve greater estimation accuracy than a single-pulse waveform using the same signal energy."}, {"label": 0, "content": "Weapon-Target Assignment (WTA) problem is the key of air defense command and control. Therefore, it is an urgent problem to complete the assignment quickly and efficiently. In this paper, an improved artificial fish swarm algorithm is proposed to improve assignment rate. Based on artificial fish swarm algorithm (AFSA), particle swarm optimization (PSO) is introduced to change the individual visual of artificial fish, and genetic operator is added to avoid the local extremum trap. The proposed algorithm is validated by the concrete cooperative air defense examples. The simulation results show that the algorithm improved the computational accuracy and the rate of convergence in solving weapon-target assignment problem in air defense."}, {"label": 0, "content": "Paper proposes a hardware support of method for detecting a periodic component in a measurement signal, based on the calculation of the signal's zero-crossing numbers. On the basis of the considered method, an algorithm with low computational costs is proposed. The algorithm can be used to indicate of the progressing of resonant phenomena, to determine the presence of a carrier signal, and so on. The structures of the basic modules required for constructing hardware support devices of the proposed algorithm are proposed. The proposed modules are relatively easy to implement and can be easily manufactured for example using FPGA technology. An example of a block diagram of a detection device of periodicity with a known frequency is given. A description of the operation of the device is given and the main time diagrams are given."}, {"label": 0, "content": "The article describes the software tools used for macroeconomic modeling of the Fuel and Energy Complex. These software tools allow you to generate and modify the relevant macroeconomic models. Its architecture and functional components provide a flexible interaction between the user and the computer simulation system. The software package includes two autonomous subsystems - CREATOR and DIGGER. The CREATOR system is used to create models and change their formal structure. The DIGGER system is intended for the organization of input and editing of data, conducting direct calculations on models, launching optimization procedures. Direct calculations realize the simulation mode of model user. They determine the values of the model indicators according to formulas of the static and dynamic modes. The optimization mode may be used only for multilinear indicators. It is divided into static and dynamic. In the static optimization mode (optimization of one current time circle) non-multilinear indicators are excluded from the optimization task. Dynamic non-multilinear indicators are excluded for the dynamic optimization mode. A direct calculation performed after the optimization procedure completes, it shows whether these indicators satisfy the boundary conditions specified for them. The macroeconomic problems related to the fuel and energy complex that these tools helps to solve online are listed. The capabilities of the described tools are illustrated with a significant example."}, {"label": 0, "content": "A special feature of tracking control system is to ensure the accuracy of the control set in dynamic modes. The presence of non-linearities in the control object, in the system drives, and especially the non-stationarity of the parameters of the system as a whole makes it difficult to achieve the desired quality indicators using standard classical control algorithms. The paper deals with a new approach to the implementation of fuzzy servo control systems. At the same time, a special emphasis is placed on building the tracking control system. This principle of building an intelligent system not only expands the functionality in the implementation of corrective control action, but also makes it possible to provide self-adjustment of the controller under the selected optimization criteria. This approach is considered on an example of the tracking system model on the basis of the DC electric drive at working off difficult in form of changing signal."}, {"label": 0, "content": "The relevance of the research is caused by the need to improve electromechanical systems with linear electromagnetic engines and accumulator food for realization of striking operations, technologies in construction, technical researches, etc. and assumes an effective use of power stock of accumulators and increase in period of operation of system without recharging the source. The main objective of the research is to estimate expediency and to define the possibility of realization of the system providing automatic correction of the electric power consumed from accumulators when the properties of loading change. It is shown that electromechanical systems with the linear electromagnetic engines equipped with sensors of limit provisions of an anchor and operated on the coordinate of a moving anchor are preferable to the technical embodiment. The advanced controlling system with the use of a programmable logic controller which without complication of the device of feedback provides automatic correction of the consumed energy from accumulators under changing properties of loading is offered."}, {"label": 0, "content": "The article is devoted to the description of approaches to the solution of the actual problem of optimization of neural network calculations using the non-position number system (residual number system) for design problems of neural network automatic control systems. The problems arising in neural network control systems of industrial objects characterized by astatic properties are identified and analyzed. The approach proposed to solve these problems is confirmed by an example of solving a real problem."}, {"label": 0, "content": "Machine learning (ML) is gaining popularity in the network security domain as more network-enabled devices get connected, as malicious activities become stealthier, and as new technologies like Software Defined Networking (SDN) emerge. From the application layer, ML-based SDN security models control the routing/switching of an entire Software Defined Network. Compromising the models is hackers' desirable goal. Previous works have been done on either adversarial machine learning without the context of secure networking environment or on the general vulnerabilities of SDNs without much consideration of the defending ML models. Through examination of the latest ML-based SDN security applications, a good look at ML/SDN specific vulnerabilities accompanied by a successful attack on StratosphereIPS, this paper makes a case for more secure developments of ML-based SDN security applications."}, {"label": 0, "content": "The article proposes the structure of an intelligent software platform for managing complex risks. It is proposed to divide the platform into a global and local part (end-point software) that provides not only the management of complex risks on the scale of a single system (enterprise, organization) but also the collection, accumulation, synthesis and dissemination of methods and models of integrated risk management that are best practices that have proven effective on practice. Within the local part, the platform provides the ability to build complexes of hybrid intelligent models by a risk management specialist without the participation of a programmer. Examples of using the proposed solution or its parts in projects aimed at risk management are considered."}, {"label": 0, "content": "This article describes the approach to implement the fuzzy interconnected control system for sheet metal forming electric drives. The displacements of forming rods of sheet metal forming electric drives system depend on various factors and some of which can only be described by qualitative characteristics (indexes). The possible way to achieve desired output parameters of all local drives systems is using compensation method. However, it is difficult to achieve desired outputs by using conventional compensation methods because of having interrelation error (effect) between local forming electric drives. In this article, suggested compensating interrelation effect between local forming electric drives by using fuzzy compensation method. The fuzzy compensation method based on theory of differential inclusion. Mathematical equations of this fuzzy compensation approach are described. Algorithm for implementation of fuzzy controller to achieve the specified control accuracy is presented."}, {"label": 0, "content": "In this paper, we propose a new method to mitigate the effect of low signal to noise ratio (SNR) on two-dimensional (2-D) Direction of arrival (DOA) estimation. This method consists to extend the antenna steering vectors before applying the MUltiple SIgnal Classification (MUSIC) algorithm for 2-D DOA estimation. The simulation results show a good location accuracy enhancement."}, {"label": 0, "content": "The controller synthesis problem for an automatic extremum-seeking system in the case of SISO plants is considered. The plant model can be represented as a serial connection of a nonlinear dynamic component and a static quality function with a certain minimum or maximum value. It is proposed for the individual plant components to organize two separate cascades with the different processes rates in the system. The controller synthesis procedures for each loop are presented. At the first stage, it is suggested to stabilize the processes in the inner contour by means of a controller based on the localization method or to organize a sliding mode in it. Such controllers allow to suppress the perturbations and nonlinear characteristics of the plant dynamic part, as well as to provide the required properties to the inner cascade. In order to the extremum seeking in the outer control loop, it is recommended to use a typical I-controller. The extremum seeking process is corresponded to the first-order linear differential equation. The main differences between the two extremum systems types are shown, depending on the inner loop controller type. The presented numerical simulation results of the two systems types in MatLab illustrate their basic properties."}, {"label": 0, "content": "Functional encryption is a recent generalization of public-key cryptography which aims at enabling secret-key owners to decrypt only functions of the encrypted data. This model is very promising in terms of applications. Yet, although general constructions of theoretical interests do exist, practical functional encryption is presently limited to the evaluation of low-degree functions of the encrypted inputs. In this paper, we investigate how Inner-Product Functional Encryption (IPFE) may enable the design of tax calculation system with built-in privacy. The paper is also concluded by performances results demonstrating the practicality of the approach on the concrete issue of carbon tax calculations."}, {"label": 0, "content": "A concept of a hybrid expert system for robot milling conditions control presented. Based on KUKA KR300 operating data. Milling conditions, measurements data, negative factors involved are considered. Expert estimations of actual milling episodes are used as criterion of successful milling. All measurable milling quality parameters are examined as possible feedback for milling conditions control. The choice is made in favor of using only real-time parameters available. Development of hybrid expert system includes method for expert estimations formalizing, system structure design, signal processing. An example of data processing and distribution within expert system given to further application in robotic milling control. The introduction of a hybrid expert control system into the robotic milling make it possible to implement two important functions for the manufacturing system. First, accumulate knowledge about the manufacturing process in form of measured data, simple integral and qualitative expert estimations - all in a solid and compact data format. Secondly, generate control signals for selection and maintenance of milling conditions using a rule's logic output system."}, {"label": 0, "content": "In the modern information warfare, the requirements for the reliability and real-time performance of the communication signal recognition technology are getting more and more strict. Although a great number of studies have been conducted in the reliability of communication signal recognition, few studies have been performed in the speed of communication signal recognition. The purpose of this study is to explore an improved feature extraction methods based on extreme learning machine (ELM) which has the advantage of higher speed in communication signal recognition. The results of simulations show that the approach in this paper not only improves the speed of recognition and ensures a high reliability, but also reach an ideal recognition accuracy at a low SNR."}, {"label": 0, "content": "Realistic knowledge of transport properties of the environment is necessary while modeling gas-dynamic processes that occur in the units of energy power systems based on the macro levels of heat and mass transfer. In this article we offer an approach based on the kinetic molecular theory of gases to determine a transport coefficient in the multicomponent gaseous environments. The authors offer a mathematical model of description of molecular interaction in the K-component gaseous environments in which their microstructure, in particular, elastic and geometric characteristics of interacting molecules are taken into consideration. This approach allows to determine transport properties of a multicomponent environment in the macrosystems at a qualitatively new level taking into account its microscopic properties."}, {"label": 0, "content": "In this paper, we review the task of improving the dynamic characteristics of vacuum arc furnace power controller. The paper proposes solution of this task by the use of fuzzy logic controller based on Sugeno algorithm within a speed loop of electrode positioning drive. It also proposes the application of compensation method for dead zone along the arc length using fuzzy logic device also based on Sugeno algorithm. The results of studying the dynamic characteristics of vacuum arc furnace power control system with classical and fuzzy controllers are shown."}, {"label": 0, "content": "Beamforming with conventional array processing utilizes linear, additive processing techniques to combine the signals from the different array elements. In previous work, a multiplicative processing technique was proposed for combining the signals from sensor arrays for super-resolution beamforming. The multiplicative processing technique is derived from standard linear array processing concepts and was shown to emulate the performance of larger array apertures. In this paper, experimental measurements from an acoustic sensor array are used to validate the proposed method for direction-of-arrival (DoA) estimation by comparing these results with those from linear processing of measurements from much larger aperture arrays. The processing results show that the multiplicative processing of experimental measurements from smaller apertures perform as well as linear processing of measurements from larger apertures."}, {"label": 0, "content": "With a rapid increase in the number of geostationary satellites around the earth's orbit, there has been a renewed interest in using Global Positioning System (GPS) to understand several phenomenon in earth's atmosphere. Such study using GPS devices are popular amongst the remote sensing community, as they provide several advantages with respect to scalability and range of applications. In this paper, we discuss how GPS signals can be used to estimate the amount of water vapor in the atmosphere. Furthermore, we demonstrate the importance of such precipitable water vapor (PWV) in the atmosphere for the task of rainfall detection. We present a detailed analysis in our dataset of meteorological data of 3 years. Test dataset shows that use of PWV in rainfall detection helps to reduce the false alarm rate by almost 12%."}, {"label": 0, "content": "From positions of the principles of training selection decomposition and collective estimation the synthesis technique of multilevel nonparametric systems of pattern recognition for the multialternate classification problem is offered. Their application provides high computing performance of information processing of big dimension. Two approaches are considered. Poorly dependent feature sets of the classified objects are in case of the former used. Considering the assumption of independence of feature sets the generalized decisive rule of maximum likelihood is under construction. The basis of the second method is made by a dichotomy method. At each its stage we form the family of the private decision functions corresponding to various feature sets of the classified objects with the subsequent their integration in the non-linear decisive rule by means of methods of nonparametric statistics. At the same time formation of the generalized decision on situation belonging to this or that class is carried out in space of values of private decision functions. The offered technique allows to use technology of parallel calculations."}, {"label": 0, "content": "Aim of this study is to develop an accurate and reliable method for estimating propagation characteristics inside and outside aircraft cabin so as to advance wireless link design for Wireless Avionics Intra-Communication (WAIC) system. This paper estimates propagation characteristics from inside cabin to exterior mounted antenna of WAIC system installed on a passenger aircraft (Airbus 320-200 model). EMF distributions excited by a 4.4 GHz wireless transmitter inside cabin are analyzed and fundamental characteristics for the WAIC system are derived from the analysis results."}, {"label": 0, "content": "The article presents a group method of data sharing, based on a two-level system of residual classes. Group methods are shown and investigated using the example of sharing a data with error correction. Various implementations of models for cryptographic and non-cryptographic information protection are presented."}, {"label": 0, "content": "The paper formulates the problem of establishing the initial data for assessing insolation and electric power generation by solar photovoltaic systems. Three sources of information are evaluated: reference books on the climate, satellite meteorological data and weather stations data. In order to improve the precision of assessment, it seems expedient to combine sources of information regarding the factors that affect electric power generation by solar photovoltaic systems. The paper proposes a method for estimating electricity generation on the basis of reference information and open-access weather stations data. Assessment of solar radiation takes into account the beam, sky-diffuse and ground-reflected components. The method incorporates the impact of the total and low-level clouds on solar radiation, as well as the impact of temperature on the efficiency of solar photovoltaic systems. Assessment of solar radiation and electric power generation by solar photovoltaic systems is conducted in Narin-Kunta, a village in the Irkutsk region on the shore of lake Baikal."}, {"label": 0, "content": "Methods of solution of a problem of an estimation of uncompensated errors of the measurements caused by presence of noises and interferences, that not submitting to statistical regularities are investigated. The model of the signal presented in the form of a composition multiparameter quasidetermined model function and fixed on interval of measurement a background function is considered. The background function is described by an arbitrary ensemble of functions whose range of variation is limited by the E-layer. The proposed model provides the calculation of interval estimates of signal parameters for confidence values close to unity. The technique of calculation of interval errors of parameters on the basis of model of E-areas and nonlinear model function is described. This technique allows calculating interval estimations of parameters for a particular of a signal realization, provided that the range of values of the background function is limited. It is shown that defined in the space of parameters E-regions characterize the state of the control object. Based on the developed algorithm for diagnosing the state of an object from the particular signal realisation using E-regions in the space of parameters of a model function, an approach to identifying abnormal situations is proposed. Based on the developed algorithm for diagnosing the state of an object from the implementation of a signal using E-regions in the space of parameters of a model function, an approach to identifying abnormal situations is proposed. The results of practical application of E-regions for monitoring the process of power consumption of electric energy and other energy resources are presented."}, {"label": 0, "content": "High bandwidth signal transmission between ASIC and memory is more and more important especially for high performance Cell Phone Application Processor (AP) application. Several advanced packaging technologies are developed and High bandwidth Package on Package (HBPoP) is one of them to take the place of FCMAPPOP for the high/middle-end mobile phone products. The structure of HBPoP is similar as sandwich, the substrate interposer (SI) substrate is stacked on silicon top side with fine pitch solder joints to routing and re-layout the I/O and provide high bandwidth requirement. However, the warpage control is the key challenge for PoP technology due to the top side memory package and bottom side HBPoP package would be mount on mother board at the same time; especially, the thinner and thinner package thickness requirement. In this work, pretreatment temperature had apparently effect on package warpage performance, a three-dimensional computational HBPoP simulation model was developed for analyzing warpage behavior. Besides, advanced metrology analyzer (aMA) based on three-dimensional digital image correlation technology, a non-contact optical displacement measurement scheme, was used to measure bare substrate in plan strain and shrinkage behavior with different temperatures. Also, thermal mechanical analyzer (TMA) was utilized to capture the substrate shrinkage properties by multi-cycles measurement. The warpage from the simulation was verified against the experimental data. The results predicted by finite-element model (FEM) indicated that the sensitivity of parameters, including thickness and materials of each laminates and the deviation comes from different suppliers would be evaluate in this study. Moreover, several design combinations are proposed to enhance the warpage control. This work provides design guidance for HBPoP technology development."}, {"label": 0, "content": "This article is devoted to computer modeling of radio systems. The paper shows a method for automated solution of direct and inverse problems of mathematical modeling of nonlinear radio engineering systems. Nonlinear radio engineering systems perform important signal generation and conversion operations: amplification, modulation, detection, frequency multiplication and conversion. Modeling of such systems is necessary to improve the efficiency of their design and research. Existing modeling systems are application packages, that are limited to a set of valid tasks. They usually solve only direct problems (problems of system analysis). Modern radio engineering requires the ability to solve various direct and inverse problems of systems modeling within a uniform program complex. Therefore, the application packages should be replaced by complexes of automated modeling using elements of artificial intelligence. The article proposes a method of using functional grammars to construct the system of automated modeling of nonlinear semiconductor systems. The result of the scientific research is the logical-mathematical model using the terms of functional context-free grammars. The model substantiated by a logical conclusion using lambda calculus. Practical application of the method is demonstrated by examples of solving direct and inverse modeling problems."}, {"label": 0, "content": "In this paper, we report the influence of vegetation in paddy fields with antenna height as a parameter for 2.4GHz (3mW) and 920MHz (20mW) propagation. From the viewpoint of practicality, a commercially available wireless communication module equipped with a small pattern antenna was used for the experimental apparatus. In order to consider the worst case of communication loss, detailed measurements were made assuming the height of rice grown 105 cm, and the antenna height was 55cm, 105cm, 155cm. Based on the measurement results, fitting was performed on the Exponential Decay (EXD) model, and the propagation characteristics in paddy fields were verified. As a result, at 2.4GHz, even under line-of-sight conditions, there was attenuation due to the influence of scattering of reflected waves due to plant arrangement, and it was found that the same degree of remarkable attenuation due to vegetation occurred regardless of the height under rice top. On the other hand, at 920MHz, which is said to have strong diffraction, it was revealed that the loss characteristic of vegetation is proportional to the height."}, {"label": 0, "content": "Energy-efficiency in Wireless Sensor Networks (WSNs) has been regarded as the core issue for designing any communication protocol. Sensor networks consist of limited battery-powered nodes and recharging or replacing is not practical being deployed in harsh environments like underground mines. So designing of WSNs should be concentrated on energy efficiency. Clustering technique is used very effectively to achieve scaling up and power saving in WSNs. It allows hierarchical structures to be built on the nodes and enables the more efficient use of scares resources. In this work, we have proposed a hybrid clustering scheme which able to meet energy constraints of WSNs. It allows data transmission from sensor nodes to the sink with reasonable consumption of energy."}, {"label": 0, "content": "This paper is devoted to the description of the storage model and the search for sound sequences based on the theory of active perception. The theory of active perception is used to form an indicative description of the sound signal. The class of problems solved by the proposed model has a wide scope and includes, among other things, the search for musical plagiarism. It shows the possibility of creating on the basis of the proposed model a software system for identifying audio signals. The software implementation of the search model is made in the programming language R. To approbate this model computational experiments have been carried out, where the database size is 10,000 musical compositions and the search accuracy reaches 96%. The stability of the proposed model is also analyzed for distortion of the sought signal by noise. A comparison is made with similar existing systems in terms of search accuracy."}, {"label": 0, "content": "Security is one of the most important issues concerning Vehicular Ad-Hoc Networks (VANETS), specifically when dealing with misbehaving vehicles to prevent them threatening the safety of others. In this paper, we present a review about revoking misbehaving vehicles based on classical Certificate Revocation List (CRL) in IEEE Standard. The main disadvantage of these algorithms resides in the fact that the Certification Authority (CA) is overwhelmed because it is responsible to distribute the whole CRL to all the requesting vehicles. To overcome this drawback in European Telecommunication Standards Institute (ETSI) standard, we propose our contribution that aims to minimize the tasks of the CA, by decomposing the CRL into different chunks that are distributed separately by the different RSUs of the same zone."}, {"label": 0, "content": "The article analyzes the application of artificial intelligence methods for control of mobile robots. The proposed method of selecting of the optimal artificial intelligence technology by the set of available technologies for control of mobile robots allows to improve the functional characteristics of the robot control system as a whole and its individual subsystems by the optimal coverage of a set of tasks and procedures solved by the robot control system. The method is based on the description of the initial information in the form of preference relations on a set of alternatives. As a solution to the problem of selecting an optimal information technology is adopted the most nondominant alternative. The proposed method was applied to the selection of the optimal artificial intelligence technology for modelling traffic control and technical operation field irrigation in developing control system of mobile robotic sprinkler system \u201cFregat\u201d, This method can be implemented in the design of software not only for mobile robots, but robots for other purposes."}, {"label": 0, "content": "Base on the fourth-order cumulants, a direction finding algorithm for noncircular sources under unknown mutual coupling is proposed. Utilizing the symmetric Toeplitz structure of the mutual coupling matrix and the noncircularity of the sources, a closed form solution for the DOA estimation is obtained by constructing a series of cumulant matrices. According to the simulation results, the proposed algorithm is effective in the presence of unknown mutual coupling. Moreover, it enjoys better estimation performance via utilizing the noncircularity of the sources."}, {"label": 0, "content": "This paper introduced a hand gesture recognition method based on convolutional neural networks (CNNs). The recognition scenario consisted in a three dimensional radar array to transmit and receive 24GHz continuous electromagnetic (EM) wave, and convert the scattered EM wave to the intermediate frequency (IF) signals. This paper used the the processed frequency spectrum as the input to the CNN. Then the CNN feature detection layer learned through data training, avoiding supervised feature extraction while learning implicitly from training data. It highlighted these features through convolution operating, pooling and a softmax function. Results showed that this system could achieve a high recognition accuracy rate higher than 96%."}, {"label": 0, "content": "The packet classification problem aims to determine the behavior of incoming packets at network devices. The linear search classification algorithm assigns each packet according to its prior actions, which are determined by comparing the packet header with classification rules until a match is found. As the processing latency of packet classification is proportional to the number of rules, a large number of rules can result in serious communication delay. This problem is generalized to Optimal Rule Ordering (ORO), which aims to identify the rule ordering that minimizes the delay caused by packet classification. If two different rules match a single packet, conventional ORO does not allow the posterior rule to be placed in a higher position than the prior rule. However, interchanging the rules does not violate the policy if the actions of those rules are the same. Thus, in this paper, we specifically consider the Relaxed Optimal Rule Ordering (RORO) problem, in which rules can be interchanged if their actions are the same. In RORO, the weight of rules may vary as they are interchanged. Hence, we propose a method of calculating the weights using a zero-suppressed binary decision diagram. We prove the difficulty of estimating the weights and propose an algorithm for RORO. This algorithm computes a rule list that ensures lower latency than in several conventional algorithms and accurately computes the latency. We demonstrate the effectiveness of our method by comparing it with previous models and reordering methods."}, {"label": 0, "content": "An FFT-based algorithm is presented for rapidly post-processing the integral-equation based solution of scattering problems to evaluate the fields at an arbitrary number of nearby points. The proposed algorithm uses a similar approach to the adaptive integral method (AIM) but contends with the fact that the fields are not Galerkin tested with basis functions but instead point tested. It reduces the computational costs compared to the brute-force method, especially when the number of observer points is large."}, {"label": 0, "content": "The paper studies an important problem how to increase the efficiency of electric energy transportation in distribution networks with the voltage range of 6-20 kV by reducing losses while transmitting electricity and ensuring its quality at the consumer nodes in accordance with the deviation criterion from nominal in the network in modern Russian electric power industry. A description of the distribution network model is given. The results of distribution networks modeling with the voltage range of 6-20 kV and the use of intelligent control devices based on the thyristor regulator of booster voltage (TRBV) are presented. The high laboriousness of applying precise control algorithms for intelligent control devices for reducing transmission losses in networks with the voltage range of 6-20 kV is shown. The comparative characteristics of the operation of the distribution electric networks before and after the adjustment of intelligent control devices are given."}, {"label": 0, "content": "The article presents the results of the research, as well as a software module that allows you to configure the fuzzy logic controller to three possible ways of including a fuzzy logic controller in the control object. The evaluation of the quality of regulation and their comparative characteristics."}, {"label": 0, "content": "A concept of a large scale Multi-Chip-Module with number of Many Core (PE), FPGA and DRAM chips is proposed. There are three major features, (1) very high speed data transmission by T-Hz $(0.1 \\sim10$ THz) radio communication chips inside the MCM module for communication among the chips, (2) MLC (Multi Layer Ceramic) or another substrate on which T-Hz radio chips in upper surface side and number of Many Core (PE), FPGA and DRAM chips in lower surface side are connected by C4, (3) T-Hz radio chips also communicate with other MCMs and external I/O devices."}, {"label": 0, "content": "The article is devoted to the discussion of the problems of development of predictive neural network models based on the residual number system and the use of modular arithmetic to improve the quality of automatic control systems by adding to the control algorithms a prognostic component, which is especially important for astatic control objects. The possibility of implementing neural network training algorithms in the residual number system is shown, which allows to significantly accelerate the work of these algorithms, which is especially important when adding new functionality to automatic control systems in the form of prognostic neural network models."}, {"label": 0, "content": "Interface design flaws are often at the root cause of use errors in medical devices. Medical incidents are seldom reported, thus hindering the understanding of the incident contributing factors. Moreover, when dealing with a use error, both novices and expert users often blame themselves for insufficient knowledge rather than acknowledge deficiencies in the device. Simulation-Based Medical Education (SBME) platforms can provide appropriate training to professionals, especially if the right incentives to keep training are in place. In this paper, we present a new SBME, particularly targeted at training interaction with medical devices such as ventilators and infusion pumps. Our SBME functions as a game mode of the PVSio-web, a graphical environment for design, evaluation, and simulation of interactive (human-computer) systems. An analytical evaluation of our current implementation is provided, by comparing the features on our SBME with a set of requirements for game-based medical simulators retrieved from the literature. By being developed in a free, open source platform, our SBME is highly accessible and can be easily adapted to specific use cases, such a specific hospital with a defined set of medical devices."}, {"label": 0, "content": "A central problem with distributed ledger technologies involves the latency that must be incurred in processing and verifying transactions to be accepted as permanent records in the ledger. In many applications, high latency is simply not a tolerable aspect of the governance of the ledger. To help reduce latency, we offer a distributed ledger architecture, Tango, that mimics the Iota-tangle design as articulated by Popov [1] in his seminal paper. A main idea is the introduction of a semi-synchronous transaction entry protocol layer. We model periodic pulsed injections into the evaluation layer from the entry layer."}, {"label": 0, "content": "A considerable disadvantage that comes with the downscaling of the CMOS technology is the ever-increasing susceptibility of Integrated Circuits (ICs) to soft errors. Therefore, the study of the radiation-induced transient faults in combinational logic has become one of the most challenging issues as the absence of appropriate error-protection mechanisms may lead to system malfunctions. This paper presents an efficient and accurate layout-based Soft Error Rate (SER) estimation analysis for ICs in the presence of both single and multiple transient faults, since the latter are more prevalent as technology downscales. The proposed tool, i.e. SER estimator, is based on Monte-Carlo simulations taking into account a detailed grid analysis of the circuit layout for the identification of the vulnerable areas of a circuit and, in addition, temperature as one of the factors that affect the generated pulse width. The widening of the fault pulses due to elevated temperature is reflected in increased SER according to our results. Finally, the comparison between the simulation results for some of the ISCAS'89 benchmark circuits obtained from the proposed framework and the respective ones obtained from SPICE indicates a fairly good correlation."}, {"label": 0, "content": "The increased power of the hardware and software complexes and the sharp improvement in the quality of television screens at the same time as the price of their television allowed to solve the problem of simulating visually observed terrain behind the simulator window for the preparation of the locomotive driver. Such a system is a imitator of the visual situation, standing from a computer image generator and 3D indicator. However, the availability of modeling does not imply the need for system analysis to select the main nodes of the 3D modeling system. When training on the simulator, the driver interacts not with real objects, but with their models. Since it is impossible to create a complete model, in studying the simulator complex it is necessary to take into account the negative consequences of the \u201cunreliability\u201d of the synthesis of visible 3D models located in the 3D space model, in comparison with real analogues. The comparison was carried out in the study of the peculiarities of the decision making by the locomotive driver, while controlling the real locomotive, using the information obtained by viewing the terrain through the cab window. Studies have shown that in order to carry out a comparative analysis, it is necessary to develop criteria for assessing the imitator of the visual situation. It was determined that at present the main criterion for the successful modeling of a three-dimensional image in the locomotive operator's integrated simulator is the requirement to train professional skills in determining the distance to visible 3D models. Earlier similar tasks were investigated and successfully solved in aviation simulators. Their solution showed that such a problem can be solved using pseudo-3D 3D imaging systems. Two classes of 3D modeling systems are used: one-channel and two-channel. Each class allows the trainee to see 3D images of sufficient quality so that he can train his visual apparatus and use it to visually distance the selected object. Each class of 3D imaging systems will allow a person to see a 3D image due to exposure to certain components of the person's vision. Single-channel, needleless 3D modeling systems affect the accommodation and convergence of human vision. Two-channel effects on the disparity of human vision. Each 3D image modeling class has its disadvantages and advantages. The choice of a specific class of modeling 3D images is determined by the tasks set in the TOR for the simulator. After selecting the 3D simulation modeling class, it is necessary to determine the composition of the main nodes of the imitator of the visual situation in order to perform the tasks set in the TOR for the simulator. The article shows one of the possible methods for assessing the main nodes of the visual environment simulator, which allows to determine the composition of the optical-software-hardware simulation complex of the required 3D image for the train simulator of the locomotive driver. The quality of the formation of the information space should ensure the formation of the trainee's professional skills in locomotive management."}, {"label": 0, "content": "This article presents a medical simulation solution. This solution allows a physician to train a clinical scenario by interacting with a graphical interface. The aim is to instigate the learning and internalization of clinical procedures. Currently these resources have been intensifying in the most diverse areas, being our focus, Medicine. Within this area, the focus is on medical simulation. There are numerous biomedical simulation centers, whose main objective is to create realistic simulations to aid health professionals. Thus, it is intended to optimize its performance, to meet the needs detected and to anticipate unexpected situations (critical or complex events). However, current simulation systems face some limitations, since they have enough difficulties in the development of new scenarios, since they are restricted to the level of modularity and the number of simulated situations. The training of these professionals is limited to simulation centers. The goal is to create a platform to simulate real scenarios and develop serious games that simulate various clinical situations, in order to facilitate access to this type of training and training."}, {"label": 0, "content": "Time-Step method and Station-Pair method are prominent techniques for estimation of spatial gradients. Since GBAS is meant to serve a limited area of about 50km of an airport for aircraft Precision Approach and landing, these two methods were considered for gradient estimation within the GBAS service area. Much of the work on these techniques has been reported for GPS-based GBAS applications. In this paper, the suitability of these methods to IRNSS-based GBAS applications is investigated. It is observed that since IRNSS satellites are either GEO or GSO, the time interval (At), of Time-Step method should be significantly high (30min for GSO), to obtain gradient data for GBAS' service area. With the Station-Pair method, a dense network of stations, each separated by not more than 1-2 kms is required."}, {"label": 0, "content": "Rapid development of internet and network technologies has led to considerable increase in number of attacks. Intrusion detection system is one of the important ways to achieve high security in computer networks. However, it have curse of dimensionality which tends to increase time complexity and decrease resource utilization. To improve the ability of detecting anomaly intrusions, a combined algorithm is proposed based on Weighted Fuzzy C-Mean Clustering Algorithm (WFCM) and Fuzzy logic. Decision making is performed in two stages. In the first stage, WFCM algorithm is applied to reduce the input data space. The reduced dataset is then fed to Fuzzy Logic scheme to build the fuzzy sets, membership function and the rules that decide whether an instance represents an anomaly or not."}, {"label": 0, "content": "We propose a system of electric drive control of the coal miner, which allows to increase the reliability and safety of its operation. The problem is solved by predicting the diagnosed parameters of the power elements and preemptive control actions on the control system of the electric drive and the organization of maintenance and repair. Forecasting values of drive parameters is carried out using a neural network model, and allows to increase the mean time between failures and availability of the electric drive compared with the control system without prediction parameters."}, {"label": 0, "content": "In article describes an intellectual-information system that will be used to make managerial decisions on the operation of technical personnel of power plants and similar process facilities. In particular, the results of its work will help to make a decision on the appropriateness of upgrading the skills of technical personnel. The methods of intellectualization used to determine the level of knowledge based on odd logic allow not only to assess the level of knowledge, but also to automate the process of its consolidation and enhancement. The use of this method allows the training of technical personnel on the job. The method of intellectualization of an estimation of a level of knowledge of the basic theory of calculation of optimum feeding parameters of regulating devices of automatic control systems is described. Solving this problem in the example of automation evaluation on the parametric synthesis of one-loop, cascade and combined control systems. Control of the level of knowledge for parametric synthesis determined by the correct choice of a point on a graph of the amplitude-phase characteristics and D-partition graphs. The adequacy of established fuzzy output system tested in a program FuzzyTECH."}, {"label": 0, "content": "In this paper, we propose a method for resolving simple signals based on the method of dividing the spectra, which makes it possible, by calculating the phase of the received signal, to specify the moment of recording the signal at the receiver. It is shown that the implementation of this method in primary data processing systems of interferometric sonar leads to an increase in the accuracy of determining the spatial position of the object at the bottom. An estimation of the resolution of the chirp signals is made, on the basis of which it was concluded that it is possible to determine the signal fixing time by linear correlation method no worse than one sampling, at a sampling frequency lower than the value of the central carrier frequency. The method of compression and recovery of simple signals used in side-scan sonar is considered. The article also contains the results of modeling the dependence of the error in determining the spatial position of the object at the bottom of the angle of rotation of the base of the interferometric sonar of the side survey, the duration of the probing premise, and the frequency of the discretization of the recorded response."}, {"label": 0, "content": "An emerging issue of GDPR (General Data Protection Regulation) and other like similar concept is facing now and future on all intellectual data base. The most secured situations are on standalone operations for data handling during the certain processing, however it is difficult to get relatively higher performance processing such as AI with deep learning. One development approach will present here for those purpose. Deep learning is consisted by the two functions which one is trainings by big data bases and other is making inferences for the objected needs. The inferences are provided by simple hardware for the processing that can make standalone module. On the other hand, training processing is structured by huge repeating calculations to require heavy hardware or cloud interface, consequent hard to produce mobile feature meant standalone module. Our development approach is with two novel functions to resolve heavy training processing. One problem would be how to access with high speed from SSD storage. Another problem is how to reduce power and calculation time by huge repeating processing. Our approach is based by lookup table (LUT) subsequent zero calculation to enable power reduction and shorten calculation-time. The architecture is with the dynamic reconfiguration by Memory Logic Conjugated System (MLCS) even easy realized for high speed execution that is a non-Neumann processor. In the conclusion, our trial demonstration module has the 1W of effective power and 400Mops of the processing performance in a commercial based right FPGA evaluation board. If it would be arranged by custom designed SoC, that would suppose to be 0.5W and 2Gops. Both cases are perfectly enough for the standalone modules for middle range of DL execution with seamless training and inference operations on real time sequences. The operation with dynamic reconfiguration is furthermore on the performance that has not been realization in any processing systems so far."}, {"label": 0, "content": "Facial expression and emotions recognition had attracted the attention of game developers and there are some game proposals, in which, by detecting faces, it is possible to detect who is playing and automatically load the players' profile. Likewise, the recognition of the players' facial expressions has been used to alter the expression of their avatars. In this work, we intended to study ways to use the recognition of a player's facial expression to change some parameters of a game, such as stimulate it when the player is bored or increasing its difficulty, for example, when it detects that the player is more stimulated and happy. This paper proposes a new approach for facial expression and emotion recognition, which divides the classification stage into two steps. First it is detected if it is a positive, a negative or a neutral emotion. Then it is applied a refinement step to recognize the particular positive (happiness or surprise) or negative (sadness, fear, anger or disgust) emotion. Our approach runs in real-time and was compared with the most common classification methods and has shown a great accuracy. We built a game to test our approach. The estimation of the player's emotional state was used to change certain parameters of the game, which increases or decreases the game difficulty."}, {"label": 0, "content": "Unified Communications (UC) is revolutionizing next generation enterprise networks by allowing human voice and video to travel over existing packet data networks along with UC services such as video teleconferencing (VTC), unified messaging, and chat. The ultimate goal for UC is an integrated, fully-converged, cloud-based architecture. The implementation of this technology brings new forensic challenges to network investigators. Recently, a cloud service delivery model known as UC as a Service (UCaaS) is gaining momentum in the field of Information Technology. This paper discusses the main challenges associated with forensic investigations in an UC cloud environment. Further, this research attempts to improve UC security by proposing two semi-formal patterns in order to create a Cloud Forensic Model. These patterns provide a systematic approach to network forensic collection and analysis of digital evidence in UCaaS architectures. The proposed cloud forensic framework will allow network investigators to specify, analyze and implement network forensic investigations for technologies under the UC umbrella."}, {"label": 0, "content": "The paper analyses using various models of fuzzy implication in 3-axis CNC milling machine with an autonomous cooling system of a cutting tool. The authors discuss the following models: Mamdani (minimum) product, product (Prod), Einstein product and the bounded difference. Simulation has showed that the best result is obtained using the operator of the fuzzy implication Mamdani (minimum). The authors have also carried out a numerical simulation of the fuzzy control system by the methods of the center of gravity and the difference in areas. The paper describes how the 3-axis CNC milling machine with autonomous cooling system of the cutting tool is connected. A fuzzy-logical system of controlling the parameters of the machine and the cooling system are used for intellectualizing the processes."}, {"label": 0, "content": "Computer networks are ubiquitous and growing exponentially, with a predicted 50 billion devices connected by 2050. This tremendous growth dramatically increases the attack surface of both private and public networks. These attacks often influence the behaviour of the system, leading to the detection of the attack. In this manuscript we model the path of an attack through the network by graphs. The model developed aims to better integer attackers intentions. Using the data produced by 5 honeypots, we apply our model. The preliminary results show that the approach is useful to rapidly detect anomalies in the experiment dataset."}, {"label": 0, "content": "The paper represents the model of AC drive control system. An analysis of conventional drive control system has been performed, its advantages and disadvantages, and preconditions for use of intelligent approaches for implementation of control systems for this kind of objects have been estimated. The intelligent system is supposed to be modeled with use of fuzzy logic controller and different inference algorithms for implementation of control laws in various control loops. Dynamic characteristics of the system both for conventional case and with use of controllers based on fuzzy set theory have been analyzed. The practicability of using the multi-cascade fuzzy systems while implementation of unified intelligent control module and different combinations of fuzzy inference algorithms has been demonstrated for complex drive systems. Application of this technology allows implementation of control system for the entire class of drive systems considering all special aspects and relationships between the coordinates in the complex control object. Additionally, modeling of spacial membership function using multi-cascade fuzzy controller will give a possibility to avoid a variety of quantitative and qualitative restrictions due to complex relations between the coordinate in such systems."}, {"label": 0, "content": "Alloy supports reasoning about software designs in early development stages. It is composed of a modelling language and a tool that is able to find valid instances of the model. Alloy is able to produce graphical representations of analysis results, which is essential for their interpretation. In previous work we have improved the representations with the usage of layout managers. Here, we further extend that work by presenting the improvements on the approach, and by introducing a new case study to analyse the contribution of layout managers, and to support validation trough a user study."}, {"label": 0, "content": "Internet of Things (IoT) systems are slowly but steadily becoming part of different aspects of our lives, with their applications ranging from smart homes, to wearable devices, to healthcare, etc. Traditional cryptographic schemes might not be suitable to be implemented on resource limited IoT devices. The decision to utilize a certain cryptographic algorithm, is mainly based on a tradeoff between security and performance, i.e. power consumption. A benchmark of these different cryptographic algorithms on IoT platforms is a must for security architects while designing their protocols and schemes. This paper presents a benchmark of the most known cryptographic algorithms on the Raspberry Pi platform, with a comparison with Arduino benchmark results provided in the literature."}, {"label": 0, "content": "The paper presents a wavelet-fuzzy controller of a field-oriented control system of belt conveyor drive. A discrete wavelet transform is used to decompose the speed error signal into various frequency components. The transformed error coefficients and scaling coefficients are used to generate the motor control signal The proposed controller is satisfied the speed control problem for a closed control system. The field-oriented control scheme with a wavelet-fuzzy controller under various dynamic operating conditions was investigated. The results of the comparison of the work of the traditional PI controller and the proposed wavelet-fuzzy controller are presented. The simulation results demonstrate the effectiveness of the proposed wavelet-fuzzy controller. Application of the proposed approach of the wavelet-fuzzy controller allows reducing of electromagnetic moment vibrations."}, {"label": 0, "content": "The statistical delay of a path is traditionally modeled as a Gaussian random variable assuming that the path is always sensitized by a test pattern. Its sensitization in various circuit instances varies among its test patterns and the pattern induced delay is non-Gaussian. It is modeled using probability mass functions. The defect coverage is improved by test pattern selection using machine learning. Experimental results demonstrate accuracy in defect coverage when comparing to existing methods."}, {"label": 0, "content": "With the growing usage of credit card transactions, financial fraud crimes have also been drastically increased leading to the loss of huge amounts in the finance industry. Having an efficient fraud detection method has become a necessity for all banks in order to minimize such losses. In fact, credit card fraud detection system involves a major challenge: the credit card fraud data sets are highly imbalanced since the number of fraudulent transactions is much smaller than the legitimate ones. Thus, many of traditional classifiers often fail to detect minority class objects for these skewed data sets. This paper aims first: to enhance classified performance of the minority of credit card fraud instances in the imbalanced data set, for that we propose a sampling method based on the K-means clustering and the genetic algorithm. We used K-means algorithm to cluster and group the minority kind of sample, and in each cluster we use the genetic algorithm to gain the new samples and construct an accurate fraud detection classifier."}, {"label": 0, "content": "As the scale of high performance computing facilities approaches the exascale era, gaining a detailed understanding of hardware failures becomes important. In particular, the extreme memory capacity of modern supercomputers means that data corruption errors which were statistically negligible at smaller scales will become more prevalent. In order to understand hardware faults and mitigate their adverse effects on exascale workloads, we must learn from the behavior of current hardware. In this work, we investigate the predictability of DRAM errors using field data from two recently decommissioned supercomputers: Cielo, at Los Alamos National Laboratory, and Hopper, at Lawrence Berkeley National Laboratory. Due to the volume and complexity of the field data, we apply statistical machine learning to predict the probability of DRAM errors at previously un-accessed locations. We compare the predictive performance of six machine learning algorithms, and find that a model incorporating physical knowledge of DRAM spatial structure outperforms purely statistical methods. Our findings both support expected physical behavior of DRAM hardware as well as providing a mechanism for real-time error prediction. We demonstrate real-world feasibility by training an error model on one supercomputer and effectively predicting errors on another. Our methods demonstrate the importance of spatial locality over temporal locality in DRAM errors, and show that relatively simple statistical models are effective at predicting future errors based on historical data, allowing proactive error mitigation."}, {"label": 0, "content": "Chaotic systems, such as Lorenz systems or logistic functions, are known for their rapid divergence property. Even the smallest change in the initial condition will lead to vastly different outputs. This property renders the short-term behavior, i.e., output values, of these systems very hard to predict. Because of this divergence feature, lorenz systems are often used in cryptographic applications, particularly in key agreement protocols and encryptions. Yet, these chaotic systems do exhibit long-term deterministic behaviors-i.e., fit into a known shape over time. In this work, we propose a fast dynamic device authentication scheme that leverages both the divergence and convergence features of the Lorenz systems. In the scheme, a device proves its legitimacy by showing authentication tags belonging to a predetermined trajectory of a given Lorenz chaotic system. The security of the proposed technique resides in the fact that the short-range function output values are hard for an attacker to predict, but easy for a verifier to validate because the function is deterministic. In addition, in a multi-verifier scenario such as a mobile phone switching among base stations, the device does not have to re-initiate a separate authentication procedure each time. Instead, it just needs to prove the consistency of its chaotic behavior in an iterative manner, making the procedure very efficient in terms of execution time and computing resources."}, {"label": 0, "content": "Cloud computing provides on-demand access to IT resources via the Internet. Permissions for these resources are defined by expressive access control policies. This paper presents a formalization of the Amazon Web Services (AWS) policy language and a corresponding analysis tool, called ZELKOVA, for verifying policy properties. ZELKOVA encodes the semantics of policies into SMT, compares behaviors, and verifies properties. It provides users a sound mechanism to detect misconfigurations of their policies. ZELKOVA solves a PSPACE-complete problem and is invoked many millions of times daily."}, {"label": 0, "content": "We revisit the two main SAT-based algorithms for checking liveness properties of finite-state transition systems: the k-LIVENESS algorithm of [1] and the FAIR algorithm of [2]. These approaches are fundamentally different. k-LIVENESS works by translating the liveness property together with fairness constraints to the form F Gq, and then bounding the number of times the variable q can evaluate to false. FAIR works by finding an over-approximation R of reachable states, so that no state in R is contained on a fair cycle. Each technique has unique strengths on different problems. In this paper, we present a new algorithm k-FAIR that builds upon both techniques, synergistically leveraging their strengths. Experiments demonstrate that this combined approach is stronger than running both in parallel."}, {"label": 0, "content": "Formal verification of arithmetic circuits checks whether or not a gate-level circuit correctly implements a given specification model. In cases where this equivalence check fails - the presence of a bug is detected - it is required to: i) debug the circuit, ii) identify a set of nets (signals) where the circuit might be rectified, and iii) compute the corresponding rectification functions at those locations. This paper addresses the problem of post-verification debugging and correction (rectification) of finite field arithmetic circuits. The specification model and the circuit implementation may differ at any number of inputs. We present techniques that determine whether the circuit can be rectified at one particular net (gate output) - i.e. we address single-fix rectification.Starting from an equivalence checking setup modeled as a polynomial ideal membership test, we analyze the ideal membership residue to identify potential single-fix rectification locations. Subsequently, we use Nullstellensatz principles to ascertain if indeed a single-fix rectification can be applied at any of these locations. If a single-fix rectification exists, we derive a rectification function by modeling it as the synthesis of an unknown component problem. Our approach is based upon the Gr\u00f6bner basis algorithm, which we use both as a decision procedure (for rectification test) as well as a quantification procedure (for computing a rectification function). Experiments are performed over various finite field arithmetic circuits that demonstrate the efficacy of our approach, whereas SAT-based approaches are infeasible."}, {"label": 0, "content": "This paper addresses the problem of vehicular localization on the road and proposes a stochastic solution that leverages vehicle-to-vehicle communication as well as the knowledge that vehicles acquire regarding their approximate locations. Such knowledge is inferred from generated GPS readings together with distance measurements calculated using the beacons broadcasted periodically by other neighboring vehicles. Furthermore, the proposed solution methodology also adopts the locations of stationary RoadSide Units (RSUs) as fixed reference points that help in determining the locations of vehicles whenever these vehicles navigate through the RSUs' coverage ranges. It is shown here that the additional position measurements received from neighboring vehicles lead to a remarkably accurate estimate of the position of a certain target vehicle. An analytical framework is established in this paper with the objective of formulating the target vehicle's position estimate problem using particle filters. The validity, reliability and accuracy of the presented mathematical formulae are verified through extensive simulations using a combination of the Network Simulator (NS-3) and the Simulation for Urban MObility (SUMO) that were used to generate realistic vehicular mobility traces."}, {"label": 0, "content": "Deploying new base stations is being used as an effective solution to satisfy the tremendous increase in mobile traffic. However, this solution is costly due to temporal and spatial variation of traffic, where mobile network operators need to deploy base stations that may work for limited periods of the day. Due to its high flexibility, low deployment time and possibility to be used in more than one location, the usage of drone base stations is a promising alternative that can reduce the operational cost of the network while providing higher bitrates compared to traditional approach. In this paper, we study the impact of using drone base stations on the performance of a cellular network. We consider a heterogeneous architecture composed of a macro-base station overlaid with base stations serving small cells. For the small cells, we compare two scenarios; the first corresponds to a specific number of fixed small base stations while the second corresponds to variable number of drone base stations serving small cells. Results show that a small number of drone base stations can replace large number of fixed ones. Moreover, using smaller number of drone base stations compared to fixed ones increases the average bit rate of the users and capacity per unit of energy up to 75 and 78 percentage points respectively."}, {"label": 0, "content": "In Lebanon, traffic problems are a major concern for the population. The rising number of cars that exceeds the capacity of the roads, the inefficiency of public transportation infrastructures and the non-adaptive traffic light systems are contributors to the traffic crisis. Most roads in Lebanon suffer from traffic jams due to the traditional static green and red times allocations that are inconsiderate to the current state of the traffic. A solution to this problem is a system that adapts to the variations of the traffic dynamically and updates the traffic signal phases accordingly. In this paper, an adaptive traffic light system is implemented using reinforcement learning and tested using real data from Lebanese traffic. For training and testing the system, a software simulation tool is used. This tool can simulate the traffic intersection and allows the neural network to interact with it. Compared with the actual traffic light system, the proposed model displayed a reduction in average queue lengths by 62.82% and in average queuing time by 56.37%."}, {"label": 0, "content": "This paper introduces a new imitation of neurons cells, based on the latest discoveries in neuroscience. After reobserving the latest revelations in the field of biological neurons, the conventional artificial models has proven strong potentials in image processing and pattern classification, but remains far from presenting a modern imitation of natural intelligent organisms. A Biomimetic cell design is thus proposed with a combination of registers to hold the inputs, outputs, and weights as information codes. These cells use binary equivalence gates to compare the inputs to the weights and deliver the required outputs. The abstraction model provided renders the training process highly simplistic, which speeds up the design phase and opens the way towards a new dimension in artificial intelligence."}, {"label": 0, "content": "In this paper, we propose a solution to simplify reading and understanding medical documents via the automatic demystification of complex medical terms found in web pages. The suggested approach detects those terms using a combination of NLP and heuristics. It computes the probability that a word is a medical and complex term through text processing and analysis. It then makes use of an ad hoc dictionary to simplify the complex terms. Finally, it exploits the Microsoft Bing cognitive API to retrieve images and videos related to the complex medical terms detected and presents them to the users so that they may have a better understanding of the document being read."}, {"label": 0, "content": "In this paper, we present a method that utilizes computer vision, specifically projective geometry, to map a known distribution of points on a sphere - with known diameter - along with an arbitrary image of these points on an image plane to identify the configuration of the camera. In other words, knowing the sets of 2D-3D corresponding points, one can extract the camera matrix and dissect it into parameters of interest: intrinsics and extrinsics. The method that is validated by code shows in detail how to setup a theoretical world and camera coordinate frame, and then through the knowledge of the correspondence, displays the solution to the optimization problem. The results are then analyzed noting the relative error between the retrieved and actual camera matrices."}, {"label": 0, "content": "5G is the next wireless technology that is expected to be launched in 2020. Electromagnetic radiations emitted by mobile phones resulted in considerably higher brain tissue exposure than other radiation sources in the radiofrequency band, which led to concerns about the possible potential health effects from exposure to Radio Frequency (RF) and Millimeter (mm) wave radiations. This paper investigates the effects of 5G radiations for different frequency candidates on human brain. This has been achieved by using Computer Simulation Technology (CST) software by conducting simulations on Specific Anthropomorphic Mannequin (SAM); a model designed according to different international standards representing the average material properties of the head by calculating the Specific Absorption Rate (SAR, a quantitative measure of interaction mechanisms of radiofrequency radiation with the living systems and expressed in watt per kilogram (W/kg) in order to check whether the resulting exposure is safe or not by comparing it to the safety limit of exposure to high frequency radiations set by different international standards."}, {"label": 0, "content": "Real life systems are mostly nonlinear. Controlling nonlinear systems is accomplished in several ways, mostly linearizing the system which may cause problems regarding the robustness of the controller. The controllers, on the other hand may be linear or nonlinear. This paper investigates the effects of nonlinearities on the robustness of various controllers (linear controllers, such as PID with first order derivative filter, state-feedback with integral control and, CRONE, and nonlinear controllers, such as smoothed first-order integral SMC) with an application on a DC motor. The motor's model includes four types on nonlinearities (voltage saturation, current saturation, dead zone, and backlash deadband). The performance of the different controllers is presented and compared using the nonlinear model, the linearized model, uncertainty in the model, and harmonic load disturbance."}, {"label": 0, "content": "Career direction is a crucial matter not to be undermined in the development of a more efficient generation of the corporate workforce. In order to obtain accurate career direction, one would think of different ways of identifying attributes that would lead to an accurate classification of personality. In this paper, the goal is extracting personality from the use of language. The paper covers all aspects of this process in terms of Text Normalization Techniques, Feature Extraction, Feature Selection, Data Pre-Processing, Data Sampling, Training Predictive Models to predict personality types, validating the results on test data, and finally, and finally,compare the findings with other approaches to personality classification. After having a personality type classified, the process is as simple as matching career paths that are most likely suitable for the user. All these processes combined by experimenting with various approaches to each operation would result in personality attribute classifiers yielding an average of 96% accuracy."}, {"label": 0, "content": "In-memory computing with nanoscale memristive devices such as phase-change memory (PCM) has emerged as an alternative to conventional von Neumann systems to train deep neural networks (DNN) where a synaptic weight is represented by the device conductance. However, PCM devices exhibit temporal evolution of the conductance values referred to as the conductance drift, which poses challenges for maintaining synaptic weights reliably. Based on the mean behavior of 10,000 GST-based PCM devices, we observe that the drift coefficient is dependent on the conductance value. Moreover, we show that PCM drift is re-initialized and the drift history is erased after the application of even partial SET pulses. This is regardless of how much the device has drifted. With models capturing these features, we show that drift has a detrimental impact on training DNNs, but drift resilience can be significantly improved with a recently proposed multi-PCM synaptic architecture."}, {"label": 0, "content": "Hardware realization of scalable neurons and synapses are essential for the implementation of large scale Spiking neural network (SNN). In this paper, first, we propose, a novel transient Joule heating based the leaky-integrate and fire neuron (LIF) in scalable PrMnO3 (PMO) RRAM device experimentally. The Joule-heating based thermal runaway is utilized to achieve rectified linear unit (ReLU) voltage dependence of spiking frequency similar to a typical LIF neuron. Second, the Jouleheating hypothesis in PMO is validated by TCAD DC and transient simulations. PMO is extremely thermally resistive semiconductor (300x cf. Si) and hence enables low energy thermal dynamics. The excellent energy, area performance with a synapse in the same material system and thermal engineering makes PMO neuron attractive. Finally, PMO neuron shows software equivalent learning accuracy in SNN on the Fischer's Iris dataset."}, {"label": 0, "content": "Write Variation is an inevitable non-ideal effects for Resistive Random Access Memory(ReRAM) based On-chip Neural Networks training, which incurs significant accuracy drops and affects the system robustness. In this paper, we have proposed a Hierarchical Crossbar(HC) to achieve a software competitive accuracy and enhanced the system robustness to random effects. The accuracy on MNIST can be enhanced 10% for 2000 iterations on-chip fine-tuning comparing with conventional crossbar."}, {"label": 0, "content": "Accurately analyzing the sources of performance anomalies in cloud-based applications is a hard problem due both to the multi tenant nature of cloud deployment and changing application workloads. To that end many different resource instrumentation and application performance modeling frameworks have been developed in recent years to help in the effective deployment and resource management decisions. Yet, the significant differences among these frameworks in terms of their APIs, their ability to instrument resources at different levels of granularity, and making sense of the collected information make it extremely hard to effectively use these frameworks. Not addressing these complexities can result in operators providing incompatible and incorrect configurations leading to inaccurate diagnosis of performance issues and hence incorrect resource management. To address these challenges, we present UPSARA, a model-driven generative framework that provides an extensible, lightweight and scalable performance monitoring, analysis and testing framework for cloud-hosted applications. UPSARA helps alleviate the accidental complexities in configuring the right resource monitoring and performance testing strategies for the underlying instrumentation frameworks used. We evaluate the effectiveness of UPSARA in the context of representative use cases highlighting its features and benefits."}, {"label": 0, "content": "Bare-metal cloud provides a dedicated set of physical machines (PMs) and enables both PMs and virtual machines (VMs) on the PMs to be scaled in/out dynamically. However, to increase efficiency of the resources and reduce violations of service level agreements (SLAs), resources need to be scaled quickly to adapt to workload changes, which results in high reconfiguration overhead, especially for the PMs. This paper proposes a hierarchical and frequency-aware auto-scaling based on Model Predictive Control, which enable us to achieve an optimal balance between resource efficiency and overhead. Moreover, when performing high-frequency resource control, the proposed technique improves the timing of reconfigurations for the PMs without increasing the number of them, while it increases the reallocations for the VMs to adjust the redundant capacity among the applications; this process improves the resource efficiency. Through trace-based numerical simulations, we demonstrate that when the control frequency is increased to 16 times per hour, the VM insufficiency causing SLA violations is reduced to a minimum of 0.1% per application without increasing the VM pool capacity."}, {"label": 0, "content": "Cloud computing and its computing as a utility paradigm offers on-demand resources, enabling its users to seamlessly adapt applications to the current demand. With its (virtually) unlimited elasticity, managing deployed applications becomes more and more complex raising the need for automation. Such autonomous systems leverage the importance to constantly monitor and analyse the deployed workload and the underlying infrastructure serving as knowledge-base for deriving corrective actions like scaling. Existing monitoring solutions, however are not designed to cope with a frequently changing topology. We propose a monitoring and event processing framework following a model-driven approach, that allows users to express i) the monitoring demand by directly referencing entities of the deployment context, ii) aggregate the monitoring data using mathematical expressions, iii) trigger and process events based on the monitoring data and finally iv) attach scalability rules to those events. We accompany the modelling language with a monitoring orchestration and distributed complex event processing framework, capable of enacting the model in a frequently changing multi-cloud infrastructure, considering cloud-specific aspects like communication costs."}, {"label": 0, "content": "The automated deployment of cloud applications is of vital importance. Therefore, several deployment automation technologies have been developed that enable automatically deploying applications by processing so-called deployment models, which describe the components and relationships an application consists of. However, the creation of such deployment models requires considerable expertise about the technologies and cloud providers used-especially for the technical realization of conceptual architectural decisions. Moreover, deployment models have to be adapted manually if architectural decisions change or technologies need to be replaced, which is time-consuming, error-prone, and requires even more expertise. In this paper, we tackle this issue. We introduce a meta-model for Pattern-based Deployment Models, which enables using cloud patterns as generic, vendor-, and technology-agnostic modeling elements directly in deployment models. Thus, instead of specifying concrete technologies, providers, and their configurations, our approach enables modeling only the abstract concepts represented by patterns that must be adhered to during the deployment. Moreover, we present how these models can be automatically refined to executable deployment models. To validate the practical feasibility of our approach, we present a prototype based on the TOSCA standard and a case study."}, {"label": 0, "content": "Server consolidation and resource elasticity are among two of the most important resource management features in cloud and edge computing. One of two forms of elasticity is often adopted. While horizontal elasticity is concerned with the acquisition and release of computational nodes in accordance with demand, vertical elasticity focuses on the distribution of a node's resources among its hosted virtual machines (VMs) or containers, by adjusting the capacity of the resource types allocated to each individual VM in accordance with its respective application's needs. In the case of vertical elasticity, when insufficient resources are available to allocate to a given VM, its application's performance may suffer degradation. For online applications, the only alternative is to live-migrate the VM to another server. On the other hand, when running batch jobs, the resource-constrained VM could also be suspended or saved to disk and revived elsewhere or on the same host, when resources become available. Given that memory availability has a significant influence on performance and system throughput, this paper investigates the viability of integrating VM migration, pausing and suspension schemes as part of a VM scheduling strategy to support the execution of both online and batch applications in a virtualized infrastructure employing memory elasticity. Results show that combining such schemes can provide utilization benefits for cloud service providers when memory is scarce."}, {"label": 0, "content": "Apache Mesos, a two-level resource scheduler, provides resource sharing across multiple users in a multi-tenant clustered environment. Computational resources (i.e., CPU, memory, disk, etc.) are distributed according to the Dominant Resource Fairness (DRF) policy. Mesos frameworks (users) receive resources based on their current usage and are responsible for scheduling their tasks within the allocation. We have observed that multiple frameworks can cause fairness imbalance in a multi-user environment. For example, a greedy framework consuming more than its fair share of resources can deny resource fairness to others. The user with the least Dominant Share is considered first by the DRF module to get its resource allocation. However, the default DRF implementation, in Apache Mesos' Master allocation module, does not consider the overall resource demands of the tasks in the queue for each user/framework. This lack of awareness can lead to poor performance as users without any pending task may receive more resource offers, and users with a queue of pending tasks can starve due to their high dominant shares. In a multi-tenant environment, the characteristics of frameworks and workloads must be understood by cluster managers to be able to define fairness based on not only resource share but also resource demand and queue wait time. We have developed a policy driven queue manager, Tromino, for an Apache Mesos cluster where tasks for individual frameworks can be scheduled based on each framework's overall resource demands and current resource consumption. Dominant Share and demand awareness of Tromino and scheduling based on these attributes can reduce (1) the impact of unfairness due to a framework specific configuration, and (2) unfair waiting time due to higher resource demand in a pending task queue. In the best case, Tromino can significantly reduce the average waiting time of a framework by using the proposed Demand-DRF aware policy."}, {"label": 0, "content": "Cloud computing can be used to provide a virtualized platform for running various services, including soft real-time applications such as video streaming. To satisfy an application's real-time requirements, CPU resources are often allocated for the worst case, resulting in system under-utilization or overpaying to the cloud provider under the pay-as-you-go model. To solve this problem, we present Pacer, a framework that provides application developers a platform to implement custom virtual machine-level resource allocation algorithms that utilize real-time application-specific performance feedback from applications running in virtual machines. We also present two example resource allocation algorithms for Pacer that are based on additive-increase-multiplicative-decrease and self-tuning PID control. We apply Pacer to video stream object detection applications to show that Pacer can save more than 50% CPU utilization and use CPU resources more efficiently, while still meeting deadlines for real-time applications."}, {"label": 0, "content": "By renting pay-as-you-go cloud resources (e.g., virtual machines) to do science, the data transfers required during the execution of data-intensive scientific workflows may be remarkably costly not only regarding the workflow execution time (makespan) but also regarding money. As such transfers are prone to delays, they may jeopardise the makespan, stretch the period of resource rentals and, as a result, compromise budgets. In this paper, we explore the possibility of trading some communication for computation during the scheduling production, aiming to schedule a workflow by duplicating some computation of its tasks on which other dependent-tasks critically depend upon to lessen communication between them. This paper explores this premise by enhancing the Heterogeneous Earliest Finish Time (HEFT) algorithm and the Lookahead variant of HEFT. The proposed approach is evaluated using simulation and synthetic data from four real-world scientific workflow applications. Our proposal, which is based on task duplication, can effectively reduce the size of data transfers, which, in turn, contributes to shortening the rental duration of the resources, in addition to minimising network traffic within the cloud."}, {"label": 0, "content": "Many algorithms in workflow scheduling and resource provisioning rely on the performance estimation of tasks to produce a scheduling plan. A profiler that is capable of modeling the execution of tasks and predicting their runtime accurately, therefore, becomes an essential part of any Workflow Management System (WMS). With the emergence of multi-tenant Workflow as a Service (WaaS) platforms that use clouds for deploying scientific workflows, task runtime prediction becomes more challenging because it requires the processing of a significant amount of data in a near real-time scenario while dealing with the performance variability of cloud resources. Hence, relying on methods such as profiling tasks' execution data using basic statistical description (e.g., mean, standard deviation) or batch offline regression techniques to estimate the runtime may not be suitable for such environments. In this paper, we propose an online incremental learning approach to predict the runtime of tasks in scientific workflows in clouds. To improve the performance of the predictions, we harness fine-grained resources monitoring data in the form of time-series records of CPU utilization, memory usage, and I/O activities that are reflecting the unique characteristics of a task's execution. We compare our solution to a state-of-the-art approach that exploits the resources monitoring data based on regression machine learning technique. From our experiments, the proposed strategy improves the performance, in terms of the error, up to 29.89%, compared to the state-of-the-art solutions."}, {"label": 0, "content": "Energy consumption constitutes a significant proportion of data centers' operational costs. Furthermore, the establishment of large scale Cloud data centers due to the fast growth of utility-based IT services made the energy usage of data centers a concern. Cloud data centers use load balancing algorithms to allocate their physical resources (CPU, memory, hard disk, network bandwidth) efficiently on demand and hence optimize their energy consumption. In the load balancing process, some Virtual Machines (VMs) are selected from over-or under-utilized physical hosts and these VMs are migrated, while live and running, to other hosts. This live migration can result in Service Level Agreement Violations (SLAVs) and consequently low Quality of Service (QoS). Thus, in this paper, we propose an energy aware VM selection policy to minimize the number of migrations and consequently decrease SLAVs. Load balancing has three stages: a) Detecting over-and under-utilized hosts; b) Selecting one or more VMs for migration from those hosts; c) Finding destination hosts for the selected VMs. The focus of this research is on the VM selection stage of CPU load balancing. Our proposed VM selection algorithm considers CPU utilization of the VMs on each host and any linear correlation between the CPU usage of the VMs. The algorithm was evaluated on two different real Cloud data sets provided by the CoMon project and Google. Its performance was compared to our benchmark policy that only considers minimum migration time for VM selection. The results showed that our proposed algorithm decreases SLAVs by 66%, ESV (SLAVs \u00d7 energy consumption) by 64% and the number of \"re over-utilized\" hosts by 81% when the CPU usage of VMs in a data set are highly correlated (e.g., as in the Google data set)."}, {"label": 0, "content": "The large-scale virtualized data centers in the Cloud environment consume huge amount of energy leading to high operational costs and emission of greenhouse gases. Energy consumption of a data center can be reduced by dynamically consolidating the virtual machines (VMs) to a minimum number of physical machines, using live migration. However, the dynamic workload of virtual machines makes the VM consolidation problem more challenging. In this paper, we have proposed a prediction based migration technique for the VMs, where we perform VM migrations based on the predicted CPU utilization. Extensive simulations show that the proposed technique substantially reduces energy consumption, number of VM migrations and Service Level Agreement (SLA) violations within a data center. The performance overheads associated with excessive migration of VMs increase the time needed by the VMs to complete their jobs. So in this paper, we have also proposed a deadline aware VM migration technique, which reduces the time taken by the VMs to execute their jobs significantly, thereby improving the Quality of Service (QoS). Such improvement in QoS is achieved at the cost of slight increase in the energy consumption within the data center. However, simulation results show that appropriate setting of deadlines for the VMs, helps in achieving a trade-off between energy consumption and the QoS."}, {"label": 0, "content": "Virtualization is one of the key enabler technologies of cloud computing in providing on-demand sharing of computing resources. Virtualization requires mechanisms and algorithms for virtual resource allocation, virtual machine deployment, migration, and servers consolidation. Most of the existing studies have only focused on how to solve the problem of virtual resource allocation among servers. However, as cloud servers with multi-core architectures become popular, the virtual machine resource allocation in a single server becomes a critical challenge. In this paper, we propose a multi-objective virtual machine placement algorithm by jointly considering energy efficiency and load balancing criteria in a multi-core server with the Network-on-Chip architecture. Our proposed algorithm is based on Markov approximation optimization theory. We perform extensive experiments to evaluate our proposed algorithm. The results show that our proposed algorithm achieves higher energy efficiency, load balancing, and calculation speed compared with the state-of-the-art algorithms."}, {"label": 0, "content": "Estimating energy costs for an industrial process can be computationally intensive and time consuming, especially as it can involve data collection from different (distributed) monitoring sensors. Industrial processes have an implicit complexity involving the use of multiple appliances (devices/ sub-systems) attached to operation schedules, electrical capacity and optimisation setpoints which need to be determined for achieving operational cost objectives. Addressing the complexity associated with an industrial workflow (i.e. range and type of tasks) leads to increased requirements on the computing infrastructure. Such requirements can include achieving execution performance targets per processing unit within a particular size of infrastructure i.e. processing & data storage nodes to complete a computational analysis task within a specific deadline. The use of ensemblebased edge processing is identifed to meet these Quality of Service targets, whereby edge nodes can be used to distribute the computational load across a distributed infrastructure. Rather than relying on a single edge node, we propose the combined use of an ensemble of such nodes to overcome processing, data privacy/ security and reliability constraints. We propose an ensemble-based network processing model to facilitate distributed execution of energy simulations tasks within an industrial process. A scenario based on energy profiling within a fisheries plant is used to illustrate the use of an edge ensemble. The suggested approach is however general in scope and can be used in other similar application domains."}, {"label": 0, "content": "Edge computing is the emerging architectural paradigm extending cloud technologies to the logical extremes of the network for on-demand and delay-sensitive services. However, once service placement on edge-enabling resources has been dealt with, a new challenge arises: how to process enormous volumes of streaming data to provide query-driven analytics while still satisfying the delay-critical servicing requirements. To overcome this challenge we introduce StreamSight, a framework for edge-enabled IoT services which provides a rich and declarative query model abstraction for expressing complex analytics on monitoring data streams and then dynamically compiling these queries into stream processing jobs for continuous execution on distributed processing engines. To overcome the resource restrictive barriers in edge computing deployments, StreamSight outputs the query execution plan so that intermediate results are reused and not continuously recomputed. In turn, StreamSight enables users to express various optimization strategies (e.g., approximate answers, query prioritization) and constraints (e.g., sample size, error-bounds) so that delay-sensitive requirements relevant to their deployment are not violated. We evaluate our framework on Apache Spark with real-world workloads and show that leveraging StreamSight can significantly increase performance by 4x while still satisfying all accuracy guarantees."}, {"label": 0, "content": "As fog computing brings processing and storage resources to the edge of the network, there is an increasing need of automated placement (i.e., host selection) to deploy distributed applications. Such a placement must conform to applications' resource requirements in a heterogeneous fog infrastructure, and deal with the complexity brought by Internet of Things (IoT) applications tied to sensors and actuators. This paper presents four heuristics to address the problem of placing distributed IoT applications in the fog. By combining proposed heuristics, our approach is able to deal with large scale problems, and to efficiently make placement decisions fitting the objective: minimizing placed applications' average response time. The proposed approach is validated through comparative simulation of different heuristic combinations with varying sizes of infrastructures and applications."}, {"label": 0, "content": "Fog computing provides a paradigm for executing Internet of Things services. Enabling the coordinated cooperation among computational, storage, and networking resources in the fog can be challenging due to the volatility of resources. For this reason, we design an architecture and implement a representative framework called FogFrame that defines the necessary communication mechanisms for instantiating and maintaining service execution in the fog. To evaluate our approach, we conduct a series of experiments that show how service placement, deployment, and execution is performed by the framework, and how the framework operates at runtime, i.e., adapts to changes in the available resources, balances the workload and recovers from resource failures and overloads."}, {"label": 0, "content": "In this paper, we address the problem of com-putational and networking virtual resources embedding across multiple Infrastructure-as-a-Service (IaaS) providers. This issue, usually referred to as the Virtual Network Embedding (VNE) problem, requires two phases of operation in such a context: the multicloud virtual network requests (VNRs) splitting, followed by the intracloud VNR segments mapping. This paper focuses on the splitting phase problem, by proposing a splitting strategy based on two optimization approaches, with the objective of improving the performance and the quality of service (QoS) of resulting mapped VNR segments. An Integer Linear Program (ILP) is used to formalize our splitting strategy as a mathematical minimization problem with constraints. The ILP model is first solved with the exact approach. Subsequently, a metaheuristic approach based on the Tabu Search (TS) is proposed in order to find optimal or near-optimal solutions in polynomial solving time. The simulation results obtained show the efficiency of the proposed VNRs splitting approaches according to several performance criteria. Solution costs of the heuristic are on average close to the exact solution, with an average cost gap ranging from 0% to a maximum of 2.05%, performed in a highly reduced computing time. In comparison with other baseline approaches, the acceptance rate and the delay are improved by approximately 15%, while preventing QoS violations."}, {"label": 0, "content": "Combining data from disparate sources enhances the opportunity to explore different aspects of the phenomena under consideration. However, there are several challenges in doing so effectively that include, inter alia, the heterogeneity in data representation and format, collection patterns, and integration of foreign data attributes in a ready-to-use condition. In this study, we have designed a scalable query-oriented distributed data integration framework, Confluence, that also dynamically generates accurate interpolations for the targeted spatiotemporal scopes along with an estimate of the uncertainty involved with such estimation in case of spatiotemporal misalignment of datapoints. Confluence efficiently orchestrates computations to evaluate spatiotemporal query joins and facilitates distributed query evaluations with a dynamic relaxation of query constraints. Query evaluations are locality-aware and we leverage model-based dynamic parameter selection to provide accurate estimation for data points. We have included empirical benchmarks that profile our system in terms of accuracy, latency, and throughput at scale and also demonstrate its improvement in performance in a distributed cloud computing environment over GeoMesa, a Spark-based geospatial analytics framework."}, {"label": 0, "content": "In Infrastructure-as-a-Service (IaaS) clouds, remote users access provided virtual machines (VMs) via the management server. The management server is managed by cloud operators, but not all the cloud operators are trusted in semi-trusted clouds. They can execute arbitrary management commands to users' VMs and redirect users' commands to malicious VMs, which is called the VM redirection attack. The root cause is that the binding of users to VMs is weak. In other words, it is difficult to enforce the execution of only users' management commands to their VMs. In this paper, we propose UVBond for strongly binding users to their VMs to solve this problem. UVBond boots user's VM by decrypting its encrypted disk inside the trusted hypervisor. Then it issues a VM descriptor to securely identify that VM. To bridge the semantic gap between high-level management commands and low-level hypercalls, UVBond uses hypercall automata, which accept the sequences of hypercalls issued by commands. We have implemented UVBond in Xen and confirmed that a VM descriptor and hypercall automata prevented attacks and that the overhead was not large."}, {"label": 0, "content": "The economics of high performance computing are rapidly changing. Commercial cloud offerings, private research clouds, and pressure on the budgets of institutions of higher education and federally-funded research organizations are all contributing factors. As such, it has become a necessity that all expenses and investments be analyzed and considered carefully. In this paper we will analyze the return on investment (ROI) for three different kinds of cyberinfrastructure resources: the eXtreme Science and Engineering Discovery Environment (XSEDE); the NSF-funded Jetstream cloud system; and the Indiana University (IU) Big Red II supercomputer, funded exclusively by IU for use of the IU community and collaborators. We determined the ROI for these three resources by assigning financial values to services by either comparison with commercially available services, or by surveys of value of these resources to their users. In all three cases, the ROI for these very different types of cyberinfrastructure resources was well greater than 1 - meaning that investors are getting more than $1 in returned value for every $1 invested. While there are many ways to measure the value and impact of investment in cyberinfrastructure resources, we are able to quantify the short-term ROI and show that it is a net positive for campuses and the federal government respectively."}, {"label": 0, "content": "In this paper, we propose a multi-cloud marketplace model for Infrastructure-as-a-Service (IaaS) layer with multiple cloud providers, intermediate brokers and end users. The brokers service end users subscribed to them by aggregating resources (virtual machines) from cloud providers while maximizing their profits. Similarly cloud providers (producers) allocate their supply of virtual machines to brokers (consumers) so as to maximize their profits. We define the notion of social welfare in this market structure and study two trading schemes. The first scheme involves centralized control which aims at maximizing social welfare but may contain unstable producer-consumer pairs who have an incentive to deviate from the current allocation. The second scheme eliminates such unstable pairs by using a generalization of stable matching algorithm but may lead to sub-optimal social welfare. The stable matching algorithm we proposed in this paper is a particular way of generalizing the original Gale-Shapley algorithm."}, {"label": 0, "content": "Energy related costs and environmental sustainability present a significant challenge for cloud computing practitioners and the development of next generation data centers. In efficient resource management is one of the greatest causes of high energy consumption in the operation of data centers today. Virtual Machine (VM) placement is a promising technique to save energy and improve resource management. A key challenge for VM placement algorithms is the ability to accurately forecast future resource demands due to the dynamic nature of cloud applications. Furthermore, the literature rarely considers placement strategies based on co-located resource consumption which has the potential to improve allocation decisions. Using real workload traces this work presents a comparative study of the most widely used prediction models and introduces a novel predictive anti-correlated VM placement approach. Our empirical results demonstrate how the proposed approach reduces energy by 18% while also reducing service violations by over 47% compared to some of the most commonly used placement policies."}, {"label": 0, "content": "We consider networks in which each individual link is characterized by two delay parameters: a (usually very conservative) guaranteed upper bound on the worst-case delay, and an estimate of the delay that is typically encountered, across the link. Given a source and destination node on such a network and an upper bound on the end-to-end delay that can be tolerated, the objective is to determine routes they typically experience a small delay, while guaranteeing to respect the specified end-to-end upper bound under all circumstances. We formalize the problem of determining such routes as a shortest-paths problem on graphs, and derive algorithms for solving this problem optimally."}, {"label": 0, "content": "As the density of sensing/computation/actuation nodes is increasing, it becomes more and more feasible and useful to think at an entire network of physical devices as a single, continuous space-time computing machine. The emergent behaviour of the whole software system is then induced by local computations deployed within each node and by the dynamics of the information diffusion. A relevant example of this distribution model is given by aggregate computing and its companion language field calculus, a minimal set of purely functional constructs used to manipulate distributed data structures evolving over space and time, and resulting in robustness to changes. In this paper, we study the convergence time of an archetypal and widely used component of distributed computations expressed in field calculus, called gradient: a fully-distributed estimation of distances over a metric space by a spanning tree. We provide an analytic result linking the quality of the output of a gradient to the amount of computing resources dedicated. The resulting error bounds are then exploited for network design, suggesting an optimal density value taking broadcast interferences into account. Finally, an empirical evaluation is performed validating the theoretical results."}, {"label": 0, "content": "Utilizing intelligent transportation infrastructures can significantly improve the throughput of intersections of Connected Autonomous Vehicles (CAV), where an Intersection Manager (IM) assigns a target velocity to incoming CAVs in order to achieve a high throughput. Since the IM calculates the assigned velocity for a CAV based on the model of the CAV, it's vulnerable to model mismatches and possible external disturbances. As a result, IM must consider a large safety buffer around all CAVs to ensure a safe scheduling, which greatly degrades the throughput. In addition, IM has to assign a relatively lower speed to CAVs that intend to make a turn at the intersection to avoid rollover. This issue reduces the throughput of the intersection even more. In this paper, we propose a space and time-aware technique to manage intersections of CAVs that is robust against external disturbances and model mismatches. In our method, RIM, IM is responsible for assigning a safe Time of Arrival (TOA) and Velocity of Arrival (VOA) to an approaching CAV such that trajectories of CAVs before and inside the intersection does not conflict. Accordingly, CAVs are responsible for determining and tracking an optimal trajectory to reach the intersection at the assigned TOA while driving at VOA. Since CAVs track a position trajectory, the effect of bounded model mismatch and external disturbances can be compensated. In addition, CAVs that intend to make a turn at the intersection do not need to drive at a slow velocity before entering the intersection. Results from conducting experiments on a 1/10 scale intersection of CAVs show that RIM can reduce the position error at the expected TOA by 18X on average in presence of up to 10% model mismatch and an external disturbance with an amplitude of 5% of max range. In total, our technique can achieve 2.7X better throughput on average compared to velocity assignment techniques."}, {"label": 0, "content": "The revolution of deep neural networks (DNNs) is enabling dramatically better autonomy in autonomous driving. However, it is not straightforward to simultaneously achieve both timing predictability (i.e., meeting job latency requirements) and energy efficiency that are essential for any DNN-based autonomous driving system, as they represent two (often) conflicting goals. In this paper, we propose PredJoule, a timing-predictable energy optimization framework for running DNN workloads in a GPU-enabled automotive system. PredJoule achieves both latency guarantees and energy efficiency through a layer-aware design that explores specific performance and energy characteristics of different layers within the same neural network. We implement and evaluate PredJoule on the automotive-specific NVIDIA Jetson TX2 platform for five state-of-the-art DNN models with both high and low variance latency requirements. Experiments show that PredJoule rarely violates job deadlines, and can improve energy by 65% on average compared to five existing approaches and 68% compared to an energy-oriented approach."}, {"label": 0, "content": "Processing and analyzing big data sets updated in real time in an increasing number of applications such as severe weather prediction and particle-physics experiments require the computational power of extreme-scale high-performance computing (HPC) systems. To address the scheduling of massive task/thread sets on these extreme-scale systems, current strategies rely on improving centralized, distributed, and parallel scheduling algorithms as well as virtualization developed for HPC systems which aim to reduce the makespan and balance the load among the computing nodes in these systems. However, these HPC schedulers provide no guarantees on meeting timing constraints such as deadlines that are required in an increasing number of these real-time science workflows. This paper describes a new project which departs from this established trend of best-effort scheduling of large-scale HPC Message Passing Interface (MPI) tasks and ensemble workloads found in fine-grain many-task computing (MTC) applications. The new approach brings real-time scheduling to address the demands of real-time science workloads. This new framework abstracts information about the tasks or threads, and continuously dispatch this workload to meet deadlines and other timing constraints associated with individual tasks or groups of tasks in extreme-scale HPC systems to reduce execution time and energy consumption. This paper introduces deadline-based scheduling in the tasking programming model."}, {"label": 0, "content": "The existing sporadic task model is inadequate for real-time systems to take advantage of Simultaneous Multithreading (SMT), which has been shown to improve performance in many areas of computing, but has seen little application to real-time systems. A new family of task models, collectively referred to as SMART, is introduced. SMART models allow for combining SMT and real time by accounting for the variable task execution costs caused by SMT."}, {"label": 0, "content": "We propose a method for designing software transactional memory that relies on the use of locking protocols to ensure that transactions will never be forced to retry. We discuss our approaches to implementing this method and tunable parameters that may be able to improve schedulability on an application-specific basis."}, {"label": 0, "content": "Machine learning (ML) on edge computing devices is becoming popular in the industry as a means to make control systems more intelligent and autonomous. The new trend is to utilize embedded edge devices, as they boast higher computational power and larger memories than before, to perform ML tasks that had previously been limited to cloud-hosted deployments. In this work, we assess the real-time predictability and consider data privacy concerns by comparing traditional cloud services with edge-based ones for certain data analytics tasks. We identify the subset of ML problems appropriate for edge devices by investigating if they result in real-time predictable services for a set of widely used ML libraries. We specifically enhance the Caffe library to make it more suitable for real-time predictability. We then deploy ML models with high accuracy scores on an embedded system, exposing it to industry sensor data from the field, to demonstrates its efficacy and suitability for real-time processing."}, {"label": 0, "content": "Security of vehicular networks has often been an afterthought since they are designed traditionally to be a closed system. An attack could lead to catastrophic effect which may include loss of human life or severe injury to the driver and passengers of the vehicle. In this paper, we propose a novel algorithm to extract the real-time model of the controller area network (CAN) and develop a specification-based intrusion detection system (IDS) using anomaly-based supervised learning with the real-time model as input. We evaluate IDS performance with real CAN logs collected from a sedan car."}, {"label": 0, "content": "Worst-case timing analysis of Networks-on-Chip (NoCs) is a crucial aspect to design safe real-time systems based on manycore architectures. In this paper, we present some potential extensions of our previously-published buffer-aware worst-case timing analysis approach to cope with bursty traffic such as real-time audio and video streams. A first promising lead is to improve the algorithm analyzing backpressure patterns to capture consecutive-packet queueing effect while keeping the information about the dependencies between flows. Furthermore, the improved algorithm may also decrease the inherent complexity of computing the indirect blocking latency due to backpressure."}, {"label": 0, "content": "The traditional mixed-criticality (MC) model does not allow less critical tasks to execute during an event of the error and exception. Recently, the imprecise MC (IMC) model has been proposed where, even for exceptional events, less critical tasks also receive some amount of (degraded) service, e.g., a task overruns its execution demand. In this work, we present our ongoing effort to extend the IMC model to the precise scheduling of tasks and integrate with the dynamic voltage and frequency scaling (DVFS) scheme to enable energy minimization. Precise scheduling of MC systems is highly challenging because of its requirement to simultaneously guarantee the timing correctness of all tasks under both pessimistic and less pessimistic assumptions. We propose an utilization-based schedulability test and sufficient schedulability conditions for such systems under earliest deadline first with virtual deadline (EDF-VD) scheduling policy. For this unified model, we present a quantitative study in the forms of speedup bound and approximation ratio. Finally, both theoretical and experimental analysis will be conducted to prove the correctness of our algorithm and to demonstrate its effectiveness."}, {"label": 0, "content": "Effective management and provisioning of communication resources is as important in meeting the real-time requirements of smart city cyber physical systems (CPS) as managing computation resources is. The communication infrastructure in Smart cities often involves wireless mesh networks (WMNs). However, enforcing distributed and consistent control in WMNs is challenging since individual routers of a WMN maintain only local knowledge about each of its neighbors, which reflects only a partial visibility of the overall network and hence results in suboptimal resource management decisions. When WMNs must utilize emerging technologies, such as time-sensitive networking (TSN) for the most critical communication needs, e.g., controlling traffic and pedestrian lights, these challenges are further complicated. An attractive solution is to adopt Software Defined Networking (SDN), which offers a centralized, up-to-date view of the entire network by refactoring the wireless protocols into control and forwarding decisions. This paper presents ongoing work to overcome the key challenges and support the end-to-end real-time requirements of smart city CPS applications."}, {"label": 0, "content": "Real-time task scheduling for wireless networked control systems provides guarantees for the quality of service. This paper introduces a new model for joint network and computing resource scheduling (JNCRS) in real-time wireless networked control systems. This new end-to-end real-time task model considers a strict execution order of segments including the sensing, the computing and the actuating segment based on the control loop of WNCSs. The general JNCRS problem is proved to be a NP-hard problem. After dividing the JNCRS problem into four subproblems, we propose a polynomial-time optimal algorithm to solve the first subproblem where each segment has unit execution time, by checking the intervals with 100% network resource utilization and modify the deadlines of tasks. To solve the second subproblem where the computing segment is larger than one unit execution time, we define the new timing parameters of each network segment by taking into account the scheduling of the computing segments. We propose a polynomial-time optimal algorithm to check the intervals with the network resource utilization larger than or equal to 100% and modify the timing parameters of tasks based on these intervals."}, {"label": 0, "content": "This paper presents a generic proof of Typical Worst-Case Analysis (TWCA), an analysis technique for weakly-hard real-time uniprocessor systems. TWCA was originally introduced for systems with fixed priority preemptive (FPP) schedulers and has since been extended to fixed-priority nonpreemptive (FPNP) and earliest-deadline-first (EDF) schedulers. Our generic analysis is based on an abstract model that characterizes the exact properties needed to make TWCA applicable to any system model. Our results are formalized and checked using the Coq proof assistant along with the Prosa schedulability analysis library. Our experience with formalizing real-time systems analyses shows that this is not only a way to increase confidence in our claimed results: The discipline required to obtain machine checked proofs helps understanding the exact assumptions required by a given analysis, its key intermediate steps and how this analysis can be generalized."}, {"label": 0, "content": "Supporting real-time communications over Wireless networks (WSNs) is a tough challenge, due to packet collisions and the non-determinism of common channel access schemes like CSMA/CA. Real-time WSN communication is even more problematic in the general case of multi-hop mesh networks. For this reason, many real-time WSN solutions are limited to simple topologies, such as star networks. We propose a real-time multi-hop WSN MAC protocol built atop the IEEE 802.15.4 physical layer. By relying on precise clock synchronization and constructive interference-based flooding, the proposed MAC builds a centralized TDMA schedule, supporting multi-hop mesh networks. The real-time multi-hop communication model is connection-oriented, using guaranteed time slots, ad enables point-to-point communications also with redundant paths. The protocol has been implemented in simulation using OMNeT++, and the performance has been verified in a real-world deployment using Wandstem WSN nodes."}, {"label": 0, "content": "The scheduling of mixed-criticality (MC) systems with graceful degradation is considered, where LO-criticality tasks are guaranteed some service in HI mode in the form of minimum cumulative completion rates. First, we present an easy to implement admission-control procedure to determine which LO-criticality jobs to complete in HI mode. Then, we propose a demand-bound-function-based MC schedulability test that runs in pseudo-polynomial time for such systems under EDF-VD scheduling, wherein two virtual deadline setting heuristics are considered. Furthermore, we discuss a mechanism for the system to switch back from HI to LO mode and quantify the maximum time duration such recovery process would take. Finally, we show the effectiveness of our proposed method by experimental evaluation in comparison to state-of-the-art MC schedulers."}, {"label": 0, "content": "Engine-triggered tasks are real-time tasks that are released when the crankshaft in an engine completes a rotation, which depends on the angular speed and acceleration of the crankshaft itself. In addition, the execution time of an engine-triggered task depends on the speed of the crankshaft. Tasks whose execution times depend on a variable period are referred to as adaptive-variable rate (AVR) tasks. Existing techniques to calculate the worst-case demand of AVR tasks are either inexact or computationally intractable. In this paper, we transform the problem of finding the worst-case demand of AVR tasks over a given time interval into a variant of the knapsack problem to efficiently find the exact solution. We then propose a framework to systematically reduce the search space associated with finding the worst-case demand of AVR tasks. Experimental results reveal that our approach is at least 10 times faster, with an average runtime improvement of 146 times, for randomly generated tasksets when compared to the state-of-the-art technique."}, {"label": 0, "content": "The interest in neural networks has increased significantly, and the application of this type of machine learning is vast, ranging from natural image classification to medical image segmentation. However, many users of neural networks tend to use them as a black box tool. They do not access all of the possible variations, nor take into account the respective classification accuracies and costs. In our work, we focus on multiclass image classification, and in this research, we shed light on the trade-offs between systems using a single multiclass classification and multiple binary classifiers, respectively. We have tested these classifiers on several modern neural network architectures, including DenseNet, Inception v3, Inception ResNet v2, Xception, NASNet and MobileNet. We have compared several aspects of the performance of these architectures during training and testing using both classification styles in terms of classification speed and several classification accuracy metrics. Here, we present the results from experiments on a total of 99 networks: 11 multiclass and 88 individual binary networks, for an 8-class classification of medical images. In short, using multiple binary classification networks resulted in a more robust model (less variance) for the task at hand. However, on average, such a multi-network style performed the classification 7.6 times slower compared to a single network multiclass implementation. These collective findings show that both approaches can be applied to modern neural network structures. Several binary networks can often give more robust and increased classification accuracy, but at the cost of classification speed and resources consumption."}, {"label": 0, "content": "Iris segmentation is a critical part in iris recognition systems. It segments the acquired image into iris and non-iris parts. It is the foundation of subsequent processing. The errors in this stage are propagated to subsequent processing stages, which will affecting the recognition rate of the whole system. A majority of iris segmentation algorithms require a significant amount of user cooperation during image acquisition process to provide good segmentation performance. However, the quality of iris images can not be guaranteed. When an iris image is acquired under non-ideal conditions (e.g., bad illumination, uncooperative subject, occluded iris, etc.), segmentation becomes a challenging task. In this paper, we present a more robust iris segmentation method using fully convolutional network (FCN) with dilated convolutions. We reduce the downsampling factor of the FCN model, and use the dilated convolutions to extract the more global features, which makes our method better at dealing with details. Moreover, our model supports end to end prediction, it does not need any pre-processing, such as adjusting the image to a fixed size. We used three datasets for training and testing, including CASIA-iris-interval-v4, UBIRIS v2 and IITD Delhi datasets. Experiments show that our model greatly reduced the error rate of the current state-of-the-arts by 79%, 84% and 79% on the CASIA-iris-interval-v4, IITD Delhi and UBIRIS v2 datasets respectively."}, {"label": 0, "content": "A key challenge in renal diagnosis using digital pathology has been the scarcity of reliable annotated datasets that can act as a benchmark for histological investigations. This paper uses a novel medical image dataset, titled Glomeruli Classification Database (GCDB), consisting of renal glomeruli images bifurcated into binary classes of normal and abnormal morphology. Based on this dataset, we direct our pioneering efforts to explore suitable deep neural network techniques related to kidney tissue slide imaging so as to establish a state of the art in this relatively unexplored domain. The paper focuses on classifying normal and abnormal categories of glomeruli which are the vital blood filtration units of the kidney. The results obtained using publicly available transfer learning models are held in comparison with supervised classifiers configured with image features extracted from the last layers of pre-trained image classifiers. Contrary to popular belief, transfer learning models such as ResNet50 and InceptionV3 are empirically proved to under-perform for this particular task whereas the Logistic Regression model augmented with features from the InceptionResNetV2 show the most promising results on the GCDB dataset."}, {"label": 0, "content": "Appearance of a small round or oval shaped in a Computed Tomography (CT) scan of lung is an alarm to suspicion of lung cancer. In order to avoid the misdiagnose of lung cancer at early stage, Computer Aided Diagnosis (CAD) assists oncologists to classify pulmonary nodules as malignant (cancerous) or benign (noncancerous). This paper introduces a novel approach for pulmonary nodules classification employing three accumulated views (top, front, and side) of CT slices and Canonical Correlation Analysis (CCA). Nodule is extracted from 2D CT slice to obtain the Region of Interest (ROI) patch. All patches from sequential slices are accumulated from three different views. Vector representation of each view is correlated with two training sets, malignant and benign sets, employing CCA in spatial and Radon Transform (RT) domain. According to the correlation coefficients, each view is classified and the final classification decision is taken based on the priority decision. For training and testing, 1010 patients are downloaded from Lung Image Database Consortium (LIDC). The final results show that the proposed method achieved the best performance with an accuracy of 90.93% compared with existing methods."}, {"label": 0, "content": "This paper presents a novel contrast measure for MSER region selection, termed Mean Intensity Difference (MID). The proposed metric is computed between the pixels of an MSER region and its surrounding pixel set. In this work we consider the complementary pixels within the bounding box of a region and alternatively the pixels of the first contour layer as surroundings. To evaluate the proposed contrast metric, a location retrieval task is performed. To this end, SURF descriptors are computed and the Bag-of-Words representation is used as global signature for each image. For the evaluation we use the Devon Island dataset, which is said to have one of the most Mars-like environments on Earth and which comes with GPS ground-truth data. We further integrate the contrast-based methods with the approach of Grid Adaptation. The experimental results show that our contrast metric outperforms state-of-the-art metrics, such as Perceptual Divergence, and yields better performance compared to random region selection. In this work we also evaluate the computational complexity of the methods."}, {"label": 0, "content": "The number of food photos posted to the Web has been increasing. Most of the users prefer to post delicious-looking food photos. They, however, do not always look delicious. A previous work proposed a method for estimating the attractiveness of food photos, that is, the degree of how much a food photo looks delicious, as an assistive technology for taking a delicious-looking food photo. This method extracted image features from the entire food photo to evaluate the impression. In our work, we conduct a preference experiment where subjects are asked to compare a pair of food photos and measure their gaze. The proposed method extracts image features from local regions selected based on the gaze information and estimates the attractiveness of a food photo by learning regression parameters. Experimental results showed the effectiveness of extracting image features from outside the gaze regions rather than inside them."}, {"label": 0, "content": "The increasing trend of using online platforms for real estate rent/sale makes automatic retrieval of similar floor plans a key requirement to help architects and buyers alike. Although sketch based image retrieval has been explored in the multimedia community, the problem of hand-drawn floor plan retrieval has been less researched in the past. In this paper, we propose REXplore (Real Estate eXplore), a novel framework that uses sketch based query mode to retrieve corresponding similar floor plan images from a repository using Cyclic Generative Adversarial Networks (Cyclic GAN) for mapping between sketch and image domain. The key contributions of our proposed approach are : (1) a novel sketch based floor plan retrieval framework using an intuitive and convenient sketch query mode; (2) A conjunction of Cyclic GANs and Convolution Neural Networks (CNNs) for the task of hand-drawn floor plan image retrieval. Extensive experimentation and comparison with baseline results authenticates our claim."}, {"label": 0, "content": "Tile-based video systems have recently emerged as a viable solution to overcome the challenges of 360-degree video. For instance, the HEVC based viewport-dependent profile of MPEG OMAF allows serving clients independently coded tiles of the 360-degree video at varying resolution to enhance fidelity within the actual user viewport. During streaming, the client constantly adapts its tile selection and feeds a single merged bitstream to the video decoder. This paper addresses the open issue of rate assignment in a distributed encoding system in such a multi-resolution tiled streaming scenario. A model for tile rate assignment based on the spatio-temporal activity of the video is presented to reduce variance of the quality distribution and experimental results are reported."}, {"label": 0, "content": "Advertisements are an integral part of internet economics and culture, and video ads are the most popular and arguably the most entertaining form of advertisements. With the recent growth in digital marketing, video ads have seen unprecedented growth and are growing in importance as an advertising means. Video ads are expensive to create and are not always effective. The effectiveness of a video ad is usually not known before its deployment, which is non-ideal for creators, advertisers, and ad platforms. In this paper, we outline an idea to provide feedback before an ad is placed on its effectiveness based on the video along with the historical data about the effectiveness of other video ads. We propose a multi-modal mixture based algorithm to predict the effectiveness automatically. Specifically, we exploit rich textual information often found with an advertisement as well as visual information to learn a finite mixture model. Our experiments on a publicly available dataset show that our approach can outperform other baseline approaches."}, {"label": 0, "content": "Image description has become a popular topic in multimedia computing and computer vision areas. Recent works have demonstrated that learning the local semantic concepts, in addition to the image features, as the contextual information can help to understand the image scene better. However, current image description methods treating the local features as the bag-of-visual-words that do not capture the interaction and structure of the objects embedded in the image. In this paper, we propose a novel captioning framework that learns to integrate local concepts with their geometry structure as the side information. We design an Object Structure Graph to encode the positions and the distribution of the objects in the image. In order to embed the graph into an efficient representation, we introduce a semantic matching schema that matches our embedded graph with their corresponding sentence. Our experiments based on the public image captioning data sets, the MS-COCO and the Flickr30k, show that our improved solution is significantly better than current state-of-the-art techniques that leverage local semantic concepts; and our best model on the same splitting has competitive results compared to other recent approaches."}, {"label": 0, "content": "We propose sparse-complementary convolution (SC-Conv) to improve model utilization of convolution neural networks (CNNs). The networks with SC-Conv achieve better accuracy than the regular convolution under similar computations and parameters. The proposed SC-Conv is paired with two deterministic sparse kernels, and one of kernels is complementary to the other one at in either spatial or channel domain or both; the deterministic sparsity increases the computational speed theoretically and practically; furthermore, by having the complementary characteristic, SC-Conv retains the same receptive field to the conventional convolution. This insightful but straightforward SC-Conv reuses of modern network architectures (ResNet and DenseNet), and at the same FLOPs and parameters, SC-Conv improves top-1 classification accuracy on ImageNet by 0.6 points for ResNet-101 and keep the same model complexity. Furthermore, SC-Conv also outperforms recent sparse networks by 1.3 points at top-1 accuracy for ImageNet, and after integrating SC-Conv with the sparse network, we further improve another 1.8 points accuracy at similar FLOPs and parameters."}, {"label": 0, "content": "Object detection is a fundamental task in computer vision. With the remarkable progress made in big visual data analytics and deep learning, Reinforcement Learning (RL) is becoming a promising framework to model the object detection problem since the detection procedure can be cast as a Markov decision process (MDP). We propose a Reinforcement Learning system with parameterized action space for image object detection. The proposed system uses an active agent exploring in a scene to identify the location of a target object, and learns a policy to refine the geometry of the agent by taking simple actions in parameterized space, which integrates the discrete actions and its corresponding continuous parameters. We then optimize the representation of the generated region proposals with the discriminative multiple canonical correlation analysis (DMCCA) [11] in preparation for classification with Fast R-CNN. Experiments on PASCAL VOC 2007 and 2012 datasets show the effectiveness of the proposed method."}, {"label": 0, "content": "High Efficiency Video Coding (HEVC) provides more compression than its predecessors. One of the modules that contributes to higher compression rates is the Motion Estimation module, which consists of Integer and Fractional pixel motion estimation. The Fractional Motion Estimation (FME) process performs interpolations to find sample values at fractional-pixel locations, which can be computationally demanding. In this paper, we propose an interpolation-free method for FME based on Artificial Neural Networks (ANNs). Our proposed method is implemented in HEVC reference software (HM-16.9). According to our results, ANNs can accomplish FME task with an average increase of 2.6% in BDRate and an average reduction of 0.09 dB in BD-PSNR."}, {"label": 0, "content": "Virtual reality (VR) applications make use of 360\u00b0 omnidirectional video content for creating immersive experience to the user. In order to utilize current 2D video compression standards, such content must be projected onto a 2D image plane. However, the projection from spherical to 2D domain introduces deformations in the projected content due to the different sampling characteristics of the 2D plane. Such deformations are not favorable for the motion models of the current video coding standards. Consequently, omnidirectional video is not efficiently compressible with current codecs. In this work, a geometry-based motion vector scaling method is proposed in order to compress the motion information of omnidirectional content efficiently. The proposed method applies a scaling technique, based on the location in the 360\u00b0 video, to the motion information of the neighboring blocks in order to provide a uniform motion behavior in a certain part of the content. The uniform motion behavior provides optimal candidates for efficiently predicting the motion vectors of the current block. The conducted experiments illustrated that the proposed method provides up to 2.2% bitrate reduction and on average around 1% bitrate reduction for the content with high motion characteristics in the VTM test model of Versatile Video Coding (H.266/VVC) standard."}, {"label": 0, "content": "Gaming as a popular system has recently expanded the associated services, by stepping into live streaming services. Live gaming video streaming is not only limited to cloud gaming services, such as Geforce Now, but also include passive streaming, where the players' gameplay is streamed both live and ondemand over services such as Twitch.tv and YouTubeGaming. So far, in terms of gaming video quality assessment, typical video quality assessment methods have been used. However, their performance remains quite unsatisfactory. In this paper, we present a new No Reference (NR) gaming video quality metric called NR-GVQM with performance comparable to state-of-the-art Full Reference (FR) metrics. NR-GVQM is designed by training a Support Vector Regression (SVR) with the Gaussian kernel using nine frame-level indexes such as naturalness and blockiness as input features and Video Multimethod Assessment Fusion (VMAF) scores as the ground truth. Our results based on a publicly available dataset of gaming videos are shown to have a correlation score of 0.98 with VMAF and 0.89 with MOS scores. We further present two approaches to reduce computational complexity."}, {"label": 0, "content": "Deep learning has successfully shown excellent performance in learning joint representations between different data modalities. Unfortunately, little research focuses on cross-modal correlation learning where temporal structures of different data modalities, such as audio and video, should be taken into account. Music video retrieval by a given musical audio is a natural way to search and interact with music contents. In this work, we study cross-modal music video retrieval in terms of emotion similarity. Particularly, an audio of an arbitrary length is used to retrieve a longer or full-length music video. To this end, we propose a novel audio-visual embedding algorithm by Supervised Deep Canonical Correlation Analysis (S-DCCA) that projects audio and video into a shared space to bridge the semantic gap between audio and video. This also preserves the similarity among audio and visual contents from different videos with the same class label and the temporal structure. The contribution of our approach is mainly manifested in the two aspects: i) We propose to select top k audio chunks by attention-based Long Short-Term Memory (LSTM) model, which can represent good audio summarization with local properties. ii) We propose an end-to-end deep model for crossmodal audio-visual learning where S-DCCA is trained to learn the semantic correlation between audio and visual modalities. Due to the lack of music video dataset, we construct 10K music video dataset from YouTube 8M dataset. Some promising results such as MAP and precision-recall show that our proposed model can be applied to music video retrieval."}, {"label": 0, "content": "Lipreading is the task of looking at, perceiving, and interpreting spoken symbols. It has a wide range of applications such as in surveillance, Internet telephony, speech reconstruction for silent movies and as an aid to a person with speech as well as hearing impairments. However, most of the work in lipreading literature has been limited to the classification of speech videos into text classes formed of phrases, words and sentences. Even this has been based on a highly constrained lexicon of words which, then subsequently translates to restriction on total number of classes (i.e, phrases, words and sentences) that are considered for the classification task. Recently, research has ventured into generating speech (audio) from silent video sequences. In spite of non-frontal views showing the potential of enhancing performance of speech reading and reconstruction systems, there have been no developments in using multiple camera feeds for the same. To this end, this paper presents a multi-view speech reading and reconstruction system. The major contribution of this paper is to present a model, namely MyLipper, which is a vocabulary and language agnostic and a real-time model that deals with a variety of poses of a speaker. The model leverages silent video feeds from multiple cameras recording a subject to generate intelligent speech for that speaker, thus being a personalized speech reconstruction model. It uses deep learning based STCNN+BiGRU architecture to achieve this goal. The results obtained using MyLipper show an improvement of over 20% in reconstructed speech's intelligibility (as measured by PESQ) using multiple views as compared to a single view visual feed. This confirms the importance of exploiting multiple views in building an efficient speech reconstruction system. The paper further shows the optimal placement of cameras which would lead to the maximum intelligibility of speech. Further, we demonstrate the reconstructed audios overlaid on the corresponding videos obtained from MyLipper using a variety of videos from the dataset"}, {"label": 0, "content": "In this paper, we propose a novel singing-voice enhancement system that makes the singing voice of amateurs similar to that of professional opera singers, where the singing voice of amateurs is emphasized by using a singing voice of a professional opera singer on a frequency band that represents the remarkable characteristic of the professional singer. Moreover, our proposed singing-voice enhancement based on highway networks is able to convert any song (that a professional opera singer does not sing). As a result of our experiments, the singing voice of the amateur singer at the middle-high frequency range which contains a lot of frequency components that affect glossiness was emphasized while maintaining speaker characteristics."}, {"label": 0, "content": "We present novel low-level audio features that are based on correlations between sub-band audio signals decomposed by undecimated wavelet transform. Under the assumption that SVM is used for classifier learning, the experimental results on GTZAN dataset showed that the proposed method demonstrated the best accuracy of 81.5%, outperforming the conventional methods."}, {"label": 0, "content": "In this paper, we introduce our recent studies on human perception in audio event classification. In particular, the pre-trained model VGGish is used as feature extractor to process audio data, and DenseNet is trained by and used as feature extractor for our electroencephalography (EEG) data. The correlation between audio stimuli and EEG is learned in a shared space. In the experiments, we record brain activities (EEG signals) of several subjects while they are listening to music events of 8 audio categories selected from Google AudioSet. Our experimental results demonstrate that i) audio event classification can be improved by exploiting the power of human perception, and ii) the correlation between audio stimuli and EEG can be learned to complement audio event understanding."}, {"label": 0, "content": "As a multimedia security mechanism, CAPTCHAs are completely automated public turing test to tell computers and humans apart. Although cracking CAPTCHA has been explored for many years, it is still a challenging problem for real practice. In this demo, we present a text based CAPTCHA cracking system by using convolutional neural networks(CNN). To solve small sample problem, we propose to combine conditional deep convolutional generative adversarial networks(cDCGAN) and CNN, which makes a tremendous progress in accuracy. In addition, we also select multiple models with low pearson correlation coefficients for majority voting ensemble, which further improves the accuracy. The experimental results show that the system has great advantages and provides a new mean for cracking CAPTCHAs."}, {"label": 0, "content": "Fine-grained visual categorization aims to distinguish objects in subordinate classes instead of basic class, and is a challenge visual task due to the high correlation between subordinated classes and large intra-class variation (e.g. different object poses). Although, deep convolutional neural network (DCNN) has brought dramatic success on generic object classification, detection and segmentation with the availability of the large-scale training samples, direct application of DCNN on fine-grained visual categorization, where only decades or at most hundreds of training samples for each subordinate class are available in most public finegrained image datasets, cannot lead to satisfactory classification results due to small number of training samples. This study explores the transfer learning strategy for finegrained dog breed categorization based on the learned CNN models with the large-scale image dataset: ImageNet, and prove promising performance with two DCNN models: AlexNet and VGG-16. Furthermore, we argue that different DCNN architecture may extract the representation of different image aspects due to the previously defined CNN kernel sizes, number and various operations in the model learning procedure, and thus result in different performance for visual categorization. This study proposes to fusion multiple CNN architectures for combining different aspect representations to give more accurate performance. We compressively study the fusion of different layers such as Fc6 and Fc7 in AlexNet and VGG-16, and manifest 2.88% improvement of the fusion architecture over the best performance of the only one DCNN model: VGG-16 from 81.2% to 84.08%."}, {"label": 0, "content": "Currently, movie trailers are edited using various methods. However, the length of each trailer is at most several minutes, and the scenes used for editing and the types of effects are limited because a trailer is created for a certain target audience. Therefore, it is difficult to edit a trailer that caters to the different preferences of various users. Moreover, potential audience may be lost if the trailer is not enticing enough. To solve this problem, we define seven editing biases that occur when movies are summarized and edited into trailers. We investigate whether these biases can be used to generate a movie trailer catering to various viewer preference."}, {"label": 0, "content": "In this paper, we propose a novel method to estimate the relative camera motions of three consecutive images. Given a set of point correspondences in three views, the proposed method determines the fundamental matrix representing the geometrical relationship between the first two views by using the eight-point algorithm. Then, by minimizing the proposed cost function with the fundamental matrix, the relative camera motions over three views are precisely estimated. The experimental results show that the proposed method outperforms the conventional two-view and three-view geometry-based method in terms of the accuracy."}, {"label": 0, "content": "Internet has brought about a tremendous increase in content of all forms and, in that, video content constitutes the major backbone of the total content being published as well as watched. Thus it becomes imperative for video recommendation engines to look for novel and innovative ways to recommend the newly added videos to their users. However, the problem with new videos is that they lack any sort of metadata and user interaction so as to be able to rate the videos for the consumers. To this effect, this paper introduces the several techniques we develop for the Content Based Video Relevance Prediction (CBVRP). We employ different architectures on the CBVRP dataset to make use of the provided frame and video level features and generate predictions of videos that are similar to the other videos. We also implement several ensemble strategies to explore complementarity between both the types of provided features. The obtained results are encouraging and will impel the boundaries of research for multimedia based video recommendation systems."}, {"label": 0, "content": "This paper attempts at improving the accuracy of Human Action Recognition (HAR) by fusion of depth and inertial sensor data. Firstly, we transform the depth data into Sequential Front view Images(SFI) and fine-tune the pre-trained AlexNet on these images. Then, inertial data is converted into Signal Images (SI) and another convolutional neural network (CNN) is trained on these images. Finally, learned features are extracted from both CNN, fused together to make a shared feature layer, and these features are fed to the classifier. We experiment with two classifiers, namely Support Vector Machines (SVM) and softmax classifier and compare their performances. The recognition accuracies of each modality, depth data alone and sensor data alone are also calculated and compared with fusion based accuracies to highlight the fact that fusion of modalities yields better results than individual modalities. Experimental results on UTD-MHAD and Kinect 2D datasets show that proposed method achieves state of the art results when compared to other recently proposed visual-inertial action recognition methods."}, {"label": 0, "content": "Annotation systems provide services that vary from adding simple information for signifying content of interest, to indicate patterns in documents for creating statistical models and applying machine learning techniques. In this paper, we argue that AI mechanisms should be part of the annotation process to collaborate with annotation systems' end users (i.e. annotators) as oppose to be just an outcome from annotators' work. Indeed, current systems do not delve into AI aspects to support the annotation process, lacking features that we argue as essential in annotation systems. That is, collaboration between annotators and AI, collaborative knowledge curation by extracting and structuring knowledge from annotations considering the context of annotation anchors (i.e. area that was selected to create an annotation and comprises the content of interest). To illustrate the gains of this approach, we present HAS, the Hyperknowledge Annotation System. HAS allows one to annotate multimedia content (e.g., text, image, and video) and its tight integration with AI-based services enables the extraction of additional semantic information from the annotated content. In our approach, both the annotation and the information extracted from the content are structured using the hyperknowledge conceptual model, which promotes the use of the spatiotemporal query capabilities of this model for retrieving annotations based on semantic queries. We argue that integrating AI-based services and using the hyperknowledge model for knowledge structuring leverage multimedia annotation systems, enabling the development of novel use cases."}, {"label": 0, "content": "With explosion of videos, action recognition has become an important research subject. This paper makes a special effort to investigate and study 3D Convolutional Network. Focused on the problem of ConvNet dependence on multiple large scale dataset, we propose a 3D ConvNet structure which incorporate the original 3D-ConvNet features and foreground 3D-ConvNet features fused by static object and motion detection. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, experimental results demonstrate that with merely 50% pixels utilization, foreground ConvNet achieves satisfying performance as same as origin. With feature fusion, we achieve 83.7% accuracy on UCF-101 exceeding original ConvNet."}, {"label": 0, "content": "Sounds are ubiquitous in our daily lives, for instance, sounds of vehicles or sounds of conversations between people. Therefore, it is easy to collect all these soundtracks and categorize them into different groups. By doing so, we can use these assets to recognize the scene. Acoustic scene classification allows us to do so by training our machine which can further be installed on devices such as smartphones. This provides people with convenience which improves our lives. Our goal is to maximize our validation rate of our machine learning results and also optimize our usage of hardware. We utilize the dataset from IEEE Detection and Classification of Acoustic Scenes and Events (DCASE) to train our machine. The data of DCASE 2017 contains 15 different kinds of outdoor audio recordings, including beach, bus, restaurant etc. In this work, we use two different types of signal processing techniques which are Log Mel and HPSS (Harmonic-Percussive Sound Separation). Next we modify and reduce the MobileNet structure to train our dataset. We also make use of fine-tuning and late fusion to make our results more accurate and to improve our performances. With the structure aforementioned, we succeed in reaching the validation rate of 75.99% which is approximately the seventh highest performing group of the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge 2017, with less computational complexity comparing with others having higher accuracy. We deem it a worthy trade-off."}, {"label": 0, "content": "With the ageing and growth of the population, some chronic diseases, such as Parkinson's disease (PD), urge the society to a health-conscious looking for better health system designs. Some recent research endeavour has been supported by solutions grounded in ubiquitous healthcare (u-Health) coupling telemedicine, context awareness and decision support capabilities. In this work, we propose a u-healthcare system to pre-diagnose PD based on the speech signal of people under voice call. The speech stream is sampled as well as processed to support the pre-diagnose using machine learning (ML). Experiments were conducted over a PD voice dataset composed of 40 individuals by using five different ML algorithms. Based on a linear Support Vector Machine (SVM) model, a false negative rate of 10% was obtained when classifying the locution of number \"three\"."}, {"label": 0, "content": "Protozoa detection and identification play important roles in many practical domains such as parasitology, scientific research, biological treatment processes, and environmental quality evaluation. Traditional laboratory methods for protozoan identification are time-consuming and require expert knowledge and expensive equipment. Another approach is using micrographs to identify the species of protozoans that can save a lot of time and reduce the cost. However, the existing methods in this approach only identify the species when the protozoan are already segmented. These methods study features of shapes and sizes. In this work, we detect and identify in the images of cysts and oocysts of various species such as: Giardia lamblia, Iodamoeba butschilii, Toxoplasma gondi, Cyclospora cayetanensis, Balantidium coli, Sarcocystis, Cystoisospora belli and Acanthamoeba, which have round shapes in common and affect seriously to human and animal health. We propose Segmentation-driven RetinaNet to automatically detect, segment, and identify protozoans in their micrographs. By applying multiple techniques such as transfer learning, and data augmentation techniques, and dividing training samples into life-cycle stages of protozoans, we successfully overcome the lack of data issue in applying deep learning for this problem. Even though there are at most 5 samples per life-cycle category in the training data, our proposed method still achieves promising results and outperforms the original RetinaNet on our protozoa dataset."}, {"label": 0, "content": "The proposed framework employs discriminative analysis for gaze estimation using kernel discriminative multiple canonical correlation analysis (K-DMCCA), which represents different feature vectors that account for variations of head pose, illumination and occlusion. The feature extraction component of the framework includes spatial indexing, statistical and geometrical elements. Gaze estimation is constructed by feature aggregation and transforming features into a higher dimensional space using the RBF kernel \u03b3 and spread factor. The output of fused features through K-DMCCA is robust to illumination, occlusion and is calibration free. Our algorithm is validated on MPII, CAVE, ACS and EYEDIAP datasets. The two main contributions of the framework are the following: Enhancing the performance of DMCCA with the kernel and introducing quadtree as an iris region descriptor. Spatial indexing using quadtree is a robust method for detecting which quadrant the iris is situated, detecting the iris boundary and it is inclusive of statistical and geometrical indexing that are calibration free. Our method achieved an accurate gaze estimation of 4.8\u00b0 using Cave, 4.6\u00b0 using MPII, 5.1\u00b0 using ACS and 5.9\u00b0 using EYEDIAP datasets respectively. The proposed framework provides insight into the methodology of multi-feature fusion for gaze estimation."}, {"label": 0, "content": "Tool wear prediction becomes increasingly important due to the growing demand for finished quality and the improvement of productivity. In this case, it is necessary to establish a well-designed monitoring system to obtain the relationship between tool wear and cutting process. Generalized regression neural network (GRNN) is able to handle non-linear problems with its memory-based character. However, it was rarely used for tool wear prediction in the past several decades. Therefore, in this paper, it was employed to tackle this problem. In addition, in order to tune the smooth parameter of the GRNN, a newly proposed evolutionary algorithm called fruit fly optimization algorithm (FOA) was adopted. Meanwhile, an improved fruit fly optimization algorithm (IFOA), in which escaping and distance control parameters were introduced to prevent FOA from falling into local optimum, was presented to enhance the search ability. Two cutting experiments showed that the IFOA-GRNN provided a comparable regression ability to the GRNN with particle swarm optimization(PSO), the least squares support vector machines(LS-SVM) and the BP neural network."}, {"label": 0, "content": "The feature information is usually corrupted by the irrelevant harmonics and background noise for bearing fault signal, which makes it difficult to identify the fault symptom in time. This paper proposes a new diagnosis method to identify the incipient periodic impulsive features of rolling bearings. Firstly, the Over-Complete Rational-Dilation Wavelet Transforms (ORDWT) is employed to decompose the original fault signal to obtain several sub-bands. Secondly, a periodic impulsive index which absorbs the advantages of ACFHNR, the kurtosis and Pearson's correlation coefficient index is proposed to adaptively track the best fault frequency band. Finally, the envelop spectrum of the best fault frequency band is gained for fault diagnosis. The simulation and experiment results demonstrate that the proposed adaptive fault frequency band detection (AFFBD) method is effective."}, {"label": 0, "content": "For intermittent fault diagnosis, it is important to extract fault features. An intermittent fault feature extraction method based on wavelet transform is proposed. Firstly, the stress-intensity model is used to describe the fault process of intermittent faults, analysing the main part of intermittent fault signal characteristics. Considering the randomness and suddenness of intermittent faults, wavelet transform is used to extract intermittent fault features based on the advantages of wavelet transform in signal mutation and singularity detection. By extracting signal energy through the wavelet coefficients, intermittent fault features are obtained, which can be used to identify and isolate the intermittent faults. Finally, by a simulating circuit, transient intermittent fault response characteristics are obtained by simulation in the circuit, and the features are extracted by wavelet transform for the intermittent fault location. Results show that the intermittent fault features can be extracted by the wavelet transform, and the features can be used for intermittent fault diagnosis."}, {"label": 0, "content": "Sensor data can be used to detect changes in the performance of a system in near real-time which may be indicative of a system fault. However, there is a need for efficient and robust algorithms to detect such changes in the data streams. In this paper, sensor data from a marine diesel engine on an ocean-going ship are used for anomaly detection. The focus is on cluster-based methods for anomaly detection. The idea is to identify clusters in sensor data in normal operating conditions and to assess whether new observations belong in any of these clusters. New observations that obviously do not belong to any of the identified clusters, may be regarded as anomalies and call for further scrutiny. The cluster-based approaches to anomaly detection presented in this paper are truly unsupervised, and they are applied to sensor data with no known faults. Being fully unsupervised, however, the cluster based approaches do not need to explicitly assume that all observations in the training data are fault-free as long as the amount of faulty data is not large enough to form a separate cluster. Moreover, anomalies in the training data may be detected. Various clustering techniques are applied in this paper to provide a simple and unsupervised approach to anomaly detection. This could then be used as an efficient initial screening of the data streams before more detailed analysis is applied to suspicious parts of the data. The methods explored in this study include K-means clustering, Mixture of Gaussian models, density based clustering, self-organizing maps and spectral clustering. The performance of the various methods is reported, and also compared with that of other methods recently proposed for anomaly detection such as auto associative kernel regression (AAKR) and dynamical linear models (DLM). Overall, cluster-based methods are found to be promising candidates for online anomaly detection based on sensor data."}, {"label": 0, "content": "Traditional intelligent diagnosis methods and current popular deep learning based diagnosis methods basically adopt the approach of batch learning, which may waste time and computing resources since they need to discard the previous learned model and retrain a new model based on the newly added data and prior data. Moreover, manual feature extraction is often a necessary step for intelligent diagnosis, and such a process relies much on prior knowledge. To solve the above mentioned problems, this paper proposes a fault diagnosis method based on class incremental learning without manual feature extraction. Based on denoising autoencoder, the proposed method obtains the autoencoders using the raw data acquired for each health state. In the class incremental learning process, only the autoencoder of new health state need to be trained while the former trained autoencoders are retained. Test data is classified according to the minimal reconstruction error calculated through the autoencoders. At the end of this paper, the proposed method is validated through vibration data of rolling bearings for rail vehicle. The experimental results show that the presented method is effective."}, {"label": 0, "content": "The presence of periodical impulses in vibration signals usually indicates the occurrence of faults in roller bearings. Unfortunately, in the complex working condition with the heavy noises, fault detection in mechanical systems is often difficult. To solve this problem, a hybrid method of ensemble empirical mode decomposition (EEMD) and L-Kurtosis clustering-based segmentation is proposed. EEMD is similar to empirical mode decomposition (EMD), which can express the intrinsic essence using simple and understandable algorithm to solve the mode mixing phenomenon. L-Kurtosis is the improved version of kurtosis to recognize the impulses without the influence of outliers. Furthermore, the L-Kurtosis value is employed as an indicator in the clustering-based segmentation method to extract the fault features from the background noises. To illustrate the feasibility of utilizing the EEMD and L-Kurtosis based clustering segmentation method, benchmark data simulations and experimental investigations are performed to detect faults in bearings. The results show that the proposed method enables the efficient recognition of faults in bearings."}, {"label": 0, "content": "Degradation modeling plays a key role in accelerated degradation test data analysis and condition-based maintenance. The degradation rate of a degradation process usually depends on both unit's age and its state. Several agestate-dependent degradation models have been proposed in the literature. A main drawback of these models is their intractability. In this paper, we propose an analytically tractable age-state-dependent degradation model. The proposed model is defined in terms of degradation rate, whose mean function is represented by a bivariate power-law model. The degradation increment is modeled by a non-homogeneous gamma process, whose mean function depends on both age and degradation level and has a closed-form expression. The model includes age-and state-dependent models as its special cases. An approach is proposed to make selection among the general case and special cases based on the Akaike information criterion. A real-world example is used to illustrate the flexibility and usefulness of the proposed model and approach."}, {"label": 0, "content": "The security of inertial navigation system (INS) is the key factor to determine the safety of air vehicle. Due to the INS contains some components of dynamic logic gate, which makes its work process presenting a series of dynamic characteristics. When using dynamic fault tree to analyze the security of INS, most of the traditional approaches are based on Markov chain and Monte-Carlo simulation. However, those methods have curse of dimensionality problem and/or require long computation time particularly when results with a high degree of accuracy are desired. In this paper, an analytical method is proposed based on sequential binary decision diagrams (SBDD) for combinatorial reliability analysis of dynamic systems. Firstly, the dynamic fault tree (DFT) model of collapse of inertial platform is built based on safety analysis used DFT. Secondly, the DFT model is modularized into independent static sub-trees and dynamic sub-trees, and then the exact expression of the occurrence probability for each minimal cut-set is determined by the static fault tree analysis method or SBDD method. Finally, the possibility of collapse of inertial platform in all its life is obtained after comprehensive analysis, and the result is compared with that based on Markov chain and Monte-Carlo simulation. The result shows that the proposed approach can generate exact possibility of collapse of inertial platform. In addition, the system model of sequential binary decision diagrams and possibility evaluation expression are reusable for the reliability analysis with different component failure parameters, the method is more applicable and more effective, and the method of calculating occurrence probability of top event for a dynamic fault tree is expanded."}, {"label": 0, "content": "In order to achieve fault diagnosis and remaining useful life prediction effectively when system states and fault parameters run at two different time-scales, a double time-scale particle filter algorithm is proposed to jointly estimate the system states and the possible fault parameters. The bond graph model that deals with various energy forms in a unified way is constructed to carry out fault diagnosis which consists of fault detection and isolation through deducing analytical redundancy relations to calculate residual of system. The fault estimation is used to identify the parameters of the isolated fault set and determine the true fault as well as remaining useful life prediction. Since the fault parameters exhibit slow-time varying characteristics and the system states show quick-time varying characteristics, the double time-scale particle filter is developed to simultaneously estimate and predict the system states and parameters on two different time scales for diagnosis and prognosis purpose. The effectiveness of the proposed double time-scale particle filter algorithm is validated by the simulation results on the nonlinear electromechanical system."}, {"label": 0, "content": "Machinery condition monitoring has entered the era of big data and some research has been done based on big data. Abnormal segments, such as missing segments and drift segments, are inevitable in big data acquired from harsh industrial environment due to temporary sensor failures, network segment transmission delays, or accidental loss of some collected data and so on. Being independent of the machinery condition, the abnormal segments not only reduce the quality of the data for condition monitoring and big data analysis, but also bring a heavy computation load. However, there are few reports to address abnormal segment detection for further data cleaning in the field of machinery condition monitoring. Therefore, an abnormal segment detection method is proposed to improve the quality of big data. First, a sliding window is used to separate the data into different segments. Then, 14 kinds of time-domain features are extracted from each segment and principle component analysis (PCA) is employed to extract the principle components from these features. In addition, local outlier factor (LOF) is calculated based on the principle components to evaluate the degree of being an outlier for each segment. Finally, the data, including a drift segment from a real wind turbine, are used to verify the effectiveness of the proposed method."}, {"label": 0, "content": "The electrostatic sensing technique has been verified to be a viable method for tribo-contact monitoring under laboratory conditions in previous investigations. The monitoring of wind turbine gearbox is a possible approach for electrostatic application. It usually under variable operating conditions while working. This paper introduced a new method called moving window local outlier factor (MWLOF) to process electrostatic monitoring signals with multi-sensors. It can detect early faults earlier and has a better sensitivity with the information fusion method than conventional techniques, which leads to a better industrial application in future."}, {"label": 0, "content": "The stochastic dependency exists in many complex systems, which cannot be ignored in reliability modeling. The stochastic dependency is analyzed from the degradation perspective in this paper. For a two-component system, a degradation interaction model is built to describe the dependency between the degradations of two components based on nonlinear Wiener process and Copula function. Then, the reliability functions of components and system are derived, and the parameter estimation approach of the developed models is given. Finally, a numerical example about fatigue crack development is presented to validate and illustrate the performance of the developed models."}]