[{"label": 0, "content": "Security is one of the most important issues concerning Vehicular Ad-Hoc Networks (VANETS), specifically when dealing with misbehaving vehicles to prevent them threatening the safety of others. In this paper, we present a review about revoking misbehaving vehicles based on classical Certificate Revocation List (CRL) in IEEE Standard. The main disadvantage of these algorithms resides in the fact that the Certification Authority (CA) is overwhelmed because it is responsible to distribute the whole CRL to all the requesting vehicles. To overcome this drawback in European Telecommunication Standards Institute (ETSI) standard, we propose our contribution that aims to minimize the tasks of the CA, by decomposing the CRL into different chunks that are distributed separately by the different RSUs of the same zone."}, {"label": 1, "content": "In this paper, we introduce a novel method for resolving simple signals by utilizing the technique of dividing the spectra. This method allows us to calculate the phase of the received signal, which then enables us to determine the precise moment when the signal was recorded at the receiver. Our research shows that implementing this method in the primary data processing systems of interferometric sonar greatly enhances the accuracy of determining the spatial position of objects at the bottom of the sea. \n\nFurthermore, we estimate the resolution of chirp signals and conclude that it is possible to determine the signal fixing time using the linear correlation method with no more than one sampling, even when the sampling frequency is below the central carrier frequency. We also investigate the compression and recovery of simple signals in side-scan sonar.\n\nAdditionally, we present our findings on a simulation that highlights the relationship between the error in determining the spatial position of objects at the bottom of the sea, and the rotation angle of the base of the interferometric sonar used for side surveys, the duration of the probing premise, and the frequency of the discretization of the recorded response. \n\nIn summary, we introduce a new method for resolving simple signals that can be used in primary data processing systems of interferometric sonar, which can significantly enhance the accuracy of determining the spatial position of objects on the sea floor. Our research also sheds light on the compression and recovery of simple signals, as well as the factors that affect errors in determining spatial position on the sea floor."}, {"label": 1, "content": "In this paper, we propose a low complexity algorithm to address the joint channel estimation problem for two-way multiple-input multiple-output (MIMO) relay systems. Our algorithm uses a unified formulation of the received signal as a Tucker-2 model at each user to estimate channels. By resorting to the proposed low complexity iterative algorithm, we derive a joint channel estimation process that provides each user with full knowledge of all channel matrices in the communications system. Moreover, even when the channel becomes strongly correlated, our algorithm can estimate the channel effectively. Simulation results demonstrate the effectiveness of our proposed algorithm."}, {"label": 0, "content": "Prohibition signs are commonly used for safety purposes in order to prevent and protect individuals from dangerous situations. These signs are placed in or around areas whereby they are clearly visible to the public. However, the visually impaired cannot visualize such signs. To help them, this paper proposes a system that combines Convolutional Neural Network (CNN) model and Computer Vision (CV) algorithms to detect and recognize prohibition signs in real scenes. The system uses pre-trained AlexNet model, fine-tuned using Prohibition Signage Boards (PSB) dataset and combined with Maximally Stable Extremal Regions (MSER) and Optical Character Recognition (OCR) techniques for text extraction and classification, to enhance the system performance. The experiments indicate that high recognition accuracies are achieved from a variety of prohibition images and prohibition texts."}, {"label": 1, "content": "In this paper, we introduce a new technique for determining the relative camera motions of three images. We achieve this by first acquiring a set of point correspondences among the three views. Using the eight-point algorithm, we determine the fundamental matrix that defines the geometrical relationship between the first two views. From this, we are able to precisely estimate the relative camera motions over the three views by minimizing the proposed cost function with the fundamental matrix. Through experimentation, we have shown that our method is more accurate than conventional two-view and three-view geometry-based methods."}, {"label": 0, "content": "In order to speed up the convergence of distributed online optimization algorithms, a Fast Distributed Online Conditional Gradient Algorithm (F-DOCG) is proposed in this paper. The Erdos-Renyi (ER) stochastic model is firstly established and an Edge Addition (AE) algorithm is proposed. Secondly, the Edge Addition algorithm and Distributed Online Conditional Gradient Algorithm are combined to propose a F-DOCG. The F-DOCG algorithm not only avoids the high cost projection problem with a linear approximation, but also improves the Regret bound based on the relationship between the underlying topology and the algebraic connectivity, and thus results in a faster convergence rate. Finally, compared with the existing Distributed Online Conditional Gradient Algorithm (DOCG), numerical simulation experiments show that the proposed F-DOCG has better performance."}, {"label": 0, "content": "Epilepsy is a chronic disorder that causes unprovoked, recurrent sudden abnormal reactions of the brain. Characterizing electroencephalogram (EEG) signals of the patient is an effective way for the early prediction of epileptic seizures. In this paper, a new method called the entropy of visibility heights of hierarchical neighbors (EVHHN) is proposed to detect seizures from the EEG signals. First, the visibility relationships of three nearest neighbors are determined by a visibility criterion. Then, we compute the visibility heights of three nearest neighbors for each data point. Next, the four different kinds of entropy associated with neighbor visibility states are calculated to characterize the EEG signals and finally these features are validated by LS-SVM classifiers. In the experiment, the normal and ictal EEG signals are classified with the accuracy of 99.6%, meanwhile, the interictal and ictal EEG signals are distinguished with the accuracy of 98.35%, which proves the effectiveness of our proposed method. Notably, the computational time of extracting features for each set is 1.751 s, which is largely reduced compared with other weighted visibility graph-based methods. In conclusion, the EVHHN can potentially be an effective method for characterizing complicated EEG signals and real-time detection of epileptic seizures."}, {"label": 1, "content": "Route randomization is a crucial topic of research for the advancement of moving target defense. This approach aims to change the forwarding routes in a network dynamically and proactively. However, implementing route randomization in traditional networks has proven challenging. To overcome these challenges and facilitate effective route randomization, a new approach has been introduced. This approach entails adding a mapping layer between routers' physical interfaces and their corresponding logical addresses. The design theories and details of the proposed approach are elaborated upon in this paper. The effectiveness and efficiency of this approach have been confirmed and analyzed through appropriate experiments."}, {"label": 1, "content": "Motivated by the outstanding performance of deep learning strategies in solving action recognition tasks, we propose a simple yet effective method for encoding spatiotemporal information of skeleton sequences into color texture images, which we refer to as Skeletal Optical Flows (SOFs). SOFs capture meaningful temporal information by representing the kinetic energy, predefined angles, and pair-wise displacements between joints over consecutive frames of skeleton data as color variations, thus making them highly interpretable. To exploit the discriminative features of SOFs for human action recognition, we employ a novel Convolutional Neural Network with Correctness-Vigilant Regularizer (CVR-CNN). Empirical results demonstrate the superior efficiency of our proposed method in terms of the generalizability of the generated model, training convergence speed, and classification accuracy on commonly used action recognition datasets such as MHAD, HDM05, and NTU RGB+D."}, {"label": 1, "content": "In outdoor vision systems, images taken by digital cameras can be significantly distorted by bad weather conditions, negatively affecting system performance. Rain is one such condition, randomly causing intensity fluctuations in images. A new low rank recovery based algorithm has been proposed to remove rain streaks from single images taken in rainy weather. This method uses weighted nuclear norm (WNN) and total variation (TV) regularization for efficient rain removal. WNN assigns different weights to different singular values based on the details each singular value holds, while TV regularization discriminates most of natural image content from sparse rain streaks by preserving piecewise smoothness of images. Simulation results illustrate that the rain streaks are more effectively eliminated by this method."}, {"label": 0, "content": "Text analytics has been widely used in many different domains to discover valuable knowledge hidden inside a specific text. In terms of power dispatching, a manual always contains a large amount of unstructured data, which makes it a tough job for dispatchers to remember and understand that information. This paper addresses the above problems by adopting text analytics. Based on the idea of Natural Language Processing, a series of key technologies are adopted to do the text analyzing job such as data structure transformation, efficient word segmentation tools for Chinese and Word2Vec calculation, which are helpful for dispatchers to deal with the dispatching manual."}, {"label": 1, "content": "The purpose of this article is to examine the power of WeChat and explore its potential for global expansion. Our introduction to WeChat occurred while residing in China, where we experienced its ability to serve as a comprehensive application for social media, including Instagram, Facebook, and Twitter, all in one platform. However, given the multitude of differences and diverse legal frameworks that exist across the globe, we question whether WeChat can achieve comparable success in other countries. It is fascinating to observe the immense role that WeChat plays in the everyday lives of Chinese individuals, serving as a central hub for payment methods, social interaction, and communication. Our study is therefore divided into three sections in an effort to address this issue."}, {"label": 1, "content": "Consensus algorithms play a crucial role in the distributed control of microgrids. However, the performance of microgrids in such control scenarios is highly dependent on the time delays of the communication network. In this study, we primarily investigate the impact of time delays on consensus-based control in microgrids. Through our analysis, we demonstrate that the stability margin of time delays can be determined. Specifically, we prove that the stability of microgrids is directly proportional to the local processing time delays of DGs, whereas communication time delays between different DGs merely influence the convergence speed. To verify these findings, we implement an AC microgrid with four DGs and various loads, and utilize MATLAB/Simulink simulation results to validate the stability margin of time delays. Therefore, this study highlights the importance of analyzing and managing time delays when utilizing consensus-based control in microgrids."}, {"label": 0, "content": "Optical combustion measurement and analysis systems using multiple sensors have received considerable attention. In particular, the image-based flame 3D reconstruction approaches using computerized tomography have been widely applied for the flame 3D reconstruction from a set of views by constructing the optimized linear combinations of the 3D scene and projected images. Previous techniques were easily computed but were weak against noise and blurring due to the underlying least square-based loss function. This paper presents a 3D density flame reconstruction method, captured from the sparse multi-view images, as a constrained optimization problem between the flame and its projected images. For effective estimation of the flame with a complicated structure in an arbitrary viewpoint, we extract the 3D candidate region of the flame and, then, estimate the density field using the compressive sensing. The objective function is a linear combination of the photo consistency cost and sparsity regularization terms, which avoids blurring in the reconstruction. The proposed approach is a powerful matrix factorization method with each voxel represented as a linear combination of a small number of basis vectors. The approach also effectively simplifies the reconstruction process and provides the whole 3D density field in one step. The experimental results verify that the proposed 3D density estimation performs favorably from the few flame images."}, {"label": 1, "content": "Kinship verification from facial features remains a challenging task, despite attracting increasing attention in recent years. Previous methods have struggled to accurately predict kinship based solely on facial appearance, while early attempts at utilizing deep convolutional neural networks have been held back by limited training data. To overcome this obstacle, our proposed approach combines color features with extreme learning machines (ELM), allowing us to better handle smaller training sets while enhancing the predictive power of our model. We tested our methodology on three popular kinship databases, KinFaceW-I, KinFaceW-II, and TSKinFace, achieving results that compare favorably to some of the best-performing techniques currently in use, including those based on deep learning."}, {"label": 1, "content": "Real world systems are predominantly nonlinear, and controlling such systems can be achieved by linearizing the system, but this approach may cause problems with the robustness of the controller. Controllers can be either linear or nonlinear, and this paper explores the effect of nonlinearities on the robustness of different controllers. In particular, linear controllers such as PID with first-order derivative filter, state-feedback with integral control, and CRONE, as well as nonlinear controllers such as smoothed first-order integral SMC are investigated, applying these controllers to a DC motor. The motor model comprises four nonlinearities including voltage saturation, current saturation, dead zone, and backlash deadband. The performance of each type of controller is evaluated and compared using the non-linear model, the linearized model, uncertainty in the model, and harmonic load disturbance."}, {"label": 0, "content": "Sensor data can be used to detect changes in the performance of a system in near real-time which may be indicative of a system fault. However, there is a need for efficient and robust algorithms to detect such changes in the data streams. In this paper, sensor data from a marine diesel engine on an ocean-going ship are used for anomaly detection. The focus is on cluster-based methods for anomaly detection. The idea is to identify clusters in sensor data in normal operating conditions and to assess whether new observations belong in any of these clusters. New observations that obviously do not belong to any of the identified clusters, may be regarded as anomalies and call for further scrutiny. The cluster-based approaches to anomaly detection presented in this paper are truly unsupervised, and they are applied to sensor data with no known faults. Being fully unsupervised, however, the cluster based approaches do not need to explicitly assume that all observations in the training data are fault-free as long as the amount of faulty data is not large enough to form a separate cluster. Moreover, anomalies in the training data may be detected. Various clustering techniques are applied in this paper to provide a simple and unsupervised approach to anomaly detection. This could then be used as an efficient initial screening of the data streams before more detailed analysis is applied to suspicious parts of the data. The methods explored in this study include K-means clustering, Mixture of Gaussian models, density based clustering, self-organizing maps and spectral clustering. The performance of the various methods is reported, and also compared with that of other methods recently proposed for anomaly detection such as auto associative kernel regression (AAKR) and dynamical linear models (DLM). Overall, cluster-based methods are found to be promising candidates for online anomaly detection based on sensor data."}, {"label": 0, "content": "A waveform design procedure for improving the estimation of Doppler frequencies in active remote sensing applications is presented. The bound on frequency estimation is analyzed in terms of a continuous waveform, and the optimal waveform is inferred. Several waveform designs are analyzed, showing that a near-optimal dual-pulse waveform can achieve greater estimation accuracy than a single-pulse waveform using the same signal energy."}, {"label": 0, "content": "Demand for food is increasing with the growing world population. Cultivable land however is decreasing by the day due to rapid urbanization, and farm-yield needs to increase to meet ever-increasing food-security needs. Rapid strides in Internet of Things (IoT) have made it possible to carry out a comprehensive monitoring of the farm at various levels to help achieve this while optimally utilizing essential resources. To have a highly granular view of the farm, we have deployed an IoT based precision farming system consisting of a cluster of devices measuring over 14 ambient parameters below the soil, at the crop level, and the ambient environment. The system has been integrated with our digital farming platform on the cloud that provides a flexible interface to gather sensor data from disparate sources and issues contextual advisory to the farm supervisor in order to carry out specific actions on the field. The setup has been used so far to monitor two horticultural crops, cabbage and capsicum that were suited to the agro-climatic zone of the deployment region, for the Rabi (Winter) season of 2017. We present our experiences with the work and learnings from the deployment over the season which led to a reduction in agri-input cost by 20% and improvement in yields by more than 10%."}, {"label": 1, "content": "This paper proposes a novel method called Approximate Power Flow (APF) to tackle the non-convergence issue in power flow calculations. The APF model incorporates an active and reactive power decoupling technique, where the branch model with a virtual midpoint is considered the fundamental component of the research. The APF equation is derived based on the branch model, and an iterative solving method with excellent convergence properties is introduced. The algorithm's robustness is enhanced by implementing automatic adjustment measures for active and reactive power. To validate the effectiveness and feasibility of the proposed model, the error and robustness analyses were conducted on practical examples. The developed APF program based on the presented method can be utilized in actual large-scale power grids."}, {"label": 0, "content": "Existing IoT services are based on data communications technologies that do not involve the public switched telephone network (PSTN). Since the telephone numbers have been assigned to machine-type devices, PSTN switches can play a role in IoT service routing. In this article we deploy a PSTN-based IoT mechanism where the interaction between the users and the IoT devices is achieved through PSTN switches. To our knowledge, this is the first PSTN-based IoT solution in the world. With this mechanism, all PSTN customer premises equipment (CPE; fixed-line and mobile phones) can access IoT services without installing any software (mobile apps). By reusing the existing PSTN infrastructure, PSTN-based IoT offers telecom-grade service, security, and network management for IoT, which are very expensive to build in non-PSTN-based IoT. Our approach conveniently enables the existing CPE to access IoT applications, which will significantly promote the IoT service industry."}, {"label": 1, "content": "In this paper, we present the findings of our study on the impact of vegetation in paddy fields on the propagation of 2.4GHz (3mW) and 920MHz (20mW) signals, with antenna height as the parameter. To carry out the experiments, we utilized a commercially available wireless communication module equipped with a small pattern antenna. To account for the worst-case scenario of communication loss, we made detailed measurements assuming a rice height of 105cm and antenna heights of 55cm, 105cm, and 155cm. We applied the Exponential Decay (EXD) model to fit the measurement results, and the propagation characteristics in paddy fields were verified. \n\nThe results of our study showed that at 2.4GHz, there was attenuation due to the scattering of reflected waves caused by plant arrangement, even under line-of-sight conditions. We observed that the same degree of significant attenuation occurred regardless of the height below the rice top. In contrast, at 920MHz, which is known for strong diffraction, we discovered that the loss characteristic of vegetation was proportional to the height.\n\nOverall, our findings suggest that vegetation can have a considerable impact on signal propagation in paddy fields, and this must be accounted for when designing wireless communication systems for such environments. Our study also underscores the importance of considering antenna height as a parameter in the design of such systems to ensure efficient and effective communication."}, {"label": 0, "content": "Nowadays, human society has entered the digital and network information era. A unique feature of this era is the digitalization of personal identity. The worldwide demand for accurate and reliable identification and authentication is steadily growing, and biometric authentication technologies, such as fingerprint, face, finger vein, iris and DNA, are playing an increasingly important role in our society. Due to the rapid development of Internet and cloud computing technologies, cloud biometric authentication technology is becoming a vital development direction for biometric technology. Among these, iris recognition is an important biometric technology. This paper presents a method for cloud-based iris recognition to illustrate the relationship between cloud computing/storage and cloud recognition."}, {"label": 1, "content": "The integration of distributed energy resources (DER) systems in the electrical grid has given rise to uncertainties that traditional deterministic methods cannot address adequately. In response to this, Load Flow (LF) analytic tools must be optimized to account for such uncertainties and dependencies. One effective solution to this issue is the implementation of Probabilistic Load Flow (PLF), which treats these uncertainties as random variables and fulfills the growing need for swift and precise sampling techniques. Among the existing PLF methods, the Unscented Transform (UT) has proven to be dependable and efficient.\n\nThis paper delves into the details of three variations of the UT-based PLF method, scrutinizing their weighting and scaling parameters, in relation to the IEEE 30 test case. The findings of this analysis provide comprehensive insights into the effectiveness of these variants of PLF in addressing uncertainties in DER systems."}, {"label": 1, "content": "Effective management and provisioning of communication resources is essential in meeting the real-time requirements of smart city cyber physical systems (CPS) as managing computation resources. In smart cities, wireless mesh networks (WMNs) are a critical part of the communication infrastructure. However, enforcing distributed and consistent control in WMNs is challenging due to the fact that individual routers of a WMN maintain only local knowledge about each of its neighbours. This results in suboptimal resource management decisions as it reflects only a partial visibility of the overall network. \n\nWhen WMNs must adopt emerging technologies, such as time-sensitive networking (TSN) for the most critical communication needs, the challenges become more complex. Therefore, a viable solution is to adopt Software Defined Networking (SDN), which provides a centralized, up-to-date view of the entire network, by refactoring the wireless protocols into control and forwarding decisions. \n\nThis paper highlights the ongoing work to overcome the critical challenges and support the end-to-end real-time requirements of smart city CPS applications. With the adoption of SDN, effective management and provisioning of communication resources can be guaranteed to meet the real-time requirements of smart city cyber physical systems (CPS)."}, {"label": 0, "content": "In recent years, clustering has emerged as a promising approach to facilitate data routing and data aggregation in Wireless Sensor Network (WSN). Although clustering based routing approaches are appropriate for small-scale networks, they do not fit large scale WSNs as it is the case in LEACH [1]. Indeed, clustering suffers from the adverse effects of isolated nodes in the network and some coverage problems. To deal with these issues, we present LEATCH-L, a Low Energy Adaptive Tier Clustering Hierarchy for Large scale WSNs. The proposed approach makes the major functions of LEACH applicable to large-scale WSNs whose dimension is much larger than the largest transmission radius of the sensor nodes. The latter imposes a dynamic decomposable structure on the network topology which results in a set of smaller subnetworks. Such decompositions are implemented through a smart m-level hierarchical clustering process. Moreover, the proposed approach involves a two level data aggregation. Evaluation results show that the introduced approach is scalable with significantly much better performance than the state-of-the art approaches."}, {"label": 1, "content": "In recent years, significant progress has been made in the field of visual object detection, a fundamental task in industrial intelligence. Most existing methods are designed to work on single, well-captured still images. However, in practical robotic applications, a single image may not provide enough information due to nuisances such as partial view or occlusion. In such scenarios, an intelligent robot can adjust its viewpoint for better images, making active object detection a crucial perception strategy. \n\nThis paper proposes a deep reinforcement learning framework for active object detection, formulated as a sequential action decision process. The paper also proposes a novel deep Q-learning network (DQN) with a unique dueling architecture, featuring two separate output channels: one predicting action type and the other predicting action range. Combining the two output channels allows for more efficient exploration of the action space. The proposed method is extensively validated and found to perform the best, predicting actions in real-time."}, {"label": 1, "content": "As IoT becomes increasingly integrated into our daily lives, security concerns have emerged regarding wireless sensor networks (WSNs), which form the backbone of IoT infrastructure. One of the most pressing security issues is location privacy, which poses a significant threat to the confidentiality of sensitive information. In this article, we present the k-means cluster-based location privacy (KCLP) scheme as a solution to this problem.\n\nTo address the issue of protecting the source location, our proposed approach involves the use of fake source nodes to simulate the functionality of the real sources. By doing so, we can eliminate the possibility of an attacker tracing the signal back to its source node.\n\nTo safeguard the privacy of the sink location, we employ fake sink nodes and a specific transmission pattern. This ensures that all transmitted packets undergo the same path, thereby eliminating the possibility of an adversary inferring the location of the sink node.\n\nIn order to improve safety time - the time during which the location privacy of a sensor node is protected - a k-means cluster is used to create clusters of sensor nodes. Fake packets are then generated and must pass through these clusters. This allows us to improve safety time without compromising energy consumption.\n\nCompared to other algorithms, our KCLP scheme has been demonstrated to increase safety time and reduce delay, while minimizing energy consumption. By providing a robust solution to location privacy protection that optimizes energy consumption, our KCLP scheme represents an important step towards securing WSNs and the broader IoT ecosystem."}, {"label": 0, "content": "Power System Stabilizers(PSSs) are well-designed devices to measure and enforce improvements in synchronous generators\u2019 system-stability, which offer overwhelmingly superior cost performance compared to other optimal reconstruction or enhancement of power systems. The techniques of PSSs have been focused by power industry and academic circles in many years. The paper presents a performance comparison of several advanced techniques based on Adaptive Fuzzy Control, Artificial Neural Network (ANN), Genetic Algorithm(GA) and Hybrid Artificial Intelligent(HAI), Fuzzy Logic and Particle Swarm Optimization(FLPSO) techniques. With their merits on dealing with PSSs\u2019 implemental structures, models with unknown or variable parameters, we study the main indices to compare the performance of the referred intelligent techniques including simplicity of prototype, robustness and response speed, complexity of algorithm, flexibility in implementation and applicability to hybrid AVRs so on. The comparison results show that intelligent techniques improve PSSs comprehensive performance of being more effective and vigorous in damping out low frequency oscillations by overcoming inherent limitations in conventional control methodologies. Intelligent techniques could be especially considered in application of smart grid with large-scale grid-connected renewable energy power and random high power loads."}, {"label": 0, "content": "When taking pictures in a low-light scene with artificial lighting, we often encounter the following problem: we can use short exposure setting which yields a dim, noise image but with a sharp outline, or we can use a longer exposure setting which yields a bright, saturated image but with blurred areas. In many cases, none of those images is good enough. Good brightness and color information are retained in longer-exposure images, whereas sharp outlines are retained in shorter ones. In this paper, we present a patch-based method to combine such image pair into a better one. In our method, we firstly perform a coarse-to-fine strategy to detect inconsistent pixels caused by moving objects, then we draw information from the two exposures based on a novel patch-based technique. Experimental results show that the proposed method effectively preserves sharp edges of the short-exposure image, and maintains the color, brightness, and details of the long-exposure image."}, {"label": 1, "content": "Overall Equipment Efficiency (OEE) is a valuable tool for measuring the true production capacity of equipment, and the Theory of Constraints (TOC) has been utilized to enhance system production efficiency. To develop an OEE improvement method using TOC, a bottleneck identification model and buffer model have been established. A multi-attribute bottleneck identification model has been created on the basis of the Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) and Entropy Method. Furthermore, a time buffer model based on the Drum-Buffer-Rope (DBR) theory has been introduced. This OEE improvement method has proven to significantly enhance the OEE of bottleneck equipment. Additionally, by optimizing bottlenecks, system production efficiency is improved. A semiconductor package process was used to validate the efficacy of this method."}, {"label": 1, "content": "Traffic is a major issue in Lebanon due to the increasing number of cars that exceed the capacity of roads, inefficient public transportation infrastructure, and non-adaptive traffic light systems. Traditional traffic light systems cause traffic jams as they do not respond to the current state of traffic. An adaptive traffic light system was implemented in this study using reinforcement learning and tested using real data from Lebanese traffic. A software simulation tool was used to train and test the system, which allowed the neural network to interact with simulated traffic intersections. Compared to the traditional system, the proposed model resulted in a reduction of average queue lengths and average queuing times by 62.82% and 56.37%, respectively. This model provides a solution to the traffic crisis in Lebanon by adapting to variations in traffic levels and updating traffic signal phases in real-time."}, {"label": 0, "content": "With the increasing popularity of cloud/fog computing and because of the limited computing capability of wireless Internet of Things (IoT) terminals, big data have been sent to clouds/ fogs for analysis and processing in wireless IoT. However, how to carry out tensor analysis and processing without compromising security and privacy is a challenge in cloud/fog-based wireless IoT applications. Tensors have emerged as powerful tools for multi-dimensional data analysis and processing in wireless IoT applications. In this article, we propose novel privacy-preserving tensor analysis and processing models in cloud/ fog computing for wireless IoT applications. More specifically, we present a privacy-preserving cyber-physical-social big data processing model in cloud, privacy-preserving tensor analysis, a processing model based on tensor train networks in cloud-fog computing, and an optimization model for privacy-preserving tensor analysis and processing. We introduce a social recommendation system in smartphones as an example demonstrating the security and effectiveness of the proposed models for wireless IoT."}, {"label": 0, "content": "We propose a hybrid probabilistic interval prediction method for short-term load forecasting. The method combines K-means clustering based feature selection approaches and online Gaussian processes regression(OGPR) to generate better prediction results. The K-means clustering algorithm based feature selection are used to select the most relevant features during a dynamical process to better capture the load characters along with time. OGRP, includes dynamically updating the hyper-parameters and training sample sets as two key features, is served as a forecasting engine to carry out load probability interval prediction. The load data from Queensland market, Australia is used to validate the model proposed. The comparative results show that the proposed approach can obtain higher quality prediction interval."}, {"label": 1, "content": "The current sporadic task model is not compatible with real-time systems utilizing Simultaneous Multithreading (SMT). Despite being proven to enhance computing performance in various areas, SMT has not been extensively implemented in real-time systems. To address this discrepancy, a family of task models called SMART has been developed. The SMART models enable a combination of SMT and real-time capabilities by taking into account the varying execution costs of tasks attributed to SMT."}, {"label": 0, "content": "Recently, the rapid development of Artificial Intelligence (AI) has attracted more and more attention. It has been over two decades since AI techniques emerged in power systems as effective tools to solve many complex problems. The new generation AI technologies will promote energy transition and support future power system with no doubt. This paper tried to grasp the research hotspots, frontiers and mainstream trends of AI research in power systems through a literature data collected from the Web of Science (WOS) database between 2010 and 2018, by using a widely used tool in knowledge mapping-CiteSpace. The collaboration networks were analyzed among different countries/regions and institutions contributing to the publications. A detailed discussion was given based on the general statistical data and the visualized knowledge maps. The pivotal and mushrooming articles were identified and reviewed by introducing the betweenness centrality and citation bursts as indicators."}, {"label": 1, "content": "Nowadays, social media is an inseparable part of people's lives, with platforms like Instagram, Twitter, Facebook, and many others. Almost everyone has multiple social media accounts on their smartphone, which makes social media an excellent source of data for collecting public opinion instantly. This paper discusses sentiment analysis focusing on the user's satisfaction with telecommunication operators' data services in Indonesia, using official accounts or relevant keywords. To improve the accuracy of classification, the author used Support Vector Machine with TF-IDF weighting, POS Tagging, and Negative Handling. This study presents a system of sentiment analysis classification regarding the operator's data service user's satisfaction level using the support vector machine method. The SVM used in this analysis utilized an RBF kernel (Radial Basis Function) following preprocessing, POS Tagging, and TF-IDF weighting. The results of the study showed an average f1-score rate of 95.43%, precision of 92.45%, recall of 93.90%, and accuracy of 99.01%."}, {"label": 0, "content": "As a multimedia security mechanism, CAPTCHAs are completely automated public turing test to tell computers and humans apart. Although cracking CAPTCHA has been explored for many years, it is still a challenging problem for real practice. In this demo, we present a text based CAPTCHA cracking system by using convolutional neural networks(CNN). To solve small sample problem, we propose to combine conditional deep convolutional generative adversarial networks(cDCGAN) and CNN, which makes a tremendous progress in accuracy. In addition, we also select multiple models with low pearson correlation coefficients for majority voting ensemble, which further improves the accuracy. The experimental results show that the system has great advantages and provides a new mean for cracking CAPTCHAs."}, {"label": 0, "content": "The article presents a group method of data sharing, based on a two-level system of residual classes. Group methods are shown and investigated using the example of sharing a data with error correction. Various implementations of models for cryptographic and non-cryptographic information protection are presented."}, {"label": 1, "content": "Classification is a commonly used modeling method in data mining, which involves creating a predictive model to predict a categorical value, known as a class. Ensemble classification modeling is a technique that involves combining several base models with a combination algorithm for their predictions. The classification modeling process uses a set of training data instances, which consist of predictor variable values and categorical labels called classes. However, if a class has a much smaller number of training instances compared to other classes, it is known as a minority class, which can result in low classification performance. Positive-versus-negative (pVn) classification is an ensemble classification method that is applicable to multi-class prediction tasks. This paper reports on experimental results for the performance of a replication method for improving classification performance for minority classes using pVn classification modeling. The findings indicate that pVn classification models can improve classification accuracy for minority classes."}, {"label": 1, "content": "As cyberspace continues to evolve, so too does the number of individuals utilizing it as a means of communication. However, this increase in accessibility has also given rise to negative consequences, such as cyberbullying. Cyberbullying involves the deliberate and continual harassment of individuals through the use of information technology. It is essential to detect and prevent cyberbullying to ensure a safe and healthy social media platform.\n\nThis paper provides an overview of cyberbullying content on the internet, categorizes cyberbullying, considers sources of data for cyberbullying research, and examines machine learning techniques for cyberbullying detection. The main challenges related to cyberbullying detection are highlighted, including the lack of multimedia content-based detection and insufficient public datasets.\n\nAs a conclusion, this paper puts forward various suggestions to address the challenge of cyberbullying, including the creation of publicly accessible datasets containing multimedia content to aid in detection efforts. Furthermore, a multi-pronged approach that involves raising awareness about the effects of cyberbullying and engaging with individuals to promote positive online communication is recommended. Ultimately, by adopting these measures, we can work towards making cyberspace a safer and more positive environment."}, {"label": 0, "content": "Phasor measurement units (PMUs) can make state estimation more accurate by providing synchronized voltage phasor and current phasor measurements. Optimal PMU placement (OPP) minimizes the number of PMUs required for the system to be completely observable. This paper presents a DC state estimation model using mixed integer semidefinite programming (MISDP) approach for the OPP problem. A comparison between MISDP and mixed integer linear programming (MILP) is conducted. Power flow measurements, injection measurements, limited communication facility, and single PMU failure are studied for each approach. A formulation for MISDP-based PMU placement considering a single PMU failure is proposed. The advantages and disadvantages of each formulation are discussed."}, {"label": 1, "content": "The agriculture sector plays a crucial role in the economic growth of countries that rely heavily on agriculture, such as South Africa. Issues related to agriculture not only affect consumers but also have a direct impact on prices of food, which can lead to food price inflation. In this paper, we will explore the problem of soil manuring, which affects the productivity of agricultural lands. Leaves droppings are an excellent source of fertilizers that could help enhance soil productivity. However, understanding the factors that may affect the decomposition process is essential to achieve optimum food productivity.\n\nTo address this issue, our project focused on designing a compact monitoring circuit that uses the Redboard and GSM/GPRS module. We utilized three TCS3200 color sensors to detect leaves droppings, which covered a large area. The colors detected were then categorized into numbers for potential data analysis. Additionally, other sensors were used to measure soil moisture, soil temperature, ambient temperature, relative humidity, and dew point. These parameters were collected to identify possible factors that may affect decomposition.\n\nOur project encompassed a continuous real-time monitoring system from the site to the cloud platform. To achieve this, we developed a cloud platform deployed on Heroku, which was synced to the monitoring circuit for remote monitoring. With our platform, we were able to continually monitor the soil conditions, enabling us to track any potential changes that may affect the decomposition process.\n\nIn conclusion, agriculture plays a critical role in the economic growth of countries like South Africa. Soil manuring is one of the challenges that need to be addressed to increase food productivity. Our project provided a potential solution by developing a compact monitoring circuit that could continuously monitor the soil conditions and track any changes that may impact decomposition. Ultimately, our project aimed to help optimize food productivity by providing accurate data that could be used to enhance the productivity of agricultural lands."}, {"label": 0, "content": "In this paper, we propose a multi-cloud marketplace model for Infrastructure-as-a-Service (IaaS) layer with multiple cloud providers, intermediate brokers and end users. The brokers service end users subscribed to them by aggregating resources (virtual machines) from cloud providers while maximizing their profits. Similarly cloud providers (producers) allocate their supply of virtual machines to brokers (consumers) so as to maximize their profits. We define the notion of social welfare in this market structure and study two trading schemes. The first scheme involves centralized control which aims at maximizing social welfare but may contain unstable producer-consumer pairs who have an incentive to deviate from the current allocation. The second scheme eliminates such unstable pairs by using a generalization of stable matching algorithm but may lead to sub-optimal social welfare. The stable matching algorithm we proposed in this paper is a particular way of generalizing the original Gale-Shapley algorithm."}, {"label": 0, "content": "Fog computing aims to bring cloud computing capabilities to the edge of the network, closer to the end user, enabling lower latencies, location awareness, and mobility support among other advantages. The combination of IoT and Fog encompasses a highly complex scenario with a huge amount of data and varying number of devices that must cooperate with each other. Fog computing networks may be designed as autonomous networks. In this case, there is a requirement for effective management and orchestration mechanisms to guarantee acceptable performance of applications and services, while still leveraging of cloud capabilities. Mechanisms typically applied to \"cloud-only\" implementations cannot naturally be migrated to the Fog given its particular characteristics. This calls for the design and development of new management and orchestration mechanisms for the Fog. In this paper we propose a design the use the finite state machine to enhance decision making in an autonomous Fog computing network. The proposed scheme is expected to optimise Fog computing networks autonomy and improve performance and cost."}, {"label": 1, "content": "Effective voltage optimization is crucial for Distribution System Operators (DSO) as the performance of the entire distribution network depends on the voltage profile of the system. With the increasing use of Distributed Energy Resources (DERs), such as PV connected systems, optimizing voltage profiles becomes even more challenging. Smart inverters play a vital role in Volt-VAR Optimization (VVO), as they have the ability to regulate voltage using reactive power injection and absorption. However, choosing the optimal Volt-VAR Curve (VVC) for the smart inverters can be complex. \n\nTo address this issue, this paper proposes the use of Genetic Algorithm (GA) and formulates four objective functions for VVC selection, in conjunction with the integration of high-level penetration of PV smart inverters. The algorithm was tested on the standard IEEE 13 node distribution test feeder without the use of traditional voltage regulating devices, and the results showed that carefully selecting the optimal VVC can minimize overall system active power losses, depending on the VVC reactive power absorption and injection axis. \n\nIn conclusion, effective Voltage optimization is critical for DSOs, especially when dealing with DERs. Smart Inverters play an essential role in VVO, and with the use of GA and objective functions, selecting the optimal VVC is possible. The results of this study highlight the potential to minimize active power losses within the network by selecting the right VVC for the given scenarios."}, {"label": 1, "content": "Social media have become an integral part of our daily lives, facilitating communication, information exchange, and global interaction across distances. As such, the extraction of events from social media platforms for specific domains has become an emerging research trend in fields such as business intelligence and national security. However, the short length of Twitter messages and their frequent use of informal and ungrammatical language pose a challenge to traditional approaches for automatically detecting and categorizing events using streamed data in Event Message Identification systems. To overcome this challenge, this paper proposes a semi-supervised approach that combines the Support Vector Machine (SVM) algorithm with a corpus to identify targeted events from Twitter messages in specific locations. Experimental results demonstrate that our proposed semi-supervised SVM model outperforms existing models such as Logic Regression, Naive Bayes, and Decision Tree, making it an efficient tool for event detection in social media domains."}, {"label": 1, "content": "A comprehensive evaluation of protocols and algorithms for (wireless) networks requires both simulation and real-system experiments. However, this usually entails implementing the same mechanisms or protocols twice - one for discrete-event simulation and the other for real hardware. To address this issue, we propose a framework based on DPDK and OMNeT++ that allows simulations and real-system experiments to be run from the same code base. This provides the best of both worlds, i.e., scalable scenarios and reproducibility when simulating, as well as realistic behavior and real-world performance metrics when running real-system experiments. Our evaluation of representative real-world networking scenarios found similarities between simulation and real-system results, and the framework performed well, facilitating productive deployment using the codebase later on. Overall, our approach provides comparable results from both worlds, and eliminates the need for double implementations."}, {"label": 0, "content": "In this paper, we propose a solution to simplify reading and understanding medical documents via the automatic demystification of complex medical terms found in web pages. The suggested approach detects those terms using a combination of NLP and heuristics. It computes the probability that a word is a medical and complex term through text processing and analysis. It then makes use of an ad hoc dictionary to simplify the complex terms. Finally, it exploits the Microsoft Bing cognitive API to retrieve images and videos related to the complex medical terms detected and presents them to the users so that they may have a better understanding of the document being read."}, {"label": 0, "content": "The aim of this research is to develop an innovative low cost and affordable platform for smart home control and energy monitoring interfaced with augmented reality. This method will educate people about energy use at a time when fuel costs are rising and create novel methods of interaction for those with disabilities. In order to increase the awareness of energy consumption, we have developed an interactive system using Augmented Reality to show live energy usage of electrical components. This system allows the user to view his real time energy consumption and at the same time offers the possibility to interact with the device in Augmented Reality. The energy usage was captured and stored in a database which can be accessed for energy monitoring. We believe that the combinations of both, complex smart home applications and transparent interactive user interface will increase the awareness of energy consumption."}, {"label": 1, "content": "This paper explores the challenge of designing controllers for networked control systems that are vulnerable to cyber attacks. To address the limited communication resources, a hybrid-triggering communication strategy is employed. However, this approach makes the system susceptible to corruption from cyber attacks. The paper proposes a closed-loop system model that accounts for these factors and develops a stability criterion using Lyapunov stability theory and stochastic analysis techniques. The paper also leverages matrix inequalities to derive the desired controller gain. A numerical example is presented to illustrate the effectiveness of this approach."}, {"label": 1, "content": "This article proposes a simplified simulation and modeling technique for microgrids, which can reduce simulation time and computational memory requirements when compared to using a detailed model. Initially, the grid-tied inverter, integrated load, and nonlinear load in the microgrid are analyzed and modeled. Then, their models are simplified into Th\u00e9venin-Norton equivalents. Furthermore, the consistency between hardware and control equivalent models of power electronics devices is demonstrated. Finally, the proposed technique is verified by testing voltage sag in the improved IEEE 13 Node Test Feeder system in MATLAB/SIMULINK. The results reveal that the output characteristics of the simplified models match those of the detailed models."}, {"label": 0, "content": "With explosion of videos, action recognition has become an important research subject. This paper makes a special effort to investigate and study 3D Convolutional Network. Focused on the problem of ConvNet dependence on multiple large scale dataset, we propose a 3D ConvNet structure which incorporate the original 3D-ConvNet features and foreground 3D-ConvNet features fused by static object and motion detection. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, experimental results demonstrate that with merely 50% pixels utilization, foreground ConvNet achieves satisfying performance as same as origin. With feature fusion, we achieve 83.7% accuracy on UCF-101 exceeding original ConvNet."}, {"label": 1, "content": "We present a methodology for designing compact behavioral models to describe communication channels through borehole pipes in measurement while drilling (MWD) microwave systems. This approach utilizes data obtained from testing the channel, specifically from measurements of radio pulse signals. As an exemplar of this process, we detail the characterization of the microwave channel for decline-directed boreholes with a transmitter on the Gunn diode. Our procedure involves collecting signal samples from the transmitter, and then accounting for various electrodynamical parameters, as well as attenuation both in the media and on the pipe walls."}, {"label": 1, "content": "Efforts are underway in the Philippines to address the issue of limited internet access, which affects 42% of the population. A service has been introduced that installs Wi-Fi hotspots in public areas and government buildings, but in order for it to be effective, accurate data on Wi-Fi access point deployment is crucial. While there are platforms like WiGLE for crowdsourced data submission and dedicated wardriving methods, these can be costly and infrequent. To improve accuracy, regular data collection on Wi-Fi access points is necessary. Opportunistic wardriving is proposed as an inexpensive and frequent data gathering method, utilizing neighborhood public utility vehicles like tricycles to gather network data. Compared to crowdsourced data, opportunistic wardriving is able to detect at least 60% of access points in an area and covers a wide range, accessing 83% of the selected location - Brgy. Teachers Village East - in just 40 trips. This study aims to offer insight into a cost-effective method for gathering information on Wi-Fi deployment in various areas."}, {"label": 1, "content": "A hybrid expert control system for robot milling conditions control based on KUKA KR300 operating data has been proposed. In this system, milling conditions, measurements data, and negative factors involved in the milling process are taken into consideration. Expert estimations of actual milling episodes are used as a criterion for successful milling. Measurable milling quality parameters are examined as possible feedback for milling conditions control, and real-time parameters are used to make the final decision. The development of the hybrid expert system involves formalizing expert estimations, designing system structure, and signal processing. An example of data processing and distribution within the expert system has been given to further application in robotic milling control.\n\nThe introduction of this hybrid expert control system into robotic milling has two important functions for the manufacturing system. Firstly, it can accumulate knowledge about the manufacturing process in the form of measured data, simple integral, and qualitative expert estimations - all in a solid and compact data format. Secondly, it can generate control signals for the selection and maintenance of milling conditions using a rule's logic output system. With the implementation of this system, the manufacturing process can be more efficient and effective."}, {"label": 1, "content": "In this paper, we implemented the architecture of the DV700, which is a deep learning-based image recognition accelerator specifically designed for edge computing applications. The performance of this accelerator was measured in an FPGA environment. Our results showed that the DV700 operates at a speed of 12.9 fps when running the GoogleNet algorithm and 15.6 fps when running the SSD algorithm, with an operating frequency environment of 79MHz. These findings suggest that the DV700 can deliver reliable and fast image recognition capabilities for edge devices, making it an excellent choice for various computer vision applications."}, {"label": 1, "content": "Blockchain technologies, specifically smart contracts, offer a distinct platform for machine-to-machine communication. These contracts offer an exceedingly secure, unchanging record that can be accessed without trust or a central controlling entity. The purpose of this study is to analyze the possibilities and limitations of using smart contracts for machine-to-machine communication. In order to accomplish this, we created AGasP, an automated gasoline purchasing application. Our goal was to determine if smart contracts could effectively address the challenges of transparency, longevity, and trust in IoT applications. \n\nOur research has shown that smart contracts do, in fact, provide a solution to these challenges. They are highly efficient in maintaining data transparency, ensuring data longevity, and promoting trust between the involved parties. However, the practical applications of smart contracts require acknowledging important trade-offs, such as privacy and the difficulty of ensuring their correct implementation. Therefore, we conclude that using smart contracts in machine-to-machine communication is a useful method but must consider and address these trade-offs accordingly."}, {"label": 0, "content": "In this paper, we consider a two-way relay (TWR) visible light communication (VLC) system consisting of two users which communicate to each other with the help of a relay. To achieve efficient transmission, we introduce network coding (NC) into VLC system and we develop two energy-efficient NC-based strategies, namely straightforward network coding over finite-alphabet sets (SNCF) scheme and physical-layer network coding over finite-alphabet sets (PNCF) scheme, which need three time slots and two time slots respectively to achieve the communication. Simulation results indicate that our proposed SNCF scheme and PNCF scheme both can achieve great performance gains over the traditional four time-slot transmission scheme."}, {"label": 1, "content": "A novel methodology has been developed to address voltage constraints in distribution generation using a penetration capability approach. The main objective of this methodology is to determine the maximum capacity and best location for DGs without violating voltage limits. Rather than using combinatorial techniques to solve the problem, a two-stage methodology has been employed to obtain solutions. In the first stage, a linear method is used to estimate the capabilities for all possible locations of DGs. In the second stage, a detailed calculation stage is utilized to determine the exact maximum penetration capability using a nonlinear optimization method, considering the best location identified in stage 1. The proposed methodology provides an easy way to calculate the maximum penetration capability and corresponding location. The effectiveness of this methodology has been demonstrated using several examples in single and multiple DGs in IEEE 33-bus power system. The numerical studies demonstrate that this methodology is effective in calculating the maximum penetration capability and determining the best location for DGs in distribution network systems."}, {"label": 0, "content": "This paper investigates the differentially private problem of the average consensus for a class of discrete-time multi-agent network systems (MANSs). Based on the MANSs, a new distributed differentially private consensus algorithm (DPCA) is developed. To avoid continuous communication between neighboring agents, a kind of intermittent communication strategy depending on an event-triggered function is established in our DPCA. Based on our algorithm, we carry out the detailed analysis including its convergence, its accuracy, its privacy and the trade-off between the accuracy and the privacy level, respectively. It is found that our algorithm preserves the privacy of initial states of all agents in the whole process of consensus computation. The trade-off motivates us to find the best achievable accuracy of our algorithm under the free parameters and the fixed privacy level. Finally, numerical experiment results testify the validity of our theoretical analysis."}, {"label": 0, "content": "The operation and maintenance management of the distribution network (DN) mainly includes fault analysis, active early-warning and differentiated operation and maintenance. In the context of multi-time-scale and multi-spatial-temporal data in DN, this paper deals with the application of data mining for distribution network operation and maintenance management. In the paper, the one-dimensional fault feature is extracted from fault information by K-means clustering algorithm. Then, we employed Apriori algorithm to mine association rules of different failure modes and establish key performance matrix. The spatial-temporal characteristics are analyzed based on high-dimensional random matrix theory (RMT). Afterwards, one-dimensional and multi-dimensional fault features are combined based on D-S evidence theory so that the fault diagnosis criteria of DN is obtained. At the same time, comprehensively considering the DN operating state and the variation for power users, health index and importance index of equipment are established, which could help to significantly reduce the decision-making risk of DN operation and maintenance. The result of simulation proves the effectiveness of the proposed method."}, {"label": 0, "content": "We present a cofactor-based endmember extraction strategy for estimating green algae area in geostationary ocean color imager multispectral images. Our strategy improves the efficiency of the widely used N-FINDR endmember extraction method from two aspects. First, our strategy exploits the cofactor matrix for searching the largest simplex volume, which just computes matrix inverse and determinant for a small number of times (or even once). This is more efficient than the enumeration of determinants for all pixels in N-FINDR. Second, our strategy empirically obtains optimal endmembers through a few recursive iterations of cofactor matrix updates, contrasting a large number of repetitive volume maximizations with random initializations in N-FINDR. Experimental evaluation in terms of green algae area estimation validates that our strategy achieves the same accuracy as N-FINDR with much more efficiency."}, {"label": 1, "content": "This paper discusses the development of a three-dimensional displacement control system that utilizes Case-Based Reasoning technology. To improve the accuracy of the system in determining the three-dimensional location of the object being studied, Fuzzy Set Theory is employed. This is accomplished through the use of a Fuzzy Logic mechanism that processes signals and converts input data into linguistic variables using a seven-value term-set. To determine membership functions, a search mechanism is implemented while a rule base is formed to ensure defuzzification of output data. Overall, this research presents important insights into the use of advanced technologies to improve the accuracy and effectiveness of three-dimensional displacement control systems."}, {"label": 1, "content": "The combination of high-resolution optical imagery and airborne LiDAR data is an effective method for extracting impervious surfaces in urban areas. In this study, the authors propose a novel hierarchical multiscale super-pixel-based classification method for impervious surface extraction using WorldView-2 and normalized digital surface model (nDSM) images. The method involves splitting ground and nonground objects based on nDSM thresholds, generating super pixels using a hierarchical multiresolution segmentation method, and constructing optimal deep residual network (ResNet) and Spatial Pyramid Pooling (SPP-net) models for training. The authors also propose an adaptive method for determining multiscale input images based on the segmentation of super pixels, which improves feature extraction. The results show that the proposed method significantly improves accuracy compared to traditional pixel-based and single scale methods."}, {"label": 1, "content": "Humans can effortlessly perceive a vast amount of information while observing a scene. In contrast, scene recognition is a challenging problem for computers due to the variability, ambiguity, and diverse illumination and scale conditions of scene images. Therefore, scene classification is an essential task that provides contextual information to guide other processes, such as browsing, content-based image retrieval, and object recognition. To better evaluate the proposed solution, a baseline model based on the traditional bag of words model is established. In addition, a model based on the idea of fine to coarse category mappings is proposed, which combines information with the fusion of feature descriptors to yield a single feature representation. This approach exploits the hierarchical relationships among the scene categories, leading to enhanced performance. Various evaluation metrics validate the effectiveness of the proposed method, which outperforms the given baseline and several state-of-the-art methods. Moreover, the proposed framework achieves an appropriate balance between the time and accuracy of the model."}, {"label": 0, "content": "In order to achieve fault diagnosis and remaining useful life prediction effectively when system states and fault parameters run at two different time-scales, a double time-scale particle filter algorithm is proposed to jointly estimate the system states and the possible fault parameters. The bond graph model that deals with various energy forms in a unified way is constructed to carry out fault diagnosis which consists of fault detection and isolation through deducing analytical redundancy relations to calculate residual of system. The fault estimation is used to identify the parameters of the isolated fault set and determine the true fault as well as remaining useful life prediction. Since the fault parameters exhibit slow-time varying characteristics and the system states show quick-time varying characteristics, the double time-scale particle filter is developed to simultaneously estimate and predict the system states and parameters on two different time scales for diagnosis and prognosis purpose. The effectiveness of the proposed double time-scale particle filter algorithm is validated by the simulation results on the nonlinear electromechanical system."}, {"label": 0, "content": "With the success of Convolutional Neural Networks (CNN) in computer vision tasks, Steganalysis, the technique of detecting hidden secret messages within images, is moving away from Feature Engineering to Network Engineering. Deep neural networks are being proposed to model and capture the weak embedded signals, in such a low Signal-to-Noise (SNR) scenario. In this paper, we propose a novel Convolutional Neural Network based on aggregated residual transformations, which generate stronger image representations helpful for steganalysis. The architecture has very few hyperparameters to set and focus on increasing the classification accuracy while keeping the depth and number of parameters fixed. The residual skip connections further help preserve the weak embedded signals and improve the gradient flow. We evaluated our proposed CNN on BOSSbase against S-UNIWARD and HILL steganographic algorithms with different payloads. Comparing with the state-of-the-art Deep Residual Learning (DRN) based on Residual Learning and the SRM plus Ensemble, our proposed CNN gives a better classification Accuracy."}, {"label": 0, "content": "This paper attempts at improving the accuracy of Human Action Recognition (HAR) by fusion of depth and inertial sensor data. Firstly, we transform the depth data into Sequential Front view Images(SFI) and fine-tune the pre-trained AlexNet on these images. Then, inertial data is converted into Signal Images (SI) and another convolutional neural network (CNN) is trained on these images. Finally, learned features are extracted from both CNN, fused together to make a shared feature layer, and these features are fed to the classifier. We experiment with two classifiers, namely Support Vector Machines (SVM) and softmax classifier and compare their performances. The recognition accuracies of each modality, depth data alone and sensor data alone are also calculated and compared with fusion based accuracies to highlight the fact that fusion of modalities yields better results than individual modalities. Experimental results on UTD-MHAD and Kinect 2D datasets show that proposed method achieves state of the art results when compared to other recently proposed visual-inertial action recognition methods."}, {"label": 0, "content": "We consider the problem of minimizing a block separable convex function (possibly nondifferentiable, and including constraints) plus Laplacian regularization, a problem that arises in applications including model fitting, regularizing stratified models, and multi-period portfolio optimization. We develop a distributed majorization-minimization method for this general problem, and derive a complete, self-contained, general, and simple proof of convergence. Our method is able to scale to very large problems, and we illustrate our approach on two applications, demonstrating its scalability and accuracy."}, {"label": 0, "content": "Device Fingerprinting (DFP) is the identification of a device without using its network or other assigned identities including IP address, Medium Access Control (MAC) address, or International Mobile Equipment Identity (IMEI) number. DFP identifies a device using information from the packets which the device uses to communicate over the network. Packets are received at a router and processed to extract the information. In this paper, we worked on the DFP using Inter Arrival Time (IAT). IAT is the time interval between the two consecutive packets received. This has been observed that the IAT is unique for a device because of different hardware and the software used for the device. The existing work on the DFP uses the statistical techniques to analyze the IAT and to further generate the information using which a device can be identified uniquely. This work presents a novel idea of DFP by plotting graphs of IAT for packets with each graph plotting 100 IATs and subsequently processing the resulting graphs for the identification of the device. This approach improves the efficiency to identify a device DFP due to achieved benchmark of the deep learning libraries in the image processing. We configured Raspberry Pi to work as a router and installed our packet sniffer application on the Raspberry Pi. The packet sniffer application captured the packet information from the connected devices in a log file. We connected two Apple devices iPad4 and iPhone 7 Plus to the router and created IAT graphs for these two devices. We used Convolution Neural Network (CNN) to identify the devices and observed the accuracy of 86.7%."}, {"label": 0, "content": "Planetary gear is an important part of the transmission system of large electromechanical equipment. Therefore, it is very important to monitor the degradation of the state of the planetary gear. A method for the degradation state recognition of planetary gear based on the features with multiple perspectives and linear local tangent space alignment (LLTSA) algorithm is presented. First, the time domain features of the original vibration signal are extracted, which have the statistical properties and global significance. Then, the detailed features which pay more attention to the detailed information of the vibration signal are extracted on the basis of improved complete ensemble empirical mode decomposition with adaptive noise, and all those features constitute high dimensional original features. In order to solve the problems of information redundancy and interference features, the original features are processed by LLTSA, and the extraction of low dimensional sensitive features can be achieved. Finally, the optimized support vector machine is studied to recognize the low dimensional sensitive features. The result shows that the proposed method can recognize different degradation states of planetary gear accurately and effectively."}, {"label": 1, "content": "Virtualization is a crucial technology for cloud computing as it allows for the sharing of computing resources. To accomplish virtualization, algorithms and mechanisms for virtual resource allocation, virtual machine deployment, migration, and server consolidation are necessary. Although previous studies focused primarily on virtual resource allocation amongst servers, the increasing utilization of cloud servers with multiple cores has raised the importance of virtual machine resource allocation in a single server. \n\nThis paper presents a novel multi-objective virtual machine placement algorithm that considers both load balancing and energy efficiency criteria in a multi-core server utilizing Network-on-Chip architecture. The algorithm relies on Markov approximation optimization theory. We conducted extensive experiments to evaluate the proposed algorithm's performance, and the results demonstrate that our approach achieves higher energy efficiency, load balancing, and calculation speed compared to state-of-the-art algorithms."}, {"label": 1, "content": "Optical combustion measurement and analysis systems have become popular, especially those utilizing multiple sensors. One such system is the image-based flame 3D reconstruction using computerized tomography. However, previous techniques were not robust against noise and blurring due to the underlying least square-based loss function.\n\nTo overcome this challenge, a new 3D density flame reconstruction method has been proposed in this paper. The method is a constrained optimization problem between the flame and its projected images, using sparse multi-view images. For more accurate estimation of the flame with a complex structure from an arbitrary viewpoint, the method involves extracting the 3D candidate region of the flame and then estimating the density field using compressive sensing.\n\nThe proposed method has an objective function that is a linear combination of the photo consistency cost and sparsity regularization terms. This combination helps to avoid blurring in the reconstruction process. The approach is well-suited for matrix factorization as each voxel is represented as a linear combination of a small number of basis vectors. This method effectively simplifies the reconstruction process and provides the whole 3D density field in one step.\n\nExperimental results show that the proposed 3D density estimation performs favorably even with limited flame images. Overall, this method is a significant improvement over previous techniques and holds promise for future flame reconstruction applications."}, {"label": 0, "content": "Modern tactical wireless network (TWN) communication technologies are not only capable of transmitting voice but also capable of transmitting data. Due to such capabilities, TWN have high security requirements as any security breach can lead to detrimental effects. Hence, securing such an environment is not only a requirement but also a virtual prerequisite to the network centric warfare operational (NCW) theory. One key to securing this environment is to promptly and accurately recognize information warfare attacks directed to the network and respond to them. This is achieved using intrusion detection systems (IDS). However, false detection of nodes in hostile environment remains a major problem that need to be addressed. Recently, machine learning methods and algorithms have shown applicability and are growing research area for cyber security and intrusion detection. Conversely, several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. The question then becomes, which one amongst these machine learning algorithms have the potential to enhance or address IDS issues in TWN. In this paper, seven machine learning classifiers are analyzed; Multi-Layer Perceptron, Bayesian Network, Support Vector Machine (SMO), Adaboost, Random Forest, Bootstrap Aggregation, and Decision Tree (J48). WEKA tool was used to implement and evaluate the classifiers. The results obtained indicate that ensemble-based learning methods outperformed single learning methods when we consider the detection accuracy metrics; AUC, TPR, and FPR. However, ensemble classifiers tend to be slower in in terms of build time and model test time."}, {"label": 0, "content": "Dynamic voltage restorers have been examined to compensate voltage sags in distribution networks to avoid production losses, especially in the Premium Power Park(PPP). If the compensation strategy can be optimized to extend the compensation time as long as possible, then the application of DVR in power system can be more popular, and the intelligent level of the power grid can also be effectively improved. As a result, this paper proposed a time-maximized compensation strategy to achieve a maximum compensation time based on the energy-minimized compensation strategy. According to the formula of dc-link discharge, the calculating formula of optimum phase angle jump of load voltage is deduced, and the principle and characteristic of the compensation strategy are analyzed respectively for single-phase and three-phase system. Simulation and experimental results are presented to confirm the effectiveness of the proposed compensation strategy."}, {"label": 0, "content": "The output frequency response for most DACs rolls off according to the sin(x)/x frequency-response envelope [1]. This paper derives a FIR filter which is designed in the minimax sense with the ripple constraints as the design criteria for the frequency response compensation of DACs. The filter order estimation function under the criterion gives a detailed filter design configuration method, which can effectively shorten the design time and provide an accurate resource configuration reference for the top-level system design [2]. The simulation example verifies the advantages of the compensation filter in minimax sense and shows the performance of the compensation filter and the accuracy of the order estimation function."}, {"label": 1, "content": "This paper puts forward an innovative method of power load modeling which utilizes multiple RBF neural network models. At first, a sub-model is established by utilizing RBF neural network to describe common load features, thus overcoming the limitations posed by load constituents while aligning with the flexible growth of modern power systems. In addition, several typical load models are amalgamated into one integrated model based on Bayesian estimation theory. This improved approach solves the traditional load model's weakness in terms of generalization competence and accommodates the fluctuation of power load over time. Ultimately, the method is put into practice within the IEEE 14-bus testing system, yielding successful results, which validates the merits of the proposed method."}, {"label": 0, "content": "Electromagnetic transient (EMT) simulation programs are often used to obtain the optimal values of circuit and control parameters. In this paper, an EMT-simulation-aided optimization technique using the Kriging method is proposed. Then, it is applied to the design of the automatic reactive power regulator (AQR) controller of a static synchronous compensator (STATCOM). It is shown that the optimal solution of the AQR controller can be obtained efficiently by the proposed technique."}, {"label": 0, "content": "The localization is the one of the most promising technology in modern society. Wireless sensor network (WSN) as a popular method to solve the problem is being studied by many scholars. The main challenge in localization problem is the non-line of sight (NLOS) propagation. To overcome the issue, we present a method combing the particle filter and residual analysis. The residual analysis is a posteriori reliable algorithm. Particle filter is a powerful technique because it does not make any presumptions about the probability density function or the linearity of the system model. The mix of the residual analysis and particle filter could improve the localization accuracy. At the same time, the randomness of the particle improves the robustness of the method. Simulation results evaluate the effectiveness and the robustness of the proposed method."}, {"label": 0, "content": "Livestock is an essential commodity, especially in Indonesia, an agricultural country located in the Asia Pacific. Indonesia has vaster Greenland area than any other ASEAN countries. One way to increase livestock production is by implementing Smart Livestock Monitoring System which relies on Internet of Things technology such as LoRa. LoRa and other Low Power Wide Area (LPWA) technologies offer great advantages such as extended transmission range, extended node's battery life, and a massive number of nodes per gateway support. However, LoRa utilizes sub 1 GHz unlicensed spectrum which has been very crowded since the last decades and will be even more crowded soon following IoT trends. We propose to use mobile LoRa gateway to mitigate this issue. This paper simulates and compares deployments of LoRa using one mobile gateway with one and multiple static gateways. LoRa is simulated using one mobile gateway and compared with one and multiple static gateway deployments. Simulation result shows that one static gateway is better for narrow livestock area because it gives sufficient Data Extraction Rate (DER) value required for data transmission and lowest Network Energy Consumption (NEC) value. Otherwise, for vast livestock area, one mobile gateway is better because of smaller deployment cost and more than sufficient DER value."}, {"label": 0, "content": "We propose a system of electric drive control of the coal miner, which allows to increase the reliability and safety of its operation. The problem is solved by predicting the diagnosed parameters of the power elements and preemptive control actions on the control system of the electric drive and the organization of maintenance and repair. Forecasting values of drive parameters is carried out using a neural network model, and allows to increase the mean time between failures and availability of the electric drive compared with the control system without prediction parameters."}, {"label": 0, "content": "Microscopic traffic simulation is associated with substantial runtimes, limiting the feasibility of large-scale evaluation of traffic scenarios. Even though today heterogeneous hardware comprised of CPUs, graphics processing units (GPUs) and fused CPU-GPU devices is inexpensive and widely available, common traffic simulators still rely purely on CPU-based execution, leaving substantial acceleration potentials untapped. A number of existing works have considered the execution of traffic simulations on accelerators, but have relied on simplified models of road networks and driver behaviour tailored to the given hardware platform. Thus, the existing approaches cannot directly benefit from the vast body of research on the validity of common traffic simulation models. In this paper, we explore the performance gains achievable through the use of heterogeneous hardware when relying on typical traffic simulation models used in CPU-based simulators. We propose a partial offloading approach that relies either on a dedicated GPU or a fused CPU-GPU device. Further, we present a traffic simulation running fully on a manycore GPU and discuss the challenges of this approach. Our results show that a CPU-based parallelisation closely approaches the results of partial offloading, while full offloading substantially outperforms the other approaches. We achieve a speedup of up to 28.7\u00d7 over the sequential execution on a CPU."}, {"label": 0, "content": "A parameter estimation algorithm for multiple frequency-hopping (FH) signals based on maximum energy difference is proposed in this paper. First, the time-frequency (TF) matrix is obtained by TF analysis such as short-time Fourier transform (STFT) and smoothed pseudo wigner-ville distribution (SPWVD). Then, the carrier frequency and the TF data with valid frequency are determined according to the distribution of energy. Next, the number of signal segments and window length in each nonzero row of the TF data is obtained. Finally, hopping time and hop cycle are estimated based on the maximum energy difference. Simulation results indicate that the proposed algorithm has better anti-noise performance than the TF pattern modification method and the STFT-SPWVD method. The new method is suitable for asynchronous and synchronous network."}, {"label": 0, "content": "Software Defined Networking (SDN) breaks the vertical integration of existing Internet architecture and makes the network programmable from a logically centralized control point. Even though the centralized network control provides several advantages, attacks toward SDN framework remain as a challenge. In this paper, we propose a method based on machine learning to detect Denial of Service (DoS) attack in data plane devices, i.e., the OpenFlow switches, resulting from flow-table overflow. We created an SDN dataset using Mininet and features are extracted from switch-controller communication trace as well as flow-table snapshots of OpenFlow switches. Further, we use three algorithms, (i) Neural Network, (ii) Support Vector Machines, and (iii) Naive Bayes, to classify the network to either malicious or benign. The results show that neural network and Naive Bayes provide 100% accuracy with the extracted features."}, {"label": 1, "content": "In IaaS clouds, users access virtual machines through a management server. However, in semi-trusted clouds, cloud operators may not be trusted and can execute arbitrary management commands, leading to VM redirection attacks. This is because the user-VM binding is weak, making it difficult to enforce the execution of only users' management commands. To address this issue, this paper proposes UVBond, which strongly binds users to their VMs. UVBond decrypts the user's VM disk within a trusted hypervisor and issues a VM descriptor to securely identify the VM. To bridge the semantic gap between high-level management commands and low-level hypercalls, UVBond uses hypercall automata. The implementation of UVBond on Xen confirms that it prevents attacks and has minimal overhead."}, {"label": 0, "content": "In order to discriminate the real targets, the clutter and the dense multi-false targets, we propose a factorized convolutional neural network-based algorithm for radar targets discrimination. We establish the factorized convolutional neural network model with depthwise separable convolution. To reduce the parameters of the model, we establish the simplified factorized convolutional neural network by reducing the numbers of both convolutional filters and connection nodes of fully connected layers. The result of the measured data demonstrates that, as compared with the existing model, the simplified factorized convolutional neural network has higher discrimination rate for the real targets, the clutter and the dense multi-false targets, and its parameters are less than ten percent counterpart of a recent proposed model."}, {"label": 1, "content": "The planetary gear plays a crucial role in the transmission system of large electromechanical equipment. Given its importance, monitoring the state of degradation of the planetary gear is critical. In this context, a new approach is proposed for recognizing the degradation state of planetary gear through multiple perspective features and the linear local tangent space alignment (LLTSA) algorithm. Specifically, the proposed method extracts time domain features from the original vibration signal, which have both statistical properties and global significance. Moreover, detailed features are extracted using an improved complete ensemble empirical mode decomposition with adaptive noise, to pay closer attention to the finer details of the vibration signal. These features together form high dimensional original features, which are processed by LLTSA to extract low dimensional sensitive features while addressing the problem of information redundancy and interference features.\n\nFinally, the optimized support vector machine is implemented to recognize the low-dimensional sensitive features. The results demonstrate that the proposed method can accurately and effectively recognize different degradation states of planetary gear, thus providing an effective tool for monitoring the health of the transmission system of large electromechanical equipment."}, {"label": 0, "content": "The line loss in power distribution network is an important index that affects the economic benefit of power supply enterprise. In order to ensure the accuracy and stability of the line loss calculation based on large amount of power measurement data, the distributed parallel processing method is applied to the line loss computing service, and the line loss calculation model in power distribution network was obtained by the fitting of BP neural network. Furthermore, many examples are given to test the algorithm proposed in this paper, the results show that the method can guarantee the stability and the accuracy of calculation results in line loss calculation."}, {"label": 1, "content": "This article presents a medical simulation solution that improves clinical training by providing a physician with a graphical interface to interact with. The objective is to enhance learning and internalization of clinical procedures. Medical simulation is a rapidly growing area of focus within the broader biomedical simulation field. Many biomedical simulation centers have emerged, primarily focused on creating highly realistic simulations to provide health professionals with an opportunity to improve their skills, optimize their performance and anticipate unexpected situations such as critical or complex events. However, these simulation systems are not without limitations. They struggle to develop new scenarios due to their limited level of modularity and number of simulated situations. Additionally, the training of professionals is restricted to simulation centers. The goal of this platform is to develop serious games that simulate a myriad of clinical situations to facilitate access to training and improve performance. By simulating real scenarios, we can better prepare health professionals to handle unexpected events, ultimately benefiting patient outcomes."}, {"label": 0, "content": "The electrostatic sensing technique has been verified to be a viable method for tribo-contact monitoring under laboratory conditions in previous investigations. The monitoring of wind turbine gearbox is a possible approach for electrostatic application. It usually under variable operating conditions while working. This paper introduced a new method called moving window local outlier factor (MWLOF) to process electrostatic monitoring signals with multi-sensors. It can detect early faults earlier and has a better sensitivity with the information fusion method than conventional techniques, which leads to a better industrial application in future."}, {"label": 1, "content": "The accuracy of measuring technology has a significant impact on the process of creating permanent joints. This is particularly true for induction heating, which can induce errors in the measurement equipment due to the different emissivity values of the materials being used. Non-contact pyrometric sensors are commonly used to take readings during induction soldering, but these readings are also prone to errors due to various factors. In order to maintain high-quality control over the induction soldering process, it is important to identify and minimize these errors.\n\nWith this in mind, this article proposes the use of intelligent methods to identify errors in measuring instruments. Specifically, the article suggests the use of artificial neural networks as a support algorithm. The software used for this approach includes the Python programming language and the Anaconda set of libraries. The article also outlines an effective process for identifying the optimal neural network structure and parameters.\n\nExperimental results have demonstrated the effectiveness of using intelligent methods for identifying errors in measuring instruments. This approach can help to improve the quality of control over the induction soldering process, as well as other methods for creating permanent joints. With the use of artificial neural networks, it is possible to minimize errors and ensure that the measurement equipment is producing accurate readings."}, {"label": 0, "content": "In order to increase the capacity and improve the spectrum efficiency of wireless communication system, this paper proposes a rate-based iterative one-to-one matching game algorithm to realize multi users access an energy-harvesting small cells considering NOMA (non-orthogonal multiple access) in heterogeneous cellular networks. First, we use a heuristic clustering based channel allocation algorithm to assign channels to small cells and reduce the interference. Then we model user access problem as an iterative one-to-one matching game with rate as its utility, where one user matches with one small cell at each matching game and so is the small cell. After that, we propose an algorithm to iteratively enable users to access small cells in terms of proposed matching game and prove its stability. Finally, a power allocation algorithm to reallocate transmission power for each user is presented to fully use the harvesting power. Simulation results show that this algorithm outperforms OMA (orthogonal multiple access) system in efficiency besides improving the system capacity."}, {"label": 1, "content": "Pulmonary and respiratory diseases are a significant global health burden, particularly in developing countries where resources for diagnosis are limited, leading to misdiagnosis and underdiagnosis. To address this issue, we have developed a mobile toolkit that consists of a low-cost peak flow meter and a mobile phone app called Pulmonary Screener. This toolkit can screen and diagnose three of the most common pulmonary and respiratory diseases, including Asthma, Chronic Obstructive Pulmonary Disease (COPD), and Allergic Rhinitis (AR).\n\nThe Pulmonary Screener app uses machine vision software to capture the reading from the peak flow meter automatically, without requiring any electronics or Bluetooth radio. The app then uses an integrated clinical questionnaire and a machine learning model to calculate individual probabilities of a patient having a specific pulmonary disease or comorbidity. The model was trained using diagnostic data from 325 patients collected over the past three years and achieves accuracy values above 0.90 for all diseases and combinations, except for Asthma with AUC = 0.84. Sensitivity and specificity values for all diseases are also greater than 0.90.\n\nOverall, this is the first clinically validated mobile tool capable of diagnosing multiple pulmonary diseases using a single app. Future versions of the Pulmonary Screener will expand support for infectious diseases such as pneumonia and tuberculosis and include features extracted from auscultation and cough sounds. With the Pulmonary Screener, health workers and general practitioners in developing countries can now diagnose pulmonary diseases more accurately, leading to improved outcomes for patients."}, {"label": 1, "content": "We present two sets of perceptual evaluations of our South African text-to-speech voices carried out by language practitioners. The first evaluation measures baseline quality, examining the comprehensibility and naturalness of the voices, as well as their usability in various settings. Based on our findings, we sought to refine the pronunciation of our voices in a second evaluation, comparing the improved versions with their baseline counterparts. Though the results of the evaluation reveal success in several areas, particularly in regard to certain languages, we have also identified areas where further improvement is needed, particularly in the case of certain African languages."}, {"label": 0, "content": "The provision of Cloud services through the use of a Cloudlet technology, provides a number of advantages with regards to Quality of Experience (QoE) for its consumers. The QoE includes, but is not limited to free of cost, low-latency, and one-hop WiFi network consumption. Most of the advantages are achieved through the deployment of a comprehensive business model. A well-defined business model is one with well-defined value logic. Value creation is a fundamental element of a business model. It helps identify relevant customer segments, value proposition for customers and mechanism that will be used to provide the created value. However, poorly defined value logic into a business model can lead to business failure in addressing their customers' needs and issues. The literature reveals that researchers delved more into the evaluation of Cloud business models and its subsequent value creation, but none has attempted on either creation or deployment of a Cloudlet Business model. Therefore, this study introduces a Cloudlet Business Model (CBM) that can be used to support the easy deployment of a Cloudlet technology by SMEs such as coffee shops and shopping malls. In order for SMEs to meet their customers' needs and be able to sustain their businesses. This study undertakes the use of a Four Box model to bring out the value perspective of a CBM. This resulted in the identification of the value as small data, how it is generated, delivered and compensated for especially cost in ensuring the effectiveness of a CBM on the SMEs."}, {"label": 1, "content": "In this paper, the authors propose a new range-free localization method for localizing sensor nodes in anisotropic networks. Traditional range-free localization methods assume that the hop-size of all links is the same, which is only true when the node distribution is balanced. However, this assumption is not practical due to the random deployment of nodes in wireless sensor networks. To overcome this issue, the proposed method uses the expected distance and hop-count between the sensor nodes to determine the hop-size. This method is applied to anisotropic networks with obstacles, which are common in real-world scenarios. \n\nExtensive simulation studies were conducted to validate the proposed method's accuracy under the effects of log-normal shadowing, a practical scenario. The results show that the proposed method outperforms the DV-Hop and Reliable anchor pair selection method (RAPS) techniques by up to 35% and 15%, respectively, in the literature. Therefore, this method significantly improves the accuracy of range-free localization in anisotropic networks with obstacles."}, {"label": 0, "content": "As a widely used power system analytical toolset in the world, Power System Simulator for Engineering (PSS/E) is also characterizing with its powerful user-defined function. This paper illustrates a study of user-defined modeling function in PSS/E through modeling a VSC-HVDC transmission system. The user-defined modeling (UDM) function of PSS/E is introduced in detail, then a modeling method for VSC-HVDC is proposed. Through comparing the simulation curves of user-defined VSC-HVDC in PSS/E with that simulated by PSCAD/EMTDC, the correctness of the VSC-HVDC user-defined model, the feasibility as well as the practicability of PSS/E's user-defined function are validated. The modeling method proposed in this paper can provide guidance and a basis for the modeling of other complex components in PSS/E for simulations."}, {"label": 0, "content": "With the wide application of power electronic technologies in power system, such as HVDC, new energy power stations and FACTS, the dynamic characteristics of the interconnected power grid have changed dramatically, and the traditional quasi steady model based electromechanical transient program cannot simulate the fast switching process of the power electronic components and its non-fundamental frequency or asymmetry characteristic, which is important to the accuracy of HVDC and FACTS models. Because of the limitation of calculation load and computing speed, traditional electromagnetic transient program cannot replace electromechanical transient program in large-scale power system stability analysis. FEMTP should be redesigned for transient stability analysis of large-scale electronic power system. Then the generator and transmission line's electromagnetic models are discussed in this paper, and the discussion and simulation shows the adequation of their electromechanical transient model in FEMTP. Afterwards, the implementation of FEMTP was discussed in detail, and the example of China East power grid shows the appropriation of simplified electromechanical models and proposed technologies."}, {"label": 0, "content": "Worst-case timing analysis of Networks-on-Chip (NoCs) is a crucial aspect to design safe real-time systems based on manycore architectures. In this paper, we present some potential extensions of our previously-published buffer-aware worst-case timing analysis approach to cope with bursty traffic such as real-time audio and video streams. A first promising lead is to improve the algorithm analyzing backpressure patterns to capture consecutive-packet queueing effect while keeping the information about the dependencies between flows. Furthermore, the improved algorithm may also decrease the inherent complexity of computing the indirect blocking latency due to backpressure."}, {"label": 0, "content": "In order to determine the accurate allowed penetration level of small hydropower (SHP), the random characteristic of the loads and SHP output are considered in this paper, and the corresponding stochastic models are established. While weather condition for different regions differences, the concept of regional synchronization feature of SHP generation is proposed, and non-parametric kernel density estimation method is used for hydropower output modeling with the consideration of randomness. The expressions for calculating capacity of SHP are presented based with the node voltage constraint. The differences between the two stochastic models are taken into consideration, and the correction coefficient is introduced in the expression. Finally, the effectiveness of the method proposed in this paper is validated by a case study based on the load data of a real distribution network in Southern China Power Grid."}, {"label": 1, "content": "The increasing demand for wireless communications has highlighted the issue of spectrum crunch, leading to the exploration of other sources of bandwidth. Visible light communication (VLC) has emerged as a promising alternative due to its high capacity, although it suffers from limited coverage. To address this challenge, we propose the integration of VLC with device-to-device (D2D) technologies in heterogeneous networks, where mobile users can act as relays to extend the coverage of VLC. However, determining optimal routes for data transmission in such networks is a difficult task, as mobile users exhibit distributed behaviors. In this paper, we present a reinforcement learning (RL) based approach to determine multi-hop data transmission routes in an indoor VLC-D2D heterogeneous network. Our method uses an equilibrium problem with equilibrium constraints to dynamically obtain rewards for the RL algorithm, and employs the alternating direction method of multipliers to solve it. The proposed technique achieves optimal routes for data transmission in a distributed way, as confirmed by our simulation results. The learning algorithm enables us to achieve transmission routes with low delays and high capacities, making it a promising strategy for indoor VLC-D2D networks."}, {"label": 1, "content": "Making decisions in the midst of a disaster can be extremely difficult, especially when human lives and key infrastructures are at risk. One crucial factor that must be taken into account when allocating resources during these high-stress situations is the inter-dependencies of critical infrastructures. To aid in this process, i2Sim\u2014a sophisticated tool built for this very purpose\u2014has been developed. With a layered architecture, this tool boasts a dedicated decision-making layer and employs a machine learning approach known as Reinforcement Learning (RL), wherein an agent learns via its experiences. The RL approach has already been successfully tested under certain dimensionality constraints. However, this paper introduces two new methods to make it even more efficient. \n\nThe first is an improved reward scheme that not only speeds up convergence but guarantees it as well. Applying shaping rewards correctly requires a deep understanding of the problem and multiple convergence tests. The second method is the implementation of a scheduler that enables training to be done in parallel through a distributed RL algorithm. The scheduler partitions the state space and assigns the matrix to different computing nodes, which then populate the matrix with trained knowledge. The resulting knowledge is then collected and used. \n\nThis work has been tested on an IBM cluster with 24 computing nodes, using an aggregated disaster model of the City of Vancouver that was configured for a variety of potential consequences. A total of 24 scenarios were identified, solved simultaneously, and their results gathered. This scheduler streamlines the training process by establishing the necessary model and learning parameters, continuously executing multiple instances, and gathering all results at the conclusion of the training period. The results of these tests demonstrate the proof of concept behind these two new improvements, ultimately highlighting their vast potential for use with highly dimensional models."}, {"label": 0, "content": "Discrete messages transmitted over the radio channels are distorted under the influence of various kinds of additive and multiplicative interferences, this being the reason for errors on the receiver-side of the radio line. Error probability in the received message characterizes the communication channel quality at a definite period of time required for the radio line adaptation to the communication conditions. An immediate estimation of the error probability requires a lot of time, which in many cases exceeds the communication channel stationary interval and makes it impossible to provide operational adaptation of the radio line to the continuously changing communication conditions. The error probability and the value of telegraph (end) distortions in the received discrete message are known to be determined by the ratio of the received signal power and noise, which cannot be estimated directly, since both of these components are found in the communication channel simultaneously. Estimation of the degree of telegraph distortions takes less time compared to the time of error probability estimation. Therefore it is practical to assess error probability in the communication channel indirectly, estimating the degree of telegraph distortions at a limited interval of time in the sliding window, which is less than the interval of stationary state of the communication channel. The paper describes a specific version of the technical (software) implementation of the device for measuring the degree of telegraph distortions. The findings of the study can be used in the design of devices for radio line adaptation to communication conditions."}, {"label": 1, "content": "External disturbances in distribution networks with ungrounded neutral points can easily trigger the low frequency nonlinear oscillation of PT, ultimately causing fuse burn-out, loss, and even explosions. To understand this phenomenon, an analysis of the PT low frequency oscillation mechanism was conducted. Using PSCAD, we simulated the phenomenon and identified key factors contributing to it. A new method for suppressing low frequency PT oscillation was developed that involves grounding the PT neutral point using a low frequency surge suppressor. Simulation results verified the effectiveness of this suppression method in controlling low frequency inrush current within safe ranges. These findings offer guidance for PT fault analysis and the formulation of countermeasures in distribution networks."}, {"label": 0, "content": "In this paper, a framework for collaborative localization of heterogenous systems is presented. Making advantage of the original MSCKF framework, we design a collaborative MSCKF filter that operates in two levels and allows a decentralized 3D collaborative localization without use of external computation systems. To achieve that, based on MSCKF localization, we first propose a range based collaboration that we optimize using the extracted environment constraints, an operation allowed by the use of a truncated unscented kalman filtering updates. The collaborative filtering is managed to not impact the original MSCKF odometry properties. The framework is applied to collaborative localization of aerial and ground robots; experimental results show the effectiveness of the proposed method."}, {"label": 0, "content": "Images have always had a significant effect on their viewers at an emotional level by portraying so much in a single frame. These emotions have also been involved in human decision making. Machines can also be made emotionally intelligent using \u2018Affective Computing\u2019, giving them the ability of decision making by involving emotions. Emotional aspect of machine learning has been used in areas like E-Health and E-learning etc. In this paper, the emotional aspect of machines has been used to perform Geo-tagging of an image. The proposed solution concentrates on a hybrid approach towards Affective Image Classification where the Elements-of-Art based emotional features (EAEF) and Principles-of-Art based emotional features (PAEF) are combined. Firstly, experiments are performed on these two sets of features individually. Then, these two sets are combined to obtain a Hybrid feature vector and same experiments are performed on this feature vector. On comparison of results, it is indicated that the hybrid approach gives better accuracy then either individual approach. Images in this research work are downloaded from Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset which contains the co-ordinates of millions of images and are free to use."}, {"label": 1, "content": "The Internet of Things (IoT) is revolutionizing the development of distributed systems by offering a platform for ubiquitous computing services. However, due to limitations in computing and storage resources, the IoT often relies on cloud-based architecture. This has raised concerns about security and trust in the cloud-based IoT context. To address these issues, we propose a novel trust assessment framework to evaluate security and reputation of cloud services.\n\nBy integrating security- and reputation-based trust assessment methods, our framework enables the evaluation of cloud services to ensure the security of the cloud-based IoT context. Our security-based trust assessment method evaluates cloud service security using cloud-specific security metrics. In addition, we use feedback ratings to evaluate cloud service reputation in our reputation-based trust assessment method.\n\nWe conducted experiments using synthesized security metrics and a real-world web service dataset to test the effectiveness of our proposed framework. Our results show that our framework efficiently and effectively assesses the trustworthiness of a cloud service and outperforms other trust assessment methods.\n\nIn conclusion, our proposed trust assessment framework provides a comprehensive solution to address the security and trust concerns in the cloud-based IoT context. By integrating security- and reputation-based trust assessment methods, our framework provides a holistic evaluation of cloud services to ensure the security and reliability of the cloud-based IoT context."}, {"label": 0, "content": "We report on two sets of perceptual evaluations of our South African text-to-speech voices by language practitioners. In the first evaluation, we measure baseline quality in terms of how understandable and human-like the voices sound. We also determine baseline usability by asking a series of questions related to accessibility and mainstream application settings. In the second evaluation, we employ the same criteria to compare pronunciation improvements against the baseline. The results indicate success in many areas, but also illuminate room for improvement in others, especially in the cases of the African languages."}, {"label": 1, "content": "Nowadays, an increasing number of people have multiple social network accounts linked to different email addresses or phone numbers, making it challenging to identify the same individual across multiple networks. This problem is known as network alignment, and it necessitates the use of anchor users, who are those with different accounts across networks. Research has shown that using known anchor users to predict potential anchor links across the entire network is an effective approach. To increase the accuracy of anchor link prediction, ISS is proposed, which is a new prediction framework that is grounded on the reality of partially aligned social networks. ISS employs supervised learning that is based on social feature extraction and strict stable matching, which boosts the accuracy of the prediction results. Moreover, ISS also uses an iterative framework to refine known information and maximize the prediction results. The effectiveness of the ISS framework was demonstrated through experiments conducted on two real-world heterogeneous social networks, namely Foursquare and Twitter. The results showed that ISS performs very well in predicting anchor links across heterogeneous social networks and outperforms other similar prediction methods."}, {"label": 0, "content": "Weapon-Target Assignment (WTA) problem is the key of air defense command and control. Therefore, it is an urgent problem to complete the assignment quickly and efficiently. In this paper, an improved artificial fish swarm algorithm is proposed to improve assignment rate. Based on artificial fish swarm algorithm (AFSA), particle swarm optimization (PSO) is introduced to change the individual visual of artificial fish, and genetic operator is added to avoid the local extremum trap. The proposed algorithm is validated by the concrete cooperative air defense examples. The simulation results show that the algorithm improved the computational accuracy and the rate of convergence in solving weapon-target assignment problem in air defense."}, {"label": 1, "content": "When it comes to simulation studies, traditionally, simulation data are the primary focus of concern regarding credibility. However, the simulation model itself is arguably even more critical. Therefore, it is necessary to assess the credibility of the simulation model. To accomplish this, information about the process of generating the simulation model is required, which is referred to as provenance. Provenance connects entities and activities involved in the generating process. The provenance of a simulation model relates to the refinement, extension, composition, calibration, and validation of simulation models to the diverse sources used in these processes. Unambiguously means for specifying entities are central to exploiting this information. Thus, a formal domain-specific language for modeling facilitates assessing and reusing simulation models. A declarative domain-specific language for specifying simulation experiments helps utilize simulation experiments done with earlier models for future models. Provenance enables us to understand the present and design the future by opening up new avenues for generating and analyzing simulation models."}, {"label": 0, "content": "In distributed environments, where unknown entities cooperate to achieve complex goals, intelligent techniques for estimating agents' truthfulness are required. Distributed Reputation Management Systems (RMSs) allow to accomplish this task without the need for a central entity that may represent a bottleneck and a single point of failure. The design of a distributed RMS is a challenging task due to a multitude of factors that could impact on its performances. In order to support the researcher in evaluating the RMS robustness against security attacks since its beginning design phase, in this work we present a distributed simulation environment that allows to model both the agent's behaviors and the logic of the RMS itself. Moreover, in order to compare at simulation time the performance of the designed distributed RMS with a baseline obtained by an ideal RMS, we introduce an omniscient process called truth-holder which owns a global knowledge all involved entities. The effectiveness of our platform was proved by a set of experiments aimed at measuring the vulnerability of a RMS to a common set of security attacks."}, {"label": 0, "content": "In this paper, a comparative analysis of various performance enhancement techniques in two-dimensional (2-D) atmospheric optical code division multiple access (OCDMA) system is studied in presence of beam divergence, multiple access interference (MAI), noise and atmospheric turbulence. Lognormal and gamma-gamma probability density functions (pdfs) are considered for evaluating fading process due to atmospheric turbulence. Further, double hard limiters, spatial diversity and error correcting code (ECC) are used for performance improvement of the 2-D atmospheric OCDMA system. Double hard limiters and ECC improve performance substantially as compared to spatial diversity. In addition, double hard limiters are cost-effective than the spatial diversity and ECC. Thus, double hard limiters are superior to the other performance improvement techniques in 2-D atmospheric OCDMA system."}, {"label": 0, "content": "Sophisticated mobile applications may require more resources than are readily available on mobile devices. Mobile device resources such as processing power and storage can be extended with cloud-based mobile augmentation. However, some resources, specifically battery life and bandwidth, cannot be augmented. This research identifies that it is important to be able to estimate the energy consumption of both offloaded and local tasks when making offloading decisions. Due to the fact that the energy consumption profile of mobile devices with diverse capabilities are not the same, this aspect needs to be considered. This research proposes the Switch framework to conserve the limited battery life on mobile devices using a device specific energy consumption profile. The evaluation of the framework suggests that Switch can successfully be used to conserve battery life on mobile devices by making intelligent offloading decisions."}, {"label": 0, "content": "Pulmonary and respiratory diseases comprise a large proportion of the global disease burden, responsible for both mortality and disability. This burden is especially concentrated in the developing world, where air pollution levels are generally high and resources for diagnosing these diseases are very limited. Health workers and many general practitioner doctors in developing countries are not trained to diagnose pulmonary diseases, leading to high rates of misdiagnosis and underdiagnosis. Motivated by this need, we have developed a mobile toolkit that can be used for screening and diagnostic guidance for three of the most common pulmonary and respiratory diseases (Asthma, Chronic Obstructive Pulmonary Disease (COPD) and Allergic Rhinitis (AR)). The toolkit consists of a mobile phone app, known as Pulmonary Screener, which is used in conjunction with a low-cost (<;US$10) peak flow meter. Machine vision software enables the phone camera to automatically track and capture the reading from the peak flow meter without the need for any electronics, Bluetooth radio, or batteries. Using the peak flow meter reading as well as an integrated clinical questionnaire, a machine learning model is then used to calculate the individual probabilities of a patient having a specific pulmonary disease (Asthma, COPD, AR, other) or comorbidities (Asthma + AR, COPD + AR). The machine learning models used in the application were trained using diagnostic data from 325 patients collected at a pulmonary clinic over the past 3 years. Based on 50 iterations of a held-out test set, the Pulmonary Screener achieves accuracy (AUC) values above 0.90 for all diseases and combinations, with the exception of Asthma with AUC = 0.84. Sensitivity and specificity values were for all diseases was also greater than 0.90. To our knowledge, this is the first clinically validated mobile tool that is capable of diagnosing multiple pulmonary diseases in a single app. Future versions of the Pulmonary Screener will make use of ongoing data collection to expand support for infectious diseases as well, including pneumonia and tuberculosis, and include features extracted from auscultation and cough sounds."}, {"label": 1, "content": "This paper presents a novel approach for determining T-equivalent parameters of a transformer using its port data. To detect internal faults of the transformer, a recognition model and adjustment model were constructed based on the model reference adaptive principle. The fitness function was created using the response output of the two models, and the particle swarm optimization (PSO) algorithm was utilized to identify the equivalent circuit parameters. The simulation platform was developed using MATLAB/Simulink, and underwent two iterations. The first iteration identified the excitation parameters, and the second iteration determined the leakage impedance parameters for both the high and low voltage sides of the equivalent circuit. Finally, the feasibility of the proposed algorithm was demonstrated on an experimental platform. Overall, this approach offers a promising solution for identifying the internal parameters of transformers for fault detection and diagnosis."}, {"label": 1, "content": "To enable online evaluation and control of transient stability following power system faults, a decision tree method and an emergency control scheme based on Fisher linear discriminant have been proposed. This approach involves initially processing offline simulation data utilizing Fisher liner discrimination to reduce data dimensions and establish the distance between data points for classification hyperplane. Training the distance data results in a more adaptive decision tree for classification, which serves as a guide to determine the stability index required for emergency control. The stability index is then used to calculate optimal load shedding and generation shedding schemes through 0-1 linear programming, while minimizing system losses and ensuring power system stability. Simulation analysis carried out on the WECC system and New England system showed that the decision tree-based classification mechanism is both accurate and interpretable. The stability margin-based emergency control strategy can deal with transient instability following faults effectively, allowing the restoration of stable operations in the system."}, {"label": 0, "content": "Consensus algorithms are widely applied in distributed control of microgrids. With distributed control, the performance of a microgrid is significantly relevant to the time delays of the communication network. This paper mainly focuses on the time delays in the consensus-based control in microgrids. The influence of time delays is analyzed and the stability margin of the time delays is given. It is proved that the stability of the microgrid depends only on the local processing time delays of DGs, while the communication time delays between different DGs would only influence the convergence speed. Moreover, an AC microgrid with four DGs and some loads are implemented and the stability margin of time delays is verified by MATLAB/Simulink simulation results."}, {"label": 0, "content": "While gait recognition is the mapping of a gait sequence to an identity known to the system, gait authentication refers to the problem of identifying whether a given gait sequence belongs to the claimed identity. A typical gait authentication system starts with a feature representation such as a gait template, then proceeds to extract its features, and a transformation is ultimately applied to obtain a discriminant feature set. Almost every authentication approach in literature favours the use of Euclidean distance as a threshold to mark the boundary between a legitimate subject and an impostor. This article proposes a method that uses the posterior probability of a Bayes' classifier in place of the Euclidean distance. The proposed framework is applied to template-based gait feature representations and is evaluated using the standard CASIA-B gait database. Our study experimentally demonstrates that the Bayesian posterior probability performs significantly better than the de facto Euclidean distance approach and the cosine distance which is established in research to be the current state of the art."}, {"label": 0, "content": "Ground loop is a serious problem in a lab or an industry. It conflicts with low-level signals instrument and sometimes endangers for human being. However, detecting the ground loop within a huge instrument is difficult task and time consuming matter. To solve this problem, a novel technique is applied for detecting ground loop in a complex situation using internet of thing (IoT). This approach consists of an exciter module with IoT device that generates a 100 kHz ground loop current and a detector to determine affected cable by receiving the test current. Multiple detectors used to give a virtual cable id in a complex area to identify fault cable. After detecting ground loop the affected cable id sent to the server for taking an action to prevent the ground loop."}, {"label": 0, "content": "The article describes the software tools used for macroeconomic modeling of the Fuel and Energy Complex. These software tools allow you to generate and modify the relevant macroeconomic models. Its architecture and functional components provide a flexible interaction between the user and the computer simulation system. The software package includes two autonomous subsystems - CREATOR and DIGGER. The CREATOR system is used to create models and change their formal structure. The DIGGER system is intended for the organization of input and editing of data, conducting direct calculations on models, launching optimization procedures. Direct calculations realize the simulation mode of model user. They determine the values of the model indicators according to formulas of the static and dynamic modes. The optimization mode may be used only for multilinear indicators. It is divided into static and dynamic. In the static optimization mode (optimization of one current time circle) non-multilinear indicators are excluded from the optimization task. Dynamic non-multilinear indicators are excluded for the dynamic optimization mode. A direct calculation performed after the optimization procedure completes, it shows whether these indicators satisfy the boundary conditions specified for them. The macroeconomic problems related to the fuel and energy complex that these tools helps to solve online are listed. The capabilities of the described tools are illustrated with a significant example."}, {"label": 0, "content": "By renting pay-as-you-go cloud resources (e.g., virtual machines) to do science, the data transfers required during the execution of data-intensive scientific workflows may be remarkably costly not only regarding the workflow execution time (makespan) but also regarding money. As such transfers are prone to delays, they may jeopardise the makespan, stretch the period of resource rentals and, as a result, compromise budgets. In this paper, we explore the possibility of trading some communication for computation during the scheduling production, aiming to schedule a workflow by duplicating some computation of its tasks on which other dependent-tasks critically depend upon to lessen communication between them. This paper explores this premise by enhancing the Heterogeneous Earliest Finish Time (HEFT) algorithm and the Lookahead variant of HEFT. The proposed approach is evaluated using simulation and synthetic data from four real-world scientific workflow applications. Our proposal, which is based on task duplication, can effectively reduce the size of data transfers, which, in turn, contributes to shortening the rental duration of the resources, in addition to minimising network traffic within the cloud."}, {"label": 1, "content": "This paper aims to compare the performance of two different methods for predicting the future values of a vehicle's slip angle - conventional adaptive finite impulse response filters and feed forward multi-layer neural networks. The study evaluates the results based on the number of inputs and prediction horizon, considering prediction error and the required number of algorithmic operations."}, {"label": 1, "content": "The article presents an overview of software tools used for macroeconomic modeling of the Fuel and Energy Complex. These tools provide the ability to create and modify macroeconomic models, with an architecture and functional components that enable flexible interaction between the user and the computer simulation system.\n\nThe software package includes two autonomous subsystems, CREATOR and DIGGER. CREATOR is used to create models and change their formal structure, while DIGGER is intended for data input and editing, direct calculations, and launching optimization procedures.\n\nDirect calculations allow the simulation of the model user, determining the values of model indicators according to formulas of static and dynamic modes. The optimization mode can only be used for multilinear indicators, which can be static or dynamic. Non-multilinear indicators are excluded from optimization tasks during static and dynamic modes.\n\nAfter optimization, a direct calculation is performed to verify whether indicators satisfy specified boundary conditions. The article lists macroeconomic problems related to the fuel and energy complex that can be solved using these tools and provides an example to illustrate their capabilities."}, {"label": 1, "content": "Limited-resource clinics often rely on paper patient records as they are easy to use, reliable, and supported by their financial and technical resources. However, electronic health record (EHR) systems provide better patient information management and reporting, but require more resources than many clinics can afford. This paper proposes that limited-resource clinics could implement a patient-record automation system on a low-cost platform that is sustainable within their resources.\n\nTo test this hypothesis, the piClinic Console was developed. This Raspberry-Pi-based system costs less than $300 USD per clinic and provides essential patient-record automation functions. Through end-user observation, participatory design, and iterative user testing, the system's most beneficial features were identified and included in the final design. The piClinic Console system was found to significantly improve clinic information processing and prepare the clinic for future transition to a more complete EHR system.\n\nInitial testing shows that the piClinic Console is a viable solution for limited-resource clinics. However, further field testing and development are necessary to improve the system's performance and functionality. Future research will focus on enhancing the system's capabilities to provide a more complete EHR system once resources become available to sustain such a platform."}, {"label": 1, "content": "Cognitive radio (CR) systems are required to accurately detect the presence of a primary user (PU) signal by sensing the spectrum area of interest. However, due to propagation effects such as fading and shadowing, spectrum sensing becomes challenging as the PU signal may be attenuated in certain areas. In this article, we will discuss a distributed spectrum sensing approach that utilizes the largest eigenvalue of correlation matrices (CMs), which are adaptively estimated based on a combine and adapt least (CTA) type of diffusion method with no fusion center.\n\nIn the proposed approach, CR nodes exchange observations with a subset of their neighboring nodes and combine the neighboring observations based on locally-estimated signal to noise ratio (SNR) values. This method enables each node to enhance its own estimation accuracy while generating a consensus estimate of the PU signal. This consensus can then be used to make a decision on the presence of the PU signal in the spectrum.\n\nWe analyzed the detection performance of the proposed approach and verified our theoretical findings through simulations. The results indicate that our approach achieves good performance in terms of detection probability, and outperforms existing approaches in realistic environments where fading and shadowing are present.\n\nIn conclusion, our proposed approach demonstrates promising potential for improving the reliability of spectrum sensing in CR systems. The use of distributed sensing and collaboration among CR nodes enables accurate detection of PU signals, which is essential for the effective utilization of the available spectrum."}, {"label": 1, "content": "In this paper, we have developed analytical expressions for the system performance, which includes outage probability and throughput, of the power splitting half-duplex power beacon-assisted energy harvesting relay network in both amplify-and-forward and decode-and-forward modes. To validate our findings, we have also used Monte-Carlo simulation, and the results of the simulation have further confirmed our analytical results. Our numerical findings indicate that the analytical and simulation results match well together, considering all possible system parameters."}, {"label": 0, "content": "Making decisions during a disaster can be challenging when human lives and infrastructures are exposed. An important factor to consider when allocating resources, in these situations, is the critical infrastructures' interdependencies. i2Sim, the Infrastructures' Interdependencies Simulator, is a tool build for that purpose. As a layered architecture, i2Sim includes a dedicated decision-making layer. The use of Reinforcement Learning (RL), a machine learning approach based on an agent learning from experience, has been successfully tested with some dimensionality constraints. This paper introduces two improvements to our previous tests aiming at increasing speed and allowing larger dimensionality problems. The first addition is an improved reward scheme for speeding up convergence while guaranteeing it. The correct application of shaping rewards requires a deep understanding of the problem and extensive convergence tests. The second improvement added is the implementation of a scheduler programmed to trigger multiple instances of the same model using different parameters. This scheduler partitions the state space, enabling the agent's training to be done in parallel via a distributed RL algorithm. With this idea, the state/action matrix representing knowledge is partitioned for training, assigned to computing nodes, populated with knowledge (trained) and collected/reconstructed for use. This work has tested on an IBM cluster with 24 computing nodes. The test model is an aggregated model of the City of Vancouver configured for a disaster with numerous consequences over the different critical infrastructures. Based on the model's configuration, 24 scenarios were identified, created and solved simultaneously. The scheduler automates the training by setting up model and learning parameters, looping execution of instances and gathering results from all nodes. The results verify a proof of concept and enable applicability to new models with highly increased dimensionalities."}, {"label": 0, "content": "Spacecraft detection is one of essential issues on aerospace information processing and control, and can provide reliable dynamic state of target, so as to support decisions made on target recognition, classification, catalogue, et al. Although numerous spacecraft detection methods exist, most of them cannot achieve real-time detection, and are still lack of better accuracy and fault-tolerance for different scenes. Recently, deep learning algorithms have achieved fantastic detection performance in computer vision community, especially the regression-based convolutional neural network YOLOv2, which has good accuracy and speed, and outperforming other state-of-the-art detection methods. This paper for the first time applies CNN to the detection of spacecraft and sets up a dataset for target detection in space. Our method starts with image annotation and data augmentation, and then uses our improved regression-based convolutional neural network YOLOv2 to detect spacecraft in an image. The experimental results have shown that our algorithm achieves 97.8% detection rate in the test set, and the average detection time of each image is about 0.018s, which has lower time overhead and better robustness to rotation and illumination changes of spacecraft."}, {"label": 1, "content": "This study aims to examine the accuracy of short-term forecast for MISO's locational marginal pricing (LMP) data sets. Various methods, including rolling average, autoregressive integrated moving average (ARIMA), and long short-term memory (LSTM), have been tested on a three-year data set and compared against MISO's own forecasting approach. The results show that the use of recurrent neural networks (RNN) can provide a 25% improvement in forecasting accuracy compared to MISO's method."}, {"label": 1, "content": "Automatic Modulation Recognition (AMR) plays a vital role in numerous civil and military applications. Cumulant-based recognition is an effective method for AMR. However, in space division multiplexing MIMO systems, cumulant-based AMR faces the issue of lower recognition rates. This is because the statistical characteristics of signals differ in these systems. To address this problem, we proposed using auto-encoding network (AEN) as the data dimension reduction algorithm, and artificial neural network (ANN) as the classifier for multiple digital modulation signals in MIMO system.\n\nIn our simulations, we utilized the proposed scheme to classify five types of digital modulation signals, namely 2PSK, 4PSK, 8PSK, 16QAM, and 32QAM. The simulation results indicate that our proposed method achieves substantially higher recognition rates compared to direct classification by cumulants."}, {"label": 1, "content": "A mixed integer linear programming model has been proposed for microgrid intra-day scheduling with an aim to improve the accuracy and computational efficiency of the existing models. The model focuses on minimizing the economic operation cost of the microgrid while considering the reactive power capability of distributed generation.\n\nCompared to the existing microgrid economical dispatching literature, the proposed model includes linearized constraint models such as power flow with voltage amplitude limitation, DG reactive power characteristic and branch capability based on analytic geometry, exchange power and power factor of point of common couple, and many others.\n\nIn order to validate the performance and accuracy of the proposed model, an experimental microgrid has been tested. The model has proved to provide accurate results and improve the overall performance of the microgrid.\n\nFurther, the study discusses the effects of different segments on the feasible region coverage of DG operating points. Overall, the proposed mixed integer linear programming model has the potential to enhance the scheduling of microgrids and improve the efficiency of their operation."}, {"label": 0, "content": "The role of technology as a key driver of sustainable development has long been recognized. Technology leverages total-factor productivity and contributes to the growth of welfare. The lack of adequate financial and human capital hampers the adoption of technology in many parts of Africa. On September 2015, the General Assembly of United Nations adopted as a consensus resolution the 2030 Development Agenda for Sustainable Development with seventeen goals (SDGs). Creating a knowledge-based economy is a key enabling factor for achieving the SDGs. If successful, this will result in a reduction of inequalities and enhance the quality of life in those countries. It is our belief that education plays a key role in effective and sustainable development. While there are many ways for doing this, we believe that strengthening budding academic institutions is a sustainable long-term strategy. Such approach needs to take the local situation into account and requires flexible frameworks with the ability to respond to local needs. We have embarked in an international effort to implement a solid proof-of-concept of our approach. Specifically, it focuses on two SDGs, \u201cgood health and well-being\u201d together with \u201cquality education\u201d. If successful, we expect a positive impact on a third goal, \u201cdecent work and economic growth\u201d. The Canary Islands are a Spanish region and a European outermost region near West Africa. Thanks to the unique combination of geolocation and level of development, the islands play an increasing role as a logistics, trade hub, and launch base for many countries in Western Africa. Many aid organizations, including UN World Food Program, Red Cross/Crescent and USAid, use the Canary Islands as logistics base. In this paper, we report our current experience within the European funded INTERREG MACBioIDi project. One of our main aims is to create a learning community in the fields of medicine and engineering. The participants live in different countries. They speak different languages and come from a variety of cultures with different social and economic boundary conditions. African participants in the project come from Cape Verde, Mauritania, Senegal, and Mozambique. Technology partners come from the outermost European regions of Canary Islands, Azores and Madeira. The main technological platform used in this project is called 3D Slicer. 3D Slicer is an extensible, free software for visualization and processing of biomedical images and was created by a community of scientists and engineers led by researchers from Harvard. Training and mentoring is provided to African physicians, biomedical engineers and researchers in the use 3DSlicer. In order to support these activities, a training center has been created at the University of Las Palmas de Gran Canaria (ULPGC). We first describe the overall strategy of 3D Slicer training, designed in response to the needs of each African country. We highlight the major hurdles faced (e.g. internet connectivity, maintenance of medical equipment, differences in linguistic and social context). Then we focus on the implementation of the training program. Training sessions were held in Africa and at ULPGC, and based on a blended learning program with \u201cface to face\u201d lessons, coaching, collaboration, multimedia, web based learning and support resources. Finally, we analyze the first results obtained in the program training and the measures adopted to readjust the action plan in the coming months."}, {"label": 0, "content": "Orthogonal frequency division multiplexer (OFDM) is a recent modulation scheme used to transmit signals across power line communication (PLC) channel due to its robustness against some known PLC problems. However, this scheme is greatly affected by the impulsive noise (IN) and often causes corruption with the transmitted bits. Different impulsive noise error correcting methods have been introduced and used to remove impulsive noise in OFDM systems. However, these techniques suffer some limitations and require much signal to noise ratio (SNR) power to operate. In this paper, an approach of designing an effective impulsive-noise error-correcting technique was introduced using three-known artificial neural network techniques (Levenberg-Marquardt, Scaled conjugate gradient, and Bayesian regularization). Findings suggest that both Bayesian regularization and Levenberg-Marquardt ANN techniques can be used to effectively remove the impulsive noise present in an OFDM channel and using the least SNR power."}, {"label": 1, "content": "After conducting a thorough analysis of the nature of energy for comprehensive energy utilization, the fundamental theory of energy networks in different energy forms has been established. This theory is based on the physical theories of analytical mechanics, thermodynamics, heat transfer, fluid mechanics (fluid network), and electromagnetic field theory (electric network). The generalized equations of energy transfer and transformation have been successfully established following this in-depth analysis. \n\nFurthermore, the generalized lumped element models that include generalized resistance, capacitor, and inductance have been derived. To establish the equations of energy networks, the Kirchhoff's Law in electric networks has been extended to energy networks. This extension is known as the Generalized Kirchhoff's Law. Finally, the equations have been unified into a comprehensive energy network equation system.\n\nIn order to demonstrate the effectiveness of the energy network theory, a numerical example has been provided."}, {"label": 1, "content": "Mixing matrix estimation is a crucial step in underdetermined blind source separation. However, existing methods suffer from low estimation accuracy. To address this issue, a novel detection method for single source points (SSPs) is proposed in this paper based on local stationarity and distribution symmetry. By utilizing a clustering algorithm, mixing matrix estimation is obtained without the need for region division of hypersphere, making it easy to operate. This method effectively eliminates pseudo SSPs, resulting in improved clustering features of observed signals. Simulation results demonstrate that the proposed method achieves higher accuracy compared to traditional methods."}, {"label": 1, "content": "In recent years, there has been a significant focus on deep learning methods for 3D geometry perception tasks such as dense depth recovery, optical flow estimation, and ego-motion estimation. One promising approach is the use of unsupervised strategies to learn from video datasets. To further improve performance, a combination of constraints and a finer architecture has been proposed for unsupervised ego-motion and depth estimation. Specifically, the approach involves the use of two neural networks: Depth-Net for monocular depth estimation and Pose-Net for ego-motion estimation. Depth-Net improves estimation accuracy with as few parameters as possible. Experiments using the KITTI driving dataset show that the proposed method outperforms some of the current state-of-the-art results in both supervised and unsupervised methods."}, {"label": 1, "content": "Consensus is a crucial aspect of distributed systems since it supports key functionalities such as distributed decision making, decentralized control and distributed information fusion. However, existing consensus algorithms require each agent to exchange explicit state information with its neighbors, leading to the unwanted disclosure of private state information. In this paper, we introduce a novel approach for undirected networks that can achieve secure and privacy-preserving average consensus in a decentralized architecture without the need for an aggregator or third party. By employing partial homomorphic cryptography to embed secrecy in pairwise interaction dynamics, our approach ensures convergence to the consensus value (subject to quantization error) in a deterministic fashion without exposing a node's state to its neighbors. Additionally, we provide a new privacy definition for dynamical systems and a new framework that proves a node's privacy can be protected as long as it has at least one legitimate neighbor participating in the consensus protocol without attempting to infer other nodes' states. Our approach not only provides resilience against passive attackers attempting to steal state information but also enables easy incorporation of defense mechanisms against active attackers who try to manipulate the content of exchanged messages. Unlike current noise-injection-based privacy-preserving mechanisms, which need network reconfiguration when the topology or number of nodes varies, our approach can be used in dynamic environments with time-varying coupling topologies. This approach is applicable to weighted average consensus as well as maximum/minimum consensus with a new update rule. Our theoretical results are validated through numerical simulations and comparison with current approaches. We also present experimental results on a Raspberry-Pi board-based microcontroller network to demonstrate the effectiveness and efficiency of our approach."}, {"label": 0, "content": "This paper investigates the distributed security constrained economic dispatch (SCED) problem in an active distribution network, pursuing the minimization of generation cost of the entire system considering network security and generation limit constraints. A feedback strategy derived from the duality-based method is proposed to achieve the distributed solution of SCED. In the proposed strategy, the distribution management system (DMS) and microgenerators (MGs) are regarded as intelligent agents in charge of fast calculation and measurement. The variation of system demand results in fluctuations on branch flows, which can be measured by DMS and then transmitted to MGs. Each MG can adjust itself active power output independently so as to respond to system demand variation, and meanwhile ensure network security. The proposed method only requires partial dual variable information transfer from the DMS agent to MG agents, and has plug-and-play features. Simulations of the IEEE 33-bus system are provided to illustrate the performance and robustness of the proposed algorithm."}, {"label": 0, "content": "Person re-identification has gradually become a popular research topic in many fields such as security, criminal investigation, and video analysis. This paper aims to learn a discriminative and robust spatial\u2013temporal representation for video-based person re-identification by a two-stage attribute-constraint network (TSAC-Net). The knowledge of pedestrian attributes can help re-identification tasks because it contains high-level information and is robust to visual variations. In this paper, we manually annotate three video-based person re-identification datasets with four static appearance attributes and one dynamic appearance attribute. Each attribute is regarded as a constraint that is added to the deep network. In the first stage of the TSAC-Net, we solve the re-identification problem as a classification issue and adopt a multi-attribute classification loss to train the CNN model. In the second stage, two LSTM networks are trained under the constraint of identities and dynamic appearance attributes. Therefore, the two-stage network provides a spatial\u2013temporal feature extractor for pedestrians in video sequences. In the testing phase, a spatial\u2013temporal representation can be obtained by inputting a sequence of images to the proposed TSAC-Net. We demonstrate the performance improvement gained with the use of attributes on several challenging person re-identification datasets (PRID2011, iLIDS-VID, MARS, and VIPeR). Moreover, the extensive experiments show that our approach achieves state-of-the-art results on three video-based benchmark datasets."}, {"label": 1, "content": "The performance of Automatic Speaker Identification (ASI) systems on Voice over Internet Protocol (VoIP) speech is highly dependent on the type of codec used in the communication. Since the codec used varies based on the service provider of the user, there is a necessity to have codec-independent ASI systems that can accurately identify the speaker. \n\nTo achieve this goal, three different modeling approaches based on the Unified Background Model (UBM)-Gaussian Mixture Model (GMM) framework and the i-vector framework have been proposed. These modeling approaches enable the identification of the speaker independent of the codec used. Further, these frameworks are also evaluated for mismatch conditions with respect to the codec used for training and testing. \n\nThe proposed approaches have been tested on VoIP speech data obtained from four different codecs with different bit rates, along with uncoded speech. The results reveal that these approaches lead to improved identification accuracy, especially in mismatched conditions. Hence, these proposed methods show significant potential for enabling more efficient and robust ASI systems for VoIP applications."}, {"label": 0, "content": "Feature extraction is of great importance for running states monitoring and performance evaluation of mechanical electro-hydraulic systems (MEHS). However, due to the complexity of the multi-domain energy conversion property of MEHS, especially during the varying operation conditions, it is quite difficult to extract the desired features effectively. In addition, the conventional signals are difficult to be collected and analyzed, as different kinds of coupled information are mixed together. Therefore, based on a power distribution analysis of MEHS, it is found that the change rate of the kinetic energy (CRKE) can be considered as a suitable index for evaluating the performance, such as energy saving and output stationarity of the considered MEHS. In order to characterize the magnitude of CRKE, a cooperation analysis method is proposed by using internal and external features. In the proposed method, the kinetic energy stiffness (KES) is selected as the internal feature, while the instantaneous speed fluctuation (ISF) is chosen as the external feature. According to a Lissajous figure-based information fusion approach and the order tracking technology, a systematic method is developed to obtain the magnitudes of KES and ISF. Furthermore, based on the complementary advantages and mutual relationship of KES and ISF, the performance of MEHS is analyzed under varying operation conditions. The proposed method is verified through experiments with a real rig. The results show that the changes of KES and ISF can effectively reflect the change in the operation condition, and lower KES loss can improve the efficiency of the system and also restrain the ISF."}, {"label": 1, "content": "In general, distributed renewable energy, energy storage, and DC loads are connected to the traditional AC distribution network using multi-stage converters, which results in low energy efficiency of the system. However, AC/DC power distribution technology can effectively reduce the intermediate link of AC/DC transformation, thereby improving the economy, reliability and operation flexibility of power distribution.\n\nTherefore, this paper aims to address the economic energy problems associated with efficient access for high-capacity distributed renewable energy and DC loads. Based on previous research, several aspects including system structure design, key equipment support technology and operation control technology are explored to provide ideas and references for further research and the application of AC/DC hybrid systems."}, {"label": 1, "content": "The inspection robot is able to accurately locate heating defects of equipment through the use of infrared images, making it an essential tool for equipment fault diagnosis. While the robot is able to capture the infrared thermal image, processing and analysis subsequently require human intervention. This paper proposes an approach for processing infrared thermal images of transformer bushings that utilizes image recognition and pattern recognition techniques, with the aim of reducing manual intervention in the processing and analysis of infrared thermal images. Firstly, the Normalized Cross Correlation (NCC) template matching method and Otsu threshold segmentation method are used to determine the Region of Interests (ROI) of the bushing. Subsequently, characteristics such as maximum temperature rise, temperature mean value, temperature variance and temperature gradient of the ROI are extracted. Finally, the support vector machine is used to determine the status of the bushing. The experimental results demonstrate that the proposed model effectively reduces manual intervention in the processing of infrared thermal images, and has high accuracy making it suitable for engineering applications."}, {"label": 1, "content": "The Time-Step method and the Station-Pair method are both widely used for estimating spatial gradients. The Time-Step method is typically used for GPS-based GBAS applications, while the Station-Pair method is used for a dense network of stations. However, this paper investigates the suitability of both methods for IRNSS-based GBAS applications. \n\nIt is important to note that IRNSS satellites are either GEO or GSO, which means that for the Time-Step method, the time interval (At) should be significantly high (30 minutes for GSO) to obtain gradient data for the GBAS service area. On the other hand, the Station-Pair method requires a dense network of stations, each separated by not more than 1-2 kilometers. \n\nOverall, both methods have their limitations when it comes to IRNSS-based GBAS applications. It is important to carefully consider the specifics of each method and their compatibility with IRNSS before making a decision on which method to use for gradient estimation within the GBAS service area."}, {"label": 0, "content": "Aimed at the problem that voltage at the end of feeder lines is always too low to start up pump loads in low-voltage distribution network (LVDN) mainly composed of agriculture loads of oxygen pumps and water pumps, a distributed reactive power compensation optimal allocation model of LVDN including pump loads characteristics is proposed. In this model, the objective function includes both the investment cost of the compensation device and the economic benefits of reducing power loss of LVDN, and various constraints of LVDN in the condition of normal operation and startup of pump loads are included in the constraints. A sigmoid function is used to approximate the discontinuous sign function in the objective function, and a nonlinear penalty function which has relative large curvature near the discrete value is used to address the discrete variables characteristics of reactive power compensation capacity, then the primal-dual interior-point algorithm is used to solve the optimization model. Numerical results of an actual LVDN show that the obtained optimal allocation scheme of the method can improve the voltage quality of distribution network, ensure the normal startup of pump loads, and decrease the power loss of LVDN effectively."}, {"label": 0, "content": "State of charge (SOC) estimation is a core technology for battery management system (BMS), which plays an important role to make electric vehicles (EVs) operate safely, reliably and economically. In this paper, a new approach based on the Spherical Simplex-Radial Cubature Kalman Filter (SSRCKF) algorithm is presented to improve the accuracy of SOC estimation. The superiority of the proposed approach has been proved through the Worldwide harmonized Light Vehicles Test Procedure, which came into effect last year in the European Union. In addition, noise are added to the measured data of current and voltage to verify the its anti-interference ability. By comparing with the Unscented Kalman Filter (UKF) and the Cubature Kalman Filter (CKF), the experimental results show that the SSRCKF algorithm estimated the SOC more accurately than the UKF and CKF."}, {"label": 1, "content": "Currently, the improved methods for social network analysis are primarily based on homogeneous networks. However, actual social networks are intrinsically heterogeneous in nature. Heterogeneous social networks can provide a better representation of the system's composition and the associated relationships than the homogeneous social network model. Although previously proposed ranking clustering has provided a new perspective, the algorithm is only capable of producing clustering results for specific target types and cannot cover the complete range of heterogeneous network types. \n\nTo address this issue, we introduce a collaborative clustering algorithm that combines ranking to propose a ranking collaborative clustering algorithm. The algorithm firstly obtains a ranking distribution matrix based on ranking clustering, followed by utilizing collaborative clustering to complete different types of clustering, ensuring the full utilization of the relationship between different types and the same types. \n\nThe experimental results on real twitter and foursquare datasets demonstrate that the ranking collaborative clustering algorithm outperforms ranking clustering in terms of modularity."}, {"label": 1, "content": "Power system analysis involves complex network theory development algorithms, which use a probabilistic perspective to interpret power flows. This is an essential and foundational topic for many new techniques, including machine learning and artificial intelligence. In this paper, we reveal that the equilibrium of probabilistic energy transfers on a graph can represent power flow mathematically. We demonstrate that this equilibrium is achievable when energy transfers follow Markov chains' random process. Our work connects the power flow models and the random walk models of complex networks, advancing the current state of the art. Furthermore, we present multiple new insights on the electrical betweenness centrality measures used in power grid vulnerability analysis from this new probabilistic perspective."}, {"label": 0, "content": "Wireless sensor networks are susceptible to report fabrication attacks, where adversary can use compromised nodes to flood the network with false reports. En-route filtering is a mechanism of dropping bogus/false reports while they are being forwarded towards the sink. Majority of the proposed en-route filtering schemes are probabilistic, where the originality of forwarded reports is checked with fixed probability by intermediate nodes. Thus, false reports can travel multiple hops before being dropped in probabilistic en-route filtering schemes. Few deterministic based en-route filtering schemes have also been proposed, but all such schemes need to send the reports through fixed paths. To overcome the above mentioned limitations of existing en-route filtering schemes, we propose a novel deterministic enroute filtering scheme. In the proposed scheme, secret keys are allocated to sensor nodes based on combinatorial design. Such design ensures direct communication between any two nodes without adding more key storage overhead. We provide in-depth analysis for the proposed scheme. The proposed scheme significantly outperforms existing schemes in terms of expected filtering position of false reports and is more buoyant to selective forwarding and report disruption attacks. Our scheme also performs neck-to-neck with existing schemes in terms of protocol overheads."}, {"label": 1, "content": "The article presents the findings of a study, along with a software module that permits the configuration of a fuzzy logic controller in three different ways to integrate it into the control object. It also analyzes the performance of regulation and offers a comparative overview of the outcomes."}, {"label": 0, "content": "Passive sound source localization (SSL) using time-difference-of-arrival (TDOA) measurements is a non-linear inversion problem. In this paper, a data-driven approach to SSL using TDOA measurements is considered. A neural network (NN) is viewed as an architecture constrained non-linear function, with its parameters learnt from the training data. We consider a three layer neural network with TDOA measurements between pairs of microphones as input features and source location in the Cartesian coordinate system as output. Experimentally, we show that, NN trained even on noise-less TDOA measurements can achieve good performance for noisy TDOA inputs also. These performances are better than the traditional spherical interpolation (SI) method. We show that the NN trained offline using simulated TDOA measurements, performs better than the SI method, on real-life speech signals in a simulated enclosure."}, {"label": 0, "content": "The aim of this project is to develop an efficient event-driven wireless sensor nodes based network for an effective and power efficient monitoring of plants health and larva population in a remote crop field. In this framework, an event-driven approach is proposed to detect larva and measure other system parameters like Acoustic Complexity Index (ACI), temperature, humidity and soil moisture. The sensors' data is collected by the front end event-driven sensing node, developed with a STM32F407VG board, via a serial port. The STM32F407VG board is based on the ARM processor. In contrast to the clock driven classical sensing nodes, the devised sensing nodes only acquire and transmits the sensors data in the case of a significant change. It significantly reduces the power consuming data acquisition and transmission activity as compared to the classical solutions. It improves the proposed solution power efficiency and autonomy as compared to the counter classical ones. The data from the node is transmitted to a base station by using a wireless ZigBee interface. The base station collects data from a group of event-driven sensing nodes. This data is transmitted to the Central Processing Unit (CPU) via the USB liaison between the base station and the CPU. On CPU this data is analyzed via the MATLAB based specifically developed application. The findings are displayed and logged on the CPU. It allows the terminal user to access this and to achieve a timely interaction and cure of the intended crop field. The system parameters are adjusted in order to achieve the effective modules integration and performance. The proposed system operation is validated with an experimental setup. Results have shown a promising system realization."}, {"label": 1, "content": "Passive sound source localization using time-difference-of-arrival (TDOA) measurements is a challenging non-linear inversion problem. This paper investigates a new data-driven approach to SSL using TDOA measurements. We propose a three-layer neural network (NN) model with TDOA measurements between pairs of microphones as input and source location in the Cartesian coordinate system as output. The NN is trained to learn the non-linear mapping between TDOA measurements and source location from the training data. Interestingly, we find that the NN model can generalize well to noisy TDOA inputs, even if it is trained on noise-free TDOA data. In comparison, the traditional spherical interpolation (SI) method for localization is shown to yield inferior performance. Furthermore, we demonstrate that the NN model trained offline using simulated TDOA measurements outperforms the SI method for localizing real-life speech signals inside a simulated enclosure."}, {"label": 0, "content": "On the premise of improving the traditional line detection method based on transient zero sequence current (TZSC) waveform comparison, a fault line detection method based on variational mode decomposition (VMD) and phase space reconstruction was proposed in this paper. First, the first modal component was extracted by VMD of the TZSC of each line running up to the first feature extraction and noise elimination. Then the phase space reconstruction of the extracted modal component was carried out by using coordinate delay method. The C-C algorithm was used to estimate the optimal phase space reconstruction dimension and time delay. The mean dimension and time delay were taken respectively for creating similar phase spaces to achieve the second feature extraction. Finally, the phase space similarity matrix was established by comparing the similarity degree of every two zero sequence current phase spaces. Calculate the comprehensive correlation coefficient, and the line with the minimum comprehensive correlation coefficient was selected as the fault line. The simulation results verified the applicability of the line selection method, high sensitivity and strong anti noise."}, {"label": 0, "content": "Anonymous Voice over IP (VoIP) communication is an important tool to provide freedom of speech. For achieving a good trade-off between quality of service (QoS) and anonymity, connecting mixes by padded links is a promising approach. However, constructing suitable overlay topologies for establishing padded links is not easy: existing approaches are either not scalable with regard to the number of mixes because of using full meshes or impractical due to computational complexity of the algorithms used to create optimal reduced topologies. This prevents them from being deployed as volunteer-based networks like Tor. In this paper, we propose and study different heuristic strategies for reduced topology construction and path selection with low computational complexity. We evaluate our approach using latency and bandwidth estimations from the Tor network and demonstrate that we can achieve appropriate performance and anonymity metrics for VoIP. Especially, the achieved properties are similar to those of a recently proposed approach that calculates optimal topologies for small networks."}, {"label": 1, "content": "Multi-objective route planning is a topic that is currently receiving a great deal of attention in research and it has wide-ranging applications in daily life. As the problem's scale continues to expand, a large number of approximate and heuristic algorithms have been proposed to solve it. In this paper, we propose a solution to multi-objective route planning with a balanced assignment of tasks. The solution can be divided into two main steps.\n\nFirstly, we introduce a clustering algorithm known as cbk-means (cluster balance k-means), which enhances the process of similarity measurement in clustering and overcomes certain limitations of traditional k-means algorithm, such as inflexible measurement criteria and an uncertain number of points. This step is crucial in achieving a fair assignment of tasks.\n\nSecondly, we use genetic algorithm to obtain an optimal route planning for each cluster. The experimental results demonstrate that the cbk-means algorithm ensures a more balanced workload for each cluster with minimal costs, thereby greatly improving the fairness of task assignment. Furthermore, this hybrid solution is time-efficient and yields better results.\n\nIn conclusion, our proposed solution addresses the issue of multi-objective route planning with a balanced assignment of tasks, and demonstrates significant improvements over existing algorithms."}, {"label": 1, "content": "In this paper, we investigate the issue of physical layer security and transmission reliability in D2D underlaying cellular networks when there is an active eavesdropper (AE) present. We propose a secrecy anti-jamming game, where the cooperation between cellular user equipment (CUE) and D2D user equipment (DUE), as well as the competition between legitimate users and AE, is formulated. In this game, DUE can either launch cooperative relaying or friendly jamming to help CUE improve its anti-eavesdropping and anti-jamming performance. CUE can offer different levels of rewards for DUE's assistance, while AE can switch between actively jamming and passively eavesdropping to maximize its destructive impact on the D2D underlaying cellular networks. We prove the existence of the pure-strategy equilibrium under perfect information and analyze the existence of the mixed-strategy equilibrium under imperfect information. We also introduce a distributed Q-learning-based algorithm that converges to the mixed-strategy equilibrium. Simulation results indicate that the proposed algorithm is convergent and that the cooperation between CUE and DUE leads to an improvement in the average utilities of legitimate users."}, {"label": 1, "content": "This paper introduces the Eigensystem Realization (ER) approach for estimating dynamic phasors, which provides synchrophasor estimates such as amplitude, phase, frequency and rate of change of amplitude in one-cycle. The proposed ER-based phasor estimator has been evaluated under both steady-state and dynamic conditions with theoretical and real signals from a commercial PMU. A comparison has been made between the performance of the ER-based phasor estimates and that of the Discrete Fourier Transform. The results demonstrate that the proposed phasor estimator produces reliable estimates even under polluted conditions with high harmonic content, and is capable of accurately tracking changes in amplitude, phase and frequency."}, {"label": 1, "content": "To meet the demands of performance evaluation for current distribution network planning, traditional correlation analysis based on complex power flow calculation is inadequate. This paper proposes a correlation mining technique using BP neural network (Back Propagation Neural Network, BPNN). By taking the reconstruction measures and performance indexes as the training sample sets, an offline learning of sample data can obtain the corresponding correlation model. Thus, the neural network training will deliver prompt and precise results of performance indexes when given reconstruction measures. To enhance the generalization mapping capability of BPNN, this paper utilizes genetic algorithm to optimize the weights and thresholds of the BPNN. The experimental result based on IEEE 33 node network demonstrates the proposed methodology to be accurate and effective."}, {"label": 1, "content": "This paper evaluates the reliability indices of a distribution network through rerouting interrupted customers during outages. The DN's circuit graph is utilized to analyze the failure mode using the FMEA method and compute system reliability. The feeder capacity and voltage deviation during rerouting are also considered for a more precise estimation. The optimization problem after an outage is to find the optimal tree by interchanging the status of NO and NC switches to restore as many interrupted customers as possible under set constraints. The proposed algorithm provides a systematic approach to solving the restoration problem, decreasing the search space, and making it suitable for analyzing big DN sizes. Additionally, a ranking method is introduced to measure the impact of each NC switch on reliability improvement."}, {"label": 0, "content": "In this paper, we consider the downlink in a K-tier heterogeneous network in the presence of Nakagami-m fading and noise. For such a system, we derive closed-form approximations of coverage probability and average rate achievable. A piece-wise linear approximation is employed in obtaining the simplified expressions. The proposed results are verified numerically through simulations. A comparison with existing work shows that the proposed work is a good approximation."}, {"label": 1, "content": "The rise in the use of mobile and wearable electronic devices has led to an increased demand for solutions to power these devices safely and conveniently for both humans and the environment. One perspective solution to this problem is the harvesting of human body energy. In this paper, the focus is on the performance of harvesting human body waste heat using thermoelectrical generators. However, the generated form of electricity is not suitable for direct use by electronic devices, and special converters are required to transform and store electrical energy.\n\nThis study analyzed the performance of three commercially available low-voltage step-up converters in conjunction with a series of five thermoelectric elements located on the lower leg. An approach was presented for defining the efficiency of energy harvesting, which was used to compare the step-up converters and find the best solution for the required conditions.\n\nOverall, this study shows that harvesting human body waste heat using thermoelectrical generators is a promising solution to power mobile and wearable electronic devices, and the efficiency of energy harvesting can be improved by using the appropriate step-up converter. Further research is needed to optimize the design of converters and thermoelectric generators for practical use."}, {"label": 0, "content": "We propose sparse-complementary convolution (SC-Conv) to improve model utilization of convolution neural networks (CNNs). The networks with SC-Conv achieve better accuracy than the regular convolution under similar computations and parameters. The proposed SC-Conv is paired with two deterministic sparse kernels, and one of kernels is complementary to the other one at in either spatial or channel domain or both; the deterministic sparsity increases the computational speed theoretically and practically; furthermore, by having the complementary characteristic, SC-Conv retains the same receptive field to the conventional convolution. This insightful but straightforward SC-Conv reuses of modern network architectures (ResNet and DenseNet), and at the same FLOPs and parameters, SC-Conv improves top-1 classification accuracy on ImageNet by 0.6 points for ResNet-101 and keep the same model complexity. Furthermore, SC-Conv also outperforms recent sparse networks by 1.3 points at top-1 accuracy for ImageNet, and after integrating SC-Conv with the sparse network, we further improve another 1.8 points accuracy at similar FLOPs and parameters."}, {"label": 0, "content": "A Ground Control Station (GCS) is an essential element to supervise and control autonomous vehicles performing complex missions in real time. In the new era of Internet of Things, where systems are highly connected, these missions demand enormous amounts of computational power to correctly manage the coordination of all the vehicles involved. In this scope, the set of Unmanned Vehicles (UVs) included in the mission must achieve more difficult tasks everyday. As a consequence, the development of a robust, reusable and adaptable GCS framework to allow a single operator to monitor and control a team of heterogeneous agents raises a number of research and engineering challenges. In this paper we introduce an adaptive event-driven framework specially designed for GCSs involved in heterogeneous multi-agent missions that takes advantage of two features: 1) it allows the GCS to add or remove both actual or simulated agents in real time, changing the number or types of monitored agents, and 2) from a software design perspective, the graphical user interface dynamically changes its view in order to minimize operators fatigue and mental workload, facilitating the success of the mission in such complex environments. We also show one of the tests performed with the adaptive framework, where after observing how a real UV deployed in a water surface performs successfully a set of previously planned trajectories, we will see how a simulated UV joins the mission in order to fulfill a leader-follower maneuver."}, {"label": 0, "content": "Security of vehicular networks has often been an afterthought since they are designed traditionally to be a closed system. An attack could lead to catastrophic effect which may include loss of human life or severe injury to the driver and passengers of the vehicle. In this paper, we propose a novel algorithm to extract the real-time model of the controller area network (CAN) and develop a specification-based intrusion detection system (IDS) using anomaly-based supervised learning with the real-time model as input. We evaluate IDS performance with real CAN logs collected from a sedan car."}, {"label": 1, "content": "The AC power grids of China are now connected by HVDC links, creating a complex network that requires precise simulation technology in order to understand its behavior and the interaction between AC and DC. Traditional power system analysis tools have simplified models of DC control and protection that may not accurately reflect the grid's behavior in the field. To achieve a detailed, panoramic view of the network, a large-scale network model must be simulated in real-time with actual DC control and protection devices linked. This requires a small simulation time step of microseconds, which is a significant challenge for power system experts. \n\nThis paper uses the power system real-time simulator HYPERSIM and the supercomputer SGI UV300 to optimize the method of large-scale grid decoupling, splitting the complex region grid model into distributed tasks. It also proposes a synthetic method for optimizing multiple auto-assigned parameters using the automatic task mapping function in HYPERSIM. The result is an electromagnetic transient network model of 17746 single phase buses that can be operated in real-time, a breakthrough in scale expanding of real-time simulation. This precise simulation gives a new technical means for gaining a better understanding of the Chinese power system, including characteristic cognition, system planning, operating, decision-making, and failure reproduction."}, {"label": 0, "content": "Advanced communications and networks greatly enhance the user experience and have a major impact on all aspects of people\u2019s lifestyles. Widely deployed sensor nodes provide support for these services. However, although energy harvesting and transfer technology provides a solution to allow the long-term survival of wireless sensor nodes for wireless sensor networks, the single collection scheme causes a lot of energy waste. Thus, efficient energy utilization and fast data collection are still serious challenges for energy harvesting wireless sensor networks. To overcome these challenges, an adaptive collection scheme based on matrix completion (ACMC) is proposed to reduce delay and to improve the energy utilization of the network. In the ACMC scheme, compared with traditional data collection schemes, the data collection schemes vary with the available energy, collecting large amounts of data when the available energy is sufficient to obtain high-quality data-based applications. Otherwise, adaptive selecting the collected data based on previously collected data, the amount of data collected can be effectively reduced based on the application requirements, thereby improving the energy utilization of the network. The ACMC scheme also proposes a method for reducing the delay by increasing the duty cycle of the nodes that are far from the CC. At the same time, the transmission reliability of these nodes increases due to the increase in the transmission frequency. Thus, the ACMC scheme can also further reduce the delay of the network. The experimental results of the ACMC scheme in planar networks show better performance than the traditional data collection schemes and can improve the energy utilization of the network by 4.26%\u20136.68% while reducing the maximum delay by 9.4%."}, {"label": 0, "content": "The article examines the effectiveness of the scheduling algorithm for solving the problem of increasing the efficiency of the operation of corporate information and computer networks based on the use of priority information processing models. The possibility of increasing the efficiency is caused by the analysis of the features of properties and characteristics of the considered network in various modes of operation, which are obtained at the modeling stage. The order of servicing of information flows of different types, determined by the introduction of priorities, allows to reduce the average waiting time for system operation and loss due to waiting. The task of analyzing of the efficiency of priority systems for the transmission and processing of information in complex information and computing systems is the perspective application-oriented task due to use of an effective dispatch algorithm."}, {"label": 0, "content": "Bare-metal cloud provides a dedicated set of physical machines (PMs) and enables both PMs and virtual machines (VMs) on the PMs to be scaled in/out dynamically. However, to increase efficiency of the resources and reduce violations of service level agreements (SLAs), resources need to be scaled quickly to adapt to workload changes, which results in high reconfiguration overhead, especially for the PMs. This paper proposes a hierarchical and frequency-aware auto-scaling based on Model Predictive Control, which enable us to achieve an optimal balance between resource efficiency and overhead. Moreover, when performing high-frequency resource control, the proposed technique improves the timing of reconfigurations for the PMs without increasing the number of them, while it increases the reallocations for the VMs to adjust the redundant capacity among the applications; this process improves the resource efficiency. Through trace-based numerical simulations, we demonstrate that when the control frequency is increased to 16 times per hour, the VM insufficiency causing SLA violations is reduced to a minimum of 0.1% per application without increasing the VM pool capacity."}, {"label": 0, "content": "The paper presents the results of the research work funded by Salt River Project Agricultural Improvement and Power District (SRP) on maximizing the economic benefits to customers installing residential rooftop PV systems in SRP territory. The optimized discharge of the battery power which would help in the reduction of Demand Charge paid by the customer was the primary goal. Machine Learning algorithms were utilized as a better load forecasting technique to the ones already in place. The improved battery discharge algorithm would also reduce the battery charge-discharge cycles (cycling aging) thus, improving the battery life. The tests were performed in the state of Arizona, on a residential rooftop grid-tied PV with storage system installed at the Tempe campus of the Arizona State University."}, {"label": 1, "content": "Wireless sensor networks are widely used in intelligent transportation systems, but the amount of data that needs to be transmitted wirelessly varies depending on the density of vehicles in different areas. To address this issue, a new data transmission scheme called the Sensor On/off scheme based on Polling Algorithm (SOP) has been proposed. The SOP incorporates a no packet loss scheduling mechanism, which is effective in different service density areas. Additionally, switching technology has been integrated into the SOP, allowing sensors to be turned off when idle, significantly reducing energy consumption. Comparative analysis between SOP, random, and sequential schemes has proven the effectiveness of SOP in reducing the energy consumption of data transmission in wireless sensor networks."}, {"label": 0, "content": "In the current practice, locational marginal prices (LMPs) are computed every few minutes by solving an optimization problem with continuous variables only. Generator on/off statuses are decided by the day-ahead unit commitment scheduling procedure. Thus, unit commitment and locational marginal price computing are two separate steps. This paper presents a model that substitutes the traditional two-step method. It is based on bilevel programming. The proposed problem determines generator on/off status and locational marginal prices simultaneously. The efficiency of the model is demonstrated on the 5- and 30-bus systems."}, {"label": 0, "content": "This study is aimed at the problem that the detection rate of intrusion detection technology based on Extreme Learning Machine (ELM) algorithm is not high and the intrusion detection technology based on Support Vector Machine (SVM) algorithm is slow. An intrusion detection method based on Kernel Principal Component Analysis (KPCA) and extreme learning machine algorithm is proposed. Using the KPCA algorithm to reduce the dimension of the extracted feature matrix, and using the ELM algorithm to perform multi-classification detection on four common types of attacks. Simulation results show that the proposed method is more efficient and faster than intrusion detection based on extreme learning machine algorithm and intrusion detection based on support vector machine algorithm. Finally, the accuracy, false alarm rate, detection rate, and detection time in intrusion detection technology are improved."}, {"label": 1, "content": "Learning to predict human body motion has become an important area of research in both computer vision and artificial intelligence. This paper presents a study on predicting human body motion from video sequences by proposing a human body motion prediction network that integrates advanced 2D feature extraction and video sequence prediction. Our network leverages temporal characteristics extracted from video sequences to make predictions about human motion. We trained our network using video-based human pose datasets, and were able to demonstrate its excellent performance through both quantitative and qualitative results. Our experimental findings definitively confirm the feasibility of our method."}, {"label": 0, "content": "With a rapid increase in the number of geostationary satellites around the earth's orbit, there has been a renewed interest in using Global Positioning System (GPS) to understand several phenomenon in earth's atmosphere. Such study using GPS devices are popular amongst the remote sensing community, as they provide several advantages with respect to scalability and range of applications. In this paper, we discuss how GPS signals can be used to estimate the amount of water vapor in the atmosphere. Furthermore, we demonstrate the importance of such precipitable water vapor (PWV) in the atmosphere for the task of rainfall detection. We present a detailed analysis in our dataset of meteorological data of 3 years. Test dataset shows that use of PWV in rainfall detection helps to reduce the false alarm rate by almost 12%."}, {"label": 1, "content": "A computational benchmark suite has been developed to assess the performance of modern RCS simulations. The suite consists of a collection of scattering problems organized across six levels of complexity, thus offering a range of basic to challenging scenarios. The suite also provides standard reference solutions, performance metrics and recommended studies to help evaluate the strengths and limitations of different simulation methods."}, {"label": 1, "content": "Currently, movie trailers are edited using various methods, but their length is often limited to just a few minutes. This can make it difficult to create a trailer that meets the preferences of all potential viewers. In addition, the scenes used for editing and the effects used are often limited, which can limit the effectiveness of the trailer in attracting and retaining viewers.\n\nTo address these issues, we have identified seven editing biases that commonly occur when movies are summarized and edited into trailers. By understanding these biases, we can use them to generate movie trailers that cater to a wider range of viewer preferences.\n\nBy doing so, we can increase the chances of attracting and retaining viewers, and ultimately, improve the overall success of a film. With this approach, we can create trailers that are more enticing and effective in promoting movies to audiences of all kinds."}, {"label": 1, "content": "The bare-metal cloud offers a dedicated set of physical machines (PMs) for scaling both PMs and virtual machines (VMs) on them dynamically. However, increasing the efficiency of the resources and reducing service level agreement (SLA) violations require quick scaling to adapt to workload changes, resulting in high reconfiguration overhead, especially for the PMs. To address this, this paper proposes a hierarchical and frequency-aware auto-scaling based on Model Predictive Control. This technique achieves an optimal balance between resource efficiency and overhead. When performing high-frequency resource control, the proposed technique improves the timing of reconfigurations for the PMs while increasing the reallocations for the VMs to adjust the redundant capacity. This process improves the resource efficiency. Trace-based numerical simulations demonstrate that when the control frequency is increased to 16 times per hour, VM insufficiency causing SLA violations is reduced to a minimum of 0.1% per application without increasing the VM pool capacity."}, {"label": 1, "content": "Accurately estimating power system frequency and amplitude is crucial for power system monitoring, control, stability, and protection. Therefore, this study presents a new strategy for estimating the frequency and amplitude of the power system based on the variational mode decomposition (VMD) algorithm and the Cheb-function (Chebfun) approximation system. The VMD extracts spectral information from power signals in the form of sub-signals or modes. The Chebfun system then interpolates each mode using Chebyshev polynomials in the continuous domain. This results in a continuous function, which allows for the instantaneous frequency and amplitude to be estimated based on the zero crossings and local extrema locations. To evaluate the robustness of this approach, various power system scenarios were tested, and the results were compared with other methods. The outcomes suggest that the proposed technique is a viable candidate for efficiently estimating power system frequency and amplitude."}, {"label": 1, "content": "Audio forensics has numerous applications that can benefit from the ability to classify audio recordings based on the device they originated from, especially on social media platforms where huge amounts of data are posted daily. In this regard, a recent paper proposes to leverage passive signatures associated with recording devices extracted from recorded audio, in identifying the source cell-phone of the recorded audio. This approach uses device-specific information present in both low and high-frequency regions of the audio recordings, even in the absence of any extrinsic security mechanism like digital watermarking. The proposed system achieves a closed set accuracy of 97.2% on the only publicly available dataset in this field, MOBIPHONE, matching the state-of-the-art accuracy reported for this dataset. Furthermore, the proposed methodology outperforms existing methods by 4% in average accuracy on audio recordings which have undergone double compression, which frequently occurs when a recording is shared on social media."}, {"label": 0, "content": "The large-scale virtualized data centers in the Cloud environment consume huge amount of energy leading to high operational costs and emission of greenhouse gases. Energy consumption of a data center can be reduced by dynamically consolidating the virtual machines (VMs) to a minimum number of physical machines, using live migration. However, the dynamic workload of virtual machines makes the VM consolidation problem more challenging. In this paper, we have proposed a prediction based migration technique for the VMs, where we perform VM migrations based on the predicted CPU utilization. Extensive simulations show that the proposed technique substantially reduces energy consumption, number of VM migrations and Service Level Agreement (SLA) violations within a data center. The performance overheads associated with excessive migration of VMs increase the time needed by the VMs to complete their jobs. So in this paper, we have also proposed a deadline aware VM migration technique, which reduces the time taken by the VMs to execute their jobs significantly, thereby improving the Quality of Service (QoS). Such improvement in QoS is achieved at the cost of slight increase in the energy consumption within the data center. However, simulation results show that appropriate setting of deadlines for the VMs, helps in achieving a trade-off between energy consumption and the QoS."}, {"label": 1, "content": "To address the issue of low detection rates of minority samples in imbalanced datasets for network intrusion detection, an optimized deep learning approach is proposed. The approach uses a hybrid sampling method during data processing, where Synthetic Minority Over-sampling Technique (SMOTE) is employed to augment the minority samples, while the majority samples are under-sampled using Neighborhood Cleaning Rule (NCL).  Once the dataset has been balanced, high-dimensional data is reduced using Deep Belief Network (DBN) to obtain a lower-dimensional representation of the preprocessed data. Finally, classification is performed via Probabilistic Neural Network (PNN). \nExperimental results on the NSL-KDD dataset indicate that the hybrid sampling approach significantly improves detection rates and classification accuracy for minority categories, and the performance of the DBN-PNN method surpasses that of traditional methods."}, {"label": 1, "content": "Research in the area of co-prime arrays and samplers has primarily focused on reconstructing the autocorrelation and spectral content of a signal at the Nyquist rate from sub-Nyquist data. These techniques have proven useful in various applications including power spectrum estimation, beamforming, direction-of-arrival estimation, and system identification. However, the potential of coprime samplers for cross-correlation estimation has not yet been fully explored. \n\nIn this paper, we propose a method for cross-correlation estimation using co-prime samplers in two different scenarios. In the first scenario, both signals are obtained using co-prime samplers, while in the second scenario, we assume one signal is known and sampled at the Nyquist rate, while the second signal is acquired using a co-prime sampler. We also determine the number of contributors available for cross-correlation estimation at each difference value, which is crucial for accurate estimation. \n\nThis work has numerous potential applications, including time-delay, range, velocity, acceleration, and cross-spectrum estimation, all of which require cross-correlation estimation. By expanding the use of co-prime samplers to include cross-correlation estimation, we hope to provide a valuable tool for a range of signal processing applications."}, {"label": 0, "content": "Advances in robotics and cloud computing have led to the emergence of cloud robotics where robots can benefit from remote processing, greater memory and computational power, and massive data storage. The integration of robotics and cloud computing has often been regarded as a complex aspect due to the various components involved in such systems. In order to address this issue, different studies have attempted to create cloud robotic architectures to simplify representation into different blocks or components. However, limited study has been undertaken to critically review and compare these architectures. As such, this paper investigates and performs a comparative analysis of existing cloud robotic architectures in order to identify key limitations and recommend on the future of cloud robotic architectures. As part of this study, 7 such architectures have been reviewed and compared and results showed limited evaluation of existing architectures in favour of security weaknesses."}, {"label": 0, "content": "The paper considers technologies of increasing energy efficiency of technological modes at robotized productions. The project of trajectories optimization module of industrial manipulator is put forward. The module is integrated into an informative space of robotized section and analyzes operating programs of CAM - system. It defines the volume of associated energy costs of technological process, puts forward corrective actions, which are aimed at increasing energy efficient of technological process while changing the program code without going beyond the limitations of the technology. The base of the computing part includes intellectual algorithms of identifying non-linear dependencies and output decisions. Results of the research are proved by experiment that was made at robotized machining section with industrial robot KUKA KR - 60."}, {"label": 0, "content": "Traffic congestion and occlusions are major problems nowadays in metropolitan cities which leads to an ever growing traffic accidents. Therefore, the need of traffic flux management in order to avoid these congestions, unnecessary time wastage and tragic accidents is very important. Traffic regulation by optimizing timing of traffic control signals is one of the solutions for this purpose. This paper presents a low cost camera based algorithm in order to control traffic flow on a road. The algorithm is based on mainly three steps: vehicle detection, counting and tracking. Background subtraction is used to isolate vehicles from their background, Kalman filter is used to track the vehicles and Hungarian algorithm is exploited for association of labels to the tracked vehicles. This algorithm is implemented on both daytime and night time videos acquiered from CCTV camera and IR camera. Experimental results show the efficacy of the algorithm."}, {"label": 1, "content": "Modular superconducting magnetic energy storage (SMES) is an innovative technology that addresses the capacity limitations of SMES by utilizing advanced control technology to achieve a modular layout. In this study, self-adaptive state of charge (SOC) feedback control was employed in the power distribution of modular SMES. This approach considered various factors such as the economy of the device, power efficiency, and other relevant constraints.\n\nTo optimize the capacity of the modular SMES, a multi-objective evolutionary algorithm was chosen as the solution algorithm. This allowed the achievement of multiple objectives without compromising on any significant factors. This new approach is expected to improve the effectiveness and efficiency of SMES applications significantly.\n\nIn summary, the adoption of a self-adaptive SOC feedback control mechanism together with a multi-objective evolutionary algorithm has enabled the development of a modular SMES with enhanced capacity efficiency. This innovative solution paves the way for more efficient and reliable utilization of SMES technology, thereby ensuring a more sustainable energy future."}, {"label": 0, "content": "A concept of a large scale Multi-Chip-Module with number of Many Core (PE), FPGA and DRAM chips is proposed. There are three major features, (1) very high speed data transmission by T-Hz $(0.1 \\sim10$ THz) radio communication chips inside the MCM module for communication among the chips, (2) MLC (Multi Layer Ceramic) or another substrate on which T-Hz radio chips in upper surface side and number of Many Core (PE), FPGA and DRAM chips in lower surface side are connected by C4, (3) T-Hz radio chips also communicate with other MCMs and external I/O devices."}, {"label": 1, "content": "Conventionally, signal frequencies have been estimated using a spectral peak search process, which has been limited by the common signal mismatch problem (SMP). However, nonparametric methods such as MVDR and CCA have been shown to improve spectral estimation through magnitude squared coherence (MSC). This paper develops a novel scalar cost function based on the CCA MSC spectrum, utilizing local peaks to estimate signal frequency. Furthermore, a gradient-based adaptive-step algorithm has been presented to identify these local peaks. Simulation results demonstrate that the proposed algorithm offers notable improvements in frequency estimation accuracy with avoidance of the SMP."}, {"label": 1, "content": "In this paper, the focus is on designing cubic splines as tone correction functions for the achromatic component of a spherical color model. This model is advantageous as it provides more perceptually smooth color changes along its coordinates compared to commonly used color models HSV and HSL. However, using a tone correction function in this model can lead to gamut issues due to the higher number of color points it contains compared to the RGB color model. \n\nThis paper shows that well-designed tone correction functions can avoid gamut issues, and the general tone correction techniques still work effectively in the spherical color model. The authors propose a specific type of cubic splines that can be adopted by the general tone correction techniques for low-key, middle-key, and high-key images by selecting suitable parameters. \n\nExperimental results demonstrate that these cubic splines work well for tone correction in the spherical color model, ensuring that the tone correction technique seamlessly integrates with this model."}, {"label": 1, "content": "In a multi-cellular WiMAX system based on Orthogonal Frequency Division Multiple Access (OFDMA), selecting an appropriate base station involves conducting a cell search. The mobile terminal has to complete various basic tasks successfully, including timing and frequency synchronization as well as identifying the frame start position, before it can establish a communication link with the base station. In this study, we suggest a dual correlation algorithm to enhance the accuracy of frame detection synchronization. Additionally, we propose a joint blind channel estimation and equalization scheme to minimize the impact of channel fading in low SNR conditions. Our proposed technique outperforms the current approach in terms of frame detection system performance, particularly under low signal to noise ratio conditions."}, {"label": 1, "content": "Localization is one of the most promising technologies in modern society, and wireless sensor networks (WSNs) are being studied as a popular method to solve the problem. However, one of the main challenges in localization is the non-line of sight (NLOS) propagation. To overcome this issue, we propose a method that combines the particle filter and residual analysis.\n\nThe residual analysis is a powerful a posteriori algorithm that can be used to improve the reliability of the localization process. On the other hand, the particle filter is a powerful technique that does not make presumptions about the probability density function or linear system models. By combining the two methods, we can improve the accuracy of localization and enhance the robustness of the method.\n\nMoreover, the randomness of the particle improves the robustness of the proposed method. The effectiveness and robustness of the proposed method are evaluated through simulation results. Overall, the proposed method appears to be effective and robust, providing a promising solution to the challenge of NLOS propagation in localization."}, {"label": 0, "content": "The ever-growing of Internet of Things (IoT) data and the new spectrum of data applications have stimulated IoT clients to outsource their data to cloud servers or datacenters. Apart from storage service, the IoT clients also desires the servers to execute functional operations per client's request. In this paper, we aim to design the secure mechanisms that allow the IoT clients to outsource their encrypted data to geographically distributed servers while supporting homomorphic computation functions. We leverage the distributed index framework to disassemble and spread data evenly across geographically distributed servers while employing the key-value store as the underlying structure for fast data retrieval. To support computing over encrypted data, we customize Shamir's secret sharing into our mechanisms to design a tunable scheme for the adaption of different IoT application scenarios. In particular, we design three tunable protocols to achieve the effective additive homomorphic computations while approaching efficiency in terms of servers utilization, computation, and storage overhead. Even the designs focus on the additive computation, we show that it can be readily extended to other types of homomorphic computations as well as verifying the correctness of stored data. Based on the proposed protocols, we design system prototypes, deploy them in Amazon Web services, and evaluate our construction experimentally. Through experimental results, we show that our designs can achieve the efficiency in various perspectives."}, {"label": 0, "content": "Nowadays, audio generation plays an important role in human-computer interactive applications. However, the audio generated by machine is far from nature sound, especially in expressiveness and complexity. Currently, conditional variational Auto-encoder (cVAE) has achieved excellent results in data generation, but original cVAE cannot avoid the defects caused by KL divergence which used in stochastic distribution measurement. This paper introduced Hellinger distance into cVAE model. First of all, the experiment shows that using Hellinger distance can improve the weakness of KL divergence effectively. And then, the relationship between the latent space parameters and the generated music quality is analyzed by experiments, and we found the best generative parameter is the distribution centroid. Finally, the generated music is subjectively evaluated and the results show that it is significantly better than the original model."}, {"label": 0, "content": "Mixing matrix estimation is very important for underdetermined blind source separation. To solve the problems of existing methods for mixing matrix estimation such as low estimation accuracy, a detection method for single source points (SSPs) is proposed based on local stationarity and distribution symmetry in this paper, and then mixing matrix estimation is obtained through clustering algorithm. The proposed method does not require region division of hypersphere and is easy to operate, so as to effectively eliminate pseudo SSPs and improve the clustering features of observed signals. The simulation results show that the proposed method has higher accuracy than the traditional methods."}, {"label": 1, "content": "This article presents a method for implementing a fuzzy interconnected control system for sheet metal forming electric drives. The system's forming rod displacements depend on various factors, and some of them can only be described through qualitative characteristics or indexes. To achieve the desired output parameters for all local drives systems, a compensation method must be used. However, the conventional compensation methods encounter difficulties due to the interrelation error between local forming electric drives. To address this, we suggest compensating for the interrelation effect between local forming electric drives by using a fuzzy compensation method based on the theory of differential inclusion. We describe the mathematical equations for this approach, and we present an algorithm for implementing a fuzzy controller that achieves the specified control accuracy."}, {"label": 0, "content": "Passive acoustic monitoring is emerging as a promising solution to the urgent, global need for new biodiversity assessment methods. The ecological relevance of the soundscape is increasingly recognised, and the affordability of robust hardware for remote audio recording is stimulating international interest in the potential for acoustic methods for biodiversity monitoring. The scale of the data involved requires automated methods, however, the development of acoustic sensor networks capable of sampling the soundscape across time and space and relaying the data to an accessible storage location remains a significant technical challenge, with power management at its core. Recording and transmitting large quantities of audio data is power intensive, hampering long-term deployment in remote, off-grid locations of key ecological interest. Rather than transmitting heavy audio data, in this paper, we propose a low-cost and energy efficient wireless acoustic sensor network integrated with edge computing structure for remote acoustic monitoring and in situ analysis. Recording and computation of acoustic indices are carried out directly on edge devices built from low noise primo condenser microphones and Teensy microcontrollers, using internal FFT hardware support. Resultant indices are transmitted over a ZigBee-based wireless mesh network to a destination server. Benchmark tests of audio quality, indices computation and power consumption demonstrate acoustic equivalence and significant power savings over current solutions."}, {"label": 1, "content": "Cooperative navigation offers a promising solution to improve the accuracy and safety of vehicle navigation, particularly in challenging environments such as urban canyons. One key technology used in cooperative navigation is the Dedicated Short-Range Communication (DSRC) standard, which facilitates the sharing of sensor measurements between nearby vehicles. GNSS and other primary sensors measurements can be shared among vehicles via DSRC or other telecommunication systems such as 3G/4G. \n\nIn addition to GNSS measurement sharing, the on-board IEEE 802.11p DSRC receiver can measure the angle between a vehicle's building axes and the direction of received signals from nearby vehicles. Utilizing DSRC signals to share GNSS measurements and mutual headings of surrounding vehicles can also contribute to an increase in navigating accuracy. By fusing the coordinate measurements and corresponding heading angles of surrounding vehicles onboard, cooperative navigation technology can significantly improve the accuracy of vehicle navigation in challenging environments. \n\nOverall, the proposed approach has the potential to provide tangible benefits for increasing the accuracy and safety of vehicle navigation in challenging environments."}, {"label": 1, "content": "In this study, we tackle the task of inferring links in a communication network through limited, passive observations of network traffic. Our method employs transfer entropy (TE) as a measure for assessing the strength of automatic repeat request (ARQ) mechanisms in next-hop routing links. Unlike other techniques, TE offers a model-free, information-theoretic approach that operates using packet arrival times that are externally available. We conduct a discrete event simulation of a wireless sensor network and demonstrate that our TE-based topology inference approach is robust to varying degrees of connection quality in the underlying network. When compared with an existing approach that employs linear regression based formulation of Granger Causality, our method exhibits a better asymptotic time complexity and significantly enhanced network topology reconstruction performance. Despite being suboptimal, our approach also boasts better time complexity, while maintaining reasonable performance, when compared to a causation entropy based optimal algorithm proposed in the literature."}, {"label": 0, "content": "A novel model parameter identification based bus-bar protection principle is proposed in this paper. An inductance model can be developed when an internal fault occurs on bus. By taking the inductance and the resistance of the model as the unknown parameters to be identified, the equivalent instantaneous impedance and the dispersion of the parameter can be calculated. Utilizing their difference, the external fault and the internal fault with different current transformer (CT) saturation extent can be distinguished correctly. Based on this, a new criterion with self-adaptive restraint characteristic for bus-bar protection is put forward. As the new principle is suitable for non-periodic component, fundamental component and harmonic component, it can operate more rapidly comparing with the traditional phasor based bus-bar protection principles. Moreover, the proposed principle is inherently immune to the impact of fault current flowing out when a fault occurs in the protection zone of the breaker-and-a-half bus-bar and is insensitive to fault resistance. Simulation results show that the presented principle has high sensitivity and reliability."}, {"label": 0, "content": "Skin cancer is the abnormal growth of skin cells that can not be controlled. Skin cancer appears when the DNA is damaged skin cells (mostly due to ultraviolet radiation from the sun) triggers mutations that skin cells grow rapidly, can not be controlled and start forming a tumor. Skin cancer can be overcome if it is detected earlier before spreading or doing metastasis. However, the tendency of people who are indifferent and reluctant to check or consult with doctors make his condition worse without realizing it. Therefore, designed an application for Skin Cancer Detection with Image Processing and Expert System using Forward Chaining and Certainty Factor method. The end result of image processing and expert system in this application is the assessment of High Risk, Low Risk, or Medium Risk of nevus conditions in patients. With the design of this system is expected to help raise awareness to detect early skin cancer. The results in this study show that the application has an accuracy rate of 100%. It shows that this system produces the same results as an expert."}, {"label": 1, "content": "Attitude motion periods of unstable satellites is an important parameter for space target surveillance. Traditional methods of period estimation are only applicable to single periods. A new double-period estimation technique based on variational mode decomposition (VMD) and mutual information is proposed for the rotation and precession of unstable satellites. Firstly, intrinsic mode functions (IMFs) and corresponding center-frequencies of the unstable satellite's radar cross-section (RCS) are determined through VMD. The rotation and precession periods of the satellite are then obtained by calculating and comparing the mutual information of the IMFs and RCS sequence. Results from experimentation show that the rotation and precession periods can be estimated correctly. Compared to spectrum analysis, autocorrelation function, and empirical mode decomposition (EMD), the approach effectively restrains the phenomenon of frequency multiplication and mode mixing, significantly improving the accuracy of period estimation."}, {"label": 0, "content": "Electric vehicles (EVs) as distributed storage devices have the potential to provide frequency regulation services due to the fast adjustment of charging/discharging power. Along with the policy incentives, it is practical for EVs to take part in the regulation market through the aggregator. An optimal control strategy based on reinforcement learning (RL) for electric vehicles (EVs) in distributed networks is proposed in this paper. The overall goal is to follow the regulation signals sent by the system operator in the real time regulation market by controlling the EVs in the parking lot. To achieve this, the reinforcement learning algorithm is employed to optimize the charge and discharge strategy of the EVs, so that the aggregator optimally allocates the regulation power and the baseline charging power to EVs to respond to the regulation signals for the best regulation performance. Comprehensive simulation studies have been carried out based on the data of PJM electricity market and the results show that the regulation performance based on the control strategy is excellent in both cases of traditional and dynamic regulation signals."}, {"label": 0, "content": "The article proposes the structure of an intelligent software platform for managing complex risks. It is proposed to divide the platform into a global and local part (end-point software) that provides not only the management of complex risks on the scale of a single system (enterprise, organization) but also the collection, accumulation, synthesis and dissemination of methods and models of integrated risk management that are best practices that have proven effective on practice. Within the local part, the platform provides the ability to build complexes of hybrid intelligent models by a risk management specialist without the participation of a programmer. Examples of using the proposed solution or its parts in projects aimed at risk management are considered."}, {"label": 0, "content": "The idea of Non-Intrusive Load Monitoring (NILM) is to monitor a network of electronic consumers through a centralized capture of current and voltage. Based on the waveform, the devices can be distinguished by their specific turn-on and turn-off events. Most methods use the power signals active power P and reactive power Q (PQ). Various types of machine learning and classification algorithms have already been applied to a variety of different datasets. The power signals can already achieve good results for different classification algorithms on different datasets. However, the information content of the power signals is limited. When calculating the power from the current and voltage signal, information is lost through integration over a period. Many publications in the field are based on proprietary, unpublished datasets. Often, only particular scenarios are in focus. Thus, real comparability between the works is difficult. The Home Equipment Laboratory Dataset 1 (HELD1) dataset introduces the current-voltage based new waveform called Frequency Invariant Transformation of Periodic Signals (FIT-PS) for use with the well-known standard machine learning algorithms, k Nearest Neighbors Classifier (kNN), Support Vector Machine Classifier (SVM) and Naive Bayes Classifier (Bayes). With FIT-PS, more than 90% of events can be correctly assigned to the devices. This work allows an equitable comparison for already applied methods in the context of NILM with the effectiveness of this new waveform."}, {"label": 1, "content": "This article presents an intellectual-information system that will be utilized for managerial decision making regarding technical personnel in power plants and other similar process facilities. The system's output will help in deciding whether the upgrading of technical staff's skills is necessary. The intellectualization methods utilized to determine the knowledge level through fuzzy logic allow not only for knowledge assessment but also to automate the process of its enhancement and consolidation. This approach enables on-the-job training for technical personnel. Additionally, the article presents a method for intellectualization to estimate the knowledge level of the basic theory of calculating optimum feeding parameters of regulating devices for automatic control systems. This approach solves the problem of automation evaluation for parametric synthesis in one-loop, cascade, and combined control systems. The level of knowledge control during parametric synthesis can be determined by selecting the correct point on the amplitude-phase characteristics and D-partition graphs. The program FuzzyTECH tested the adequacy of the established fuzzy output system."}, {"label": 1, "content": "In this paper, we present a method that uses computer vision, specifically projective geometry, to map a known distribution of points on a sphere with a known diameter, along with an arbitrary image of these points on an image plane, to identify the configuration of the camera. In simpler terms, this method can extract the camera matrix and its parameters - intrinsics and extrinsics - by knowing the sets of 2D-3D corresponding points. We validated this method by code, which explains in detail how to set up a theoretical world and camera coordinate frame. By applying our knowledge of the correspondence, we can display the solution to the optimization problem. We analyzed the results and noted the relative error between the retrieved and actual camera matrices."}, {"label": 0, "content": "On battery-operated devices, energy and power consumption are main concerns. With the recent advancement of technology, mobile devices can be integrated with traditional systems for running complex computations. In fact, mobile devices can easily become part of computational networks and share their computational and memory resources. Despite this, traditional simulation frameworks are not designed to perform well on heterogeneous networks. This is mainly due to the limited computational resources that are available on mobile devices. In this paper, we propose SEECSSim (SEECSSim is derived from School of Electrical Engineering and Computer Science (SEECS)) that is a simulation framework specifically designed for mobile devices. SEECSSim includes state-of-the-art distributed synchronization algorithms that are implemented to run on mobile or embedded devices. To benchmark the proposed framework, the well-known PHOLD model is used and performance results are reported in terms of execution time, CPU usage, memory and energy consumption."}, {"label": 0, "content": "Dual- (or multiple) rear cameras on hand-held smartphones are believed to be the future of mobile photography. Recently, many of such new has been released (mainly with dual-rear cameras: one wide-angle and one telephoto). Some of the notable ones are Apple iPhone 7 and 8 Plus, iPhone X, Samsung Galaxy S9, LG V30, Huawei Mate 10. With built-in dual-camera systems, these devices are capable of not only producing better quality picture but also acquiring 3D stereo photos (with depth information collected). Thus, they are capable of capturing the moment in life with depth just like our two eye system. Thanks to this current trend, these phones are now getting cheaper while becoming more power complete. In this paper, we describe a system that makes use of the commercial dual rear-camera phones such as the iPhone X, to provide aids for people who are visually impaired. We propose a design to place the phone on the chest centre of the user who has one or two Bluetooth headphone(s) plugged into the ears to listen to the phone audio outputs. Our system is consist of three modules: (1) the scene context recognition to audio, (2) the 3D stereo reconstruction to audio, and (3) the interactive audio/voice controls. In slightly more detail, the wide-angle camera captures live photos to be investigated by a GPS guided Deep Learning process to describe the scene in front of him/herself (module 1). The telephoto camera captures the more narrow-angle and thus to be stereo reconstructed with the aids of the wide angle's one to form a depth map (densed area-based distance map). The map helps determine the distance to all visible object(s) to notify the user with critical ones (module 2). This module also makes the phone vibrate when an object(s) located close enough to the user, e.g. within hand reach distance. The user can also query the system by asking various questions to get automatic voice answering (module 3). In addition, a manual rescue module (module 4) is also added when other things have gone wrong. An example of the vision to audio could be \u201dOverall, likely a corridor, one medium object is 0.5 m away - central left\u201d, or \u201dOverall, city pathway, front cleared\u201d. Audio command input may be \u201dread texts\u201d, and the phone will detect and read all texts on closest object. More details on the design and implementation are further described in this paper."}, {"label": 1, "content": "Discrete messages transmitted over radio channels experience distortion due to various additive and multiplicative interferences, leading to errors in the received message. The probability of error in the received message reflects the quality of the communication channel at a given time, and the radio line must adapt to changing communication conditions. However, the immediate estimation of error probability is time-consuming and not feasible within the communication channel stationary interval. Thus, it is practical to indirectly assess error probability by estimating telegraph distortions within a sliding window of limited time. This method requires less time compared to the direct estimation of error probability. The ratio of received signal power and noise determines error probability and telegraph distortions, but it cannot be estimated directly. In this paper, a specific software implementation of a device for measuring telegraph distortions is described, which can be used to design devices for radio line adaptation to communication conditions."}, {"label": 1, "content": "The popularity of consumer Internet of Things (IoT) platforms has been on the rise, especially in the smart home domain. However, these platforms face various new security and privacy issues due to the open nature of wireless communications. This article explores the architecture of popular smart home platforms and their components' functions. It focuses on the security and privacy challenges that arise from these platforms and reviews the state of the art countermeasures.\n\nSeveral new attacks targeting the voice interface of smart home platforms have surfaced, aiming to gain unauthorized access and compromise the user's privacy. To combat such attacks, we propose a novel voice liveness detection system that utilizes wireless signals generated by IoT devices and the received voice samples for user authentication analysis. \n\nA comprehensive survey of the aforementioned attacks and their related countermeasures is conducted, followed by the implementation of a real-world testbed on Samsung's SmartThings platform to evaluate the proposed system's performance. The results demonstrate its effectiveness in thwarting the targeted attacks, highlighting the importance of integrating security and privacy measures into smart home platforms."}, {"label": 0, "content": "In this manuscript a transmission technique, which can save energy thanks to a supportive transmission in the feedback channel is presented. It could be useful in future telemetry networks embedded into IoT or stand-alone WSNs or DSC. The basic round condition for its practical exploitation is that a central node collecting information has enough energy-much more than the supported node. It is suitable in a scenario in which the data collected by the central node and transmitted could be modelled as a Poisson distributed random variable. It is a modification of the technique presented in [1]. In contrast to [1] in this new method the central node does not need to have any prediction capabilities. Beyond this it allows us to make a tradeoff between the achieved gain in savings and delay in data delivery. A proof of concept is given by an example in which approximately 1 bit on average is needed to transmit one value of the Poisson distributed random variable."}, {"label": 0, "content": "Distributed energy resources (DER) systems introduce uncertainties in the electrical grid that cannot be addressed by classical deterministic methods. Power system analytic tools, such as Load Flow (LF), should be revisited to address such uncertainties and dependencies. Probabilistic Load Flow (PLF) provides a solution to this problem by handling these uncertainties as random variables, which addresses the rising need for fast and accurate sampling methods. Among the existing methods, the Unscented Transform (UT) has provided reliable and fast estimations. In this paper, three variants of PLF based on the UT method, with the effects of their weighting and scaling parameters are described, analyzed and compared in the IEEE 30 test case."}, {"label": 0, "content": "This paper discusses an optimal joint placement problem of phasor measurement units (PMUs) and flow measurement devices for ensuring topological observability of power systems under N-2 transmission contingencies. Previous relevant studies focus on topological observability with the least-cost deployment of PMUs and flow measurement devices. In comparison, besides minimizing the total investment cost of PMUs and flow measurements, the proposed optimal location-based joint placement model also optimizes their locations for avoiding PMUs on radial nodes and encouraging flow measurements incident to nodes that are adjacent to PMU installed buses. The proposed model can be equivalently formulated as a mixed-integer linear programming (MILP) problem and solved via a decomposition based algorithm. Effectiveness of the proposed model is verified via a 14-bus IEEE power system. Numerical results show that, compared to traditional optimal joint placement models, optimal locations obtained by the proposed approach lead to noticeable improvements in state estimation accuracy when time skew errors are considered."}, {"label": 0, "content": "The main purpose of this paper is to build an embedded platform based on TMS320VC5509A processor, and finally realizes the recognition of snore and controls pillow change the height to relieve snoring symptoms. The whole embedded hardware platform collects the sounds through the microphone module, and completes data storage, data processing and data interaction through other peripherals. After a series of pre-processing operations, short-time energy and short-time zero crossing rate dual threshold detection is selected as endpoint recognition algorithm, MFCC(Mel Frequency Cepstral Coefficient) is selected as feature extraction and KNN(k-nearest neighbors algorithm) is selected as recognition algorithm. The experimental results show that this hardware system can run well and the design is reasonable. At the same time, it can also achieve a good accuracy in snore analysis and recognition."}, {"label": 1, "content": "This paper presents a simplified critical AC-DC system voltage interaction factor (SCADVIF) index and a method to evaluate the commutation failure risk of multi-infeed HVDC systems quickly. The proposed method does not require modeling power system components in detail for dynamic simulation. By comparing all the ADVIFs and SCADVIFs of an AC/DC system, the receiving-end AC buses where faults are applied, causing commutation failure of multi-infeed HVDC system can be identified. If ADVIFjm \u2265 SCADVIFjm, a fault occurring at receiving-end system AC bus m would result in commutation failure at HVDC j. The proposed approach significantly reduces the computing time of researchers and enhances their work efficiency. The validity and accuracy of the proposed method are demonstrated by comparing simulation results using an actual planning large power grid. The index and method proposed can be widely used in the planning, design, and operation of AC/DC power grids."}, {"label": 0, "content": "Fine-grained classification is challengeable due to the small inter-class variance and large intra-class distance between fine-grained categories. The key to solve this problem is to locate the discriminative part in the image. In this paper we propose a weakly supervised method, which only need image-level label for fine-grained classification. In our model, the convolutional neural network (CNN) can location the discriminative region through attention and automatically focus on subtler features by zooming the discriminative region and feeding it to the next CNN. A Squeeze and Excitation (SE) module is employed for channel-wise attention, and a spatial constrain loss is utilized to keep the diversity of located part. We conduct experiments on CUB-2011-200, Stanford Dogs and Stanford Cars datasets to evaluate the performance of our model. The experimental results demonstrate the effectiveness of the proposed method as compared other methods."}, {"label": 1, "content": "This paper proposes an improved face recognition method based on two-directional 2DPCA (two-dimensional principal component analysis) in each block of face images. The face image is divided into several sub-images, and the sub-image features of each corresponding block are extracted by two-directional 2DPCA according to the number of sub-images. The recognition rate is then improved using the support vector machine. The experimental results on ORL face database and YALE face database demonstrate that the proposed method outperforms any other 2DPCA method in terms of face recognition rate."}, {"label": 1, "content": "Intelligent transportation systems (ITS) will play a critical role in the development of smart cities. However, to unlock the true potential of ITS, there is a need for real-time ultralow latency and reliable analytics solutions that can combine data from a diverse range of sources. Conventional cloud-centric data processing techniques are not suitable for this task due to their high communication and computing latency. This calls for the development of edge-centric solutions that are customized to the unique ITS environment. \n\nIn this article, we introduce an edge analytics architecture for ITS that processes data at the vehicle or roadside smart sensor level, thus overcoming the reliability and latency challenges inherent in ITS. This distributed edge computing architecture leverages deep-learning techniques to provide reliable mobile sensing. We explore the various challenges facing mobile edge analytics in ITS, including data heterogeneity, autonomous control, vehicular platoon control, and cyberphysical security. We then discuss various deep-learning solutions that can help overcome these challenges. \n\nThese deep-learning solutions endow ITS devices with powerful computer vision and signal processing functions, enabling the ITS edge analytics to overcome the challenges. Initial results show that this combined approach provides a transportation environment that is secure, reliable, and truly smart. \n\nIn conclusion, the future of ITS will be integrated with smart cities, and edge analytics architecture powered by deep-learning techniques will be a critical component. This architecture will provide reliable, secure, and ultralow latency data analytics solutions for the ITS environment, thus benefiting drivers, passengers, and city planners alike."}, {"label": 0, "content": "The errors in the means of measuring technology have a significant impact on the technological processes of creating permanent joints. This is also true for the processes of creating permanent joints based on induction heating. In this technological process take place an induction heating of materials with different values of emissivity, which introduces significant errors in the indications of measuring equipment. Non-contact pyrometric sensors are used to take readings of the induction soldering process, which also causes the occurrence of a large number of factors that introduce errors into the readings of these sensors. To ensure high quality control of the induction soldering process, it is necessary to carry out identification of errors. This article suggests the use of intelligent methods for identifying errors in measuring instruments. As an algorithmic support, it is proposed to use the apparatus of artificial neural networks. The software offers the use of the programming language \u201cPython\u201d and the set of libraries \u201cAnaconda\u201d. In the article is described the process of effective neural network structure and parameters search. The results of experimental study confirm that the usage of intelligent methods for identifying errors allows to improve the quality of induction soldering process control as well as other methods of permanent joints creation."}, {"label": 0, "content": "This paper focuses on the application of the protective clothing detection system by using S-HOG+C operator for substation workers in order to meet the practical needs in a complex environment. An image is firstly divided into three cells including a helmet, upper and lower part of a body according to the characteristics of objects. The HOG and HOC features of the three cells are extracted individually to train a classifier. A linear Support Vector Machine is used in our study. The S-HOG+C operator is used to improve the detection accuracy on the protective clothing of substation workers. Three evaluation indicators are used to analyze the performance of the S-HOG+C operator. The experimental results suggest that the S-HOG+C operator perform better than the HOG+C operator in the protective clothing detection for substation workers. This work is supported by Science and Technology Project of Guangdong Power Grid Co., Ltd. (GDKJXM20162351)."}, {"label": 1, "content": "The aim of this study is to enhance the accuracy of epoch extraction from emotional speech by introducing a post-processing technique for the conventional Zero Frequency Filtering (ZFF) method, utilizing Variational Mode Decomposition (VMD) based spectral smoothing. Identifying the epochs in emotional speech signals is a challenging task due to the fast and uncontrolled variations of pitch. In the proposed method, the spectra of the ZFFS short frames are decomposed using VMD to obtain component spectra in five modes. A smoothed short-time spectra is generated by eliminating the spectra from the two higher VMD modes that contain high spectral variations. The modified ZFFS is then reconstructed utilizing the sinusoidal parameters corresponding to a single dominant frequency present in the smoothed spectra through parameter interpolation based sinusoidal synthesis using VMD. As compared to the conventional ZFF approach, the resulting re-synthesized ZFFS has reduced spurious zero crossings for emotive speech signals. This is evident from the improved epoch identification accuracy and rate for all the emotive utterances (with 7 emotions) in the German emotion speech database with simultaneous speech and electroglottographic (EGG) signal recordings. The proposed VMD based spectral post-processing approach is confirmed to be more effective than other existing ZFF based post-processing techniques for emotive speech signals in terms of the accuracy of the epoch identification when compared to the corresponding reference epochs estimated from EGG signals."}, {"label": 0, "content": "Network modeling of high-dimensional time series data is a key learning task due to its widespread use in a number of application areas, including macroeconomics, finance, and neuroscience. While the problem of sparse modeling based on vector autoregressive models (VAR) has been investigated in depth in the literature, more complex network structures that involve low rank and group sparse components have received considerably less attention, despite their presence in data. Failure to account for low-rank structures results in spurious connectivity among the observed time series, which may lead practitioners to draw incorrect conclusions about pertinent scientific or policy questions. In order to accurately estimate a network of Granger causal interactions after accounting for latent effects, we introduce a novel approach for estimating low-rank and structured sparse high-dimensional VAR models. We introduce a regularized framework involving a combination of nuclear norm and lasso (or group lasso) penalties. Subsequently, we establish nonasymptotic probabilistic upper bounds on the estimation error rates of the low-rank and the structured sparse components. We also introduce a fast estimation algorithm and finally demonstrate the performance of the proposed modeling framework over standard sparse VAR estimates through numerical experiments on synthetic and real data."}, {"label": 0, "content": "This paper presents an approach that combines conventional image processing with deep learning by fusing the features from the individual techniques. We hypothesize that the two techniques, with different error profiles, are synergistic. The conventional image processing arm uses three handcrafted biologically inspired image processing modules and one clinical information module. The image processing modules detect lesion features comparable to clinical dermoscopy information-atypical pigment network, color distribution, and blood vessels. The clinical module includes information submitted to the pathologist- patient age, gender, lesion location, size, and patient history. The deep learning arm utilizes knowledge transfer via a ResNet-50 network that is repurposed to predict the probability of melanoma classification. The classification scores of each individual module from both processing arms are then ensembled utilizing logistic regression to predict an overall melanoma probability. Using cross-validated results of melanoma classification measured by area under the receiver operator characteristic curve (AUC), classification accuracy of 0.94 was obtained for the fusion technique. In comparison, the ResNet-50 deep learning based classifier alone yields an AUC of 0.87 and conventional image processing based classifier yields an AUC of 0.90. Further study of fusion of conventional image processing techniques and deep learning is warranted."}, {"label": 1, "content": "This paper introduces a rate-based iterative one-to-one matching game algorithm for multi-user access in energy-harvesting small cells, with the consideration of NOMA in heterogeneous cellular networks. The proposed algorithm aims to increase the capacity and spectrum efficiency of wireless communication systems. To reduce interference, the paper employs a heuristic clustering-based channel allocation algorithm to assign channels to small cells. The user access problem is then modeled as an iterative one-to-one matching game with rate as its utility, where each user matches with one small cell at each matching game. An algorithm is proposed to enable users to access small cells iteratively, and its stability is proven. Furthermore, a power allocation algorithm is presented to reallocate transmission power for each user, in order to fully utilize the harvesting power. Simulation results demonstrate that the proposed algorithm outperforms the OMA system in terms of efficiency and improves the system capacity."}, {"label": 1, "content": "High-speed railway communications have become a topic of widespread interest in the world. However, the Doppler shift caused by train movement induces inter-carrier interference, and the doubly selective channel increases the complexity of training and beamforming. To address these issues, this paper proposes an angle domain channel tracking scheme. The base station is equipped with a large-scale uniform linear antenna array (ULA) to provide high angular resolution. The spatial property is used to decompose the channel into angular information and beam gain. The former is obtained by aligning beams towards the signal direction, which compensates for the Doppler frequency offset (DFO). The latter is tracked using a linear Kalman filter, which is optimal for minimizing the mean square error (MSE). The proposed scheme combines the angular information and beam gain to recover channel state information (CSI). The simulation results demonstrate the superiority of our proposed scheme."}, {"label": 0, "content": "Aiming at the irrational size of LEACH protocol cluster and the imbalance of network energy consumption, an energy-equalized unequal clustering routing protocol is proposed. By introducing the energy decision factor and the density decision factor of node, the proposed method improves a threshold calculation formula of candidate cluster heads, and it is more likely that the nodes with more residual energy and more neighbor nodes become candidate nodes. The candidate cluster heads are elected to become the real cluster heads according to a certain competition radius. Since we consider the influence of the number of cluster heads and the distance between the nodes and the base station on the network energy consumption when determining the competition radius, then, the algorithm can divide the nodes into clusters of different sizes and make the size of the cluster close to the base station smaller, thereby effectively solving the \u201chot spots\u201d problem. Finally, the number of cluster heads and the cluster head distribution are more reasonable. The simulation results show that the improved algorithm can effectively balance the network consumption and prolong the network life cycle."}, {"label": 1, "content": "Nowadays, cities around the world are undergoing a transformation towards becoming smart cities. Among the key concerns that these smart cities need to address is the issue of air pollution. In order to provide clean air, a scalable and cost-effective air monitoring system becomes a critical requirement for smart city development. The impact of air pollution is felt across the globe, affecting the well-being of populations, the global atmosphere, and the worldwide economy.\n\nThis paper presents a scalable smart air quality monitoring system equipped with low-cost sensors and long-range communication protocol. The sensors collect parameters such as temperature, humidity, dust, and carbon dioxide present in the air. The proposed system has been implemented and deployed in Yangon, the business capital of Myanmar, as a case study since June 2018. The end-to-end system design allows users to login and monitor the real-time status of air quality through an online dashboard.\n\nMoreover, the collected air quality parameters from the past two months have been used to train a machine learning model that can predict future air quality parameters. This will enable proactive actions to be taken to alleviate the impacts of air pollution. \n\nIn summary, the proposed smart air quality monitoring system is an essential tool for smart city development. It provides a low-cost and scalable method of monitoring air quality, thereby enabling timely interventions to mitigate the negative effects of air pollution on our population, economy and environment."}, {"label": 1, "content": "As information technology continues to advance, the need for more efficient personal authentication systems, such as those used for ATMs, becomes increasingly important. Popular authentication methods currently in use, including IC cards, passwords, and biometrics authentication, have proven to have several issues. Thus, new and improved systems are urgently needed.\n\nTo address this issue, we propose a revolutionary new method of authentication \u2013 the use of aerial handwriting data. The Leap motion, which captures handwriting movements in mid-air, is utilized to carry out personal authentication from the handwritten numerals from 0 to 9 written by three different subjects. To ensure accuracy, we applied some pre-processing to inputs, and learning and identification were carried out using CNN, or Convolutional Neural Network, which is a machine learning method. \n\nWe found that the average identification accuracy was an impressive 90.3%. This illuminates the potential that aerial handwriting data can be authenticated for personal authentication, leading the way for the development of a more advanced personal authentication system."}, {"label": 1, "content": "Classification results of SAR images are often compromised by speckle noise and complex scattering phenomena, resulting in noisy and shattered outputs that are difficult to apply in practical settings. Deep-learning-based semantic segmentation offers a way to achieve smooth and fine-grained classification maps by segmenting and categorizing images simultaneously. However, this approach demands large data sets with pixel-wise categorical annotations, which can be tedious and time-consuming to collect. Furthermore, manually annotating SAR data is considerably more difficult compared to photographs and optical remote sensing images, which hamper the use of relevant techniques. \n\nTo address these challenges, a new data set is introduced to facilitate semantic segmentation for high-resolution PolSAR images. However, the data set is limited to only 50 image patches due to the aforementioned problems. To overcome this, two transfer learning strategies are proposed, utilizing the fully convolutional network (FCN) and U-net architecture, respectively, and pretraining data sets that can adapt to different situations. Experimental results demonstrate the promising potential of using small training sets for both methods, and even though trained with small patches, both networks can efficiently apply on large images. \n\nThe proposed data set and transfer learning methods are expected to serve as useful baselines to support various PolSAR applications."}, {"label": 1, "content": "This paper introduces Navion, an energy-efficient accelerometer that helps miniaturized robots (such as nano drones) and portable devices navigate autonomously for virtual and augmented reality. The chip comprises of an integrated system that processes inertial measurements and mono/stereo images to estimate the 3-D map of the surrounding environment and drone trajectory. The approach used in this study is based on a state-of-the-art VIO algorithm that needs large irregularly structured memories and heterogeneous computation flow, which led to the development of Navion with an optimized structure that leverages compression and both structured and unstructured sparsity for reducing energy consumption and memory footprint by 4.1\u00d7. Using parallelism helped in increasing the throughput by 43%, and the chip can process 752 \u00d7 480 stereo images from the EuRoC data set in real-time at 20 frames per second (fps). Navion works at peak performance, processing stereo images at up to 171 fps and inertial measurements at up to 52 kHz, consuming an average of 24 mW. The configurable chip can adapt to different environments to maximize accuracy, throughput, and energy efficiency trade-offs. To the best of our knowledge, Navion is the first fully-integrated VIO system that uses an application-specified integrated circuit (ASIC)."}, {"label": 0, "content": "Nowadays, cities all over the globe are transforming into smart cities. Smart cities initiatives need to address environmental concerns such as air pollution to provide clean air. A scalable and cost-effective air monitoring system is imperative to monitor and control air pollution for smart city development. Air pollution has notable effects on the well-being of the population a whole, global atmosphere, and worldwide economy. This paper presents a scalable smart air quality monitoring system with low-cost sensors and long-range communication protocol. The sensors collect four parameters, temperature, humidity, dust and carbon dioxide in the air. The proposed end-to-end system has been implemented and deployed in Yangon, the business capital of Myanmar, as a case study since Jun 2018. The system allows the users to log in to an online dashboard to monitor the real-time status. In addition, based the collected air quality parameters for the past two months, a machine learning model has been trained to make predictions of parameters such that proactive actions can be taken to alleviate the impacts from air pollution."}, {"label": 1, "content": "This paper presents an ultra-low power acoustic wake-up detector that is based on high frequency signal analysis. The purpose of this detector is to detect the presence of specific animal species or drones in real-time, and generate alerts for triggering power-consuming tasks such as high-frequency signal recording only when needed. The detector has a good frequency selectivity and a high frequency detection capability, and it continuously monitors the presence of specific frequencies in an analog acoustic signal. The system is based on an ultra-low analog frequency to voltage converter that uses a current-mirror, analog timers, and comparators. The emphasis has been put on reducing the power consumption to limit the size and weight of the system, and the power consumption has been reduced to 34\u03bcW. The detector can be powered by 3 coin cell CR2032 batteries, providing a full year of autonomy including the microphone. This wake-up detector is quite suitable for long-term stealth environmental or military surveys."}, {"label": 0, "content": "Despite the diversity of automated management systems and the wide range of their functional features, there is a lack of informational and decision support of production management at the operational level. Researches of both Russian and foreign authors confirm that informational aspect of management is the foundation of its efficiency. The design and the usage of software which collect, modify, interpret information and forecast processes development on its basis are the key to enterprises efficiency when transferring to the sixth techno-economic paradigm. At the operational level of assembling management, one of the major processes is defect management. The paper proposes the way of increasing efficiency of engineering technologist work related to the processing of requests for registering defects revealed during aircraft assembling. Commonly, informational support of defect management processes is provided by paper or electronic registers, catalogs and manuals along with the data of different corporate information systems. Considering increasing intensification of production processes and therefore workers' informational workload, to increase speed and improve quality of processing information required for making decisions related to defect processing, it is proposed to use an expert system which allows accumulating data about typical solutions and provides simple decision support resources. The first results of the expert system implementation, which was developed by the authors and allowed improving engineering technologist work related to the processing of defect registration requests, confirm the relevance of further researches of expert systems as a tool of informational and decision support at the operational level of production management."}, {"label": 0, "content": "Machine learning (ML) is gaining popularity in the network security domain as more network-enabled devices get connected, as malicious activities become stealthier, and as new technologies like Software Defined Networking (SDN) emerge. From the application layer, ML-based SDN security models control the routing/switching of an entire Software Defined Network. Compromising the models is hackers' desirable goal. Previous works have been done on either adversarial machine learning without the context of secure networking environment or on the general vulnerabilities of SDNs without much consideration of the defending ML models. Through examination of the latest ML-based SDN security applications, a good look at ML/SDN specific vulnerabilities accompanied by a successful attack on StratosphereIPS, this paper makes a case for more secure developments of ML-based SDN security applications."}, {"label": 0, "content": "In wireless sensor network (WSN), a secure link in the key pre-distribution (KP) scheme may be compromised when sensor nodes are captured. Accordingly, the KP q- composite scheme was proposed to tackle this node-capture problem. Recently, Bechkit et al. proposed a hash chain based KP (HCKP) q-composite scheme to further enhance network resiliency against node capture. However, Bechkit et al.'s HCKP q-composite scheme has to perform too many hash operations to establish a secure link between two nodes for the case that the difference of node identifiers is large. This computational overhead is more serious for the large value of q. In this paper, we propose a computational overhead invariant HCKP q-composite scheme by storing one additional hashed value in sensor node. When compared with Bechkit et al.'s HCKP q-composite scheme, the proposed scheme reduces the number of hash operations, and meanwhile the storage overhead remains insignificant."}, {"label": 0, "content": "A vehicular ad-hoc network (VANET) can improve the flow of traffic to facilitate intelligent transportation and to provide convenient information services, where the goal is to provide self-organizing data transmission capabilities for vehicles on the road to enable applications, such as assisted vehicle driving and safety warnings. VANETs are affected by issues such as identity validity and message reliability when vehicle nodes share data with other nodes. The method used to allow the vehicle nodes to upload sensor data to a trusted center for storage is susceptible to security risks, such as malicious tampering and data leakage. To address these security challenges, we propose a data security sharing and storage system based on the consortium blockchain (DSSCB). This digital signature technique based on the nature of bilinear pairing for elliptic curves is used to ensure the reliability and integrity when transmitting data to a node. The emerging consortium blockchain technology provides a decentralized, secure, and reliable database, which is maintained by the entire network node. In DSSCB, smart contracts are used to limit the triggering conditions for preselected nodes when transmitting and storing data and for allocating data coins to vehicles that participate in the contribution of data. The security analysis and performance evaluations demonstrated that our DSSCB solution is more secure and reliable in terms of data sharing and storage. Compared with the traditional blockchain system, the time required to confirm the data block was reduced by nearly six times and the transmission efficiency was improved by 83.33%."}, {"label": 0, "content": "In this paper, the authors propose a novel range-free localization method to localize the sensor nodes in anisotropic networks. The basic methods of range-free localization assume the hop-size of all links to be the same. This assumption is valid only in scenarios where the node distribution is fairly balanced. This is not practically accurate due to the random deployment of nodes in wireless sensor networks. Hence, the method of finding hop-size using the expected distance and hop-count between the sensor nodes is used in our work. This method is applied to anisotropic networks where obstacles are present. Extensive simulation studies have been conducted to validate the accuracy of the proposed method under the effects of log-normal shadowing which is practically relevant. The results are compared with DV-Hop and Reliable anchor pair selection method (RAPS). This method gives up to 35% performance improvement over DV-Hop technique and 15% performance improvement over RAPS technique in the literature."}, {"label": 0, "content": "Hand detection is critical in gesture recognition for conveying information or control commands between persons and computers. The accuracy of hand detection from images plays an important role in these applications. Extraction of effective features is the main factor in this task. The features should be discriminative, robust to different variations and easy to compute. This paper presents the experimental comparison of features commonly used in object detection, such as Haar-like features, a histogram of oriented gradient (HOG), and local binary pattern (LBP), using hand detection as the test platform. The adaptive boost (AdaBoost) cascade classification method is employed to combine \"weak learners\" to a strong classifier. The classifier was trained using 300 positive images, which are images containing the hand (region of interest (ROI)) and 10000 negative images, which are images that do not contain a hand on them. Different parameter combinations of the classifier are considered for comparative experiments. The performance of the classifier using Haar, HOG and LBP features were evaluated with 320 static test images. The results show that parameter combinations have significant effects on the hand detection accuracy, which also differ when different features are used."}, {"label": 1, "content": "Classic DC power flow and Generalized Generation Distribution Factors (GGDF) are utilized to model transmission network constraints in a DC optimal power flow (OPF) scenario. The former is praised for its straightforwardness, precision, and stability, while the latter is celebrated for its capacity to convey transmission power flows as a function of power generation with fewer equality constraints. This research study compares the efficacy and efficacy of both methods by analyzing their performance on a PJM 5-bus system and an IEEE 57-bus system, incorporating transmission losses, using various commercial optimization solvers."}, {"label": 0, "content": "Internet of Things has gained the attention of almost everybody due to its capability of monitoring and controlling the environment. IoT helps making decisions supported by real data collected using large number of ordinary day-to-day devices that have been augmented with intelligence through the installation of sensing, processing and communication capabilities. One of the main and important aspects of any IoT device is its communication capability for transferring and sharing data between other devices. IoT devices mainly use wireless communication for communicating with other devices. The industry and the research community have proposed many communication technologies for IoT systems. In this paper, the authors present the results of an in-depth study carried out on the benefits and limitations of these communication technologies."}, {"label": 1, "content": "As credit card transactions become increasingly commonplace, the financial industry faces a growing threat of fraudulent activity. In order to minimize losses, it is crucial for banks and other institutions to implement effective fraud detection systems. However, traditional classifiers often struggle with the highly imbalanced data sets associated with credit card fraud, in which instances of fraud are significantly outnumbered by legitimate transactions. To address this challenge, this paper proposes a sampling method that leverages K-means clustering and the genetic algorithm to improve classification performance for minority class objects. Specifically, we use K-means to cluster minority samples and then apply the genetic algorithm to generate new samples and refine the fraud detection classifier. By improving the accuracy of fraud detection for imbalanced data sets, this approach can help financial institutions protect themselves against losses due to credit card fraud."}, {"label": 1, "content": "In recent years, learning methods for single image super-resolution have shown great success. Convolutional neural networks (CNNs) have proven to be robust, achieving state-of-the-art performance. Building upon this, we introduce a Dynamic Multi-mapping Convolutional Network (DMCN) that further improves SR performance. Rather than relying on fixed kernels like Bicubic interpolation, we utilize dynamic filters to resize the LR input in our pre-trained module. By using an end-to-end approach, we learn more accurate and effective features from middle layers of the network. Additionally, our multi-mapping module provides extra information and high-frequency details, resulting in sharper, high-resolution images. Through extensive quantitative and qualitative evaluations, our algorithm effectively improves image resolution."}, {"label": 0, "content": "With the rapid development of ultra-high voltage ac and dc projects (UHVAC/DC), a large-scale UHVAC/DC hybrid network has formed in China, increasing the control complexity for power grid operation. To maintain such a complex power grid operate in the stable and safe states, the operation characteristics analysis of it is highly necessary and significant. However, until now, all the existing power grid simulation tools cannot complete this challenging mission suitably because of the requirements for high accuracy modeling, large-scale electro-magnetic transient simulation, and massive off-line calculation, etc. In order to solve the above-mentioned problems, the new generation UHVAC/DC power grid simulation platform (NGSP) architecture is proposed in this paper, and the detailed design schemes of NGSP is also presented. With the help of the proposed designs, the computing scale of NGSP is larger than 6000 nodes in digital-analog hybrid simulation, and the speed-up ratio is more than 3000 in digital simulation, which represents the most powerful power grid digital simulation capacity all over the world. And the comprehensively comparative results between simulation and engineering recorded data are presented to verify the correctness and effectiveness of NGSP. Moreover, the application effects of NGSP are also described in this paper to verify the practicability of NGSP."}, {"label": 0, "content": "The aim of this paper is to develop an intelligent event-driven Electrocardiogram (ECG) processing module in order to achieve a computationally efficient solution for diagnosis of the cardiac diseases. The suggested method acquires the signal with an event-driven A/D converter(EDADC). The output of EDADC is passed through the activity selection and interpolation blocks. It allows focusing only on the important signal parts and resampling it uniformly by using the Simplified Linear Interpolator. Later on, the signal is de-noised. The autoregressive (AR) method is used to extract the classifiable features of the de-noised signal. Afterwards, the output is classified by employing different robust classification techniques such as support vector machines (SVMs), K- Nearest Neighbor (KNN) and Artificial Neural Network (ANN). The event-driven feature enables to adapt the system processing load according to the signal temporal variations. This interesting feature of the devised system aptitudes a drastic reduction in its processing activity and therefore in the power consumption as compared to the counter traditional ones. A comparison of the performance of different classifiers is also made in terms of accuracy. Results show that the proposed system is a potential candidate for an automatic diagnosis of the cardiac diseases."}, {"label": 0, "content": "To estimate the level of 10kV distribution network line losses more integrally and precisely, an evaluation method based on BP neural network (BPNN) improved by particle swarm optimization (PSO) has been proposed. Making full use of existing data resources in the State Grid Corporation of China, the relevant information in the process of10 kV distribution network line and line loss has been collected and integrated, to formulate the power grid equipment data and operation data in the process of power distribution and utilization. Firstly, the electrical characteristic indices have been selected and established to reflect the structure and operation state of 10kV distribution network. Secondly, the inertia weight and the acceleration coefficient of PSO have been dynamically adjusted so that the weights and biases of BPNN are searched more effectively. Then nonlinear relation between electrical characteristic indexes and line losses is fitted through the learning of training sample sets so as to predict the line losses of test sample sets. Finally, the PSO-BPNN is proved to be effective and proper through an actual 10kV distribution network sample data."}, {"label": 1, "content": "The development of Internet of Things (IoT) has presented various challenges, and its actual implementation is still a topic of debate. While several studies have been conducted on IoT-based smart home systems, there is a need for improved resources that focus on the practical implementation of energy-saving and resource-efficient technology for smart homes. This study presents field experiment results related to low-power node module implementation using sub 1 GHz LPWAN (low-power wide area network) connectivity. LPWAN technology has unique characteristics that offer cellular-like coverage range while maintaining a low transmit power, and it supports star topology by default, making it possible to overcome the problems of power usage inconsistency, network delay, and the complexity of routing management found in conventional mesh-based sensor networks. The field experiment results indicate that the furthest distance under outdoor conditions could reach up to 350 meters (RSSI -85 dBm) while the furthest distance under indoor conditions could reach up to 150 meters (RSSI -95 dBm). Additionally, based on the power usage test, the life-span estimation could reach up to 1 year or 17 years, depending on the scenarios and battery types."}, {"label": 0, "content": "Non-contact assessment of breathing is a valuable tool to assist in the health screening of infants and neonates, very old frail patients, or patients with infectious diseases, where it is not practical or advisable to use any device that attaches to the patient. Low-cost mobile phone-based methods to assess breathing are of particular interest in global health applications where alternative instrumentation is not available or affordable, and also for home use by consumers in wealthier areas. In this context, we have developed a smart phone mobile application which uses a thermal camera module to automatically measure respiration rate (RR) and respiration rate variability (RRV). The mobile application makes use of machine vision algorithms to detect a human face and then find the nostril points under the nose which are used for measurement of temperature. Temperature fluctuations in the nostrils are then used to approximate air flow rates and calculate timing parameters. Our device was tested with eleven human subjects in a clinic and validated against a gold-standard impedance pneumography belt using three intensities of breathing (normal breathing, shallow breathing, and deep breathing) as well as two different positions of the camera (camera at 0 degrees and -20 degrees angle respectively). With the thermal camera located at a height slightly lower than the face (20-degree angle), the results from the mobile phone tool had good agreement with the clinical device, with the mean standard error (MSE) of 0.8, 1.4, and 1.8 bpm, for normal breathing, shallow breathing, and deep breathing respectively."}, {"label": 1, "content": "This article discusses computer modeling of radio systems and presents a method for automated solutions for direct and inverse problems of mathematical modeling of nonlinear radio engineering systems. Nonlinear radio engineering systems perform crucial signal generation and conversion operations such as amplification, modulation, detection, frequency multiplication, and conversion. The modeling of such systems is essential for improving the design and research efficiency. However, existing modeling systems are limited to a set of valid tasks and usually solve only direct problems. Therefore, there is a need for automated modeling systems that can solve various direct and inverse problems of systems modeling within a single program complex using elements of artificial intelligence, replacing the existing application packages.\n\nTo address this need, the article proposes a method to use functional grammars to construct an automated modeling system for nonlinear semiconductor systems. The scientific research result is the development of a logical-mathematical model using terms of functional context-free grammars, substantiated through a logical conclusion using lambda calculus. The practical application of the method is demonstrated through examples of solving direct and inverse modeling problems. By using this modeling method, radio engineers can improve the efficiency of their design and research and provides a more comprehensive range of solutions for direct and inverse problems of systems modeling."}, {"label": 1, "content": "We are proposing an innovative electric drive control system that aims to enhance the safety and reliability of coal miner operation. The system's effectiveness lies in its ability to predict and control the diagnosed parameters of the power elements, which allows for preemptive maintenance and repair. We achieve this forecast by using a neural network model, which enables us to increase the mean time between failures and the availability of the electric drive. Compared to traditional control systems, our proposed solution ensures a hassle-free operation of the electric drive."}, {"label": 1, "content": "This paper focuses on the storage model and search for sound sequences using the theory of active perception. The theory of active perception is utilized to generate an indicative description of the sound signal. The proposed model solves problems within a broad range of applications, including the search for musical plagiarism. Additionally, the model can be used to develop a software system for audio signal identification. The search model was implemented using the programming language R and was verified through computational experiments with a database of 10,000 musical compositions, achieving a search accuracy of 96%. The model's stability was also examined by analyzing the impact of noise on the sought signal. A comparison with other existing systems was conducted in terms of search accuracy."}, {"label": 1, "content": "The prevalence of sleep apnea hypopnea syndrome (SAHS) has been steadily rising, with a rate of up to 14% in 2016. This disease can pose a serious threat to human sleep safety, making it vital to detect and pre-alert individuals who may be at risk. To address this need, a new method for diagnosing SAHS has been developed that utilizes respiratory signals.\n\nIn this method, nonlinear characteristics such as fractal dimension and sample entropy are introduced based on time-domain characteristics, including variance and clinical zero passing numbers, as well as frequency-domain characteristics such as wavelet coefficients and wavelet energy. These characteristics are used to create a 6-dimension feature vector that is input into the support vector machine (SVM), back propagation neural network (BPNN), and improved back propagation neural network (IBPNN) based on morphology.\n\nThe proposed method was validated using eight data sets, which consisted of 3840 samples of the Physionet Apnea Database. The experimental results showed that classification accuracy of SVM and BPNN were 71.2% and 84.5% respectively. The IBPNN based on morphology improved the classification results by 85% and increased accuracy by 7.3%.\n\nOverall, this study shows that the feature vector and IBPNN based on morphology can effectively detect SAHS, providing an efficient and convenient method for diagnosing this disease. As the prevalence of SAHS continues to rise, this method has the potential to help address this growing public health issue."}, {"label": 0, "content": "Various neuroimaging studies had demonstrated that multiple brain regions would activate during execute cognitive task. Meanwhile, Real-time functional magnetic resonance imaging neurofeedback (rtfMRI-NF) can assist subject self-regulation brain activity. However, the neural mechanisms about rtfMRI-NF were unclear. To investigate this problem, we combined graph theory with resting state fMRI to explore the topological properties of functional brain networks. In our study, subjects were provided with ongoing functional connectivity information which was related to emotion regulation. Our results showed that rtfMRI-NF training could alter the small-world properties and nodal degree in the temporal lobe, frontal lobe, limbic system. Together, our results suggested that rtfMRI-NF training was associated with alters in the topological properties of functional brain networks."}, {"label": 0, "content": "While enjoying the convenience brought by the Internet of Things (IoT), people also encounter many problems with wireless sensor networks (WSNs), the foundation of IoT. Security problems are especially of concern. In this article, we focus on location privacy, which is a major security issue in WSNs, and propose a k-means cluster-based location privacy (KCLP) protection scheme for IoT. To protect the source location, fake source nodes are used to simulate the function of the real sources. Then, to protect the sink location privacy, fake sink nodes and a specific transmission pattern are utilized. In order to improve safety time, a k-means cluster is applied to create clusters and fake packets that must pass through the area. Compared to contrasting algorithms, the KCLP scheme can increase the safety time and reduce delay at minor expense in energy consumption."}, {"label": 0, "content": "Suffering from speckle noise and complex scattering phenomena, classification results of SAR images are usually noisy and shattered, which makes them difficult to use in practical applications. Deep-learning-based semantic segmentation realizes segmentation and categorization at the same time, and thus can obtain smooth and fine-grained classification maps. However, this kind of methods require large data sets with pixel-wise categorical annotations, which are time consuming and tedious to retrieve. Compared with photographs and optical remote sensing images, manually annotating SAR data is even harder, which results in a delay of using relevant techniques in this field. In this letter, a new data set is proposed to support semantic segmentation for high-resolution PolSAR images. Limited by the aforementioned problems, the data set is only a small one with 50 image patches. Therefore, two transfer learning strategies are proposed, which adopt the fully convolutional network (FCN) and U-net architecture, respectively, and use distinct pretraining data sets to adapt to different situations. The experiments demonstrate the good performance of both methods and a promising applicability of using small training sets. Moreover, although trained with small patches, both networks can perfectly apply on large images. The new data set and methods are hopeful to support various PolSAR applications as baselines."}, {"label": 1, "content": "In this paper, we present a framework for collaborative localization of heterogeneous systems. Our approach builds on the original MSCKF framework and introduces a collaborative MSCKF filter, operating at two levels, that enables decentralized 3D collaborative localization without any external computation systems. To achieve this, we propose a range-based collaboration, optimized using environment constraints extracted through truncated unscented Kalman filtering updates. Our collaborative filtering is designed to preserve the original MSCKF odometry properties. We apply the framework to collaborative localization of aerial and ground robots and demonstrate its effectiveness through a series of experiments."}, {"label": 0, "content": "Classification is a commonly used modelling method for data mining. A classification model is a predictive model which is used to predict a categorical value, called a class. Ensemble classification modelling involves the creation of several base models and a combination algorithm for the base model predictions. The classification modelling process uses a set of instances called the training data. Each instance consists of values for the predictor variables and a categorical label called the class. A class is called a minority class if it has a much smaller number of training instances compared to the other classes. This results in a low level of correct classification compared to the classification performance for the majority classes. Positive-versus-negative (pVn) classification has been reported in the literature as an ensemble classification method which is applicable to classification modelling for multi-class prediction tasks. The purpose of this paper is to report on experimental results for the performance of a replication method for improving classification performance for minority classes, using pVn classification modelling. The experimental results demonstrate that the classification accuracy for minority classes can be improved through the use of pVn classification models."}, {"label": 1, "content": "Intelligent video surveillance technology can significantly reduce the burden on safety inspectors by detecting irregular operations of operators at substation job sites. However, the traditional radial basis function neural network (RBFNN) algorithm has limited accuracy in identifying objects in outdoor complex working environments, leading to high missing alarm rates. To address this issue, this study proposes a robust algorithm for dress recognition based on the classifier output sensitivity of the RBFNN.\n\nThe proposed approach utilizes shape and color feature vectors of the operator's helmet, top, and bottom images, and then uses the Monte Carlo method to randomly sample points in the neighborhood of training samples. Subsequently, a loss function that incorporates the sensitivity of the sample neighborhood is established, and the weights from the hidden layer to the output layer are calculated using the Gauss-Newton method. Finally, an RBFNN classifier based on Gaussian function is established.\n\nThe simulation results indicate that the proposed sensitivity RBFNN (S-RBFNN) algorithm can effectively reduce the missing alarm rate and is more robust in practical settings. Through this approach, intelligent video surveillance technology can become more accurate and efficient in detecting irregular operations of operators."}, {"label": 0, "content": "The paper presents a wavelet-fuzzy controller of a field-oriented control system of belt conveyor drive. A discrete wavelet transform is used to decompose the speed error signal into various frequency components. The transformed error coefficients and scaling coefficients are used to generate the motor control signal The proposed controller is satisfied the speed control problem for a closed control system. The field-oriented control scheme with a wavelet-fuzzy controller under various dynamic operating conditions was investigated. The results of the comparison of the work of the traditional PI controller and the proposed wavelet-fuzzy controller are presented. The simulation results demonstrate the effectiveness of the proposed wavelet-fuzzy controller. Application of the proposed approach of the wavelet-fuzzy controller allows reducing of electromagnetic moment vibrations."}, {"label": 0, "content": "Reliability classification of gear safety has long been a challenging issue in transmission industry because of complicated calculations and great classification errors of coupled parameters with insufficient data. This paper proposes a model based on generative adversarial network (GAN) as pretreatment to improve the accuracy of reliability classification. First, we present bounded-GAN to generate gear data within required boundaries without massive computations. In bounded-GAN, three bounded layers are designed to bound generated data in terms of different data characteristics; smooth targets are developed to enhance the ability of generating high-quality instances by the generator; Adam optimizer is used to train both generator and discriminator to avoid nonconvergence. Second, to overcome unlabeling defect of bounded-GAN, a mean-covariance labeling scheme is introduced to label the data according to the nearest classes of gear reliability within specific ranges. Finally, original and qualified data are combined to train classifiers. Simulations on gear data from industry show that our proposed model outperforms other methods on operational metrics."}, {"label": 0, "content": "During the pilot project that is constructed based on the edge ecology incubation and edge service platform, China Unicom works along with ZTE, Intel, Tencent video to build an edge data center test bed in the university town located in Tianjin, China. This is a combination of OTT business and edge computing technology for the first time. This paper focuses on the full introduction of overall architecture and construction scheme of edge vCDNs. Also the introduction of infrastructure and business flow are included. The test results of vCDN demonstrate that the deployment of applications to the network edge not only reduces the bandwidth pressure (resulting in network transmission and multiple layers of forwarding) and latencies but also optimizes user experience and helps content providers reduce costs. This pilot project begins the edge computing collaboration between China Unicom and OTT, and is of great guidance value for the reconstruction of the equipment rooms of edge DCs and the commercial incubation of 5G innovative services."}, {"label": 1, "content": "The interest in neural networks has grown remarkably, and their applications are vast ranging from natural image classification to medical image segmentation. However, many neural network users tend to utilize them as a black box tool, disregarding potential variations and respective classification accuracies and costs. Our research specifically focuses on multiclass image classification, where we shed light on the trade-offs between systems using a single multiclass classification versus multiple binary classifiers. We conducted experiments on several modern neural network architectures, including DenseNet, Inception v3, Inception ResNet v2, Xception, NASNet, and MobileNet, comparing both classification styles in terms of classification speed and accuracy metrics. In total, 99 networks were tested, consisting of 11 multiclass and 88 individual binary networks, for an 8-class classification of medical images. The results showed that utilizing multiple binary classification networks resulted in a more robust model with less variance. However, on average, such a multi-network style performed the classification 7.6 times slower compared to a single network multiclass implementation. These findings indicate that both approaches can be applied to modern neural network structures. Several binary networks can often improve classification accuracy, but at the expense of classification speed and resource consumption."}, {"label": 1, "content": "Energy flows in houses used to be simple - they were connected to the electrical grid and relied on gas or oil heating systems. However, the rise of alternative energy systems has made energy flows much more complex. Photovoltaic systems, heat pumps, combined heat and power units, home batteries, and electric cars are just some of the systems that are being integrated into houses to generate and consume energy. In Germany, a model has been implemented to simulate the technical and economic outcomes of these systems in single- and multi-family houses. The simulation uses literature data for climate and energy demand and is capable of providing economic and ecological estimates. The goal is to create a web interface that provides an easy-to-use tool for homeowners to make informed decisions about energy usage in their homes."}, {"label": 1, "content": "To address the challenge of underwater object image classification with limited training data availability, a new classification approach based on Convolutional Neural Network (CNN) is proposed. The method adopts an advanced Markov random field-Grabcut algorithm to partition the images into two parts: shadow and sea-bottom. To account for the specific properties of the data, a CNN model is constructed with two parts: a convolutional part and a classification part, following the structure of Alexnet. Finally, the transfer learning technique is employed to train the CNN model to classify three distinct shapes of underwater objects (cylinder, truncated cone, and sphere). The method is validated on synthetic aperture sonar (SAS) datasets, and its accuracy is compared to that of Support Vector Machine (SVM) and CNN models that leverage only trial data. The proposed method achieves superior accuracy compared to the SVM and CNN models with limited training data, demonstrating its effectiveness in addressing the challenge of underwater object image classification."}, {"label": 1, "content": "To meet the demanding network security requirements, designers must ensure that all network components can withstand any contingencies. This results in significant cost implications for network development. Therefore, it is crucial to take network security into consideration when pricing the network. In the original long-run incremental cost (LRIC) pricing method, a security factor is defined to facilitate charging for network security. This method assumes that the security factor remains constant before and after the nodal injection. However, this assumption may not accurately reflect the reality. In response to this, a new improved LRIC pricing method is proposed in this paper that considers the changing security factor of each network component with nodal injection.\n\nThis paper aims to analyze the factors affecting the security factor of a network, and present the improved LRIC method with dynamic security factor. Simulation studies are conducted on a three-nodes system and the IEEE 14-nodes system to demonstrate the differences between the improved LRIC pricing method and the original LRIC pricing method. The results show that the improved LRIC method can more accurately reflect the impact of nodal injections on network costs and ensure a more reasonable allocation of network costs.\n\nIn conclusion, the improved LRIC pricing method, with dynamic security factor, is a more accurate approach to pricing networks that considers the changing security factor of each network component with nodal injections. It will ensure more rational pricing of network costs and facilitate the adoption of appropriate security measures."}, {"label": 0, "content": "Paper proposes a hardware support of method for detecting a periodic component in a measurement signal, based on the calculation of the signal's zero-crossing numbers. On the basis of the considered method, an algorithm with low computational costs is proposed. The algorithm can be used to indicate of the progressing of resonant phenomena, to determine the presence of a carrier signal, and so on. The structures of the basic modules required for constructing hardware support devices of the proposed algorithm are proposed. The proposed modules are relatively easy to implement and can be easily manufactured for example using FPGA technology. An example of a block diagram of a detection device of periodicity with a known frequency is given. A description of the operation of the device is given and the main time diagrams are given."}, {"label": 0, "content": "An analysis of existing methods for solving the problems of risk assessment showed that they are based on the lack of computational capabilities and the lack of necessary information about the conditions of the problem. Therefore, in such cases it is advisable to use fuzzy mathematical methods. In this paper, we consider approaches to solving the problem of risk assessment with fuzzy source information."}, {"label": 0, "content": "Chaotic systems, such as Lorenz systems or logistic functions, are known for their rapid divergence property. Even the smallest change in the initial condition will lead to vastly different outputs. This property renders the short-term behavior, i.e., output values, of these systems very hard to predict. Because of this divergence feature, lorenz systems are often used in cryptographic applications, particularly in key agreement protocols and encryptions. Yet, these chaotic systems do exhibit long-term deterministic behaviors-i.e., fit into a known shape over time. In this work, we propose a fast dynamic device authentication scheme that leverages both the divergence and convergence features of the Lorenz systems. In the scheme, a device proves its legitimacy by showing authentication tags belonging to a predetermined trajectory of a given Lorenz chaotic system. The security of the proposed technique resides in the fact that the short-range function output values are hard for an attacker to predict, but easy for a verifier to validate because the function is deterministic. In addition, in a multi-verifier scenario such as a mobile phone switching among base stations, the device does not have to re-initiate a separate authentication procedure each time. Instead, it just needs to prove the consistency of its chaotic behavior in an iterative manner, making the procedure very efficient in terms of execution time and computing resources."}, {"label": 0, "content": "With the addition of new distributed energy sources, a new and more accurate load model need to be established. Based on MATLAB/Simulink, this paper researches on the modeling and simulating of photovoltaic power generation system and battery energy storage and their equivalent model. First, a detailed model of the photovoltaic system is built. Boost circuit and MPPT control are selected on the DC side of photovoltaic power generation system. The double closed loop control method of current internal loop and external voltage loop is used for grid inverter control. The model is improved considering the capability of LVRT (low voltage ride through). Based on the above precise models, the model is equaled. The equivalent transfer function is calculated by the grid connected control loop, so the model is simplified. The battery energy storage module establishes a general mathematical model based on the charging and discharging principle. Charge and discharge are controlled through the DC/DC converter. And it is connected to the power grid after inverting and filtering. The common PQ control are selected in the inverter control mode. According to the same principle, the energy storage model is equaled. Finally, based on the traditional load model, the above load model is added to set up a new generalized integrated load model. And the actual model is replaced to build up an equivalent model. The equivalent model is compared with the actual model to verify the equivalent description ability."}, {"label": 1, "content": "Classroom observations are a crucial part of professional development programs for teachers, but traditional observation systems can be expensive and suffer from biased feedback and the Hawthorne effect. To address these challenges, an automated classroom observation system has been developed based on audio data collected from a teacher's smartphone. The system uses machine learning techniques to label classroom activities into observation categories, such as lecture, classwork, classroom management, practice, question/answer, and reading aloud, and can provide intelligent recommendations on how to best allocate class time. The system was trained on data collected from semi-rural primary schools in Pakistan and achieved an accuracy of approximately 69% using the Random Forest algorithm. These results suggest that automated observation is a viable and cost-effective alternative to physical observation in low-resource settings."}, {"label": 1, "content": "Computer networks are expanding at an alarming rate, with an estimated 50 billion connected devices predicted by 2050. This exponential growth results in a significant rise in the attack surface of both private and public networks. These attacks can have a profound impact on the system's behavior, ultimately leading to the detection of the attack. In this manuscript, we introduce a graph-based approach for modeling the path of an attack through the network. The model is designed to provide a better understanding of the attacker's intentions. To test the efficacy of the model, we applied it to data gathered from 5 different honeypots. The results showed that our approach was able to quickly detect anomalies in the experimental dataset, making it a valuable tool for identifying and preventing network attacks."}, {"label": 0, "content": "In this paper, we review the task of improving the dynamic characteristics of vacuum arc furnace power controller. The paper proposes solution of this task by the use of fuzzy logic controller based on Sugeno algorithm within a speed loop of electrode positioning drive. It also proposes the application of compensation method for dead zone along the arc length using fuzzy logic device also based on Sugeno algorithm. The results of studying the dynamic characteristics of vacuum arc furnace power control system with classical and fuzzy controllers are shown."}, {"label": 1, "content": "Cognitive radio networks (CR) represent a novel approach to utilizing unlicensed spectrum bands due to the limited availability of licensed spectrum bands. The essential features of CR include spectrum sensing, management, sharing, and mobility. While spectrum mobility, also known as spectrum handoff, is crucial in military networks, it refers to the process of CR users changing the frequency of operation to avoid spectrum occupancy by licensed users. Given the importance of reliable communications in military systems, we propose the use of Fuzzy logic to develop a decision-making handoff scheme that can minimize interruption and reduce handoff latency. After analyzing previous research, we have determined that most existing schemes can perform handoff successfully, but they tend to be slow and difficult to implement. Our proposed scheme aims to address these issues and ensure mission success in military networks."}, {"label": 1, "content": "Real-time task scheduling is of significant importance for ensuring quality of service in wireless networked control systems (WNCSs). This paper introduces a new model, Joint Network and Computing Resource Scheduling (JNCRS), for real-time task scheduling in WNCSs. The JNCRS model takes into account a strict execution order of the sensing, computing, and actuating segments based on the control loop of WNCSs.\n\nThe JNCRS problem is known to be NP-hard, and four subproblems are identified for its solution. The first subproblem involves unit execution time for each segment, and a polynomial-time optimal algorithm is proposed to solve it. The algorithm checks the intervals with 100% network resource utilization and modifies the deadlines of tasks accordingly.\n\nFor the second subproblem where the computing segment exceeds one unit execution time, new timing parameters for each network segment are defined. A polynomial-time optimal algorithm is presented to check the intervals with network resource utilization greater than or equal to 100% and modify the timing parameters of tasks based on these intervals.\n\nOverall, the JNCRS model and the proposed algorithms provide an effective solution for real-time task scheduling in WNCSs, ensuring the quality of service and enhancing the overall system performance."}, {"label": 1, "content": "This paper presents a novel approach to imitating neurons cells based on the latest breakthroughs in neuroscience. It is evident that the traditional artificial models have excelled in image processing and pattern classification, but they are still lacking when it comes to evolving into a modern imitation of natural intelligent organisms. To overcome this, a Biomimetic cell design is suggested through registering the inputs, outputs, and weights as information codes. By utilizing binary equivalence gates, these cells can then compare the inputs to the weights and generate the required output. With the provied abstraction model, the training process becomes significantly more straightforward, which accelerates the design stage and leads to a new dimension in artificial intelligence."}, {"label": 0, "content": "In a long term evolution (LTE) based cellular network, the mobility management entity (MME) is responsible for non-data signaling between user equipment of multiple base stations in a geographic region and the core network. Thus, the MME residence time (MRT) is a key parameter required to improve the performance of an LTE based cellular network. The impact of various mobility and network scenarios on cell residence time has been studied in the literature. However, the MRT has not been suitably modeled. Hence, in this paper, we consider diverse mobility and network scenarios. For these scenarios, we model the MRT using various probability distributions. We analyze and evaluate the statistical performance of these distributions in modeling MRT. Finally, we show through exhaustive simulations that the Lognormal and Generalized Pareto distributions are best suited to model the MRT for specific network and mobility scenarios."}, {"label": 0, "content": "This paper describes a modelling and analysis experience concerning time synchronization in wireless sensor networks (WSN). A fully distributed algorithm is formally modelled and its properties assessed through statistical model checking. The described work is based on the Theatre framework which rests on actors and asynchronous message passing. Theatre can be reduced to the Uppaal Statistical Model Checker (SMC). The paper discusses the chosen time synchronization algorithm, outlines the Theatre modelling features and its mapping on to Uppaal Smc, and shows a Theatre model for the selected time synchronization algorithm enhanced with a new adaptation mechanism for energy saving. The model is then analyzed through simulations."}, {"label": 0, "content": "The paper applies image classification techniques to protect users of the Internet from inappropriate information. It describes a modular hierarchical classification system and proposes an approach to integrate image classification modules without additional retraining. The experiments devoted to the integration of an image classification module into the current infrastructure of the Web classification system are outlined and analyzed."}, {"label": 1, "content": "Despite the importance of higher education institutions in sustainable development paradigms and the availability of numerous publications in this area, there are still two major issues that hinder the successful implementation of this paradigm. These issues include the complexity of the subject domain and the lack of a methodological base that can enable the reuse of existing models in conjunction with future models.\n\nTo address these challenges, this paper presents an innovative approach that aims to reduce the complexity of the subject domain by replacing it with a set of formalized abstract infrastructure systems. This approach also seeks to reduce the problem of model reusability by leveraging a simulation umbrella as a united methodological base for the implementation of the paradigm.\n\nTo demonstrate how the proposed approach works, the paper provides a simulation case-study that is associated with a fast-growing higher education institution in the United States. This study helps to illustrate the effectiveness of the proposed approach and how it can be applied in real-world scenarios to achieve sustainable development objectives in higher education institutions.\n\nOverall, this paper provides a sound and effective solution to the challenges facing the implementation of sustainable development paradigms in higher education institutions. Through the use of abstract infrastructure systems and simulation umbrellas, it is possible to reduce complexity and ensure the effective reuse of existing models, thereby achieving more sustainable development outcomes."}, {"label": 1, "content": "Understanding the dynamics of a power system requires meaningful information presentation. In this study, large-scale modal results for a large interconnected power system are presented using visualization methods to uncover the underlying system oscillations. By employing visualization tools, the quality of mode estimation among several bus signals is captured, different modal interactions existing in the system are identified, and modal power flows are visualized to track sources of grid oscillations. Wide-area visualization is used in a synthetic large interconnected power grid to reveal critical information about the dynamic state of the system that would not have been captured from a graphical plot of the time-varying signals."}, {"label": 0, "content": "Multi-objective route planning is a hot issue in current research, and it applies all aspects of life. With the expansion of the scale of the problem, large numbers of approximate algorithms and heuristic algorithms proposed to solve the problem. In this paper, a solution of a multi objective route planning with a balanced assignment of tasks is proposed. The solution can divide into two steps. First., a clustering algorithm cbk-means (cluster balance k-means) is proposed, which improves the similarity measurement in the clustering process, and overcomes the shortcomings of traditional k-means algorithm, such as uncertain number of points and inflexible measurement criteria, which is the key step to achieve fair assignment of tasks. Second, this paper use genetic algorithm to obtain an optimal route planning for each cluster. Experimental results show that the cbk-means algorithm makes the workload of each cluster more balanced at the expense of negligible cost, which improves the fairness of task assignment greatly. Besides, this hybrid solution can save computational time and get better results."}, {"label": 0, "content": "Deep learning has successfully shown excellent performance in learning joint representations between different data modalities. Unfortunately, little research focuses on cross-modal correlation learning where temporal structures of different data modalities, such as audio and video, should be taken into account. Music video retrieval by a given musical audio is a natural way to search and interact with music contents. In this work, we study cross-modal music video retrieval in terms of emotion similarity. Particularly, an audio of an arbitrary length is used to retrieve a longer or full-length music video. To this end, we propose a novel audio-visual embedding algorithm by Supervised Deep Canonical Correlation Analysis (S-DCCA) that projects audio and video into a shared space to bridge the semantic gap between audio and video. This also preserves the similarity among audio and visual contents from different videos with the same class label and the temporal structure. The contribution of our approach is mainly manifested in the two aspects: i) We propose to select top k audio chunks by attention-based Long Short-Term Memory (LSTM) model, which can represent good audio summarization with local properties. ii) We propose an end-to-end deep model for crossmodal audio-visual learning where S-DCCA is trained to learn the semantic correlation between audio and visual modalities. Due to the lack of music video dataset, we construct 10K music video dataset from YouTube 8M dataset. Some promising results such as MAP and precision-recall show that our proposed model can be applied to music video retrieval."}, {"label": 1, "content": "Possible revised text:\n\nIn addition to voice, sign language and artificial larynx can serve as communication options for people with speech disorders. However, these methods may have limitations such as requiring extensive training and high costs. To address these challenges, researchers have explored the use of electromyography (EMG) analysis of lip movements to detect speech sounds.\n\nOne important application of EMG-based speech recognition is personal authentication, which can confirm the identity of a speaker based on their unique muscle signals. In this study, a small and dry sensor with two electrodes was used to capture muscle activity in the orbicularis, zygomatic, and depressor muscles during the production of Japanese vowels. The EMG signals were recorded and processed using P-EMG plus software, which involved signal cutting, spectrums analysis, noise removal, and reconstruction.\n\nTo train a machine learning model for speaker identification, a convolutional neural network was employed to analyze the EMG data sets. However, the accuracy of the model was found to vary significantly depending on the measurement date and the intra-individual variation of the subjects. Therefore, further refinement of the data and normalization of the individual differences are needed to improve the reliability of the system in the future.\n\nOverall, EMG-based speech recognition shows promising potential as a non-invasive and accessible solution for individuals with speech impairments. By enhancing the accuracy and usability of the technology, we can help more people to communicate effectively and confidently in various situations."}, {"label": 1, "content": "In a deregulated electricity market, it is crucial for the power system operator to identify the most profitable schedule for renewable distributed generation (DG) units while ensuring network stability. To address this issue, this paper proposes a parallel computation-based methodology using fuzzy logic embedded within a genetic algorithm (GA) framework. Unlike the classic GA, the proposed fuzzy-based parallel computation GA (FPCGA) improves the convergence speed and quality of results by enabling efficient communication among the processors during the optimization process. \n\nThe proposed optimization algorithm is applied to determine the optimal daily schedule for the system operator, incorporating the energy purchased from the power grid, each wind turbine DG, and each photovoltaic DG. The effectiveness of the proposed methodology is validated by implementing it on a 136-bus distribution system and comparing its performance with similar existing methods. \n\nOverall, the FPCGA approach offers a viable solution to address the challenges of optimal scheduling of renewable DG units in a deregulated electricity market, balancing profitability with network stability."}, {"label": 0, "content": "The interpretation of power flows from a probabilistic perspective is an important topic in the development algorithms of complex network theory for power system analysis, as it is conceived as the theoretical foundation for many new techniques such as machine learning and artificial intelligence. In this paper, we first show that power flow can be mathematically represented as the equilibrium of probabilistic energy movements on a graph. We demonstrate that such an equilibrium can be reached if the energy movements follow the commonly known random process of Markov chains. This work advances the current state of the art by showing a way that connects the power flow models and the random walk models of complex networks. Moreover, several new insights on the electrical betweenness centrality measures used in power grid vulnerability analysis are presented from this new probabilistic perspective."}, {"label": 0, "content": "In this paper, we propose a deep reinforcement learning (DRL)-based method that allows unmanned aerial vehicles (UAVs) to execute navigation tasks in large-scale complex environments. This technique is important for many applications such as goods delivery and remote surveillance. The problem is formulated as a partially observable Markov decision process (POMDP) and solved by a novel online DRL algorithm designed based on two strictly proved policy gradient theorems within the actor-critic framework. In contrast to conventional simultaneous localization and mapping-based or sensing and avoidance-based approaches, our method directly maps UAVs' raw sensory measurements into control signals for navigation. Experiment results demonstrate that our method can enable UAVs to autonomously perform navigation in a virtual large-scale complex environment and can be generalized to more complex, larger-scale, and three-dimensional environments. Besides, the proposed online DRL algorithm addressing POMDPs outperforms the state-of-the-art."}, {"label": 0, "content": "Wind power has been promoted to mitigate the energy crisis worldwide. It has beneficial impacts on economic and environment. However, owing to the chaotic nature of atmospheric movement, wind power generation always exhibits nonlinear and non-stationary uncertainties, which brings great challenges for power system operation and planning. To meet the challenges, a novel deep learning based combined approach is proposed for uncertainties. In this approach, a start-or-art point forecast approach is proposed based on wavelet transform and RNN-RBM deep learning. Raw wind power is decomposed into different frequencies. The nonlinear patterns are used to improve the forecast accuracy by this approach. Consequently, the probabilistic distribution of wind power data can be statistically formulated via the non-parametric approach. The experimental results demonstrate that this combined approach reduces the forecast error, which has a better performance than the other two comparison forecast approaches. The average accuracy of MAPE has increased by about 3.2%."}, {"label": 1, "content": "The rapid development of internet and network technologies has led to a considerable increase in the number of attacks on computer networks. As a result, intrusion detection systems have become increasingly important for achieving high security. However, one of the major challenges of intrusion detection systems is the curse of dimensionality, which can lead to increased time complexity and decreased resource utilization. \n\nTo address these issues and improve the ability of detecting anomalous intrusions, a combined algorithm has been proposed based on the Weighted Fuzzy C-Mean Clustering Algorithm (WFCM) and Fuzzy Logic. The decision-making process is conducted in two stages. Firstly, the WFCM algorithm is utilized to reduce the input data space. Afterwards, the reduced dataset is fed to the Fuzzy Logic scheme to build fuzzy sets, membership functions, and rules to determine if an instance is representative of an anomaly. \n\nThe combination of the WFCM algorithm and Fuzzy Logic scheme provides a more efficient approach to the detection of anomalous intrusions which results in increased accuracy in identifying and responding to potential attacks. This new approach has significant implications for ensuring the security of computer networks and provides a promising solution for addressing the challenges of intrusion detection systems."}, {"label": 0, "content": "Appearance of a small round or oval shaped in a Computed Tomography (CT) scan of lung is an alarm to suspicion of lung cancer. In order to avoid the misdiagnose of lung cancer at early stage, Computer Aided Diagnosis (CAD) assists oncologists to classify pulmonary nodules as malignant (cancerous) or benign (noncancerous). This paper introduces a novel approach for pulmonary nodules classification employing three accumulated views (top, front, and side) of CT slices and Canonical Correlation Analysis (CCA). Nodule is extracted from 2D CT slice to obtain the Region of Interest (ROI) patch. All patches from sequential slices are accumulated from three different views. Vector representation of each view is correlated with two training sets, malignant and benign sets, employing CCA in spatial and Radon Transform (RT) domain. According to the correlation coefficients, each view is classified and the final classification decision is taken based on the priority decision. For training and testing, 1010 patients are downloaded from Lung Image Database Consortium (LIDC). The final results show that the proposed method achieved the best performance with an accuracy of 90.93% compared with existing methods."}, {"label": 0, "content": "Flower plays an extremely important role in our life, which has high research value and application value. The traditional methods of flower classification is mainly based on shape, color or texture features, and this methods needs people to select features for flower classification lead to the accuracy of classification is not very high. This paper aims to develop an effective flower classification approach using convolution neural network and transfer learning. In this paper, based on VGG-16, VGG-19, Inception-v3 and ResNet50 models were used to compare the network initialization model with the transfer learning model. The results show that transfer learning can effectively avoid deep convolution networks are prone to local optimal problems and over-fitting problems. Compared with the traditional methods, the accuracy of flower recognition on Oxford flowers dataset is obviously improved, and has better robustness and generalization ability."}, {"label": 1, "content": "Wireless IoT is a highly promising field that involves the collection, exchange, and storage of large quantities of data and information. However, the benefits of this technology also come with significant risks as attackers seek to exploit wireless IoT for their own purposes. The resource-constrained environment of wireless IoT makes it easy for devices to be vulnerable to various attacks, and confidential information is both valuable and easily compromised.\n\nThe lack of privacy-preserving mechanisms in wireless IoT has become a major obstacle for its adoption and popularization. End users struggle to trust wireless IoT and adopt related applications due to this lack of protection. In this article, we explore the classical application scenarios of wireless IoT and the related security and privacy attack models.\n\nWe then provide an overview of privacy-preserving schemes in wireless IoT, which have been developed to address the lack of privacy protection. Based on the classification of application scenarios, we further discuss recent advancements in privacy-preserving mechanisms in wireless IoT. Finally, we highlight open issues and future research directions for these application scenarios in wireless IoT.\n\nOverall, as the use of wireless IoT continues to expand, it is crucial to focus on privacy-preserving mechanisms to ensure that valuable confidential information is protected from attackers seeking to exploit the technology. By developing more effective privacy-preserving mechanisms, we can overcome the current obstacle to adoption and popularization of wireless IoT applications."}, {"label": 0, "content": "Mission-critical wireless sensor networks are attractive for information gathering in various complex environments and support many mission-critical applications, such as industrial automation and security surveillance. However, in order to fully exploit these networks for such applications, agile and scalable network management for data transfer and computing task implementation are essential. Thus, in this paper, we propose a software-defined mission-critical wireless sensor network (MC-SDWSN) which can solve the existing challenging issues in tradition WSNs, such as resource utilization, data processing, system compatibility, and strict latency requirement. The architecture is based on the idea of SDN architecture, combining the hierarchical cloud and edge computing technologies. Based on the MC-SDWSN architecture, a novel centralized computation offload strategy in sensor network application is proposed to show the feasibility. The simulation results in confirm the MC-SDWSN architecture, and the edge offloading strategy could support the critical missions effectively."}, {"label": 1, "content": "Line loss in power distribution networks is a crucial factor that impacts the economic benefits of power supply enterprises. Therefore, ensuring accuracy and stability in line loss calculation, based on a large amount of power measurement data, is of utmost significance. To achieve this, we have applied a distributed parallel processing approach to the line loss computing service. Additionally, we have fitted a BP neural network to obtain the line loss calculation model in power distribution networks.\n\nExtensive experimentation has been performed to test the algorithm proposed in this paper, and the results demonstrate that this method can guarantee the stability and accuracy of calculation results in line loss calculation. Overall, the use of distributed parallel processing and BP neural network fitting provides an efficient and reliable approach to accurately and stably calculate line loss in power distribution networks."}, {"label": 0, "content": "This article presents a medical simulation solution. This solution allows a physician to train a clinical scenario by interacting with a graphical interface. The aim is to instigate the learning and internalization of clinical procedures. Currently these resources have been intensifying in the most diverse areas, being our focus, Medicine. Within this area, the focus is on medical simulation. There are numerous biomedical simulation centers, whose main objective is to create realistic simulations to aid health professionals. Thus, it is intended to optimize its performance, to meet the needs detected and to anticipate unexpected situations (critical or complex events). However, current simulation systems face some limitations, since they have enough difficulties in the development of new scenarios, since they are restricted to the level of modularity and the number of simulated situations. The training of these professionals is limited to simulation centers. The goal is to create a platform to simulate real scenarios and develop serious games that simulate various clinical situations, in order to facilitate access to this type of training and training."}, {"label": 1, "content": "In this paper, we explore the finite-state approximation of a discrete-time constrained Markov decision process (MDP) under both the discounted and average cost criteria. By using the linear programming formulation of the constrained discounted cost problem, we demonstrate that the optimal value of the finite-state model converges asymptotically to the optimal value of the original model. In addition, we establish a method to compute approximately optimal policies by employing a continuity condition on the transition probability. \n\nFor the average cost criterion, we opt for the original problem definition instead of the finite-state linear programming approximation method. We establish a finite-state asymptotic approximation of the constrained problem and compute approximately optimal policies. \n\nMoreover, under Lipschitz-type regularity conditions on the components of the MDP, we obtain explicit rate of convergence bounds that quantify how the approximation enhances as the size of the approximating finite-state space increases."}, {"label": 1, "content": "This paper proposes a method for parameter identification of synchronous generator based on rough set theory and particle swarm difference algorithm. The paper starts by highlighting the importance of considering the weight and sensitivity of the identified elements' parameters. \n\nThe first step of the proposed method involves constructing the synchronous generator's output by utilizing rough set theory. Following this, attribute reduction of the decision table is carried out to obtain the influential attribute parameters for the simulation results. \n\nNext, the dependence of the calculated parameters is considered for the pruning dimension of the identification parameters. Finally, a generator model is built on the Matlab/Simulink platform, and the model parameters are identified by utilizing the particle swarm difference algorithm as well as model input and output data. \n\nThe simulation results demonstrate that the proposed method is efficient and accurate in the identification of parameters. Overall, this study contributes to the advanced research into parameter identification techniques for synchronous generators."}, {"label": 0, "content": "Lately, the problem of code-switching has gained a lot of attention and has emerged as an active area of research. In bilingual communities, the speakers commonly embed the words and phrases of a non-native language into the syntax of a native language in their day-to-day communications. The code-switching is a global phenomenon among multilingual communities, still very limited acoustic and linguistic resources are available as yet. For developing effective speech-based applications, the ability of the existing language technologies to deal with the code-switched data cannot be over emphasized. The code-switching is broadly classified into two modes: inter-sentential and intra-sentential code-switching. In this work, we have studied the intrasentential problem in the context of code-switching language modeling task. The salient contributions of this paper includes: (i) the creation of Hindi-English code-switching text corpus by crawling a few blogging sites educating about the usage of the Internet, and (ii) the exploration of the parts-of-speech features towards more effective modeling of Hindi-English code-switched data by the monolingual language models trained on native (Hindi) language data."}, {"label": 1, "content": "This paper describes a new approach to modeling distributed photovoltaic (PV) power station clusters using Deep Learning (DL). In particular, a Deep Belief Network (DBN) consisting of multiple layers of restricted Boltzmann machines (RBM) is proposed for dynamic equivalent modeling after first-step clustering using an improved version of the K-means algorithm. The input variables of the neural network include irradiance variation, voltage fluctuation, and reactive power reference of the dual-loop controller, while the output variables are the output active and reactive power of PV clusters. To train the network, datasets are obtained through 6560 experiments, and a layer-by-layer unsupervised learning method is used to pre-train the network, followed by fine-tuning using a supervised back-propagation (BP) method. The resulting DBN-based equivalent model is then applied to a typical distribution network in Anhui Province, and its validity and accuracy are verified in three different disturbance cases. The proposed model reduces both computational complexity and simulation time significantly."}, {"label": 0, "content": "This paper develops a data fusion technology based modeling framework for classifying the underlying cause of power quality (PQ) disturbances. First, the moving-window technique is used to cluster disturbance period with the consideration of the temporal propagation of disturbance energy. Secondly, the PQ disturbance measurements, equipment switching action data and alarm events are integrated by utilizing entity matching method. Then, the distributed mining of association rules is designed to obtain strong association rules within integrated data for describing the relationship between PQ features and event causes. The analysis results have good generalization performance. Finally, the real grid data were taken as an example to verify the effectiveness and practicability of the proposed method. The test results show that the proposed method can analyze the relationship between the typical PQ disturbance features and event causes effectively. This relationship is meaningful for power quality improvement."}, {"label": 1, "content": "In this paper, we present a solution to a double control task involving a group of second-order dynamic agents. We adopt the leader-follower paradigm to maintain a desired formation and collectively track a velocity reference provided by an external source. By persistently selecting the online leader among the agents, we prove that it is possible to optimize the group performance. We define a suitable error metric to capture the tracking performance of the multiagent group while maintaining a desired formation through a communication-graph topology that may even be time-varying. We then demonstrate that this metric depends on the algebraic connectivity and maximum eigenvalue of the Laplacian matrix of a directed graph that is specific to the selected leader. Using these theoretical results, we devise a fully distributed adaptive procedure that can periodically select the optimal leader among the neighbors of the current one. Numerical simulations demonstrate the effectiveness of our proposed solution's ability to outperform other strategies."}, {"label": 0, "content": "Collisions between vehicles and pedestrians usually result in the fatality to the vulnerable road users (VRUs). Thus, new technologies are needed to be developed for protecting the VRUs. Based on the high density of pedestrians and limited computing capability of base stations, in this paper the cloud computing technologies are adopted to handle the huge amounts of safety-critical messages. Moreover, the wireless multi-hop backhaul technology is adopted to overcome the bottlenecks of limited transmission capability and queueing delay of the transmitted safety-critical messages between base stations and clouds. Based on the multi-hop wireless transmission scheme, the signal transmission success probability and delay between pedestrians and clouds are derived for performance analysis. Furthermore, numerical simulations are performed to illustrate the relationship between the transmission success probability and the received signal to interference plus noise ratio (SINR) threshold."}, {"label": 0, "content": "The paper formulates the problem of establishing the initial data for assessing insolation and electric power generation by solar photovoltaic systems. Three sources of information are evaluated: reference books on the climate, satellite meteorological data and weather stations data. In order to improve the precision of assessment, it seems expedient to combine sources of information regarding the factors that affect electric power generation by solar photovoltaic systems. The paper proposes a method for estimating electricity generation on the basis of reference information and open-access weather stations data. Assessment of solar radiation takes into account the beam, sky-diffuse and ground-reflected components. The method incorporates the impact of the total and low-level clouds on solar radiation, as well as the impact of temperature on the efficiency of solar photovoltaic systems. Assessment of solar radiation and electric power generation by solar photovoltaic systems is conducted in Narin-Kunta, a village in the Irkutsk region on the shore of lake Baikal."}, {"label": 0, "content": "The controller synthesis problem for an automatic extremum-seeking system in the case of SISO plants is considered. The plant model can be represented as a serial connection of a nonlinear dynamic component and a static quality function with a certain minimum or maximum value. It is proposed for the individual plant components to organize two separate cascades with the different processes rates in the system. The controller synthesis procedures for each loop are presented. At the first stage, it is suggested to stabilize the processes in the inner contour by means of a controller based on the localization method or to organize a sliding mode in it. Such controllers allow to suppress the perturbations and nonlinear characteristics of the plant dynamic part, as well as to provide the required properties to the inner cascade. In order to the extremum seeking in the outer control loop, it is recommended to use a typical I-controller. The extremum seeking process is corresponded to the first-order linear differential equation. The main differences between the two extremum systems types are shown, depending on the inner loop controller type. The presented numerical simulation results of the two systems types in MatLab illustrate their basic properties."}, {"label": 1, "content": "In recent years, deep learning object detectors such as Fast/Faster R-CNN, SSD, R-FCN, and Mask R-CNN have exhibited significant performance for general object detection, except for pedestrians. The Faster R-CNN's Region Proposal Network (RPN) functions effectively but lacks adaptability. Therefore, we suggest an adaptive real-time pedestrian detection and attribute identification scheme based on Caffe. The first contribution is the Adaptive Threshold Adjustment (ATA) algorithm for intelligent monitoring, which adjusts the threshold using pedestrian movement information. Moreover, to overcome the time-consuming defect, we analyzed the influences of the number of layers, the size of convolution kernels, and the number of feature maps to reduce redundant computation, while maintaining satisfactory performance. By optimizing the neutral network structure, selecting model parameters, and data augmentation, we obtained a stable and well-performed model with fast detection rates and high accuracy. Furthermore, pedestrian information can be identified in our program, providing better service in security monitoring, intelligent robots, and other fields. Extensive experimental results demonstrate that even in complex and athletic scenarios, our method can make quality and speed improvements over current state-of-the-art techniques."}, {"label": 1, "content": "Evolutionary circuit design is a process that requires a lot of time, especially for circuits that are large in scale. In order to address this issue, a distributed computation framework for evolutionary circuit design via parallel genetic algorithm has been designed in this study. The framework enables tasks to be distributed and collected by utilizing a communication network structure using client-server mode. Results from a series of experiments have shown that this framework has a better performance in seeking circuits that satisfy designer specified performance goals with a high efficiency, great flexibility, and strong fault tolerance."}, {"label": 0, "content": "Rapid development of internet and network technologies has led to considerable increase in number of attacks. Intrusion detection system is one of the important ways to achieve high security in computer networks. However, it have curse of dimensionality which tends to increase time complexity and decrease resource utilization. To improve the ability of detecting anomaly intrusions, a combined algorithm is proposed based on Weighted Fuzzy C-Mean Clustering Algorithm (WFCM) and Fuzzy logic. Decision making is performed in two stages. In the first stage, WFCM algorithm is applied to reduce the input data space. The reduced dataset is then fed to Fuzzy Logic scheme to build the fuzzy sets, membership function and the rules that decide whether an instance represents an anomaly or not."}, {"label": 1, "content": "This paper explores the variations in photoplethysmography (PPG) morphology for pregnancies with preeclampsia (PE). The study collected PPG data from 16 hypertensive pregnancies with PE and 26 normotensive pregnancies using a standard medical monitor. To segment and quantify the descending domain of a pulse, the study introduced a novel hierarchical area ratio (HAR) parameter. The paper provides a detailed explanation and discussion of the algorithm and features of HAR. The results of a rough PE distinction based on the statistics of HAR calculated from the original PPG signals were promising, with a precision of 72.7%, sensitivity of 100%, specificity of 76.9%, and accuracy of 85.7%. The proposed HAR has shown potential for the quick and accurate distinction of PE."}, {"label": 0, "content": "Despite having one of the most efficient transportation systems in the world, Singapore is still faced with congestion issues regularly, especially during peak hour periods, due to a number of reasons. We investigate some of the factors contributing to this issue and propose a simulator supplied with predictive travel times through congestion prediction, in order to evaluate and improve bus utilization through effective scheduling. We introduced a conceptual framework to integrate neural network models into simulation so as to improve real-time supply based on several possibilities of demands. This paper will delineate the steps taken to produce the simulator and discuss the evaluation of these models."}, {"label": 0, "content": "This paper presents a data assimilation technique for social agent-based simulation to fit real world data automatically by a reinforcement learning method. We used the hidden Markov model in order to estimate the states of the system during the reinforcement learning. The proposed method can improve simulation models of the social agent-based simulation incrementally when new real data are available without total optimization. In order to show the feasibility, we applied the proposed method to a housing market problem with real Korean housing market data."}, {"label": 1, "content": "To address the challenges and changing needs of the modern power grid, the deployment of Phasor Measurement Units (PMUs) is becoming increasingly important. To support utilities and Independent System Operators (ISOs), this paper proposes an architecture design that utilizes cloud computing technology as a new information infrastructure. This design establishes a cloud-based synchrophasor analytical application service for power system operation. The effectiveness and feasibility of using the Software as a Service (SaaS) service model at grid control centers have been demonstrated through implementation."}, {"label": 1, "content": "Prohibition signs serve as an important safety measure to prevent and protect people from hazardous situations. They are typically placed in areas where they can be easily visualized by the public. However, visually impaired individuals may not be able to see these signs, posing a risk to their safety. In order to address this issue, this paper proposes a system that combines Convolutional Neural Network (CNN) and Computer Vision (CV) algorithms to detect and recognize prohibition signs in real-life scenarios.\n\nTo accomplish this, the system utilizes a pre-trained AlexNet model that has been fine-tuned using Prohibition Signage Boards (PSB) dataset. The model is then combined with Maximally Stable Extremal Regions (MSER) and Optical Character Recognition (OCR) techniques to improve the overall performance of the system. The proposed system has been tested on a variety of prohibition images and texts, and it consistently achieves high recognition accuracies.\n\nIn summary, the combination of CNN and CV provides an effective solution for recognizing prohibition signs in real-life scenarios, which can significantly enhance the safety of visually impaired individuals."}, {"label": 1, "content": "This paper introduces a novel distributed demand response algorithm that takes into account the unpredictability of resident behavior. By using the alternating directions method of multipliers (ADMM), the optimization process can be distributed among multiple servers/cores, which safeguards users' privacy and minimizes the computational complexity of the demand response. Moreover, the robust optimization method is implemented to handle the uncertainty of the response process, so as to mitigate the influence of resident behavior unpredictability. The simulation results validate the efficacy of the proposed algorithm."}, {"label": 0, "content": "The emerging Fifth Generation (5G) mobile networks have been attracting enormous attention from various stakeholders around the world. In particular, in the research community, prototyping 5G infrastructures and deploying 5G services have gained gears recently towards realising market-oriented 5G trials. However, accessing to and programming on real-world 5G infrastructure is almost prohibitive for most 5G researchers especially in academia. Therefore, it is critical to build realistic yet cost-efficient 5G infrastructure emulators for 5G research labs to enable credible 5G research activities. This paper proposes such a 5G infrastructure emulator that is able to emulate a realistic 5G network in a lab setting based on a small number of commercial-off-the-shelf servers by leveraging virtualization and other technologies. Moreover, this emulator allows a service provider to automatically deploy 5G services from `empty' machines through advanced automation. The emulation platform is described in details with the 5G infrastructure and service deployment procedure highlighted. Empirical results are presented to show the performance of the proposed emulator."}, {"label": 0, "content": "The 13 articles in this special section focus on security and privacy in wireless Internet of Things (IoT). IoT is a paradigm that involves networked physical objects with embedded technologies to collect, communicate, sense, and interact with the external environment through wireless or wired connections. With rapid advancements in IoT technology, the number of IoT devices is expected to surpass 50 billion by 2020, which has also drawn the attention of attackers who seek to exploit the merits of this new technology for their own benefits. There are many potential security and privacy threats to IoT, such as attacks against IoT systems and unauthorized access to private information of end users. As IoT starts to penetrate virtually all sectors of society, such as retail, transportation, healthcare, energy supply, and smart cities, security breaches may be catastrophic to the actual users and the physical world. To tackle the security challenges in the design of future wireless IoT systems, we have organized this Special Issue focusing on the security, privacy, and performance of future wireless IoT. "}, {"label": 0, "content": "In wireless sensor network (WSN), the appearance of coverage hole reduces the efficiency of data collection and has a serious impact on network quality of service. Detection of coverage holes is foundation of patching the sensor network to guarantee network quality of service. The paper proposes a distributed coverage hole detection algorithm based on hole boundary nodes (HPNs-CHD). The algorithm firstly uses sensing disk model to identify the HBN nodes in WSN and then exploits probabilistic message mechanism to detect coverage hole. The simulation results indicate that the proposed algorithm outperforms other two algorithms in terms of average energy consumption and average time of coverage holes detection."}, {"label": 1, "content": "High Efficiency Video Coding (HEVC) is widely used in modern video compression systems, thanks to its ability to provide better compression performance than its predecessors. HEVC's Motion Estimation module, which includes both Integer and Fractional pixel motion estimation, plays a crucial role in achieving higher compression rates.\n\nHowever, performing Fractional Motion Estimation (FME) tasks can be highly demanding computationally, as it involves interpolating sample values at fractional-pixel locations. To resolve this challenge, a team of researchers proposed an interpolation-free FME method based on Artificial Neural Networks (ANNs).\n\nTheir proposed method was implemented in the HEVC reference software, HM-16.9, and their findings showed that ANNs can achieve the FME task with an average increase of 2.6% in BDRate and an average reduction of 0.09 dB in BD-PSNR. This new method could prove to be instrumental in improving compression performance while reducing computational costs."}, {"label": 0, "content": "Image retrieval is gaining prominence in the area of medical image processing especially in the domain of fundus images. This work aims to propose a proficient algorithm for features mining in Fundus images and thereby extract the information through Content Based Image Retrieval process. The automated extraction of important features such as exudates aids medical practitioners in effectively overcoming various diseases pertaining to the patient. Although multiple methods of extracting these features are available, they lack in retrieval aspect of the information or the accuracy of the feature extraction."}, {"label": 0, "content": "In this paper, we propose a novel singing-voice enhancement system that makes the singing voice of amateurs similar to that of professional opera singers, where the singing voice of amateurs is emphasized by using a singing voice of a professional opera singer on a frequency band that represents the remarkable characteristic of the professional singer. Moreover, our proposed singing-voice enhancement based on highway networks is able to convert any song (that a professional opera singer does not sing). As a result of our experiments, the singing voice of the amateur singer at the middle-high frequency range which contains a lot of frequency components that affect glossiness was emphasized while maintaining speaker characteristics."}, {"label": 0, "content": "Applications such as autonomous driving and virtual reality (VR) require low-latency transfer of high definition (HD) video. The proposed ultra-low-latency video coding method, which adopts line-based processing, has 0.44\u03bcs latency at minimum for Full-HD video. With multiple line-based image-prediction methods, image-adaptive quantization, and optimized entropy coding, the proposed method achieves compression to 39.0% data size and image quality of 45.4dB. The proposed basic algorithm and the optional 1D-DCT mode achieve compression to 33% and 20%, respectively, without significant visual degradation. These results are comparable to those for H.264 Intra despite one-thousandth ultra-low-latency of the proposed method. With the proposed video coding, the autonomous vehicles and VR devices can transfer HD video using 20% of the bandwidth of the source video without significant latency or visual degradation."}, {"label": 1, "content": "The safe operation of transmission overhead lines is often threatened by the fast growing and tall trees that grow under the line corridor. The limited height of transmission network results in insufficient safe distance between the line and the trees, leading to tree-related faults and tripping. To prevent damage to overhead lines caused by the growth of extra tall trees, it is necessary to understand the growth rule of these trees and predict their growth height. \n\nThis study employs the deep learning algorithm to investigate the growth rule of extra tall trees under overhead transmission lines. Various deep learning and artificial neural network algorithms, including Deep Belief Network, Auto-Encoder, and Long-Short-Term-Memory Algorithm, are used to predict tree height. These algorithms have been verified, and the accuracy of the combined algorithm is found to be higher than that of the single multilayer perceptron and mathematical statistical model. By using these methods, we can effectively prevent tree-related faults and ensure the safe operation of transmission overhead lines."}, {"label": 0, "content": "To realize ubiquitous wireless network environment, the unmanned aerial vehicle (DAV) is considered as good candidate for the next generation network communication infrastructure. UAVs wireless sensor network is composed of low cost and extremely power constrained sensor nodes scattered over spatial region. In this network, users can use UAVs to transfer data. In addition, good maneuverability and wide range of coverage improve UAVs communication efficiency. Mobile Ad-hoc Network (MANET) and Delay / Disruption Tolerant Network (DTN) are considered good supporter for UAVs network. However, the performance of UAVs network may change with the change of surrounding environment, such as UAV density and mobility. Neither single network architecture can always perform well in UAVs network. Therefore, we propose a novel method to improve UAVs network capabilities. In our proposed method, each source node selects the network architecture (MANET or DTN) according to the feature of the data that need to be sent and the network environment. Additionally, we have implemented this architecture and measured data in real world to verify the reliability of the theory. Experimental results show that the bandwidth that our proposed adaptive architecture is 150\\% better than DTN architecture. In addition, compare to MANET, the adaptive architecture can effectively transfer data in the high latency and easily interrupted network."}, {"label": 1, "content": "Automated system inspection with defect detection is a crucial task in large-scale photovoltaic (PV) farms. In this regard, this paper proposes a transfer learning based solution for visible module defects diagnosis. The solution comprises three parts: a normative, time-updated, and sundry dataset; a pre-trained deep learning model; and a transfer learning strategy. The images in the dataset are enhanced with image augmentation technology. The proposed solution employs transfer learning to extract defect features from local to global representation and low to high levels, making it more robust than the enhanced CNN based method. The effectiveness of the proposed solution is validated through extensive experiments, and the results demonstrate its superiority."}, {"label": 1, "content": "A fast Fourier transform (FFT) algorithm is introduced for efficiently refining the integral-equation solution of scattering problems, enabling the evaluation of fields at a multitude of nearby points. The method bears resemblance to the adaptive integral method (AIM), but with the consideration that the fields are point tested rather than Galerkin tested with basis functions. By doing so, the algorithm effectively minimizes computational costs, particularly when the number of observer points is high, in contrast to a brute-force approach."}, {"label": 1, "content": "This study aims to investigate the problem of distributed security constrained economic dispatch (SCED) in an active distribution network. The main objective is to minimize the system's generation cost while considering network security and generation limit constraints. To achieve distributed SCED solution, a feedback strategy based on the duality method is proposed. In this strategy, intelligent agents are deployed for fast calculation and measurement. The distribution management system (DMS) and microgenerators (MGs) act as agents to measure branch flows and respond to the variations in system demand. By adjusting their active power output, each MG ensures network security while responding to load variations. The proposed approach requires partial dual variable information transfer from the DMS agent to MG agents and is easy to implement. Simulations on the IEEE 33-bus system demonstrate the robustness and performance of the proposed algorithm."}, {"label": 1, "content": "Sophisticated mobile applications often require more resources than mobile devices can readily provide. Cloud-based mobile augmentation can help extend mobile device processing power and storage, but resources like battery life and bandwidth cannot be augmented. This study emphasizes the importance of estimating the energy consumption of offloaded and local tasks when making offloading decisions. It is crucial to consider the energy consumption profile of mobile devices, which depends on their individual capabilities. This study proposes the Switch framework to help conserve battery life on mobile devices by using a device-specific energy consumption profile. The evaluation of the framework shows that Switch can successfully make intelligent offloading decisions, thus conserving battery life on mobile devices."}, {"label": 1, "content": "In the field of Lithium-ion (Li-ion) battery technology, one of the biggest challenges is increasing battery longevity. To address this challenge, we propose the development of a health-conscious advanced battery management system that would implement monitoring and control algorithms to increase battery lifetime while maintaining performance. The key to these algorithms is real-time battery capacity estimates. To this end, we present an online capacity estimation scheme for Li-ion batteries. \n\nThe novelty of our approach lies in two key aspects: first, we leverage thermal dynamics to estimate battery capacity; and second, we develop a hierarchical estimation algorithm that has provable convergence properties. Our algorithm consists of two stages that work in cascade to provide accurate estimates of battery capacity. \n\nIn the first stage, our algorithm estimates battery core temperature and heat generation based on a two-state thermal model. In the second stage, the estimated core temperature and heat generation values are used to estimate state-of-charge and capacity. Numerical simulations and experimental data demonstrate the effectiveness of our proposed capacity estimation scheme."}, {"label": 1, "content": "This paper showcases recent research on audio event classification and human perception. To process audio data, the pre-trained model VGGish is used as a feature extractor, while DenseNet is trained and utilized as a feature extractor for EEG data. By learning the connection between audio stimuli and EEG in a shared space, we can provide accurate classifications. In our experiments, we recorded EEG signals from multiple individuals while they listened to music-events from 8 audio categories derived from Google AudioSet. Our findings confirmed that using human perception can boost audio event classification, and the correlation linking audio stimuli and EEG data can support audio event comprehension."}, {"label": 0, "content": "Urban gun violence in cities across the world is a serious issue for public safety agencies and disaster management organizations. This led us to the development of the EDNA drone, an aerial robotics solution designed to equip first responders in high-risk settings with lifesaving-edge tools for situational awareness and non-lethal conflict resolution. The EDNA is an unmanned aerial vehicle (UAV) that delivers the patent-pending \u201cPredictive Probable Cause\u201d technology. The EDNA drone is designed to provide automated real-time analysis to assist teams entering high-risk situations where gun violence may occur. By leveraging machine learning, biometric sensors, and advanced materials in the field and routing feedback to an intuitive augmented-reality interface, the EDNA will provide autonomous threat detection and bullet-stopping capabilities wherever those features are needed--to groups such as Police and Sheriff's Departments, Fire Departments, and EMT and emergency rescue teams. Data from the EDNA drone's sensors is fed to machine learning algorithms running on the drone in real-time. Through a neural network trained on past data, the EDNA is able to detect the presence and location of firearms and explosives, even through walls or other obstacles. Through the use of advanced metal foams and composite materials, the armored drone can even stop bullets-functionality which has obvious benefits for humanitarian deployment."}, {"label": 1, "content": "Autism spectrum disorder (ASD) is a developmental disorder that has received a lot of attention from researchers due to its urgency and pervasiveness. However, the diagnosis and intervention of ASD remain complicated and hard to handle. Fortunately, the rapid development of technology has led to new methods for the auxiliary diagnosis of ASD, such as face detection, gaze estimation, and action recognition. This paper proposes a preliminary visual system for assisting with the diagnosis of ASD in a core clinical testing scenario, namely response to name. The system uses eye center localization and gaze estimation to measure the subject's responses. The purpose of this paper is to analyze the feasibility of the system and optimize the sensing structure and evaluation indicators."}, {"label": 1, "content": "We introduce innovative low-level audio characteristics that rely on the connections between sub-band audio signals, which are decomposed using undecimated wavelet transform. Our experimental results on the GTZAN dataset indicate that the proposed approach, assuming the use of SVM for training purposes, achieved an exceptional accuracy rate of 81.5%, outperforming traditional methods."}, {"label": 0, "content": "Crowd simulation can play a crucial role when it comes to the design of Smart Environments. Crowd simulation can give insights on the flow of pedestrian in particular facilities and explore the interplay between ambient intelligence deployments and users. Most researchers develop crowd simulations using commercial game engines built with the editors they usually provide. This prevents a deeper experimentation with the problems of crowd simulation and enforces to stick to the development paradigm of the tool. As a consequence, it couples the scientific experimentation that produces the crowd model with the actual construction of the simulation tool. Besides, a crowd simulation may require more resources than those available to the scientist. A solution would be to conceive crowd simulation as a service that, on the one hand, it allowed scientists to experiment with the latest advances without the burden of installing elements or acquiring expensive computational resources; and, on the other hand, it enabled developers to evolve the tool in a scalable way. The contribution of the paper is a framework that enables the \u201csimulation as a service\u201d approach for crowd simulations when they are run with a 3D representation. As a proof of concept, the paper illustrates how crowd simulations can be used to generate datasets that allow studying the deployment of sensors in a large facility."}, {"label": 1, "content": "Most existing traffic simulation methods focus on simulating vehicles on freeways or city-scale urban networks, but relatively little research has been done to simulate intersectional traffic. This is despite the broad potential applications of such simulations. In this paper, we propose a novel deep learning-based framework to simulate and edit intersectional traffic. Specifically, we employ a combination of convolution network (CNN) and recurrent network (RNN) to learn the patterns of vehicle trajectories in intersectional traffic, based on an in-house collected dataset. \n\nOur approach not only simulates intersectional traffic but also allows for editing of existing intersectional traffic patterns. Through extensive experiments and comparative user studies, we demonstrate that the results produced by our method are visually indistinguishable from ground truth, and our method can outperform existing methods in terms of accuracy and realism."}, {"label": 0, "content": "Often, images captured by digital camera in outdoor vision system may be significantly distorted by bad weather conditions. Such visual distortions may negatively affect the performance of the system. One such bad weather condition is rain, which randomly makes intensity fluctuations in the images. This paper proposes a new low rank recovery based algorithm to remove the rain streaks from single image taken in rainy weather. This method makes the use of weighted nuclear norm (WNN) and total variation (TV) regularization for efficient rain removal. WNN assigns different weights to different singular values based on the details each singular value holds. TV regularization is used to discriminate most of natural image content from sparse rain streaks by preserving piecewise smoothness of images. Simulation result shows that the rain streaks are more efficaciously eliminated by our method."}, {"label": 1, "content": "When estimating the direction of arrival (DOA) of LFM signals using traditional algorithms, issues such as large sampled data and low estimation accuracy under low SNR are common. To address these challenges, a new DOA estimation method based on compressed sensing theory has been proposed in this paper. Through simulation experiments, the validity of this method's estimation of the DOA of LFM signals has been verified."}, {"label": 0, "content": "A Physically Unclonable Functions (PUFs) extracts the manufacturing variations of integrated circuits for key generation and authentication. It can be used to address the security issue in traditional non-volatile memory (NVM)-based key generation and authentication system. However, the powerful modeling attack based on machine learning has become a new threat of Strong PUFs-based authentication scheme. In this paper, we proposed a novel reconfigurable XOR Arbiter Physical Unclonable Functions (R-XOR APUFs) to resist this modeling attack. In this paper, the R-XOR APUF consist of multiplexers and inverters. The structure of generating two responses is configured according to challenges. The response of R-XOR APUFs is generated by XORing the two response. Therefore, R-XOR APUFs does not have a uniform model and effectively resists machine learning-based modeling attacks. The experiment results reveal that the uniqueness of R-XOR APUFs is 42.15% (the idea value is 50%) and the prediction rate of R-XOR APUFs reduces from 95% to 55% (the idea value is 50%) compared to the traditional APUFs."}, {"label": 0, "content": "Increasing popularity of drones inspires some people and companies to start using them as end-to-end package delivery tools. Despite reducing delivery time, drones running on batteries typically have a high power consumption relative to their battery capacity to provide power for motors, flight controller, and communication systems. Most drones do not have communication systems that provide a long-range coverage while preserving the power consumption. Developing a long-range and energy-efficient communication system becomes a main concern of this research. In terms of wireless physical layer technology, LoRa becomes one of the possible options due to its power efficiency. LoRaWAN, a de-facto standard protocol for LoRa intended for wide area networking, can be used for drone delivery application. However, it is not suitable for real-time and control-heavy applications. In this paper, the limits of LoRaWAN as a secondary communication mode for drone delivery system are evaluated. The results show that LoRaWAN protocol can still be used for a semi-real-time telemetry purpose in which it can send 10-20 bytes payload regularly with minimum of 2-3 seconds interval. In terms of coverage, the system can achieve up to 8 km in an urban area as tested, using the lowest spreading factor, considering the imperfection factor from the hardware. The percentage of packet loss using this configuration is still tolerable, i.e., up to 5%."}, {"label": 1, "content": "Annotation systems offer a range of services, from providing basic information to creating statistical models and applying machine learning techniques. However, current systems lack essential features that could be enabled through collaboration between annotators and AI mechanisms. To address this issue, we propose an approach in which AI is integrated into the annotation process to extract and structure knowledge from annotations based on the context of annotation anchors. To illustrate the benefits of this approach, we introduce the Hyperknowledge Annotation System (HAS), which allows for the annotation of multimedia content and the extraction of additional semantic information from the annotated content. Both the annotation and extracted information are structured using the hyperknowledge model, which enables semantic queries for retrieving annotations. We argue that integrating AI-based services and using the hyperknowledge model can enhance multimedia annotation systems, opening new possibilities for their use."}, {"label": 0, "content": "In this paper, a deep reinforcement learning-based robust control strategy for quadrotor helicopters is proposed. The quadrotor is controlled by a learned neural network which directly maps the system states to control commands in an end-to-end style. The learning algorithm is developed based on the deterministic policy gradient algorithm. By introducing an integral compensator to the actor-critic structure, the tracking accuracy and robustness have been greatly enhanced. Moreover, a two-phase learning protocol which includes both offline and online learning phase is proposed for practical implementation. An offline policy is first learned based on a simplified quadrotor model. Then, the policy is online optimized in actual flight. The proposed approach is evaluated in the flight simulator. The results demonstrate that the offline learned policy is highly robust to model errors and external disturbances. It also shows that the online learning could significantly improve the control performance."}, {"label": 0, "content": "By jointly conducting sparse coding and classifier training, supervised sparse coding has shown its effectiveness in a variety of recognition tasks. However, the existing supervised sparse coding methods often consider linear classification, which limits their discrimination in handling highly nonlinear data. In this letter, we propose a new supervised sparse coding model by incorporating decision tree classifiers. Since decision trees can well deal with the non-linear properties of data, the introduction of decision trees to sparse coding can noticeably improve the discrimination of coding. Meanwhile, sparse coding is able to produce sparse de-correlated features that decision tree is in favor of. For further improvement, we close the loop of sparse coding and decision tree learning with an ensemble framework, which alternatively learns a dictionary for sparse coding and a decision tree for classification. The resulting series of decision trees as well as series of dictionaries are used to construct a decision forest for classification. The proposed method was applied to face recognition and scene classification, and the experimental results have demonstrated its power in comparison with recent supervised sparse coding methods."}, {"label": 1, "content": "This paper presents a novel method for achieving energy-neutral operation on energy harvesting wireless sensor nodes (WSN) to cope with unpredictable and fluctuating environmental energy. The proposed method utilizes adaptive duty cycling and energy management circuits to achieve energy-neutral operation based on the available environmental energy and the instantaneous energy state of the node. The MicaZ mote was used as the WSN, and two different types of vibration-based harvesters (piezoelectric and electromagnetic) were employed in the implementation. \n\nThe experimental results demonstrate that the node incorporating a piezoelectric harvester can only operate for a limited period of time with a fixed duty-cycle of 0.21% and requires a long inactive time of 93.5 s for charging. However, with the proposed strategy, the node can achieve energy-neutral operation by self-adjusting to a duty-cycle of 0.17%, leading to significant improvements in energy efficiency. Furthermore, the proposed strategy can also achieve energy-neutral operation by incorporating an electromagnetic energy harvester attached to the wrist of a runner. Under conditions where no energy is available for harvesting, the proposed strategy demonstrates about 64% lifespan increment before going into sleep mode.\n\nOverall, the proposed energy management policy is an effective and efficient approach to achieving energy-neutral operation in energy harvesting WSNs. The results of this study can contribute to the development of new energy management strategies and techniques for improving the energy efficiency of these systems."}, {"label": 1, "content": "Compared to in-clinic balance training, in-home training is not as effective due to the lack of feedback from physical therapists (PTs). However, a recent study explored the use of trunk sway data and machine learning techniques to automatically evaluate balance, providing accurate assessments outside of the clinic. The study recruited sixteen participants to perform standing balance exercises, and recorded trunk sway data while a PT rated balance performance on a 1-5 scale. The trunk sway data was then used to extract a 61-dimensional feature vector representing the performance of each exercise. Using these labeled data, a multi-class support vector machine (SVM) was trained to map trunk sway features to PT ratings. The evaluation of the model achieved a classification accuracy of 82% in a leave-one-participant-out scheme. The SVM outputs were significantly closer to PT ratings compared to participant self-assessment ratings. This pilot study demonstrates the feasibility of using ML techniques to provide accurate assessments during standing balance exercises in the absence of PTs. Automated assessments could reduce PT consultation time and increase user compliance outside of the clinic."}, {"label": 1, "content": "Critical infrastructure is at risk from a wide range of hazards, making timely and effective recovery after extreme events vital. However, planning for recovery from such disasters is complicated and features both domain and user-centered complexities. Current recovery planning leans heavily on expert judgment, rather than using quantitative computer-based tools. While simulation modeling can help to simplify domain-centered complexities, it does not address issues related to human factors. Human-centered design, on the other hand, focuses on end-users and their requirements. The combination of simulation modelling and human-centered design is known as human-centered simulation modeling. This approach has the potential to make recovery planning simpler and more easily understood by critical infrastructure, emergency management experts, and other decision-makers. We have analyzed several resilience planning initiatives, post-disaster recovery assessments, and relevant journal articles to gain insights into experts and decision-makers' perspectives. We propose a conceptual design framework for creating human-centered simulation models for critical infrastructure disaster recovery planning. This framework comprises three constructs: user interaction with design features, system representation, and computation core. The first construct involves end-users' interactions with features such as model parameters assignments, decision-making support, task queries, and usability. The second construct addresses system components, interactions, and state variables, while the third construct focuses on computational methods required for performing processes."}, {"label": 1, "content": "Cardiovascular diseases continue to be the leading cause of death worldwide, with atrial fibrillation (AF) being the most common type of cardiac arrhythmia. As such, detecting AF early is a crucial goal for healthcare systems worldwide in order to reduce associated risks and expenses. Monitoring techniques based on wearable devices can significantly reduce hospital observation requirements and costs. However, the challenge lies in analyzing the large amounts of patient data generated by these devices, requiring the use of computational intelligence techniques. The selection of features from the data is critical to improving heart rhythm classifiers, and this paper demonstrates that accurate detection of AF can be achieved using a small number of simple features from photoplethysmographic signals. This enables the use of inexpensive wearable devices with limited processing and data storage resources for extended periods of time. These findings were validated through real patient data under medical supervision."}, {"label": 0, "content": "With the development of intelligent substation, substations rely more on network communication to transmit substation information. High substation automations with reliable communication avoid the traditional complicated operation which needs to be done manually, and improve the efficiency of substation operation. But higher requirements for the reliability of information integrity are put forward. The protection setting of intelligent substation directly affects the correctness of the protection devices (Intelligent Electronic Device) and further affects the normal operation of the entire power system. Because the protection setting of protective devices is affected by power system operation mode, season climate and many other factors, the protection setting often needs to be adjusted. When the number of protection devices in substation is increasing rapidly, the frequent change of the protection setting can easily cause the missing or omission of protection setting for certain protection devices, resulting in the potential danger of the power system. On view of the above problems, combined with the development trend of IEC61850, this paper focuses on the on-line management of protection devices. By studying the data attribute of the online management of protection device settings and combining the layered network structure of the intelligent substation, the online management system for protection device settings is constructed, with the aim to eliminate the hidden danger caused by the missing or omission of protection setting."}, {"label": 1, "content": "In this paper, we present a new method for automatically estimating the mass function and selecting focal elements, which are fundamental steps in the application of the Dempster-Shafer Theory (DST). Our approach leverages the centroids and membership distributions obtained by applying the Fuzzy-C-Means algorithm (FCM) to define the mass function. The proposed methodology enables the identification of composite focal elements that represent the highest uncertainty and ambiguity.\n\nWe conducted experiments on multi-spectral and multi-temporal images for change detection by integrating the proposed method of mass function estimation in a process of post-classification by DST. The resulting change detection system is characterised by a multi-level approach to handling imperfections, where ambiguity is modelled by FCM and uncertainty and imprecision are addressed in the mass function estimation step.\n\nOur proposed methodology was highly effective in detecting transformed regions within two Landsat images, with high rates of classification. Overall, our method offers a promising new approach to automatic mass function estimation and focal element selection for DST applications."}, {"label": 1, "content": "This special section comprises 13 articles that delve into the issue of security and privacy in wireless Internet of Things (IoT). IoT, which involves the use of physical objects with embedded technologies to communicate and interact through wireless or wired connections, is rapidly gaining popularity. In fact, the number of IoT devices is expected to exceed 50 billion by 2020. However, as the use of IoT technology becomes more widespread, it has also caught the attention of malicious attackers who are looking to exploit its benefits for their own gain. \n\nThere are numerous potential security and privacy threats to IoT, including unauthorized access to personal or confidential information, and attacks against IoT systems. As IoT technology penetrates various sectors like transportation, retail, energy supply, healthcare, and smart cities, security breaches could be catastrophic not only to the end users but also to the physical world. \n\nTo address these security challenges, we have compiled this Special Issue that focuses on the security, privacy, and performance of future wireless IoT systems. The articles in this section aim to shed light on the crux of the problem and offer solutions to help mitigate these security challenges."}, {"label": 0, "content": "Partial Discharge (PD) pattern recognition is one of the most important steps of PD based condition monitoring of high voltage cables, which is challenging as some types of the PD induced by cable defects are with high similarity. In recently years, deep learning based pattern recognition methods have achieved impressive pattern recognition accuracy on speech recognition and image recognition, which is one of the most potential techniques applicable for PD pattern recognition. The Stacked Denoising Autoencoder (SDAE) based deep learning method for PD pattern recognition of different insulation defects of high voltage cables is presented in the paper. Firstly, five types of artificial insulation defects of ethylene-propylene-rubber cables are manufactured in the laboratory, based on which PD testing in the high voltage lab is carried out to produce 5 types of PD signals, 500 samples for each defect types. PD feature extraction is carried out to generate 34 kinds of PD features, which are the input parameters of the PD pattern recognition methods. Secondly, the principle and network architecture of SDAE method and the flowchart of SDAE based PD pattern recognition are presented in details. Thirdly, the SDAE method is evaluated with the experimental data, 5 different types of PD signals, which achieves a recognition accuracy of 92.19%. Finally, the proposed method is compared with the traditional pattern recognition methods, Support Vector Machine (SVM) and Back Propagation Neural Network (BPNN). The results show that the pattern recognition accuracy of the proposed method is improved by 5.33% and 6.09% compared with the SVM method and the BPNN method respectively, which is applicable for pattern recognition of PD signals with high similarity."}, {"label": 1, "content": "An emerging issue in the realm of intellectual data bases is the General Data Protection Regulation (GDPR) and similar concepts that are pertinent to present and future data handling operations. While standalone operations provide security in this matter, there is a significant challenge in getting higher performance processing such as Artificial Intelligence (AI) with deep learning. To tackle this challenge, a development approach is presented in this article. \n\nDeep learning consists of two functions: training with big databases and making inferences for the specific needs. While the inferences can be processed using simple hardware that can create a standalone module, the training process requires significant computing power and cloud interfaces, making it problematic to create mobile features in a standalone module. To resolve these issues, our development approach offers two novel solutions. \n\nThe first problem was addressed by figuring out how to access data quickly from an SSD storage without any delay in processing. The second issue resolved how to reduce power and calculation time utilizing massive repeating calculation. To enable power reduction and shorten calculation time, our approach incorporates a lookup table (LUT) subsequent zero calculation architecture. Additionally, our architecture uses dynamic reconfiguration through the Memory Logic Conjugated System (MLCS), which can execute at high speeds for seamless training and inference operations in real-time sequences. It is worth mentioning that our operation with dynamic reconfiguration outperforms any other processing system currently available.\n\nThe trial demonstration module showcased an effective power of 1W with a processing performance of 400Mops in a commercially available FPGA evaluation board. Using a custom-designed SoC, this achievement can be enhanced to 0.5W with 2Gops. Both cases are adequate for standalone modules, allowing for middle-range deep learning execution with impeccable training and inference operations on real-time data sequences. \n\nOverall, our development approach appears to be a significant leap forward to resolving the issue of heavy training processing for deep learning. Utilizing the power of dynamic reconfiguration through MLCS and LUT architecture, this approach provides a cost-effective solution to reducing power consumption and calculation time while maintaining high performance."}, {"label": 1, "content": "In order to enhance the stability of linear systems in observer canonical form, we have devised a state observer that utilizes time-varying gains. These gains increase infinitely as the convergence time approaches a set value, as predetermined by the user. With this observer, fixed-time stability is achievable regardless of initial conditions, ensuring a high level of predictability. Additionally, the error injection terms for output estimation remain uniformly bounded, converging to zero at the specified time."}, {"label": 0, "content": "The variations of photoplethysmography (PPG) morphology for the pregnancies with preeclampsia (PE) were studied in this paper. PPG data from 16 hypertensive pregnancies with PE and 26 normotensive pregnancies were acquired by the standard medical monitor. A novel hierarchical area ratio (HAR) parameter was invented to segment and quantitate the descending domain of a pulse based on the data acquired. The algorithm and features of HAR are fully explained and discussed in the paper. A rough PE distinction based on the statistics of HAR calculated from the original PPG signals was conducted with the precision of 72.7%, sensitivity of 100%, specificity of 76.9% and accuracy of 85.7%. The HAR we proposed in the paper showed favorable prospects in the quick distinction of PE."}, {"label": 1, "content": "The technology of physical-layer secret key generation (PSKG), which is based on reciprocal wireless channel, has been extensively studied in point-to-point (P2P) scenarios. This approach is known to be effective in solving the conventional security mechanism's key distribution problem. However, the complexity and energy consumption of PSKG increase when it is applied to group key distribution. Therefore, the issue of using PSKG to ensure group secret communication is still unresolved. To address this problem, we present a lightweight group key distribution (LGKD) technique designed for star network topology environment. In our LGKD method, the center node and each child node extract high-correlated channel characteristics instead of generating identical P2P keys, respectively. Then, the group key is transmitted to each child node, protected by the P2P channel characteristics that have high similarities. Our simulation results confirm the feasibility and effectiveness of our proposed LGKD approach."}, {"label": 0, "content": "The increasing trend of using online platforms for real estate rent/sale makes automatic retrieval of similar floor plans a key requirement to help architects and buyers alike. Although sketch based image retrieval has been explored in the multimedia community, the problem of hand-drawn floor plan retrieval has been less researched in the past. In this paper, we propose REXplore (Real Estate eXplore), a novel framework that uses sketch based query mode to retrieve corresponding similar floor plan images from a repository using Cyclic Generative Adversarial Networks (Cyclic GAN) for mapping between sketch and image domain. The key contributions of our proposed approach are : (1) a novel sketch based floor plan retrieval framework using an intuitive and convenient sketch query mode; (2) A conjunction of Cyclic GANs and Convolution Neural Networks (CNNs) for the task of hand-drawn floor plan image retrieval. Extensive experimentation and comparison with baseline results authenticates our claim."}, {"label": 1, "content": "The emergence of high-resolution video has presented a serious challenge to the conventional video coding standard. Fortunately, the advent of new-generation standards has provided a solution to this problem. However, it has also increased the coding complexity, especially in the area of motion estimation, which is considered to be a module with a relatively high computational complexity.\n\nThis paper proposes a parallel motion estimation implementation that utilizes pre-motion estimation, integer motion estimation, and fractional motion estimation. These techniques are then accelerated using GPU, based on the AVS2 standards. To achieve more efficient data access, a rapid mapping table algorithm is introduced. Additionally, a quasi-integral-graph algorithm is applied to efficiently calculate the SAD or SATD for blocks of varying sizes. Both of these techniques enable better thread utilization and exploit the properties of GPU to enhance overall efficiency.\n\nExperimental results indicate that the proposed parallel method is effective in accelerating motion estimation. Ultimately, this technique can greatly enhance video coding standards and enable faster and more efficient video processing."}, {"label": 1, "content": "Head detection is a crucial task in identifying individuals from visual data. However, due to the difficulty of building local and global information under unconstrained pose and orientation conditions, existing methods have had limitations in effectively detecting heads. To address these issues, this study proposes an adaptive relational network capable of capturing context information. The fundamental contextual features, such as global shape priors and local adjacent relationships, can be quantified by visual operators. The authors propose a two-step search algorithm to quantify intergroup conflict, while also introducing a structured feature module to capture local intraindividual stability. Finally, the global priors and local relation are integrated into a single-stage head detector. The study shows that the proposed method achieves state-of-the-art results on two challenging datasets, HollywoodHeads and Brainwash. An extensive ablation analysis further confirms the efficacy of the approach."}, {"label": 0, "content": "The customized wireless devices play a key role in information exchanging for various electrical facilities. However, the development of the devices is hindered by diversity of interfaces (RS-232/485, Ethernet, USB, etc.), high cost, outdoor environment, etc. In this work, we proposed a \u2018system on module\u2019 solution to address above issues. We optimize and spare the storage space of baseband chip for embedded control system to replace the independent chips, which dramatically reduces the size, cost and power consumption of devices. Meanwhile, our SOM wireless devices can support multi-interfaces and thus realize \u2018plug and play\u2019. Moreover, our developed wireless devices allow multimode wireless communications including operator's wireless networks LTE FDD/TDD, UMTS, GSM and self-built LTE-G 1800MHz network. Last but not least, we can embedded positioning chip and the client of management system in the wireless devices for the sake of operation and maintenance."}, {"label": 1, "content": "As a special form of Mobile Ad hoc Network (MANET), vehicles in the network can connect to other vehicles on the road and the Internet, providing stable and high-speed wireless data access services for vehicles with high velocity. VANET has become an effective technology that guarantees vehicle safety, provides intelligent traffic management, and facilitates high-speed data communication and vehicle entertainment. However, the rapidly changing network topology, highly dynamic channel conditions, and node competition in channel accessing in VANET have posed challenges to data transmission.\n\nTo address this problem, a cluster algorithm based on vehicle mobility for VANET has been proposed. The algorithm formulates the objective function of cluster head selection based on vehicle mobility, including position, velocity, and packet forwarding capability, and presents the clustering algorithm's process. Simulations have shown that compared to previously proposed algorithms, the proposed algorithm offers better performance in terms of packet delivery rate, average transmission delay, and total throughput."}, {"label": 1, "content": "A proposal has been put forward for a large scale Multi-Chip-Module consisting of a number of Many Core (PE), FPGA, and DRAM chips. This innovative design boasts three major features. Firstly, T-Hz $(0.1 \\sim10$ THz) radio communication chips have been included within the MCM module to enable very high speed data transmission for seamless communication among the chips. Secondly, the MLC (Multi-Layer Ceramic) or another substrate, which is layered with T-Hz radio chips on the upper surface side, is utilized to connect the Many Core (PE), FPGA, and DRAM chips on the lower surface side using C4 technology. Finally, T-Hz radio chips are also employed in communicating with other MCMs and external I/O devices, thereby facilitating efficient data transfer between them."}, {"label": 1, "content": "In this article, we present a rapid and effective multitemporal despeckling technique that employs the ratio image obtained from an image and the temporal average of the stack. The approach\u2019s fundamental premise is that the ratio image is easier to remove noise from than a single image, thanks to its better stationarity. Additionally, the multitemporal mean aids in maintaining thin, temporally stable structures. The proposed method comprises three stages: (1) estimation of a \u201csuperimage\u201d through temporal averaging and possibly spatial denoising; (2) noise reduction of the ratio between the noisy image under consideration and the \u201csuperimage\u201d; and (3) calculation of the denoised image by re-multiplying the denoised ratio by the \u201csuperimage.\u201d Due to the improved spatial stationarity of ratio pictures, removing noise from them with a speckle reduction method is more effective than removing noise from original multitemporal stacks. This approach also decreases the amount of data that is jointly processed compared to other techniques by employing the \u201csuperimage\u201d that sums up the temporal stack. In comparison to several state-of-the-art reference techniques, the results show enhanced numerical outcomes (peak signal-noise ratio and structure similarity index) as well as visually on simulated and synthetic aperture radar (SAR) time series. The proposed ratio-based denoising methodology extends single-image SAR denoising methods to time series by exploiting the persistence of several geometric structures effectively."}, {"label": 0, "content": "The frequency-domain Kalman filter (FKF) has been utilized in many audio signal processing applications due to its fast convergence speed and robustness. However, the performance of the FKF in under-modeling situations has not been investigated. This letter presents an analysis of the steady-state behavior of the commonly used diagonalized FKF and reveals that it suffers from a biased solution in under-modeling scenarios. An effective improvement of the FKF is proposed, having the benefits of the guaranteed optimal steady-state behavior at the cost of a very limited increase of computational burden. The convergence behavior of the proposed algorithm is also analyzed. Computer simulations are conducted to validate the improved performance of the proposed method."}, {"label": 1, "content": "Currently, locational marginal prices (LMPs) are computed every few minutes through an optimization process that only utilizes continuous variables. The scheduling of generators is determined separately through a day-ahead commitment procedure. However, a new model based on bilevel programming has been proposed to combine the two processes. The new model determines both generator on/off status and LMPs simultaneously, resulting in potentially greater efficiency. The effectiveness of the model was demonstrated on both 5- and 30-bus systems."}, {"label": 1, "content": "The implementation of large-scale Spiking neural networks (SNN) requires hardware realization of scalable neurons and synapses. In this study, a novel transient Joule heating based the leaky-integrate and fire neuron (LIF) in scalable PrMnO3 (PMO) RRAM device is proposed experimentally. The Joule-heating based thermal runaway is utilized to achieve rectified linear unit (ReLU) voltage dependence of spiking frequency, similar to a typical LIF neuron. \n\nFurthermore, the Jouleheating hypothesis in PMO is validated by TCAD DC and transient simulations. PMO is an extremely thermally resistive semiconductor, being 300x more thermally resistive than Si, which enables low energy thermal dynamics. The excellent energy, area performance with a synapse in the same material system and thermal engineering makes PMO neuron an attractive option. \n\nFinally, PMO neuron shows software-equivalent learning accuracy in SNN on the Fischer's Iris dataset. These findings suggest that PMO RRAM device-based spiking neuron and synapse can play a pivotal role in the hardware implementation of large-scale SNNs."}, {"label": 0, "content": "Camera model identification has been attracting a lot of attention lately, as a powerful forensic method. With the promising breakthroughs in the artificial intelligence applications, such systems were revisited to increase the expected accuracy or to solve the still persisting deadlocks. One of the most still-to-be-solved dilemmas is the image manipulations effect on the overall accuracy of the identification systems. A huge degradation in the performance is noticed, when images are post-processed using commonly used methods as compression, scaling and contrast enhancement. Using the state of the art Convolutional Neural Network (CNN) architecture proposed by Bayar et al to estimate the manipulation parameters, and dedicated feature extractor models to estimate the source camera. Multiplexers are used to shift the input image between the dedicated models through the output of the CNNs. Our proposed methods significantly outperform state of the art methods in the literature, especially in case of heavy compression and down sampling. The images used for testing were extracted from 10 different cameras, including different models from the same manufacturer. Different devices were used to investigate the methodology robustness. Moreover, such generic approach could revolutionary change the whole design methodology for camera model identification systems."}, {"label": 1, "content": "Multi-energy flow coupled network computing plays a crucial role in ensuring optimal operation of regional integrated energy systems by providing constraints for network balance and system energy security. It is also a fundamental basis for analyzing system security and stability. This article focuses on constructing a network model for combined calculation of electric, gas, and thermal/cold multi-energy flow with consideration of the dynamic response characteristics of the multi-energy coupling system with multiple time scales.\n\nThe model is developed from two aspects of steady state and dynamic, facilitating efficient computation of the multi-energy coupling system. To improve computational efficiency while maintaining simulation accuracy, a hybrid step time domain simulation method is proposed. The method employs electromechanical transient simulation for the electric power system and medium-long term transient simulation for non-electrical systems, supporting the network computation of the multi-energy coupling system.\n\nThe model and algorithm are verified by simulating an example of an electro-thermal coupling network. The results demonstrate the effectiveness of the model in optimizing operation and enhancing system security and stability. The proposed hybrid step time domain simulation method improves computational efficiency without sacrificing simulation accuracy, making it a valuable tool for analyzing multi-energy coupling systems."}, {"label": 1, "content": "With the increasing utilization of power electronic technologies, such as HVDC, new energy power stations, and FACTS, the dynamic characteristics of the interconnected power grid have undergone significant changes. However, the traditional quasi-steady model-based electromechanical transient program cannot accurately simulate the fast switching process of power electronic components, and its non-fundamental frequency or asymmetry characteristic, which affects the accuracy of HVDC and FACTS models. Additionally, traditional electromagnetic transient programs cannot replace electromechanical transient programs in large-scale power system stability analysis due to computational limitations.\n\nTo address these issues, FEMTP should be redesigned for transient stability analysis of large-scale electronic power systems. This paper discusses the electromagnetic models of generators and transmission lines and evaluates the appropriateness of their electromechanical transient model in FEMTP through simulation. The implementation of FEMTP is also discussed in detail, and an example of China East power grid demonstrates the utility of simplified electromechanical models and proposed technologies."}, {"label": 1, "content": "Virtual bidding is an important factor that impacts electricity markets, particularly the Locational Marginal Price (LMP) in the day-ahead market of two-settlement electricity markets. However, an excessive amount of virtual load may lead to congestion on certain transmission lines and LMP differences between specific nodes. This presents an opportunity for market participants to manipulate markets such as the Financial Transmission Rights (FTRs) market. Therefore, it is critical for Independent System Operators (ISOs) and regulators to calculate LMP differences caused by virtual bidding in advance to monitor the market's behavior and improve its efficiency.\n\nThe traditional enumeration-based method of calculating LMP differences is time-consuming and prone to inaccuracies. Therefore, this paper proposes a fast algorithm using the parametric programming method to calculate LMP differences accurately and efficiently. Instead of repetitive calculations, this algorithm seeks out the turning points where LMP differences change. Based on a modified IEEE-9 bus system, the case study demonstrates that the proposed algorithm performs better in both accuracy and efficiency compared to traditional methods.\n\nIn conclusion, the proposed algorithm offers a more efficient and accurate approach to calculate LMP differences in two-settlement electricity markets. As virtual bidding continues to impact the electricity market, the proposed algorithm can be instrumental in providing regulators and ISOs with the necessary data to monitor market behavior and enhance market efficiency."}, {"label": 1, "content": "The paper introduces a wavelet-fuzzy controller for a field-oriented control system used in belt conveyor drives. The controller employs a discrete wavelet transform to break down the speed error signal into different frequency components. The error and scaling coefficients obtained from the transformation are then utilized to produce the motor control signal. This new controller satisfactorily addresses the speed control problem for a closed control system. The efficacy of the field-oriented control scheme in combination with the wavelet-fuzzy controller was evaluated under diverse dynamic operating conditions. The outcomes of the comparison between the conventional PI controller and the proposed wavelet-fuzzy controller are presented. The simulation results provide evidence of the effectiveness of the proposed wavelet-fuzzy controller. Moreover, the proposed approach of the wavelet-fuzzy controller can effectively reduce electromagnetic moment vibrations."}, {"label": 0, "content": "Cognitive Radio networks (CR) is a new technique that uses available unlicensed spectrum band this is due to the limited number of fixed licensed spectrum bands [1]. The main features of the CR are spectrum sensing, management, sharing, and mobility [2]. This paper focuses on spectrum mobility for military systems often referred to as spectrum handoff for military systems, which is a process when the CR user changes its frequency of operation due to spectrum occupancy by the licensed user. Spectrum mobility is important in military networks since mission success may depend on reliable communications. In this paper, we propose the use of Fuzzy logic to come up with a decision making handoff scheme. The proposed scheme avoids interruptions caused by the movement of a secondary user and minimizes handoff latency if the SU selects a correct channel. It was observed through the analysis of literature that even though most of the existing schemes can perform handoff successfully, these existing schemes result in slow handoff and are complicated to implement."}, {"label": 1, "content": "This article aims to examine the strengths and weaknesses of artificial intelligence (AI) in relation to enhancing both online and offline customer experiences, as well as explore potential methods for measuring its effectiveness. The retail industry has increasingly recognized the role of AI in shaping customer experiences. By analyzing various data extraction tools, the article seeks to provide guidance on interpreting information, assisting companies in turning such insights into tangible improvements in the online and in-store retail experience. The research not only presents a snapshot of the current state of customer experiences, but it also provides predictions on how AI will become pivotal in the future. Ultimately, the aim is to improve overall customer satisfaction through the provision of AI-powered experiences."}, {"label": 1, "content": "With the internet advancing at a rapid rate, the amount of network data has increased exponentially. Due to the dynamic nature of society, numerous hot topics emerge on networks at any time. To address this, a general framework based on statistical modeling of HMM was devised to solve the issue of hot topics. The framework models and assesses the probability of a network theme to become a hot topic. The experiments revealed that this framework is superior in comparison to traditional algorithms concerning the convergence speed and accuracy when discovering network hot topics."}, {"label": 1, "content": "The cloud's relational database service typically uses virtualization technology to achieve energy efficiency. However, the consolidation of multiple independent database systems into a single physical machine leads to resource contention on the shared device, hurting disk I/O performance. To address this issue, we propose VMSQL, a novel disk I/O model for virtualized RDBMS.\n\nVMSQL incorporates two innovations over the original disk model of virtualized database systems. First, it enforces synchronous operation in the guest operating system to handle transaction commitment, freeing up CPU cycles that can be used to serve upcoming requests, thereby improving their response times. Second, it asynchronizes the storage path of transactions committed from different co-located guest databases in the host system. This procedure allows the host system to apply improvements to the disk I/O performance of virtualized RDBMS, relieving random I/O and enhancing the overall system throughput.\n\nWe have implemented a prototype of this Sync-Async model in QEMU-KVM hypervisor with the InnoDB engine deployed in the guest operating system. Extensive experiments have been conducted to verify its advantages, and the results have been positive without any loss of ACID-compliance. However, VMSQL does incur moderate overhead at the hypervisor layer."}, {"label": 1, "content": "Accurate knowledge of the transport properties of the environment is essential for modeling gas-dynamic processes in energy power systems based on heat and mass transfer at the macro level. In this paper, we propose an approach based on the kinetic molecular theory of gases to determine transport coefficients in multicomponent gaseous environments. The model considers the molecular interaction within the K-component gaseous environments and takes into account the elastic and geometric properties of interacting molecules. This approach enables the determination of transport properties of multicomponent environments in macro systems by considering their microscopic properties. As such, it provides a new level of accuracy for modeling gas-dynamic processes."}, {"label": 1, "content": "Server consolidation and resource elasticity are two critical resource management features of cloud and edge computing. Elasticity can take either the form of horizontal or vertical elasticity. While horizontal elasticity allows for the acquisition and release of computational nodes based on demand, vertical elasticity involves adjusting the capacity of the resource types allocated to each individual virtual machine (VM) in accordance with its respective application's requirements. However, in the case of vertical elasticity, when there are insufficient resources available to allocate to a VM, its application's performance may suffer degradation.\n\nFor online applications, the only option is to live-migrate the VM to another server when insufficient resources are allocated. Alternatively, for resource-constrained VMs running batch jobs, they could be suspended or saved to disk and revived elsewhere when resources become available. Since memory availability significantly affects system throughput and performance, this paper investigates the feasibility of integrating VM migration, pausing, and suspension schemes as part of a VM scheduling strategy to support the execution of both online and batch applications in virtualized infrastructures utilizing memory elasticity. The results show that combining such schemes can provide utilization benefits for cloud service providers when memory resources are scarce."}, {"label": 0, "content": "Channel estimation for Multi-input Multi-output/Orthogonal Frequency Division Multiplexing (MIMO/OFDM) in fast linear-time-varying (LTV) multi-path channel using special frequency-division (FD) pilot is proposed. Unlike linear -time-invariant (LTI) channel, OFDM systems in LTV channel may suffer significant inter-carrier interference (ICI), which strictly depend on Doppler frequency shift caused by relative movement of transmitters and receivers. In this paper, we redesign frequency-division pilot for LTV channel, so we can effectively estimate the channel tap of intermediate instant of each OFDM symbol with relatively low ICI. Finally, we use well-known basis expansion model (BEM) to fit the whole channel. Numerical results indicate that our new frequency-division pilot combined with BEM fitting can obtain high precise channel estimation for fast LTV multi-path channels."}, {"label": 1, "content": "Fine-grained visual categorization poses a challenge in distinguishing objects in subordinate classes as opposed to basic classes. This task is difficult due to the high correlation between the subordinated classes and the large intra-class variation, such as different object poses. The deep convolutional neural network (DCNN) has been successful in generic object classification, detection, and segmentation with the availability of large-scale training samples. However, in fine-grained visual categorization, where only a few training samples are available for each subordinate class in most public fine-grained image datasets, direct application of DCNN does not lead to satisfactory classification results. This study explores the use of transfer learning for fine-grained dog breed categorization based on learned CNN models with the large-scale image dataset, ImageNet, and demonstrates promising performance with two DCNN models: AlexNet and VGG-16. Additionally, the study proposes fusing multiple CNN architectures for combining different aspect representations to give more accurate performance. The fusion of different layers, such as Fc6 and Fc7 in AlexNet and VGG-16, improved the fusion architecture's performance by 2.88% over the best performance of the only one DCNN model: VGG-16 from 81.2% to 84.08%. The study argues that different DCNN architecture can extract the representation of different image aspects due to the previously defined CNN kernel sizes, number, and various operations in the model learning procedure, and thus result in different performance for visual categorization."}, {"label": 0, "content": "Since the traditional correlation analysis based on complex power flow calculation cannot meet the requirements of performance evaluation of current distribution network planning, the BP neural network (Back Propagation Neural Network, the BPNN) based correlation mining is proposed in this paper. With the reconstruction measures and performance indexes as the training sample sets, the corresponding correlation model through the offline learning of sample data can be obtained. As a result, when given reconstruction measures in practical application, the neural network training can give the results of performance indexes quickly and accurately. In addition, in order to improve the generalization mapping capability of BPNN, the genetic algorithm is used to optimize the weights and thresholds of the BPNN. Experimental result based on the IEEE 33 node network shows the accuracy and effectiveness of the presented methodology."}, {"label": 1, "content": "The company responsible for incremental distribution network operation under the electricity reform will focus on the operating income of this network. To address this issue, this article proposes a dynamic economic dispatch model that takes into account the power-to-gas (P2G) technology. Additionally, it presents a catastrophe genetic algorithm that uses a double iterative optimization genetic algorithm to solve time coupling problems, including those associated with energy storage and demand-side response. To illustrate the effectiveness of this model, the IEEE33 node model was used, and various regulatory methods were investigated. The results confirm that incorporating P2G and demand-side response is crucial for enhancing the operating income of incremental distribution networks."}, {"label": 0, "content": "Unified Communications (UC) is revolutionizing next generation enterprise networks by allowing human voice and video to travel over existing packet data networks along with UC services such as video teleconferencing (VTC), unified messaging, and chat. The ultimate goal for UC is an integrated, fully-converged, cloud-based architecture. The implementation of this technology brings new forensic challenges to network investigators. Recently, a cloud service delivery model known as UC as a Service (UCaaS) is gaining momentum in the field of Information Technology. This paper discusses the main challenges associated with forensic investigations in an UC cloud environment. Further, this research attempts to improve UC security by proposing two semi-formal patterns in order to create a Cloud Forensic Model. These patterns provide a systematic approach to network forensic collection and analysis of digital evidence in UCaaS architectures. The proposed cloud forensic framework will allow network investigators to specify, analyze and implement network forensic investigations for technologies under the UC umbrella."}, {"label": 0, "content": "When expressing concerns about the credibility of simulation studies, simulation data have been traditionally in the focus. However, what about another and, some might argue, even more central product of simulation studies, i.e., the simulation model itself? How can the credibility of a simulation model be assessed? Therefore, information about the process of generating a simulation model is needed. This provenance relates entities (or artifacts) and activities involved in the generating process. Based on simulation studies we will illuminate how the provenance of a simulation model relates the refinement, extension, composition, calibration and validation of simulation models to the diverse sources used in these processes. To exploit this information, unambiguously means for specifying entities play a central role. For example, a formal domain-specific language for modeling facilitates assessing and reusing simulation models. Similarly, a declarative domain-specific language for specifying simulation experiments, helps utilizing simulation experiments done with earlier models for future models. Thus, provenance, information about the past, does not only allow to understand the present, but also to design the future, in opening up new avenues for generating and analyzing simulation models."}, {"label": 1, "content": "An increasing number of wireless intelligent devices are being used in ICS networks, but enhancing the security level of legacy equipment in these networks is a challenge due to their weak computing and storage capabilities. To address this issue, a hybrid-augmented device fingerprinting approach has been developed to improve traditional intrusion detection mechanisms in ICS networks. The program process is simple, and the hardware configuration is stable. The approach measures inter-layer data response processing time and analyzes network traffic to filter abnormal packets, enabling intrusion classification and detection. The device fingerprinting-based approach was evaluated using data collected from a lab-level micro-grid, and tests were performed to assess its robustness and effectiveness against forgery attacks and intrusions."}, {"label": 1, "content": "Engine-triggered tasks are tasks that are triggered in real-time and are initiated when the engine's crankshaft completes a rotation. The execution time of such tasks is heavily dependent on the speed and acceleration of the crankshaft. When the execution times of tasks depend on a variable period, they are referred to as adaptive-variable rate (AVR) tasks. In the past, attempting to calculate the worst-case demand of AVR tasks proved to be either inexact or computationally prohibitive. \n\nHowever, in this study, we propose a new method to solve this problem. We transform the problem into a variant of the knapsack problem to find the exact solution during a given time interval. Moreover, we design a framework that can systematically reduce the search space, making it feasible to find the worst-case demand of AVR tasks. Our experimental results demonstrate that our approach is at least 10 times faster than the state-of-the-art technique with an average runtime improvement of 146 times, even when it comes to randomly generated tasksets."}, {"label": 0, "content": "With more and more disturbance sources such as high-speed railway and renewable energy generation, the power quality problem has become increasingly complex, which seriously affects the reliable operation of the power grid. Identifying the types of disturbance sources that cause power quality events based on power quality monitoring data will support for targeted control disturbance sources, also provide evidences for determining contribution between customers and operators. This paper proposes a method for identification of disturbance sources based on random forests. Firstly, it chooses analysis indices and extracts both temporal and statistical characteristics of selected indicators from historical data. After balancing datasets, the data are used as eigenvectors of training random forests. Secondly, combining with OOB(out-of-bag), one of evaluation indicators, adjustment of random forest parameters in a closed-loop is used to construct cost-optimized random forest classifier. Thirdly, the type of disturbances is on-line identified using the classifer. Based on data from a power quality monitoring system in a regional grid of China, it is verified that the method has a high accuracy for identifying disturbance sources such as electric railways, converter stations, wind power, photovoltaics, and smelters."}, {"label": 0, "content": "The development of the city transportation system provides us lots of conveniences, but it also can brings traffic congestion causing environmental pollution and increasing the travel cost. The current research mainly focuses on traffic volume prediction, route recommendation. However, it is also very meaningful and instructive to select a suitable less time-consuming alternative route on a specified congested road. In general, skilled taxi drivers who are relatively familiar with the traffic condition would like to choose the less time routes to avoid peak congestion road segments. Inspired by above ideas, in this paper, we propose a novel hybrid framework which integrates both urban traffic flow characteristics theory and machine learning techniques. We first describe the problem definition, then capture a typical set of congestion road features from the GPS trajectories, the features include traffic volume, road speed limit, route distance, traffic light, and weather features. After that, the most commonly used top-k candidate alternate routes based on historical data are generated, then the feature representations for congestion are feed to train the deep learning model, and the best alternative route is selected after the training process. Extensive experiments on realistic datasets derived from realistic car services demonstrate the superiority of our methodologies."}, {"label": 1, "content": "The growth of IoT data and its diverse range of applications has led to the outsourcing of data by IoT clients to cloud servers and datacenters. In addition to storage, clients require these servers to perform functional operations as per their requests. This paper proposes secure mechanisms that support homomorphic computation functions, allowing clients to outsource their encrypted data to geographically distributed servers. The distributed index framework is leveraged to break down and distribute data uniformly across servers, with key-value store being used as the underlying structure for fast data retrieval. Shamir's secret sharing is customized into our mechanisms to implement a tunable scheme for different IoT application scenarios. Three tunable protocols are designed to enable effective additive homomorphic computations, with attention given to server utilization and computation and storage overheads. Although the primary focus is on additive computation, the designs can be extended to other types of homomorphic computations and data verification. The proposed protocols are evaluated through experimental deployment in Amazon Web Services, demonstrating their efficiency in various areas."}, {"label": 0, "content": "In this paper, we propose a new method to mitigate the effect of low signal to noise ratio (SNR) on two-dimensional (2-D) Direction of arrival (DOA) estimation. This method consists to extend the antenna steering vectors before applying the MUltiple SIgnal Classification (MUSIC) algorithm for 2-D DOA estimation. The simulation results show a good location accuracy enhancement."}, {"label": 1, "content": "This paper presents a novel data fusion technology-based modeling framework for classifying the root cause of power quality (PQ) disturbances. The proposed framework comprises three main steps. Firstly, the moving-window technique is utilized to cluster the disturbance period considering the temporal propagation of disturbance energy. Subsequently, the PQ disturbance measurements, equipment switching action data, and alarm events are integrated using an entity matching method. Finally, the distributed mining of association rules is designed to obtain strong association rules within the integrated data for describing the relationship between PQ features and event causes. The analysis results demonstrate good generalization performance.\n\nTo validate the effectiveness and practicability of the proposed approach, real grid data were taken as an example. The test results show that the proposed method can effectively analyze the relationship between the typical PQ disturbance features and its causes, which is invaluable for power quality improvement. Overall, the proposed data fusion technology-based modeling framework holds great promise for accurately identifying the root cause of PQ disturbances, thereby enabling effective PQ management."}, {"label": 0, "content": "Modeling of spectrum occupancy is important for better channel utilization, accurate spectrum sensing, and enhanced Quality of Service (QoS) to the primary user (PU) in a cognitive radio (CR) system. Existing models are highly dependent on the spatio-temporal variations of the PU activity as the statistical behavior of the PU changes with respect to the location, spectrum band, and the varying load time. In this work, a generalized Gaussian Mixture model (GMM) has been investigated for characterizing the spectrum occupancy of the PU in three spectrally different CR scenarios, viz. VHF/UHF band, GSM band, and ISM band. The goodness of fit performance of GMM is compared with the widely used spectrum occupancy model based on Beta distribution. Further, the robustness of GMM has been validated through learning based prediction via Recurrent Neural Networks (RNN), thereby proposing a hybrid approach of statistical and predictive modeling of spectrum occupancy for enhanced dynamic spectrum access."}, {"label": 1, "content": "IP source spoofing can result in DoS and DDoS attacks due to the lack of packet level authentication in the Internet. To prevent such attacks, source address validation filtering is deployed. In this study, we propose a new scheme to analyze the deployment of source address validation-filtering by using special path backscatter messages generated by the spoofed traffic. The absence of these messages from an AS over a long period can classify it as a non-spoofer AS. Using Caida's backscatter dataset, we provide a list of spoofer and non-spoofer ASes and provide a mathematical analysis for determining the time required before declaring an AS as a non-spoofer. We also use the normal approximation of binomial distribution to calculate confidence intervals for the proportion of ASes allowing spoofing and test hypotheses regarding spoofing activity."}, {"label": 0, "content": "In the future scenario of multiple wireless network coverage, the choice of vertical handoff decision algorithm will directly affect the continuity of the session, the mobility of the user, and seamless roaming under heterogeneous wireless networks. Therefore, the study of vertical handover related algorithms is the key to the success of various wireless access networks in the future. This paper proposes an optimized algorithm which combines two multiple attribute decision making (MADM) methods, the Entropy and the improved Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS). The Entropy method is applied to obtain objective weights and the improved TOPSIS method is used to rank the alternatives. The simulation results show that the proposed technique can make the distribution of weights more reasonable, and effectively reduce the number of handoffs."}, {"label": 1, "content": "The human heart is an essential organ, and accurate diagnosis of heart activities is crucial. To estimate heart parameters, various parameter estimation techniques have been developed. In this study, we utilized the Ensemble Kalman Filter (EnKF) and Particle Filter (PF) for dynamic assimilation of human heart parameters. EnKF and PF are modified filters specifically designed for state prediction of nonlinear systems with large data samples. A third-order mathematical heart model was employed to estimate three heart parameters, including movements of heart muscle fiber, tension in heart muscle, and electrochemical activity of the heart. The EnKF and PF were applied to the heart model, and different case studies were performed to observe the prediction accuracy by comparing the sum squared error values. The case studies were conducted with variable state and measurement noise values. The proposed approach demonstrated promising results in accurately predicting the human heart parameters."}, {"label": 0, "content": "Existing inefficient traffic light cycle control causes numerous problems, such as long delay and waste of energy. To improve efficiency, taking real-time traffic information as an input and dynamically adjusting the traffic light duration accordingly is a must. Existing works either split the traffic signal into equal duration or only leverage limited traffic information. In this paper, we study how to decide the traffic signal duration based on the collected data from different sensors. We propose a deep reinforcement learning model to control the traffic light cycle. In the model, we quantify the complex traffic scenario as states by collecting traffic data and dividing the whole intersection into small grids. The duration changes of a traffic light are the actions, which are modeled as a high-dimension Markov decision process. The reward is the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is employed to map states to rewards. The proposed model incorporates multiple optimization elements to improve the performance, such as dueling network, target network, double Q-learning network, and prioritized experience replay. We evaluate our model via simulation on a Simulation of Urban MObility simulator. Simulation results show the efficiency of our model in controlling traffic lights."}, {"label": 0, "content": "The relevance of the research is caused by the need to improve electromechanical systems with linear electromagnetic engines and accumulator food for realization of striking operations, technologies in construction, technical researches, etc. and assumes an effective use of power stock of accumulators and increase in period of operation of system without recharging the source. The main objective of the research is to estimate expediency and to define the possibility of realization of the system providing automatic correction of the electric power consumed from accumulators when the properties of loading change. It is shown that electromechanical systems with the linear electromagnetic engines equipped with sensors of limit provisions of an anchor and operated on the coordinate of a moving anchor are preferable to the technical embodiment. The advanced controlling system with the use of a programmable logic controller which without complication of the device of feedback provides automatic correction of the consumed energy from accumulators under changing properties of loading is offered."}, {"label": 1, "content": "Power distribution systems require continuous monitoring due to the increasing integration of renewable energy resources and the growing load demand. To enhance situational awareness and develop new monitoring algorithms, synchrophasors have been implemented in distribution systems. This paper proposes a voltage monitoring algorithm based on the synchrophasor-based linear state estimation method. The algorithm combines a set of early warning indicators and the BDS independence test to detect voltage instability in a timely manner while avoiding false alarms when the system is away from the stability boundary. In numerical studies conducted on the Quebec test feeder, the proposed method has shown to be effective in identifying and preventing voltage instability."}, {"label": 0, "content": "The learning methods have recently achieved great success for single image super-resolution. Due to the robust network, convolutional neural networks exhibit the state-of-the-art performance. In this paper, we propose a Dynamic Multi-mapping Convolutional Network (DMCN) that improve the SR performance. Instead of fixed kernels like Bicubic interpolation, we choose several dynamic filters to resize the LR input as the pre-trained module. Based on an end-to-end manner, more accurate and effective features from middle layers can be learned. Furthermore, multi-mapping module can deliver extra information and high-frequency details, which generates sharper high-resolution images from the network. Extensive quantitative and qualitative evaluations have been demonstrated that our algorithm improves effectively the resolution of the image."}, {"label": 0, "content": "With the development of science and technology, wind speed prediction plays an important role in smart grid. In order to improve the efficiency of wind power and ensure the safety of the new smart grid, the accurate prediction of wind power is very important. Because the variation of wind speed is nonlinear, and wind speed is influenced by many factors, Scholars all over the world have proposed a variety of forecasting methods to achieve a relatively accurate prediction of wind speed.In this paper, a novel method of wind speed prediction based on long short-term memory network with peephole (peephole LSTM) and wavelet decomposition is proposed. Firstly, the wind speed data are processed by wavelet dynamic decomposition, using the peephole LSTM structure to analyze the data sequence, after that a new model of peephole network is established. Through the forward training and the back-propagation algorithm update the weights iteratively, as a result get the optimal parameters.Selecting one month's data and simulating with MATLAB. High prediction accuracy is obtained by simulation. Predict the next 5 days through the first 25 days of training, and the MAPE is used to analyze prediction data. The feasibility of this method is proved by simulation results."}, {"label": 1, "content": "Despite having one of the most efficient transportation systems in the world, Singapore still faces regular congestion issues, especially during peak hours. Several factors contribute to this issue, and in order to combat it, we propose a simulator that is equipped with predictive travel times through congestion prediction. By implementing effective scheduling, the simulator seeks to improve bus utilization.\n\nTo achieve this goal, we have introduced a conceptual framework that integrates neural network models into simulation. This framework enables us to improve real-time supply based on several possibilities of demand. This paper outlines the steps taken to produce the simulator and discusses the evaluation of these models.\n\nBy using the simulator, we can evaluate and identify effective scheduling strategies that can alleviate congestion issues during peak hour periods. The predictive travel times offered by the simulator will also assist commuters in planning their journeys more efficiently.\n\nOverall, the simulator represents a significant step towards improving the transportation system in Singapore. By utilizing cutting-edge technology, we can proactively address congestion issues and improve the overall efficiency of the system."}, {"label": 0, "content": "This article is devoted to computer modeling of radio systems. The paper shows a method for automated solution of direct and inverse problems of mathematical modeling of nonlinear radio engineering systems. Nonlinear radio engineering systems perform important signal generation and conversion operations: amplification, modulation, detection, frequency multiplication and conversion. Modeling of such systems is necessary to improve the efficiency of their design and research. Existing modeling systems are application packages, that are limited to a set of valid tasks. They usually solve only direct problems (problems of system analysis). Modern radio engineering requires the ability to solve various direct and inverse problems of systems modeling within a uniform program complex. Therefore, the application packages should be replaced by complexes of automated modeling using elements of artificial intelligence. The article proposes a method of using functional grammars to construct the system of automated modeling of nonlinear semiconductor systems. The result of the scientific research is the logical-mathematical model using the terms of functional context-free grammars. The model substantiated by a logical conclusion using lambda calculus. Practical application of the method is demonstrated by examples of solving direct and inverse modeling problems."}, {"label": 1, "content": "Advertisements are an integral part of internet economics and culture, and video ads have become the most popular form of advertising in recent years. However, they can be expensive to create and are not always effective. This poses a problem for creators, advertisers, and ad platforms, who would like to know the effectiveness of a video ad before deployment.\n\nTo address this problem, we propose a novel algorithm that provides feedback before an ad is placed based on historical data about the effectiveness of other video ads. Our approach leverages a multi-modal mixture based algorithm to predict the effectiveness of the video automatically. We use both textual and visual information to learn a finite mixture model, which can provide more accurate predictions.\n\nIn our experiments, we tested our approach on a publicly available dataset and found that our algorithm outperformed other baseline approaches. By providing feedback on the potential effectiveness of a video ad before deployment, our approach can help creators, advertisers, and ad platforms make better-informed decisions about their advertising strategies."}, {"label": 1, "content": "Association rule mining is a crucial technique in data mining that enables the discovery of significant attribute relationships, which can be valuable for decision-making. Typically, association rule mining uses an item-set manipulation approach that relies on categorical data types. When numerical attributes are present in the dataset, discretization is necessary before rule mining can take place. However, most unsupervised data discretization methods currently available do not account for data distributions, necessitating the use of different methods and discretization settings to enhance rule mining results.\n\nIn this paper, we propose the use of TwoStep clustering for data discretization. Unlike traditional discretization methods, TwoStep automatically identifies the discretization intervals by considering the unique data distribution properties of each attribute. In our experiments, we evaluated the efficiency of the Apriori algorithm on four datasets after pre-processing each dataset using TwoStep and three other commonly used discretization methods. Our findings indicate that TwoStep produced the greatest number of high-quality rules compared to traditional discretization methods."}, {"label": 1, "content": "As fog computing continues to evolve and bring resources to the edge of the network, there is a growing need for automated placement processes that can deploy distributed applications efficiently. These placement processes must take into account the resource requirements of applications in a heterogeneous fog infrastructure, and must deal with the complexity of IoT applications that are linked to sensors and actuators. In this paper, we introduce four heuristics that address the problem of placing distributed IoT applications in the fog. Our approach combines these heuristics to deal with large scale problems and to make efficient placement decisions that minimize the average response time of placed applications. To validate our approach, we conducted comparative simulations of different heuristic combinations with varying sizes of infrastructures and applications. Through these simulations, we demonstrate the effectiveness of our approach in achieving optimal placement decisions in complex fog computing environments."}, {"label": 1, "content": "Many techniques have been proposed to solve the simultaneous localization and mapping (SLAM) problem, and among them, the Particle Filter (PF) is considered to be one of the most effective ways. However, the PF algorithm requires a large number of samples to approximate the posterior probability density of the system, which makes the algorithm complex. Furthermore, the judgment of resampling is imperfect. In light of these challenges, this paper proposes an improved PF algorithm that introduces a population diversity factor and a genetic algorithm into the process of resampling. \n\nThe improved PF algorithm uses the effective sample size and the population diversity factor to determine whether to resample the particle set. When resampling is needed, the genetic algorithm is utilized to optimize the particle set. The simulation results demonstrate that the estimation accuracy of the improved algorithm is superior to that of the traditional particle filter, not only in terms of accuracy but also in efficiency."}, {"label": 1, "content": "In order to effectively prevent cardiovascular disease, it is important to continuously monitor cross-clock ECG signals as well as the activity status of the patient. However, traditional ECG Holter devices are very heavy and cumbersome for patients to carry around, often requiring them to stay in the hospital for extended periods of time. To address these issues, a small ECG Holter device has been developed that can detect arrhythmias in real-time using an Android mobile application.\n\nThe device is equipped with a three-electrode sensor that directly obtains ECG signals, which are then transmitted to an Android smartphone via Bluetooth. Prepossessing ECG signal algorithms are implemented on an Arduino Device, while an Android mobile application is used for the analysis and classification of patient data to detect abnormal signs. The system was tested using data acquired from El-Monofia University, with 303 cases used for training and testing. Of these cases, 162 were normal and 141 were abnormally divided into 57 cases of Coronary Artery Disease, 36 cases of Old Anterior Myocardial Infarction, and 48 cases of Sinus tachycardia.\n\nExperimental results demonstrate that the presented system has significantly improved the accuracy of diagnosis for arrhythmias, as well as the identification of various anomalies during different activities. By utilizing this small and portable ECG Holter device, patients can enjoy greater flexibility and mobility while receiving continuous monitoring for cardiovascular disease prevention."}, {"label": 0, "content": "Facial expression and emotions recognition had attracted the attention of game developers and there are some game proposals, in which, by detecting faces, it is possible to detect who is playing and automatically load the players' profile. Likewise, the recognition of the players' facial expressions has been used to alter the expression of their avatars. In this work, we intended to study ways to use the recognition of a player's facial expression to change some parameters of a game, such as stimulate it when the player is bored or increasing its difficulty, for example, when it detects that the player is more stimulated and happy. This paper proposes a new approach for facial expression and emotion recognition, which divides the classification stage into two steps. First it is detected if it is a positive, a negative or a neutral emotion. Then it is applied a refinement step to recognize the particular positive (happiness or surprise) or negative (sadness, fear, anger or disgust) emotion. Our approach runs in real-time and was compared with the most common classification methods and has shown a great accuracy. We built a game to test our approach. The estimation of the player's emotional state was used to change certain parameters of the game, which increases or decreases the game difficulty."}, {"label": 0, "content": "Audiovisual speech synchrony detection is an important part of talking-face verification systems. Prior work has primarily focused on visual features and joint-space models, while standard mel-frequency cepstral coefficients (MFCCs) have been commonly used to present speech. We focus more closely on audio by studying the impact of context window length for delta feature computation and comparing MFCCs with simpler energy-based features in lip-sync detection. We select state-of-the-art hand-crafted lip-sync visual features, space-time auto-correlation of gradients (STACOG), and canonical correlation analysis (CCA), for joint-space modeling. To enhance joint space modeling, we adopt deep CCA (DCCA), a nonlinear extension of CCA. Our results on the XM2VTS data indicate substantially enhanced audiovisual speech synchrony detection, with an equal error rate (EER) of 3.68%. Further analysis reveals that failed lip region localization and beardedness of the subjects constitutes most of the errors. Thus, the lip motion description is the bottleneck, while the use of novel audio features or joint-modeling techniques is unlikely to boost lip-sync detection accuracy further."}, {"label": 1, "content": "This paper presents a novel method for detecting abnormal states in low signal-to-noise ratio (SNR) environments. The proposed method is based on the spiked population model and draws inspirations from random matrix theory to develop the theory and method of grid situation awareness using big data technology. \n\nTo implement this method, the paper first constructs a data source matrix and obtains a split-window matrix and its standard matrix, followed by acquiring the sample covariance matrix. The global SNR estimation is performed using the classical spectral estimation method, which is corrected by the Kaiser window function to derive a corresponding dynamic threshold. \n\nBy calculating the maximum eigenvalue of the sample covariance matrix and comparing it with the dynamic threshold, the proposed method enables situation awareness and early warning for interconnected power systems. \n\nIn order to validate the effectiveness of the proposed methodology, case studies were carried out on an IEEE 50-machine system, involving two main working conditions: abnormal load mutation and short circuit fault. The results showed that the proposed methodology has higher noise resistance compared with the traditional mean spectral radius based method and it is robust under incomplete information. \n\nOverall, the proposed method provides a robust and efficient solution for identifying abnormal states in low SNR environments, which has important implications for situation awareness and early warning in interconnected power systems."}, {"label": 1, "content": "This paper proposes a unified framework for designing sliding-mode control to stabilize delayed memristive neural networks (DMNNs) with external disturbances. The use of this framework enables the DMNNs to achieve finite-time and fixed-time stabilization through the selection of specific control parameter values. It has also been proven that the DMNN responses can reach the sliding-mode surface in finite and fixed time and remain on it. Additionally, the sliding-mode control can reject external disturbances. Numerical simulations and comparisons with related works confirm the effectiveness and superiority of the proposed approach."}, {"label": 0, "content": "Modular superconducting magnetic energy storage (SMES) is designed to achieve a modular layout of SMES through advanced control technology to solve the problem of SMES capacity limitation. In this paper, self-adaptive state of charge (SOC) feedback control is used to distribute the power of the modular SMES, the economy of the device and power efficiency and other factors are restrained as constraints. The multi-objective evolutionary algorithm is chosen as the solution algorithm to optimize the capacity of the modular SMES."}, {"label": 0, "content": "In this paper, we present a method using an unmanned aerial vehicle (UAV) to track the specified walking person and automatically capture a frontal photo of the target. The proposed method is composed by three parts: person detection and recognition, face detection and feature points localization, and vision based UAV control. In the person tracking part, we employ the deep neural network YOLOv3 for person detection and Locality-constrained Linear Coding (LLC) method to match the specified target person. In terms of frontal face perception, we use Multi-task Cascaded Convolutional Neural Networks (MTCNN) for face detection. Based on the vision information obtained from the two modules, the UAV can fly around the target person and obtain the target's frontal face image. The outdoor experiments based on a Parrot Bebop2 drone verify the effectiveness and practicability of our method."}, {"label": 1, "content": "We present our research on the innovation of education in two secondary schools in Rwanda. Our project focuses on developing spatial thinking skills through the use of Geographic Information and Communication Technologies (GeoICT). We used 2D, vector-based maps on Android tablets and commercial desktop GIS software to train teachers and students. Our goal was to develop new approaches for education innovation in Rwanda.\n\nWe conducted a Web-based survey to gather qualitative feedback from teachers and students about their experience with the innovation for education process. The survey focused on the broader opportunities spatial thinking provides, as well as feedback on the process itself. We also conducted extensive group interviews with teachers at both schools based on the data collected from the surveys.\n\nOur research shows that our innovation for education approach impacted teachers and students in four main ways. Firstly, it created broader societal benefits and individual opportunities. Secondly, the teachers identified education and societal benefits for problem solving and reasoning stemming from increased thinking ability, GeoICT training, and space-time thinking ability. Thirdly, teachers found new roles and identities for themselves through incorporation of spatial thinking-oriented curriculum and GeoICT training. Lastly, the importance of certificates and recognition artifacts as tools for students and teachers to establish their new competencies.\n\nOur focus on innovation for education, spatial thinking, and GeoICTs adds to the literature on broader technology-enhanced quality education delivery research on the value of spatial thinking and GeoICTs. Overall, our project shows the potential of innovation for education in developing new approaches to teaching and learning that benefit both individuals and society as a whole."}, {"label": 1, "content": "The paper introduces a novel methodology for automating the construction of software for solving heating system design problems. The approach is founded on the Model-Driven Engineering paradigm, which generates software through the use of formal models. The knowledge pertaining to heating systems, associated issues, and relevant software is formalized by ontologies. The software system is constructed using a heating system computer model, ontologies, and modern metaprogramming technologies. The approach solves the problem of separating the methods for problem-solving and heating system element models. In this respect, the methods are implemented as software components that are not linked to specific equipment models. As well, the heating system models are automatically compiled into software components. Throughout the software system construction process, software components that implement models and methods are dynamically integrated. Consequently, an automated software system geared towards addressing specific applied problems is produced. The approach is exemplified through the development of SOSNA software, which is utilized for designing urban heating systems."}, {"label": 1, "content": "The world of high performance computing is rapidly evolving, and demand for computing resources is increasing steadily. As more and more organizations invest in various kinds of cyberinfrastructure resources, it has become crucial to analyze expenses and investments thoroughly to optimize the return on investment. This paper presents an analysis of the ROI for three different kinds of cyberinfrastructure resources: the eXtreme Science and Engineering Discovery Environment (XSEDE), the NSF-funded Jetstream cloud system, and the Indiana University (IU) Big Red II supercomputer.\n\nOur analysis involved assigning financial values to services either by comparing them with commercially available services or by surveying the value of the resources to users. Despite the different natures of these resources, we found that the ROI for all three was greater than 1. This means that investors are getting more than $1 returned for every $1 invested. \n\nWhile there are multiple ways to calculate the value and impact of investment in cyberinfrastructure resources, measuring the short-term ROI provides a clear picture of the net positive impact these resources have on universities, research institutions, and the federal government. As such, it is becoming increasingly critical to analyze investments in these resources to ensure organizations make the most of their investments."}, {"label": 0, "content": "Railway detection task produces a large number of images, the lack of effective image classification method makes it difficult to analyze detection image deeply. Using convolutional neural networks (CNN) to realize railway image scene classification is an effective technical means. This paper propose a method to reduce the bias of database by Gradient-weighted Class Activation Mapping(Grad-CAM) to effectively improve scene classification accuracy, and achieve accuracy of 95.3%(top3) on Railway12 database. Our approach combines two insights: (1) Small quantity of railway scene database make it hard for CNN to achieve high performance, transfer pre-trained ImageNet-CNN to fine-tune on railway scene database (2) Introduce Grad-CAM visualization method to analyze model's classification pattern and intuitively show the possible bias of database, provide an intuitive way to reduce bias of dataset."}, {"label": 0, "content": "To address the limited speed and scale of the simulation of modular multilevel converter (MMC) in electromagnetic transient simulation program (EMTP), this paper put forward a MMC fast electromagnetic transient model based on rotation transformation. The crucial advantage of the rotation transformation is to reduce the fundamental frequency of the original signal via the rotation transformation of coordinate system. With proposed method implemented in the EMTP of MMC, the simulation step size can be relatively increased, and the fast electromagnetic transient simulation can be achieved. In this paper, the equivalent model of MMC switch function model expressed by the state equation is derived. Based on this, the novel MMC model based on rotation transformation is further derived combined with the concept of rotation transformation. Then, the state equation of proposed novel MMC model is calculated in Matlab, and the simulation results and simulation time-consuming are compared with which of 51-level topological MMC electromagnetic transient model established in PSCAD/EMTDC. The results of comparative analysis show that the simulation duration of the MMC model based on rotation transformation is far less than the traditional MMC electromagnetic transient model without sacrificing accuracy."}, {"label": 1, "content": "A major obstacle in digital pathology for renal diagnosis has been the lack of reliable annotated datasets that can serve as a reference for histological analyses. To address this challenge, this paper presents a novel medical image dataset named Glomeruli Classification Database (GCDB), which includes bifurcated renal glomeruli images classified into two binary classes of normal and abnormal morphology. Utilizing this dataset, we aim to explore appropriate deep neural network techniques in kidney tissue slide imaging to establish a state-of-the-art in this unexplored domain.\n\nThe paper focuses on categorizing normal and abnormal glomeruli, which are vital blood filtration units of the kidney. We compare the outcomes acquired via publicly available transfer learning models to supervised classifiers configured with image features extracted from the last layers of pretrained image classifiers. Surprisingly, our results show that transfer learning models, such as ResNet50 and InceptionV3, underperformed in this particular task. Instead, the Logistic Regression model that is augmented with features from the InceptionResNetV2 model demonstrated the most promising outcomes on the GCDB dataset.\n\nOverall, our study demonstrates the significance of dataset quality and the importance of exploring alternative techniques, as widely utilized models may not always perform optimally in specific domains. The GCDB dataset can serve as a valuable benchmark for future studies and pave the way for more accurate and reliable renal diagnosis using digital pathology."}, {"label": 1, "content": "Power System Stabilizers (PSSs) are devices designed to improve the stability of synchronous generators' systems, and they offer superior cost performance compared to other methods. The techniques used in PSSs have been the focus of the power industry and academic circles for many years. In this paper, we compare several advanced techniques which include Adaptive Fuzzy Control, Artificial Neural Network (ANN), Genetic Algorithm (GA) and Hybrid Artificial Intelligent (HAI), Fuzzy Logic and Particle Swarm Optimization (FLPSO) techniques. We compare these techniques based on their simplicity of prototype, robustness and response speed, complexity of algorithm, flexibility in implementation, and applicability to hybrid AVRs. The comparison shows that intelligent techniques can improve the performance of PSSs, making them more effective in damping out low-frequency oscillations by overcoming the limitations of conventional control methodologies. These intelligent techniques can be considered in the application of smart grids with large-scale grid-connected renewable energy power and random high power loads."}, {"label": 1, "content": "In this paper, we present a state-of-the-art brain-computer interface (BCI) system based on motor imagery and electroencephalogram (EEG) signals. The system utilizes supervised machine-learning algorithms to classify four different motor imagery tasks (left hand, right hand, foot, and tongue) using two categories of features extracted from pre-processed EEG signals: a high-dimensional feature set from 22 EEG channels and a feature set from two EEG channels (C3 and C4). \n\nTo enhance the performance of the signal classifiers, signal-to-noise ratio of EEG signals is essential. Thus, a pre-processing technique is applied on filtered EEG signals to eliminate contamination in the form of artifacts. Then, useful signal features are extracted from the artifact-free EEG signals, and relevant subsets with high predictive power are selected using feature selection techniques. \n\nFour signal classifiers, namely KNN, Regression tree, NB, and LDA, are applied on wavelet-based EEG signal features. The highest average classification accuracy of 73.06% and 72.95% was achieved using NB for both feature sets acquired from 2 and 22 EEG channels, respectively. \n\nOverall, our BCI system has demonstrated efficient discrimination of motor imagery tasks using non-invasive EEG signals and supervised machine-learning algorithms. This system has the potential to be used in various applications, such as rehabilitation and prosthetics, and can greatly improve the quality of life for individuals with disabilities."}, {"label": 1, "content": "Interface design flaws often lead to use errors in medical devices, and reporting medical incidents is crucial for understanding the causing factors. However, incidents are rarely reported, and users sometimes blame themselves rather than acknowledging issues with the device. Simulation-Based Medical Education (SBME) can offer appropriate training if incentives are in place, especially for professionals who interact with medical devices like ventilators and infusion pumps. Our SBME, integrated with PVSio-web, a graphical environment for human-computer system design, evaluation, and simulation, supports game-based learning. Our implementation was evaluated against a set of requirements from game-based medical simulators. Our open-source platform is highly accessible and easily customizable for specific use cases, like different hospitals with diverse sets of medical devices."}, {"label": 0, "content": "Estimation of machinery health, identification of defects and deviations of dynamic mechanical and technological equipment, formation and determination of parameters and criteria of technical condition are based on the measurement of parameters of oscillatory processes. As a rule, as a source of information vibroacoustic oscillatory processes are used. Such processes are characteristic both for vibroacoustic-based diagnostics, and for acoustic-emission control. Many state evaluation criteria are based on the measurement of process peak parameters. Given the random nature of the processes, estimating the measurement error and determining the peak values of signals becomes an urgent task. The task of the given work is presentation of methodical bases of estimation of measurement error and determination of peak values of vibroacoustic signals taking into account laws of distribution of their instantaneous values. In particular, the paper presents a solution to the problem of determining the dependence of the asymptotic error estimate of the sample quantile on the value of discrete values in the sample (time realization of the signal). This allows us to justify and verify the metrology of measuring instruments. Besides it allows to estimate reliability of measurements of peak values of the vibroacoustic signal."}, {"label": 1, "content": "The revolution in information technology has created vast collections of music on digital platforms, leading to a growing interest in accessing music based on various characteristics using information retrieval technologies. However, the lack of meta-tags and annotations has created a need for technologies to automatically extract relevant properties of music from audio. Automatic identification of percussion artists, especially in Carnatic music concerts with mridangam audio, is a challenging task for humans, let alone machines. Unlike speaker identification, where the voice is unique, identifying the timbre of percussion instruments is difficult. However, the distinctive characteristics of a musician are found in their playing style. To identify the artist, a single Gaussian mixture model (GMM) is built using tonic normalized cent-filterbank-cepstral-coefficients (CFCC) features from all musician data. Each percussion audio is converted into a sequence of GMM tokens, and sub-string matching between train and test data is used to identify the musician. In a dataset of 10 mridangam artists, the model identifies the artist with an accuracy of 72.5%."}, {"label": 0, "content": "To solve the simultaneous localization and mapping (SLAM) problem, many techniques have been proposed, and the Particle Filter (PF) is one of effective ways. However, the PF algorithm needs a large number of samples to approximate the posterior probability density of the system, which makes the algorithm complex. What's more, the judgment of resampling is imperfect. Based on this, an improved PF algorithm which introducing population diversity factor and genetic algorithm into the process of re-sampling is proposed in this paper. The effective sample size and the population diversity factor are used to determine whether to re-sampling. When re-sampling is needed, the genetic algorithm is used to optimize the particle set. The simulation result shows that estimation accuracy of the improved algorithm is better than that of traditional particles filter, not only in accuracy, but also in efficiency."}, {"label": 0, "content": "This paper is devoted to the description of the storage model and the search for sound sequences based on the theory of active perception. The theory of active perception is used to form an indicative description of the sound signal. The class of problems solved by the proposed model has a wide scope and includes, among other things, the search for musical plagiarism. It shows the possibility of creating on the basis of the proposed model a software system for identifying audio signals. The software implementation of the search model is made in the programming language R. To approbate this model computational experiments have been carried out, where the database size is 10,000 musical compositions and the search accuracy reaches 96%. The stability of the proposed model is also analyzed for distortion of the sought signal by noise. A comparison is made with similar existing systems in terms of search accuracy."}, {"label": 0, "content": "The most efficient solvers use composite procedures that adaptively rearrange computation algorithms to maximize simulation performance. A similar concept can be integrated into a process of electronic circuit analysis, where the combination of different algorithms allows scalability of simulation performance. In this paper, we propose a new adaptive internal solver based on Biconjugate gradient stabilized method for the iterative solution of nonsymmetric linear systems supplemented with incomplete LU factorization as an efficient replacement for the direct solver implemented in program Spice for solving large-scale circuits. We describe basic concepts of a simulation of electronic circuits with nonlinear time dependent devices and present implementation examples of the proposed methods. Optimal setting of the method and its application in program Spice is shown in comparison to other modern iterative solvers for nonsymmetric linear systems."}, {"label": 1, "content": "The Weapon-Target Assignment (WTA) problem is a critical aspect of air defense command and control, and thus it is imperative to find an efficient solution. This paper proposes an improved Artificial Fish Swarm Algorithm (AFSA) for this purpose. The AFSA is enhanced by incorporating Particle Swarm Optimization (PSO) to modify individual vision and a genetic operator to avoid the local extremum trap. The proposed algorithm is validated with cooperative air defense examples, and simulation results show an improved computational accuracy and rate of convergence in solving the WTA problem."}, {"label": 1, "content": "With the explosion of videos, action recognition has become a crucial research area. To address the problem of ConvNet dependence on multiple large scale datasets, this paper focuses on studying and investigating the 3D Convolutional Network. We propose a 3D ConvNet architecture that combines the original 3D-ConvNet features and foreground 3D-ConvNet features fused by static object and motion detection. Our approach is evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51. Experimental results demonstrate that with just 50% pixel utilization, the foreground ConvNet achieves a performance that is as satisfying as the original ConvNet. Furthermore, our feature fusion approach achieves an accuracy of 83.7% on UCF-101, which exceeds that of the original ConvNet."}, {"label": 1, "content": "With the increasing popularity of drones as delivery tools, there is a growing demand for a long-range and energy-efficient communication system for these devices. While LoRa has emerged as a possible option due to its power efficiency, its standard protocol LoRaWAN is not suitable for control-heavy and real-time applications. As such, this research evaluates the limitations of LoRaWAN as a secondary communication mode for drone delivery systems.\n\nThe study finds that LoRaWAN can be used for semi-real-time telemetry purposes, where it can send 10-20 bytes payload regularly with a minimum of 2-3 seconds interval. The system can achieve coverage of up to 8 km in an urban area, using the lowest spreading factor, while the percentage of packet loss with this configuration remains tolerable at around 5%.\n\nIn conclusion, while LoRaWAN may not be suitable for control-heavy applications for drone delivery systems, it can still be used as a secondary communication mode for telemetry purposes. The results of this research offer valuable insights for practical implementation of communication systems for drone delivery."}, {"label": 1, "content": "Virtual reality (VR) applications are designed to provide users with an immersive experience by making use of 360\u00b0 omnidirectional video content. However, such content must be projected onto a 2D image plane in order to use existing 2D video compression standards. This projection often leads to deformations in the content due to the different sampling characteristics of the 2D plane, which can negatively impact the motion models of current video coding standards. As a result, omnidirectional video is not efficiently compressible with current codecs.\n\nTo address this issue, a new geometry-based motion vector scaling method has been proposed. This method involves using a scaling technique based on the location of the 360\u00b0 video to adjust the motion information of neighboring blocks, ensuring uniform motion behavior in a certain part of the content. This uniform motion behavior provides optimal candidates for efficiently predicting motion vectors in the current block. \n\nExperiments conducted with the VTM test model of Versatile Video Coding (H.266/VVC) standard have shown that the proposed method can provide a bitrate reduction of up to 2.2%, with an average reduction of around 1% for high-motion content. This represents a significant improvement in the compression efficiency of omnidirectional video, making it more accessible for use in VR applications."}, {"label": 1, "content": "This paper proposes a cross-realm authentication scheme based on federated identity, which is aimed at addressing the issue of allowing Windows domain users to access cloud computing resources. The scheme is founded on the concept of declaration, utilizing a federated identity provider to replace the traditional gateway-based cross-realm authentication model. This results in users being able to access the cloud resources without having to re-authenticate. The SAML protocol is employed to exchange user identity information between different domains, thus ensuring the versatility and security of the system, and enabling secure communication between different security domains. Additionally, this paper presents the design of the key components of the three modules: claim provider, federated identity provider, and application service provider. Finally, the feasibility of the scheme is verified by testing it with the popular cloud platform OpenStack."}, {"label": 0, "content": "Estimating energy costs for an industrial process can be computationally intensive and time consuming, especially as it can involve data collection from different (distributed) monitoring sensors. Industrial processes have an implicit complexity involving the use of multiple appliances (devices/ sub-systems) attached to operation schedules, electrical capacity and optimisation setpoints which need to be determined for achieving operational cost objectives. Addressing the complexity associated with an industrial workflow (i.e. range and type of tasks) leads to increased requirements on the computing infrastructure. Such requirements can include achieving execution performance targets per processing unit within a particular size of infrastructure i.e. processing & data storage nodes to complete a computational analysis task within a specific deadline. The use of ensemblebased edge processing is identifed to meet these Quality of Service targets, whereby edge nodes can be used to distribute the computational load across a distributed infrastructure. Rather than relying on a single edge node, we propose the combined use of an ensemble of such nodes to overcome processing, data privacy/ security and reliability constraints. We propose an ensemble-based network processing model to facilitate distributed execution of energy simulations tasks within an industrial process. A scenario based on energy profiling within a fisheries plant is used to illustrate the use of an edge ensemble. The suggested approach is however general in scope and can be used in other similar application domains."}, {"label": 1, "content": "In this article, we address the issue of embedding computational and networking virtual resources across multiple Infrastructure-as-a-Service (IaaS) providers. This problem, known as Virtual Network Embedding (VNE), involves two phases: multicloud virtual network requests (VNRs) splitting and intracloud VNR segment mapping. This paper focuses on the splitting phase problem and proposes a splitting strategy based on two optimization approaches to improve the performance and quality of service (QoS) of mapped VNR segments. An Integer Linear Program (ILP) is used to formalize the splitting strategy as a mathematical minimization problem with constraints. The ILP model is solved using the exact approach and a metaheuristic approach based on Tabu Search (TS) is proposed to find optimal or near-optimal solutions in a polynomial solving time. Simulation results show that the proposed VNRs splitting approaches are efficient based on several performance criteria. The heuristic's solution costs are on average similar to the exact solution, with an average cost gap ranging between 0% and a maximum of 2.05%, and the computing time is significantly reduced. Compared to other baseline approaches, the acceptance rate and delay are improved by approximately 15%, while preventing QoS violations."}, {"label": 1, "content": "The active participation of end-users (prosumers) and demand response are expected to be vital components in the future power grids. With the utilization of renewable generations and microgrid architectures, market-based transactive exchanges between prosumers are becoming more prevalent. Transactive Energy Systems (TES) are a solution for dynamically balancing demand and supply across the electrical grid, employing economic and control mechanisms. However, effective transactive mechanisms are heavily reliant on a large number of distributed edge-computing and a communication architecture, which can leave them vulnerable to various threats due to the widespread use of digital devices. To detect possible anomalies within transactive energy frameworks, this paper employs a machine learning technique. For detecting anomalies in the market and physical system measurements, an ensemble-based methodology is implemented. The proposed technique has been found to perform well in detecting anomalies and triggering further investigation for root cause analysis and mitigation."}, {"label": 1, "content": "The recognition of Partial Discharge (PD) patterns is a crucial step in the condition monitoring of high voltage cables. It poses a challenge due to the high similarity of some of the PD types induced by cable defects. In recent years, deep learning based pattern recognition methods have shown tremendous potential in achieving impressive pattern recognition accuracy in fields such as speech and image recognition. The study presented in this paper proposes a Stacked Denoising Autoencoder (SDAE) based deep learning method for PD pattern recognition of different insulation defects of high voltage cables.\n\nThe laboratory constructed five types of artificial insulation defects of ethylene-propylene-rubber cables to produce 500 samples of PD signals for each defect type. To generate the 34 types of PD features that form the input parameters of the PD pattern recognition methods, the researchers carried out PD feature extraction.\n\nThe study elaborates on the principle and network architecture of the SDAE method and presents the flowchart of SDAE-based PD pattern recognition. Evaluating the experimental data of the five different types of PD signals demonstrated that the SDAE method achieved a recognition accuracy of 92.19%.\n\nThe paper concludes by comparing the proposed method with traditional pattern recognition methods namely Support Vector Machine (SVM) and Back Propagation Neural Network (BPNN). The results show that the SDAE-based method improves pattern recognition accuracy by 5.33% and 6.09% compared to the SVM and BPNN methods, respectively. The SDAE-based method has potential applications in PD signal recognition with high similarity."}, {"label": 0, "content": "In recent years, Deep Learning based method for 3-Dimension (3D) geometry perception tasks, like dense depth recovery, optical flow estimation and ego-motion estimation, is attracting significant attention. Inspired by recent advances in unsupervised strategies to learning from video datasets, we present a reasonable combination of constrains and a finer architecture, used for unsupervised ego-motion and depth estimation. Specifically, we introduce our effective neural networks Depth-Net (for monocular depth estimation) and Pose-Net (for ego-motion estimation), which are trained with monocular images. Depth-Net is proposed by us, improving the accuracy of estimation with as few parameters as possible. Finally, extensive experiments are implement on the KITTI driving dataset, proving our method outperforms some state-of-the-art results in unsupervised even supervised method."}, {"label": 1, "content": "Recently, tile-based video systems have emerged as a practical solution for dealing with the challenges associated with 360-degree video. One example is the HEVC-based viewport-dependent profile of MPEG OMAF, which allows clients to receive independently coded tiles of the video at varying resolutions, depending on the user's viewport, in order to enhance fidelity.\n\nDuring streaming, the client continuously adapts its tile selection, and combines the selected bits into a single merged bitstream to feed the video decoder. However, an open issue in this type of streaming scenario is how to distribute the encoding rates in a distributed encoding system. \n\nTo address this challenge, this paper presents a model for tile rate assignment based on the spatio-temporal activity of the video, in order to reduce the variability of quality distribution. Experimental results are reported, which demonstrate the effectiveness of the proposed model in improving the performance of a multi-resolution tiled streaming system."}, {"label": 1, "content": "Accurately estimating the state of a power system is crucial for secure operation in today's highly interconnected and nonlinear systems. However, inaccurate measurements make this process challenging. Therefore, an efficient and reliable state estimator must be able to detect and correct instances of bad-data during the estimation process. The Least Measurement Rejected (LMR) estimator has been identified as a robust estimation method with high computational efficiency and reliability, but its performance is highly dependent on the tolerance value assigned to each measurement.\n\nTo address this problem, this paper proposes an efficient method for selecting the tolerance value for the LMR estimator. This method ensures that the estimator is robust in terms of its estimation accuracy, while also providing greater computational efficiency. The performance of the proposed approach is compared with the Weighted Least Square (WLS) and Weighted Least Absolute Value (WLAV) estimators using the IEEE 30-bus system, under different bad measurement scenarios (single and multiple).\n\nOverall, the proposed method demonstrates superior estimation accuracy and computational efficiency compared to the WLS and WLAV estimators. Therefore, it is recommended for accurate and reliable state estimation in complex power systems."}, {"label": 1, "content": "Insulators play a crucial role in power transmission and transformation equipment in power systems. The identification of insulators is essential to evaluate the insulation status of insulators in computer vision. With the advancement of big data and cloud computing technologies, the application of insulator image recognition in power systems has become possible. Deep learning algorithms, such as the recent YOLO (You Only Look Once) convolutional neural network algorithm, have made terminal-to-terminal picture recognition achievable. \n\nIn this study, insulator image databases for train and test were established, and the images of training insulator images were preprocessed with the TensorFlow platform. With the YOLO algorithm, the training of the image database for 5 days was completed, and a good recognition result was achieved. The Fast R-CNN algorithm and the YOLO algorithm were compared in terms of identify speed and accuracy. \n\nFurthermore, the accuracy of insulator image recognition was defined based on relevant papers, and the factors that affect the accuracy of insulator image recognition were discussed. It was concluded that the accuracy of identification increases with the increase in the number of training insulators, which is the next step in the identification of insulators. Overall, this study shows the potential of deep learning algorithms in insulator image recognition and their future application in power systems."}, {"label": 1, "content": "This paper discusses the key technologies in multi-terminal DC power distribution technology engineering, based on the background of the DC normal education project in the Jinji Lake core area of Suzhou Industrial Park. Firstly, the Suzhou four-terminal DC project was briefly introduced, and the selection principle of critical electrical equipment, including converter valves, bridge arm reactors, link transformers, and DC monitoring and protection systems, were analyzed. Secondly, a simulation model for Suzhou's four-terminal DC distribution system was developed on the RTLAB real-time simulation platform. Finally, single-ended converter startup, four-terminal stable operation, active power step, and reactive power step were all simulated using the RTLAB model. The simulation results showed that the four-terminal DC grid model delivered good steady-state performance and active/reactive power step response performance, providing useful references for future DC distribution network construction."}, {"label": 1, "content": "We present NavREn-RL, an innovative approach that makes use of end-to-end reinforcement learning (RL) for unmanned aerial vehicles to navigate through indoor environments. In order to meet the constraints of small-sized, cost-efficient drones with minimal sensing capabilities, we have designed a suitable reward function that takes into account these factors. To aid convergence, we have integrated a collection of expert data and knowledge-based data aggregation into the RL process.\n\nWe conducted experimentation using a Parrot AR drone in various indoor arenas, and compared the results with other baseline technologies. Our approach demonstrated effective obstacle avoidance and successful navigation across different arenas. A video of the drone navigating using NavREn-RL can be viewed at https://youtu.be/yOTkTHUPNVY."}, {"label": 1, "content": "This research paper discusses the utilization of a protective clothing detection system, employing the S-HOG+C operator, specifically designed for substation workers. The system is intended to cater to the pragmatic needs in a complex environment. The image is divided into three cells, comprising a helmet, upper, and lower body, keeping the characteristics of the objects in mind. The HOG and HOC features of the three cells are extracted and used to train a classifier. In our study, we have utilized a linear Support Vector Machine. We have employed the S-HOG+C operator to improve the detection accuracy of protective clothing worn by substation workers. We have used three evaluation indicators to analyze the performance of the S-HOG+C operator. Our experimental results indicate that the S-HOG+C operator performs better than the HOG+C operator concerning protective clothing detection for substation workers. This project was supported by the Science and Technology Project of the Guangdong Power Grid Co., Ltd. (GDKJXM20162351)."}, {"label": 0, "content": "In this paper, we present a state-of-the-art motor imagery brain computer interface system (BCI) based on non-invasive approach in the form of electroencephalogram (EEG) with an objective of evaluating the performance of supervised machine-learning algorithms applied on features extracted from pre-processed EEG signals. Two categories of features were utilized namely a high dimensional feature set extracted from 22 EEG channels and a feature set extracted from two EEG channels (C3 and C4). Four signal classifiers namely KNN, Regression tree, NB and LDA are applied on wavelet-based EEG signal features for discrimination of four classes of motor imagery (MI) tasks (left hand, right hand, foot and tongue). Efficient discrimination of motor imagery tasks is significantly dependent on signal-to-noise ratio of EEG signals to enhance the performance of signal classifiers. A pre-processing technique is firstly applied on filtered EEG signals to remove contamination in the form of artifacts. Then, useful signal features are extracted from artifact free EEG signals, whereby relevant subsets with high predictive power are selected using feature selection technique. The best features subsets are fed into signal classifiers for classification purposes. A highest average classification accuracy of 73.06% and 72.95% was achieved using NB while classifying both features acquired from 2 and 22 EEG channels respectively."}, {"label": 0, "content": "Heterogeneous IoT enabled networks generally accommodate both jitter tolerant and intolerant traffic. Optical Burst Switched (OBS) backbone networks are handling the resultant volumes of such traffic by transmitting it in huge size chunks called bursts. Because of the lack or limited buffering capabilities within the core network, contentions as well as congestion may frequently occur and thus affecting overall supportable quality of service (QoS). Both contention and congestion will be characterized by frequent burst losses especially when traffic levels surge. The congestion is normally resolved by way of deflecting contending bursts to other less congested paths even though this may lead to differential delays incurred by bursts as they traverse the network. This will contribute to undesirable jitter that may ultimately compromise overall QoS. Noting that jitter is mostly caused by deflection routing which itself is a result of poor wavelength and routing assigning, in this paper we propose a controllable deflection routing (CDR) scheme that allows the deflection of bursts to alternate paths only after controller buffer preset thresholds are surpassed. In this way bursts intended for a common destination are always most likely to be routed on the same or least cost path end-to-end. We describe the scheme as well as compare its performance to other existing approaches. Both analytical and simulation results overall show that the proposed scheme does lower both congestion as well as jitter, thus also improving throughput as well as avoiding congestion on deflection paths."}, {"label": 0, "content": "Cardiovascular diseases are the primary cause of deaths in the world. Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. Due to its high prevalence and associated risks, early detection of AF is an important objective for healthcare systems worldwide. The growing demand for medical assistance implies increased expenses, which could be limited by implementing ambulatory monitoring techniques based on wearable devices, thus, reducing the number of people requiring observation in hospitals. One of the main challenges in this context is related to the large amount of data from patients to be analyzed, which points to the suitability of using computational intelligence techniques for it. The selection of the features to be extracted from data plays a key role in order for any classifier of heart rhythm to provide good results in this regard. This paper demonstrates that it is possible to achieve an accurate detection of AF using a very low number of relatively simple features extracted from photoplethysmographic signals, enabling the use of affordable wearable devices (with scarce processing and data storage resources) with this purpose over long periods of time. This fact has been validated in experiments using data from real patients under medical supervision."}, {"label": 0, "content": "In this paper a performance of conventional adaptive finite impulse response filters and a feed forward multi-layer neural network is examined for predicting the future values of the slip angle of a vehicle. The obtained results depending on a number of inputs and a prediction horizon are compared in terms of prediction error and a required number of operations for executing the algorithms."}, {"label": 0, "content": "Combining data from disparate sources enhances the opportunity to explore different aspects of the phenomena under consideration. However, there are several challenges in doing so effectively that include, inter alia, the heterogeneity in data representation and format, collection patterns, and integration of foreign data attributes in a ready-to-use condition. In this study, we have designed a scalable query-oriented distributed data integration framework, Confluence, that also dynamically generates accurate interpolations for the targeted spatiotemporal scopes along with an estimate of the uncertainty involved with such estimation in case of spatiotemporal misalignment of datapoints. Confluence efficiently orchestrates computations to evaluate spatiotemporal query joins and facilitates distributed query evaluations with a dynamic relaxation of query constraints. Query evaluations are locality-aware and we leverage model-based dynamic parameter selection to provide accurate estimation for data points. We have included empirical benchmarks that profile our system in terms of accuracy, latency, and throughput at scale and also demonstrate its improvement in performance in a distributed cloud computing environment over GeoMesa, a Spark-based geospatial analytics framework."}, {"label": 1, "content": "Orthogonal Frequency Division Multiplexer (OFDM) is a modern modulation scheme that is utilized for transmitting signals through power line communication (PLC) channels. It is preferred due to its robustness against some of the known PLC problems. However, the impulsive noise (IN) greatly affects the scheme and often leads to corrupted transmitted bits. Several impulsive noise error correcting techniques have been proposed and implemented to remove IN in OFDM systems. Nevertheless, these techniques are not without limitations and require a significant amount of signal to noise ratio (SNR) power to function effectively. \n\nIn this paper, a new approach was introduced leveraging three well-known artificial neural network techniques - Levenberg-Marquardt, Scaled Conjugate Gradient, and Bayesian Regularization - to develop an effective impulsive-noise error-correcting technique. The findings indicate that both Bayesian regularization and Levenberg-Marquardt ANN techniques are effective in removing the impulsive noise present in OFDM channels utilizing the least SNR power."}, {"label": 1, "content": "In this article, we address the issue of combating jamming in a fading channel environment by employing channel selection techniques. Unlike previous works that overlooked the impact of channel fading, our approach employs a Markov channel model for capturing the variable transmission rate due to channel fading. We frame the channel selection problem as a Markov decision process and propose an online reinforcement learning algorithm (Q-learning) for intelligent channel selection. Simulation results show that our proposed algorithm significantly outperforms sensing algorithms, as it learns both the jamming pattern and channel condition to select optimal channels for data transmission. To validate our algorithm's applicability, we develop a dynamic spectrum anti-jamming testbed based on the USRP platform for indoor environments. Our results show the algorithm's effectiveness in a real-life scenario."}, {"label": 1, "content": "This paper proposes an improvement to the basic genetic algorithm by constructing a more suitable fitness function and optimizing genetic operators to address its tendency to converge slowly and get local optimum easily. The improved genetic algorithm is then utilized to select the optimal access network for a heterogeneous integrated wireless network, taking into consideration the Quality of Service (QoS) requirements of different types of businesses. Through simulation, the improved genetic algorithm is shown to outperform both the basic genetic algorithm and a combination of the analytic hierarchy process (AHP) and the technique for order preference by similarity to an ideal solution (TOPSIS) in finding a network that better meets different QoS requirements and has higher fitness."}, {"label": 1, "content": "Energy consumption is a major concern for data centers as it constitutes a significant proportion of their operational costs. With the fast growth of utility-based IT services, Cloud data centers have become increasingly common and their energy usage is a key issue. These data centers use load balancing algorithms to allocate resources efficiently, but the live migration of Virtual Machines (VMs) during this process can result in Service Level Agreement Violations (SLAVs) and low Quality of Service (QoS). In this paper, an energy aware VM selection policy is proposed to minimize the number of migrations and decrease SLAVs. \n\nLoad balancing has three stages: detecting over-and under-utilized hosts, selecting one or more VMs for migration, and finding destination hosts. The focus of this research is on the VM selection stage of CPU load balancing. The proposed algorithm considers the CPU utilization of VMs on each host and any linear correlation between their usage. It was evaluated on two real Cloud data sets and compared to a benchmark policy that only considers minimum migration time for VM selection. \n\nThe results showed that the proposed algorithm decreases SLAVs by 66%, ESV (SLAVs \u00d7 energy consumption) by 64%, and the number of \"re over-utilized\" hosts by 81% when the CPU usage of VMs in a data set are highly correlated. This highlights the importance of an energy aware VM selection policy that considers the overall impact on energy consumption and SLAVs."}, {"label": 1, "content": "The wind power curve is a crucial tool in characterizing wind power output features and is essential for wind power planning and operation research. However, the wind power curve is a high dimension matrix data that possesses local properties. Therefore, an effective technique needs to be found to reduce the dimension of the curve. This paper introduces the latest techniques of artificial intelligence and deep learning to explore a new method for reducing the dimension of the wind power curve. The typical deep learning framework's convolutional autoencoder is redesigned, and it learns feature representation from massive historical data. The experimental result demonstrates that the proposed autoencoder is better suited for the wind power curve dimensionality reduction study."}, {"label": 0, "content": "Ad-hoc Mobile Cloud (AMC) came up as a result of the need to alleviate the inadvertent and incessant connectivity challenges that are experienced in Mobile Cloud computing (MCC). Thus, mobile devices teamed up in groups to share their resources among one another which are majorly Web Services, Storage and Computing resources. However, potential participants in AMC often feel a level of insecurity as they envisaged loss of control over various personal and confidential data in their mobile devices which can occur in the course of sharing of resources. Therefore, AMC as a service provisioning paradigm for mobile device users has a need for an implementation of a trust management system to serve as a protection and guarantee of confidence for intended AMC participants. This study proposed a Multi-Criteria Trust Management system (MCTM) architecture for AMC to protect the interest of various participating mobile devices. This system provides an avenue to identify trustworthy mobile devices to whom they can carry out resource sharing and as well alert or notify them of any malicious act that could have happened. We carried out a simple evaluation procedure with a number of incorrect service matches in the course of responding to a query as well as comparing the level of precision attained by the proposed architecture with already existing research. The proposed system proves to give a maximum confidence to mobile users to show high interest in participating in the AMC system."}, {"label": 0, "content": "This research proposes a significant reduction in the processing time to solve the concurrent AC multistage transmission network expansion and reactive power planning problem with security constraints, by an innovative search space reduction strategy. Initially, the concurrent planning problem is modeled as a mixed-integer linear programming (MILP) problem, using an AC branch flow formulation to represent the steady-state operation of the transmission network. The innovative strategy consists of obtaining a stage-by-stage solution pool of the MILP model as a static problem, to identify the significant candidate lines. Therefore, those lines considered insignificant would not be considered as candidates in the multistage problem. Then, with the updated database, it is possible to solve the multistage MILP problem with a reduced search space. The evaluation of the proposed methodology is done using the IEEE 24- and 118-bus test systems, showing the performance of the proposed methodology."}, {"label": 1, "content": "Gait recognition involves mapping a sequence of gaits to a known identity in a system, while gait authentication determines whether a given gait sequence belongs to the claimed identity. Typically, a gait authentication system uses a feature representation, such as a gait template, to extract features, and applies a transformation to obtain a discriminant feature set. The current literature frequently employs the Euclidean distance as a threshold to differentiate between legitimate subjects and impostors.\n\nIn this article, we propose an alternative method that uses the posterior probability of a Bayes' classifier in place of the Euclidean distance for gait authentication. Specifically, we apply this framework to template-based gait feature representations and evaluate its performance using the standard CASIA-B gait database.\n\nOur findings suggest that the Bayesian posterior probability outperforms both the Euclidean distance method and the current state-of-the-art cosine distance. This improvement in performance offers significant promise for future developments in gait authentication technology."}, {"label": 0, "content": "In article describes an intellectual-information system that will be used to make managerial decisions on the operation of technical personnel of power plants and similar process facilities. In particular, the results of its work will help to make a decision on the appropriateness of upgrading the skills of technical personnel. The methods of intellectualization used to determine the level of knowledge based on odd logic allow not only to assess the level of knowledge, but also to automate the process of its consolidation and enhancement. The use of this method allows the training of technical personnel on the job. The method of intellectualization of an estimation of a level of knowledge of the basic theory of calculation of optimum feeding parameters of regulating devices of automatic control systems is described. Solving this problem in the example of automation evaluation on the parametric synthesis of one-loop, cascade and combined control systems. Control of the level of knowledge for parametric synthesis determined by the correct choice of a point on a graph of the amplitude-phase characteristics and D-partition graphs. The adequacy of established fuzzy output system tested in a program FuzzyTECH."}, {"label": 1, "content": "In recent years, the application of micro-phasor measurement unit (\u03bcPMU) and advanced metering infrastructure (AMI) in distribution networks has enabled the acquisition of large amounts of measurement data. This data has made it possible to estimate the parameters of distribution networks containing T-connection lines. This paper proposes a parameter estimation method for T-connection lines based on \u03bcPMU and AMI. Firstly, a T-connection line model is constructed taking into account the actual installation of the \u03bcPMU and AMI in the distribution network. Virtual measurements are then determined using real-time measurement data from the \u03bcPMU and AMI. Secondly, the measurement function equation and Jacobi matrix are listed based on the augmented state estimation method. Thirdly, a weighted least square model is established with voltage phasor and T-connected line parameters as augmented state variables, based on multi-interval \u03bcPMU and AMI measurement data. Finally, the proposed algorithm is validated using four T-connection line zones of the NEV test feeder, and the accuracy of the estimation parameters of different lines is analyzed. This method overcomes the difficulty of parameter estimation for T-connection lines in traditional distribution networks with insufficient data, and lays the foundation for real-time optimization operation of distribution networks."}, {"label": 0, "content": "Local binary descriptors, such as local binary pattern (LBP) and its various variants, have been studied extensively in texture and dynamic texture analysis due to their outstanding characteristics, such as grayscale invariance, low computational complexity and good discriminability. Most existing local binary feature extraction methods extract spatio-temporal features from three orthogonal planes of a spatio-temporal volume by viewing a dynamic texture in 3D space. For a given pixel in a video, only a proportion of its surrounding pixels is incorporated in the local binary feature extraction process. We argue that the ignored pixels contain discriminative information that should be explored. To fully utilize the information conveyed by all the pixels in a local neighborhood, we propose extracting local binary features from the spatio-temporal domain with 3D filters that are learned in an unsupervised manner so that the discriminative features along both the spatial and temporal dimensions are captured simultaneously. The proposed approach consists of three components: 1) 3D filtering; 2) binary hashing; and 3) joint histogramming. Densely sampled 3D blocks of a dynamic texture are first normalized to have zero mean and are then filtered by 3D filters that are learned in advance. To preserve more of the structure information, the filter response vectors are decomposed into two complementary components, namely, the signs and the magnitudes, which are further encoded separately into binary codes. The local mean pixels of the 3D blocks are also converted into binary codes. Finally, three types of binary codes are combined via joint or hybrid histograms for the final feature representation. Extensive experiments are conducted on three commonly used dynamic texture databases: 1) UCLA; 2) DynTex; and 3) YUVL. The proposed method provides comparable results to, and even outperforms, many state-of-the-art methods."}, {"label": 1, "content": "Formal verification is an essential technique to check if a gate-level circuit correctly implements the given specification model. In the case where the equivalence check fails, bugs or errors in the circuit must be debugged. Post-verification debugging involves identifying a set of nets and computing the corresponding rectification functions at those locations. This paper addresses the problem of post-verification debugging and correction of finite field arithmetic circuits, focusing on single-fix rectification.\n\nThe authors use an equivalence checking setup modeled as a polynomial ideal membership test. By analyzing the ideal membership residue, potential single-fix rectification locations are identified. The team uses Nullstellensatz principles to ascertain if a single-fix rectification can be applied at any of these locations. If so, the team derives a rectification function using the synthesis of an unknown component problem. The Gr\u00f6bner basis algorithm is used both as a decision and quantification procedure.\n\nThe approach is demonstrated over various finite field arithmetic circuits, showing effectiveness, whereas SAT-based approaches are infeasible."}, {"label": 1, "content": "Chaotic systems, such as Lorenz systems or logistic functions, are widely recognized for their rapid divergence property, meaning that they exhibit unpredictable output values for only a small variation in the initial conditions. Due to this feature, Lorenz systems are particularly suitable for use in cryptographic applications, particularly in key agreement protocols and encryptions, as they guarantee secure data transmission. Even though these systems are hard to predict in the short term, they exhibit long-term deterministic behaviors that could be adapted to form a known shape over a significant amount of time.\n\nIn this research, we propose a dynamic device authentication method that utilizes the divergence and convergence features of Lorenz systems. The proposed scheme is aimed at proving the authenticity of a device by confirming its position within a predetermined trajectory of a given Lorenz chaotic system. This authentication procedure is secure since an attacker cannot predict the short-range function output values yet relatively simple for a verifier to validate because the function is deterministic.\n\nMoreover, the proposed device authentication scheme has additional significance in multi-verifier situations, as it eliminates the need for each mobile device to initiate a separate authentication procedure each time it switches among base stations. Instead, the device can demonstrate the consistency of its chaotic behavior iteratively, making the process efficient in terms of execution time and computing resources."}, {"label": 1, "content": "This paper investigates inter-cell interference suppression in a multi-cell heterogeneous network system. Using game theory, the model of frequency and timeslot selection is established. Our analysis reveals that the proposed resource reuse model is a potential game, with the ability to enhance the network's throughput."}, {"label": 1, "content": "In recent years, the risk posed by cyber-attacks to the stability of power supply has become a pressing concern. To detect such threats, data-driven approaches that utilize machine learning techniques are often employed. However, in order to achieve effective detection rates, these approaches must have a comprehensive understanding of power system operations, both under normal conditions and when contingencies such as short-circuit faults arise.\n\nOne major challenge faced by machine learning models is the scarcity of contingency data in the historical measurement database, which means that such data appears far less frequently than normal data in the training datasets. To overcome this challenge, the authors of this paper propose a data preparation method that combines Random Matrix Theory (RMT) and Adaptive Synthetic Sampling (ADASYN) algorithm.\n\nRMT is used to extract rare contingency data from the historical operating database, while ADASYN synthetically generates new contingency examples based on the existing ones. Case studies show that this method can enhance the learning of the statistical characteristics of power system operating data and improve the detection of cyber-attacks.\n\nOverall, the proposed combined method offers a promising way to address the challenge of detecting cyber-attacks and can contribute to enhanced security in the power supply sector."}, {"label": 1, "content": "Today, numerical controls (CNC) are the standard for controlling machine tools and industrial robots in production, enabling highly flexible and efficient production, especially for frequently changing production tasks. However, a major limitation with numerical controls is the need to analytically describe curves for the calculation of the position setpoints and jerk limitation. This leads to a performance overhead and limits the NC channel's calculation of new position setpoints, resulting in a drop in production speed and longer production times.\n\nTo overcome this challenge, we propose a new approach in this paper. Our approach is based on using deep generative models that allow for the direct generation of interpolated toolpaths without the calculation of continuous curves and subsequent discretization. The generative models are trained to create curves of certain types such as linear and parabolic curves or splines directly as discrete point sequences. This approach is highly feasible with regard to its parallelization and reduces the computing effort within the NC channel.\n\nOur preliminary results with straight lines and parabolic curves demonstrate the feasibility of this new approach for the generation of CNC toolpaths. With deep generative models, we can create highly flexible and efficient production systems without the performance overhead associated with traditional numerical controls. This approach can significantly improve production speed and decrease production times, making it a highly promising solution for the manufacturing industry."}, {"label": 1, "content": "In this paper, the authors propose an adaptive sub-synchronous resonance (SSR) damping controller for AC/DC hybrid system with high penetration wind. The controller uses Kalman filters for online estimations and tracking of operating points caused by variable wind power output. \n\nTo design the controller, first, a supplementary damping coordinated controller of doubly fed induction generator (DFIG) and high-voltage direct current (HVDC) is designed using multiple input multiple output (MIMO) linear matrix inequalities (LMIs). Then, an adaptive method using Kalman online estimations is used to track the operating points caused by variable wind power output. \n\nThe proposed adaptive SSR damping control method is tested using an AC/DC hybrid system integrated with wind power generation. The characteristic of variable SSR modes with wind power outputs is investigated. The simulation results demonstrate that the proposed method not only tracks the operating condition, but also provides robust and effective control in the case of large fluctuation of wind power output."}, {"label": 0, "content": "Energy related costs and environmental sustainability present a significant challenge for cloud computing practitioners and the development of next generation data centers. In efficient resource management is one of the greatest causes of high energy consumption in the operation of data centers today. Virtual Machine (VM) placement is a promising technique to save energy and improve resource management. A key challenge for VM placement algorithms is the ability to accurately forecast future resource demands due to the dynamic nature of cloud applications. Furthermore, the literature rarely considers placement strategies based on co-located resource consumption which has the potential to improve allocation decisions. Using real workload traces this work presents a comparative study of the most widely used prediction models and introduces a novel predictive anti-correlated VM placement approach. Our empirical results demonstrate how the proposed approach reduces energy by 18% while also reducing service violations by over 47% compared to some of the most commonly used placement policies."}, {"label": 1, "content": "Currently, power system networks face a significant challenge posed by the injection of false data (FDI) which adversely affects state estimation and reduces system reliability. However, high frequency and accurate data collected by the WAMS system can effectively deter FDI attacks. This study aimed to optimize the configuration of PMU to address the problem of FDI. By ensuring the overall observability of the system and taking the zero injection node into account, PMU configuration was optimized to mitigate the effect of FDI attacks, improving state estimation data accuracy to the maximum extent possible. The IEEE14, IEEE30, and IEEE57 standard nodes were used as examples, demonstrating the improved data accuracy and feasibility of the proposed method with the most minimal number of PMUs."}, {"label": 1, "content": "This paper explores the technologies that increase energy efficiency in robotized productions. The focus is on optimizing the trajectories of industrial manipulators to reduce energy costs. To achieve this, a trajectories optimization module is designed and integrated into the informative space of the robotized production section. The module analyses the operating programs of the CAM-system, identifies energy costs, and proposes corrective actions to improve energy efficiency while keeping within technology limits. The computing part of the module employs intelligent algorithms to identify non-linear dependencies and generate output decisions. The results are verified through experiments conducted at a robotized machining section with the KUKA KR-60 industrial robot."}, {"label": 0, "content": "The interest in neural networks has increased significantly, and the application of this type of machine learning is vast, ranging from natural image classification to medical image segmentation. However, many users of neural networks tend to use them as a black box tool. They do not access all of the possible variations, nor take into account the respective classification accuracies and costs. In our work, we focus on multiclass image classification, and in this research, we shed light on the trade-offs between systems using a single multiclass classification and multiple binary classifiers, respectively. We have tested these classifiers on several modern neural network architectures, including DenseNet, Inception v3, Inception ResNet v2, Xception, NASNet and MobileNet. We have compared several aspects of the performance of these architectures during training and testing using both classification styles in terms of classification speed and several classification accuracy metrics. Here, we present the results from experiments on a total of 99 networks: 11 multiclass and 88 individual binary networks, for an 8-class classification of medical images. In short, using multiple binary classification networks resulted in a more robust model (less variance) for the task at hand. However, on average, such a multi-network style performed the classification 7.6 times slower compared to a single network multiclass implementation. These collective findings show that both approaches can be applied to modern neural network structures. Several binary networks can often give more robust and increased classification accuracy, but at the cost of classification speed and resources consumption."}, {"label": 0, "content": "The purpose of this article is to analyze the power of WeChat and question ourselves on the hypothetical expansion. We discovered WeChat by living in China. For us, it is an application which gather everything. It is Instagram, Facebook, Twitter in just one app. But since we are living in a world with so many differences and different laws, we want to know if this app can really encounter the same success in different countries. WeChat has a big place in the life of Chinese people and this is something which is very interesting. Everything is around this app. The payment method, the social life and the way of interacting with people in general. We decided to split our study in three parts in order to answer the problematic."}, {"label": 0, "content": "Studying the propagation mechanism of cascading failure and identifying the vulnerable lines in the power grid is of great significance for the prevention control of cascading failure. The existing works pay more attention to vulnerable lines and high-risk path, which are hard to explain the overall propagation characteristic of cascading failure. This paper applies sequential pattern mining technology to cascading failure analysis. The concept of cascading failure pattern (CFP) is defined firstly, and a cascading failure pattern mining algorithm (CFPMA) based on PrefixSpan is proposed. The relevance of lines is then calculated based on the result of mining algorithm. Test result on the IEEE 39-bus system shows that the proposed method can find out the CFP which can clearly show the overall propagation mechanism of cascading failure. Furthermore, test results under different power flow snapshots indicate that the CFP doesn't change with system status."}, {"label": 0, "content": "An increasing number of wireless intelligent equipment is applied to ICS networks. However, it is virtually impossible to use regular encryption methods and security patches to enhance the security level of legacy equipment in ICS networks due to weak computing and storage capabilities of the equipment. To address these concerns, a hybrid-augmented device fingerprinting approach is developed to enhance traditional intrusion detection mechanisms in the ICS network. Taking the advantage of the simplicity of the program process and stability of hardware configurations, we first measure inter-layer data response processing time, and then analyze network traffic to filter abnormal packets to achieve the intrusion classification and detection in ICS networks. The device fingerprinting- based intrusion classification and detection approach is evaluated using the data collected from a lab-level micro-grid, and forgery attacks and intrusions are launched against the proposed method to investigate its robustness and effectiveness."}, {"label": 0, "content": "Considering the actual operation conditions and the general conditions of simulation calculations, this paper presents a simplified critical AC-DC system voltage interaction factor (SCADVIF) index and a method to fast evaluate the commutation failure risk of multi-infeed HVDC systems. The method still does not require modeling power system components in detail for dynamic simulation. By calculating and comparing all the ADVIFs and SCADVIFs of an AC/DC system, the receiving-end AC buses at which faults are applied may cause commutation failure of multi-infeed HVDC system can be found quickly: if ADVIFjm \u2265 SCADVIFjm, a fault occurring at receiving-end system AC bus m would result in commutation failure at HVDC j. In this way, it is possible to quickly identify all AC buses where a three phase fault would cause single or multiple commutation failures. The validity and accuracy of the proposed approach are demonstrated by comparing with simulations results using an actual planning large power grid. The proposed method instead of time-domain simulation analysis tool greatly reduces the computing time of researchers, and improves their work efficiency. The proposed index and method can be widely used in the planning, design and operation of AC/DC power grids."}, {"label": 1, "content": "In this paper, a new ultra low power wake-up system is proposed for use in synchronizing and triggering environmental and military Internet of Things (IoT) wireless networks. The system works by analyzing different frequency bands and generating a separate output interrupt for each of them, allowing for the detection of signals in multiple bands. The average power consumption of this wake-up detector is only 34\u03bcW, making it incredibly efficient.\n\nFurthermore, the paper presents applications of the wake-up system in military and environmental contexts. For example, it can be used for data retrieval in a military smart dust using a drone on a battlefield. It can also be used for activation and synchronization of an environmental wireless sensor network, even in difficult conditions such as a dense rain forest. To achieve this, a multi-frequency light pulses burst propagation algorithm is used to trigger the network in just 50ms, with a timing precision of less than 20\u03bcs in a large network.\n\nOverall, this wake-up system offers an innovative solution for the growing need to synchronize and trigger IoT wireless networks in a variety of contexts. Its ultra low power consumption and ability to detect signals in multiple frequency bands make it a valuable tool for military, environmental, and other IoT applications."}, {"label": 1, "content": "Recently, power grid user-side data has become increasingly massive and complex, posing challenges to traditional anomaly detection models. Despite the widespread use of neural networks and machine learning methods in anomaly detection, these approaches have high training sample requirements and struggle when faced with missing sample tags in power datasets. In response, this paper proposes an unsupervised model for detecting power data anomalies based on the isolated forest algorithm. This model incorporates feature extraction, feature reduction, and isolated forest computing modules, and can process large amounts of data in a short amount of time while accommodating the lack of training samples. The results indicate that this model is practical and effective for power sector needs."}, {"label": 0, "content": "Data infrastructure and quality are vital organs influencing the health of any organizations and are essential for creating and delivering business insights. Due to this fact, stakeholders expect a flawless experience, real-time solutions and support. However, these expectations are normally too high for IT departments that are always immersed with various users' requests. The Big Data era and its analytics have put organizations in positions to predict and forecast users' needs to produce a great user experience. This is achieved through intuitive design, error-free coding and quality performance. There is increasing demand for better business analytics so as to enable organizations establish solid foundation by building out a data management ecosystem that delivers flexibility and performance required by cognitive solutions. The objective of this paper was to highlight how universities can leverage better analytics to process amount of continuously generated data from various sources and generate actionable insights needed to achieve academic business goals. This study followed an interpretive paradigm taking a case study of a South African university and employed semi-structured interviews. This study found that data value and the use thereof in the management of academic divisions of the university should be consistent with faculty business plans. In this way, business analytics is not haphazardly used but is strategically aligned with faculty plans and overall institutional strategic objectives and targets."}, {"label": 0, "content": "A Flexible Machine Vision (FMV) Inspection System has been developed that requires minimal retuning in hardware and software as applications are changed up. The flexibility of the system was evaluated by applying it to an inspection problem with three different types of small parts: plastic gears, plastic connectors and metallic coins, with minimal retuning when moving from one application to the others. The system was required to differentiate between 4 different known styles of each part plus one unknown style, for a total of 5 classes. In previous work, a hybrid Support Vector Machine (SVM) classifier was developed for the connector application. When applied to the coin application, the hybrid SVM could not achieve the target performance of 95% accuracy. A new hybrid that method that combines SVM and an Artificial Neural Network (ANN) or ANN-SVM classifier was subsequently developed to overcome this problem and the results are presented in this paper. The image library used in this study is available at http://my.me.queensu.ca/People/Surgenor/Laboratory/Database.html."}, {"label": 1, "content": "We propose a novel approach to estimate the area of green algae in geostationary ocean color imager multispectral images using a cofactor-based endmember extraction strategy. Our approach improves the efficiency of the N-FINDR, which is widely used for the same purpose, from two perspectives. Firstly, by using the cofactor matrix to search the largest simplex volume, we only need to compute matrix inverse and determinant for a small number of times, unlike in N-FINDR where determinants are to be enumerated for all pixels. This makes the process significantly more efficient. Secondly, we obtain optimal endmembers empirically through a few recursive iterations of cofactor matrix updates, instead of maximizing volumes with random initializations as in N-FINDR. Our experimental results demonstrate that our approach produces green algae area estimates that are as accurate as those obtained using N-FINDR, but with much higher efficiency."}, {"label": 1, "content": "The estimation of the fundamental matrix using the RANSAC algorithm can be challenging when the outlier ratio is high due to issues related to computational inefficiency and low accuracy. However, an optimized approach based on modifications to the RANSAC algorithm has been introduced in this article. The proposed technique makes use of an isolation forest algorithm to detect outliers in putative SIFT correspondences based on the consistency of features in location, scale, and orientation. The detection of these outliers eliminates several obvious correspondences, effectively improving the inlier ratio. Ultimately, the fundamental matrix estimation is performed using the optimized set, leading to impressive results, as demonstrated through multiple experiments. It was observed that this method resulted in a remarkable increase in speed and accuracy compared to the RANSAC algorithm."}, {"label": 0, "content": "The residential rooftop solar penetration in the U.S. has increased rapidly over the past few years. This increase, if not properly accounted for, can lead to operational and reliability challenges for the electric power industry in the form of under-utilization of available energy, increase in costs, and reduction in environmental benefits, as demonstrated by the California Independent System Operator (CAISO) Duck Curve. The authors of this paper had previously developed a bottom-up approach for computing season-wise household-level residential energy consumption profiles using a synthetic population resource. In this paper, that model is enhanced to account for the effects that increasing percentages of rooftop solar penetration can have on the residential energy demand profiles of different regions. This information will be very useful to electric power utilities because it will help them efficiently manage the increasing numbers of residential rooftop solar installations in their supply areas."}, {"label": 1, "content": "In recent years, there has been a growing importance for active distribution networks as more emphasis is placed on renewable energy sources. Integrating these sources into the power system introduces various uncertainties, highlighting the significance of evaluating line loading and line active power. To provide comprehensive details for power system planning, this paper presents two primary approaches for comparing computational times and accuracy for line loading and active power in an active distribution network.\n\nNumerical methods include Monte Carlo and quasi-Monte Carlo simulations, and an analytical approach is the combined cumulants and Gram-Charlier expansion. The analysis employed the IEEE 13-bus test system, and it was determined that the combined cumulants and Gram-Charlier expansion method was both accurate and computationally efficient in comparison to Monte Carlo and quasi-Monte Carlo methods."}, {"label": 1, "content": "In this article, the well-known \"knapsack problem\" is discussed in the context of distributing virtual servers within a cloud infrastructure. The main focus is on allocating resources to virtual machines, including those with different access capabilities, such as random access memory and processor power, as well as data storage systems and their related memory capacity and access speed. A mathematical model is proposed to optimize resource allocation. This model was tested on a real network infrastructure and has practical implications in allowing the use of low-power devices available at data centers for virtual machine distribution."}, {"label": 0, "content": "In-memory computing with nanoscale memristive devices such as phase-change memory (PCM) has emerged as an alternative to conventional von Neumann systems to train deep neural networks (DNN) where a synaptic weight is represented by the device conductance. However, PCM devices exhibit temporal evolution of the conductance values referred to as the conductance drift, which poses challenges for maintaining synaptic weights reliably. Based on the mean behavior of 10,000 GST-based PCM devices, we observe that the drift coefficient is dependent on the conductance value. Moreover, we show that PCM drift is re-initialized and the drift history is erased after the application of even partial SET pulses. This is regardless of how much the device has drifted. With models capturing these features, we show that drift has a detrimental impact on training DNNs, but drift resilience can be significantly improved with a recently proposed multi-PCM synaptic architecture."}, {"label": 0, "content": "At present, using distance function to learn image pairs is a widely used method in the field of computer vision, and Euclidean distance is widely used. But traditional Euclidean distance has disadvantage of distinguishing ability in the feature similarity measure. In this paper, we propose a weighted Pairwise Constrained Component Analysis (wPCCA) algorithm based on weighted Euclidean distance for person reidentification (Re-ID). The algorithm of wPCCA is based on the PCCA using the weighted Euclidean distance to measure the characteristics. The experiments were conducted on two challenging datasets named i-LIDS and CAVIAR, and gained good results."}, {"label": 1, "content": "With the rapid advancement of science and technology, wind speed prediction has become extremely essential in the development of the smart grid. The accurate prediction of wind power is crucial in ensuring the safety and efficacy of the new smart grid, considering the nonlinear variation of wind speed and the effect of several influencing factors.\n\nTo attain an accurate wind speed prediction, numerous techniques have been documented globally. In this paper, we propose a novel method that employs long short-term memory network with peephole (peephole LSTM) and wavelet decomposition to predict wind speed. This approach involves processing the wind speed data through wavelet dynamic decomposition, peephole LSTM structure is then utilized to analyze the data sequence, and a new model of peephole network is established. By iteratively updating the weights via the back-propagation algorithm and forward training, the optimal parameters are obtained.\n\nSimulation was carried out on a month's wind speed data using MATLAB, and it showed high prediction accuracy. Predictions were made for the next five days using the first 25 days of data for training. The mean absolute percentage error (MAPE) was utilized to analyze prediction data, which proved the feasibility of this method.\n\nIn conclusion, we propose a new technique to predict wind speed, which can guarantee a high level of accuracy in the smart grid. The integration of peephole LSTM and wavelet decomposition enables a better understanding of the wind speed data sequence, leading to a more precise prediction."}, {"label": 0, "content": "This paper reviews the development and application architecture of an expert system to assist the Mauritian population with queries they may have about labor or employment law. The expert system makes use of Machine Learning, Speech Recognition/Synthesis and Natural Language Processing techniques to converse with users through a web interface. The expert system also takes advantage of a large knowledge base, that allows the system to teach itself employment law principles. The knowledge base is created from \"Understanding Employment Law and Remuneration Orders in Mauritius\", written by Ved Prakash Torul [1], which is a simplified version of the Employment Relations Act and the Employment Rights Act. The book explains employment law in common language, to help the public understand their constitutional rights. The expert system allows users to communicate and express their employment issues, so that they are aware of their next course of action, either they are an employer, employee, or a union. The paper also reviews the evaluation period, which consisted of a preliminary testing period. Through the evaluation, it was concluded that the expert system was able to respond to individual responses with a Precision of 66% and Recall of 85%. While the Expert System is able to converse with users on certain topics on Employment Law, further evaluation would need to be conducted. Additionally, the knowledge base will need to be updated over time."}, {"label": 0, "content": "In this letter, a sensor localization technique for highly deformed partially calibrated arrays with multiple moving targets is proposed. The deformed array is divided into several subarrays. The first subarray is composed of the pre-calibrated sensors, whereas the sensors in the other subarrays are uncalibrated. The positions of the sensors are estimated from the phase differences between the pre-calibrated sensors and the uncalibrated sensors. The phase ambiguities caused by the highly deformed sensor positions can be solved using the subspace orthogonality and the movement of multiple targets. Simulation results evaluate the performance of the proposed method, and the Cramer-Rao bounds are compared."}, {"label": 1, "content": "This paper presents the Fast Distributed Online Conditional Gradient Algorithm (F-DOCG) as a means of accelerating the convergence of distributed online optimization algorithms. The authors first establish the Erdos-Renyi (ER) stochastic model and propose an Edge Addition (AE) algorithm. They then combine this algorithm with the Distributed Online Conditional Gradient Algorithm to create the F-DOCG. \n\nBy using linear approximation, the F-DOCG algorithm avoids the high cost projection problem and simultaneously improves the Regret bound. This is achieved through an understanding of the relationship between the underlying topology and algebraic connectivity, resulting in a faster convergence rate. The numerical simulation experiments conducted demonstrate the superiority of the F-DOCG compared to the existing Distributed Online Conditional Gradient Algorithm (DOCG)."}, {"label": 1, "content": "To address the challenges of data reliability in NAND flash storage, a new decoding algorithm called variable-node-based belief-propagation with message pre-processing (VNBP-MP) has been proposed for binary low-density parity-check (LDPC) codes. The algorithm utilizes the unique characteristics of the NAND flash channel to perform message pre-processing (MP), which effectively prevents the spread of unreliable messages and speeds up the propagation of reliable messages. Additionally, the VNBP-MP algorithm includes a treatment for oscillating variable nodes (VNs) to further accelerate decoding convergence. \n\nSimulation results demonstrate that the proposed VNBP-MP algorithm has significantly improved convergence speed without compromising error-correction performance, in comparison to existing algorithms."}, {"label": 0, "content": "The issue of creating high-performance computing systems based on heterogeneous computer systems is relevant, since the volume of processed information, calculations and studies with large data sets is constantly increasing. The aim of the work is to develop a model for predicting the performance of heterogeneous computing systems and its experimental evaluation in the simulation of access to the memory and in modeling fundamental parallel algorithms. As a result, the use of the developed model allows making an adequate estimate of the time of the parallelized task execution using heterogeneous computer systems based on graphics processors."}, {"label": 0, "content": "Passwords form part of our daily routine and even though there are alternative authentication mechanisms, such as biometrics, passwords stubbornly persist. Passwords have been around for the last few decades, but also the various problems associated with users trying to create passwords that are strong and secure. Users are faced with a cognitive burden in managing passwords which often leads to poor password practices or users recycling passwords across various accounts. While there is no anticipated end to the use of passwords, scholars have identified that passwords need to be better supported - one such method is using a password manager. There is a wealth of technical research relating to password managers, which has led to drastic improvements and the maturing of the technology. However, there is a little research on why people would choose to adopt password managers. To explore these factors, this research uses an adapted version of the Unified Theory of Acceptance and Use of Technology (UTAUT2) that includes trust as an additional construct. Using empirical data, the results of the study show that performance expectancy, habit, and trust are key factors in the intention to adopt a password manager."}, {"label": 1, "content": "A new approach has been developed to accelerate the convergence of Sommerfeld integral tails in planar stratified media containing a perfect electrically conducting layer. This method involves a novel spectral-domain singularity subtraction technique that mitigates catastrophic cancellation in the spatial domain and yields a rapidly decaying spectral tail. The results from numerical simulations demonstrate that this technique enables more accurate calculations of the Green's functions."}, {"label": 1, "content": "In the modern engineering phase of manufacturing systems, simulation-based methods and tools have been established to meet the increasing demands for time-efficiency and profitability. To apply these solutions, model-based digital twins are created as multi-domain simulation models that describe the behavior of the manufacturing system. As production processes become more complex, data-driven digital twins have emerged in the context of industry 4.0, based on increasingly networked and cloud-based technologies. \n\nRecent advancements in machine learning have opened up new possibilities in connection with the digital twin. These include using data-based learning of models, as well as learning control logic for complex systems. This paper proposes a combined model-based and data-driven concept for digital twins, using machine learning techniques to shorten the development time of manufacturing systems. \n\nOverall, this approach provides an efficient way to simulate manufacturing processes and improve manufacturing system design in terms of time and profitability."}, {"label": 0, "content": "Research on data confidentiality, integrity and availability is gaining momentum in the ICT community, due to the intrinsically insecure nature of the Internet. While many distributed systems and services are now based on secure communication protocols to avoid eavesdropping and protect confidentiality' the techniques usually employed in distributed simulations do not consider these issues at all. This is probably due to the fact that many real-world simulators rely on monolithic, offline approaches and therefore the issues above do not apply. However, the complexity of the systems to be simulated, and the rise of distributed and cloud based simulation, now impose the adoption of secure simulation architectures. This paper presents a solution to ensure both anonymity and confidentiality in distributed simulations. A performance evaluation based on an anonymized distributed simulator is used for quantifying the performance penalty for being anonymous. The obtained results show that this is a viable solution."}, {"label": 1, "content": "This paper presents an expert system designed to assist individuals in Mauritius with queries related to labor or employment law. The expert system utilizes a variety of advanced technologies, including Machine Learning, Speech Recognition/Synthesis, and Natural Language Processing, to engage with users through a web interface. One of the key features of the system is its large knowledge base, which is derived from an accessible and easy-to-understand source, \"Understanding Employment Law and Remuneration Orders in Mauritius,\" written by Ved Prakash Torul. The knowledge base enables the system to impart employment law principles and assist users in navigating their employment issues, regardless of whether they are an employer, employee, or association member. \n\nThe evaluation period of the expert system involved a preliminary testing period, which yielded impressive results. With a Precision score of 66% and Recall score of 85%, the system was found to be highly responsive to individual responses. However, the paper concludes that further evaluation needs to be conducted to explore the system's performance in depth. An important factor in the success of the system over time will be updating the knowledge base to keep pace with evolving labor and employment laws in Mauritius. \n\nOverall, this paper highlights the potential of expert systems and related technologies to enhance access to critical information on employment law for individuals in Mauritius. With continued development and evaluation, it is hoped that such systems will improve access to legal resources and empower individuals to make informed decisions about their employment situations."}, {"label": 1, "content": "Current IoT services are typically reliant on data communication technologies that do not utilize the public switched telephone network (PSTN). However, with the assignment of telephone numbers to machine-type devices, PSTN switches can play a significant role in IoT service routing. To facilitate interaction between users and IoT devices utilizing PSTN switches, we have developed a PSTN-based IoT mechanism. This represents the world's first such solution. Through this mechanism, all PSTN customer premises equipment (CPE), including fixed-line and mobile phones, can access IoT services without the need for installation of additional software or mobile applications. Through the reuse of existing PSTN infrastructure, PSTN-based IoT provides telecommunication-grade service, security, and network management for IoT, all of which would be prohibitively expensive to develop using non-PSTN-based IoT methods. Our approach makes it convenient for existing CPE to access IoT applications, which will significantly boost the growth of the IoT service industry."}, {"label": 1, "content": "This paper proposes a new insulator contamination prediction model that is based on back-propagation (BP) neural network, optimized by genetic algorithm. To overcome the inadequacies of the slow convergence rate and weak global optimization ability, the genetic algorithm is used to improve the performance of prediction results. In addition to considering meteorological factors such as temperature, wind, precipitation, and relative humidity as input layers, the model also incorporates air quality index (AQI) factors like PM2.5 and PM10 values. Furthermore, the model uses equivalent salt deposit density (ESDD) and non-soluble deposit density (NSDD) as output variables. The results indicate that the proposed model exhibits superior optimization ability compared to the BP neural network, resulting in more accurate prediction results."}, {"label": 0, "content": "The problem of events detecting and classifying in the continuous video stream of information and telecommunication security systems is solved. As a basic algorithm, it is proposed to use the ensemble of deep neural networks - convolutional networks and feedback networks, in particular, for classification and annotation, which makes it possible to recognize abnormal emergency situations. A training set of abnormal situations was collected, various architectures of deep neural networks were trained and tested. It is shown that the use of the indRNN layers achieves up to 70% when recognitions multi-class events in the video stream. The new is strengthening of classification estimation of a video segment by extracting the keywords from the automatic annotation. The developed software package can be implemented in the integrated security system of abnormal situations recognition in real time."}, {"label": 0, "content": "Due to the various advantages that the cloud can offer to robots, there has been the recent emergence of the cloud robotics paradigm. Cloud robotics permits robots to unload computing and storage related tasks into the cloud, and as such, robots can be built with smaller on-board computers. The use of cloud-robotics also allows robots to share knowledge within the community over a dedicated cloud space. In order to build-up robots that benefit from the cloud-robotics paradigm, different cloud-robotics platforms have been released during recent years. This paper critically reviews and compares existing cloud robotic platforms in order to provide recommendations on future use and gaps that still need to be addressed. To achieve this, 8 cloud robotic platforms were investigated. Key findings reveal varying underlying architectures and models adopted by these platforms, in addition to different features offered to end-users."}, {"label": 0, "content": "Cognitive radio (CR) systems have to be able to detect the presence of a primary user (PU) signal by sensing the spectrum area of interest. Due to radiowave propagation effects like fading and shadowing, spectrum sensing is often complicated, because the PU signal can be attenuated in a particular area. In this paper, we explore a distributed spectrum sensing approach that exploits the largest eigenvalue of correlation matrices (CMs) that are adaptively estimated, based on the combine and adapt least (CTA) type of diffusion method with no fusion center (FC). More specifically, CR nodes exchange also observations with a subset of neighbouring nodes and combine the neighbouring observations based on the locally estimated signal to noise ratio (SNR) values. We analyse the resulting detection performance and verify the theoretical findings through simulations."}, {"label": 1, "content": "Passwords have become an integral part of our daily routine, and despite the availability of alternative authentication mechanisms like biometrics, they persist stubbornly. Over the last few decades, passwords have come under increasing scrutiny due to the various problems associated with users trying to create strong and secure passwords. Managing passwords can be a cognitive burden, and this often leads to poor password practices, including the reuse of passwords across various accounts.\n\nAlthough there is no anticipated end to the use of passwords, scholars have identified the need for better support to make them more secure. One such method is the use of a password manager. However, little research has been conducted to explore why people would choose to adopt password managers.\n\nTo address this gap in research, an adapted version of the Unified Theory of Acceptance and Use of Technology (UTAUT2) was used, which included trust as an additional construct. Empirical data was collected to explore the factors that drove the intention to adopt a password manager.\n\nThe results of the study indicated that performance expectancy, habit, and trust were key factors in the intention to adopt a password manager. The findings suggest that people are more likely to adopt a password manager if they perceive that it will enhance their performance in managing their passwords, have a habit of using them, and trust the technology and the provider.\n\nIn conclusion, while passwords will continue to be an important part of our daily routine, there is a need for better support to make them more secure. The study highlights the importance of understanding the factors that drive the adoption of password managers, and the role that trust plays in this process. By addressing these factors, we can make passwords more secure and easier to manage."}, {"label": 1, "content": "Orthogonal frequency division multiplexing (OFDM) is a widely used technology in modern wireless communication systems. However, conventional channel estimation methods struggle to effectively suppress noise, which ultimately affects the quality of received OFDM signals. To address this issue, this paper proposes utilizing raise cosine (RC) and square root raise cosine (SRRC) filters in OFDM-RC and OFDM-SRRC systems to suppress inter-symbol-interference and noise. The proposed method filters out noise using either RC or SRRC, then obtains the noise standard deviation as the threshold to further reduce noise impact. Simulation results demonstrate the effectiveness of this approach in multipath propagation conditions."}, {"label": 0, "content": "The incremental distribution network operating income will be the focus of attention of the company that has the power of incremental distribution network operation under the electricity reform. Based on this, this passage establishes the dynamic economic dispatch model of incremental distribution network considering P2G. It also proposes a catastrophe genetic algorithm based on double iterative optimization genetic algorithm to solve the time coupling problem under the constraints of control means, such as energy storage and demand-side response. Taking the improvement IEEE33 node model as an example, the influence of various regulatory methods on the operating income of incremental distribution network is analyzed and discussed. It is verified that the consideration of 2PG and demand-side response is essential to improve the operating income of incremental distribution network."}, {"label": 1, "content": "When capturing photos in low-light situations with artificial lighting, a common challenge is choosing between a short exposure setting that results in a dim, noise image with sharp outlines, or a longer exposure setting that yields a bright, saturated image yet with blurred areas. Unfortunately, neither option is ideal for most cases. Longer-exposure images typically retain better brightness and color information, whereas shorter-exposure images preserve sharp outlines. In this study, we introduce a patch-based approach that combines these images to produce a better final image. The method employs a coarse-to-fine approach to identify inconsistent pixels resulting from moving objects, followed by extracting information from both exposures using a novel patch-based technique. Our experimental results demonstrate that this approach effectively maintains the sharp edges of the short-exposure image while retaining the color, brightness, and details of the long-exposure image."}, {"label": 1, "content": "Wireless Sensor Network (WSN) is an emerging next-generation sensor network that has a wide range of application prospects. Localization technology is one of the critical key functionalities of WSN. However, in complex indoor environments, fluctuations in received signal strength can significantly degrade positioning accuracy. To address this problem, this paper proposes a fingerprint localization method based on the received signal strength (RSS) distance and an improved weighted k-Nearest Neighbor (KNN) algorithm.\n\nIn the offline phase, a fingerprint database is established. In the online phase, the RSS values of the measurement points are measured in real-time, and a two-stage RSS distance calculation using the Euclidean distance is performed. Finally, the improved weighted KNN algorithm is used to solve the problem of non-Gaussian distribution of measurement noise and calculate the final position coordinates of the measurement point.\n\nSimulation results illustrate that this method can substantially reduce the influence of signal strength fluctuations and improve the positioning accuracy. Therefore, the proposed method has potential applications in various IoT-based indoor location-based services, such as real-time location tracking of people, assets, and vehicles, indoor navigation, and emergency response systems."}, {"label": 1, "content": "In recent years, Indonesia has seen an increase in internet users. Many product and service providers offer internet access services with various tariff options and features. To gauge the public satisfaction level towards data services provided by telecommunication operators in Indonesia, sentiment analysis was conducted on social media data. The sentiment data was collected using Twitter's API, and pre-processing was conducted to clean and tag the data. The words were then weighted using the TF-IDF calculation method and classified using the Naive Bayes Classifier method. The study resulted in an average precision rate of 94.5%, recall rate of 93.3% and accuracy rate of 99.09%."}, {"label": 0, "content": "In order to realize the transient and accident analysis of reactor system and verification of system control scheme, the latest computer technology is used to develop advanced graphical modeling tool for TRANTH code, the human-machine interaction interface, and integration research between software and digital reactor design and verification platform, so as to the design and simulation system of digital reactor fluid instruction control system is built up. The nuclear power plant (ACP1000) is chosen as a research object some typical operating conditions are selected and executed. Results show that the design and simulation system can implement the modeling, calculation, interactive manipulation and display of reactor fluid instruction control system, which brings a multi-professional joint simulation of reactor fluid instruction control system to fruition, and provides technology support for rapid generation, evaluation, verification and optimization of design scheme about reactor fluid instruction control system."}, {"label": 1, "content": "The statistical delay of a path is typically modeled as a Gaussian random variable, assuming that the path is always sensitized by a test pattern. However, in various circuit instances, the sensitization of the path may vary among its test patterns, and such pattern-induced delay is non-Gaussian. As a result, probability mass functions are used to model the path's delay.\n\nTo improve defect coverage, machine learning is applied to select the best test patterns. This approach has been shown to result in higher accuracy in detecting defects when compared to existing methods, as demonstrated by experimental results."}, {"label": 1, "content": "This article focuses on the use of non-position number systems, specifically the residual number system, for optimizing neural network calculations in automatic control systems. The objective is to address the challenges associated with control systems of industrial objects that display astatic properties.\n\nVarious issues that arise in such systems are identified and analyzed. To overcome these challenges, the proposed approach is demonstrated through the example of a real problem. The efficacy of this approach in improving the optimization of neural network calculations is evident."}, {"label": 1, "content": "This letter proposes a novel 2-D square-root-based memory polynomial behavioral model for predistortion. The model uses a new set of square-root-based basic functions to describe the predistorter characteristic. With only two nested summations, the proposed model greatly reduces the number of coefficients compared to models with three nested summations. Experimental results show that the proposed model reduces coefficients by over 66.7% compared to the 2-D digital predistortion model and achieves better adjacent channel power ratio (ACPR) performance. The proposed model also improves normalized mean-square error by 11 dB compared to the 2D-DPD model and simple online coefficient update model. Additionally, the proposed model obtains similar ACPR performance as the 2-D modified memory polynomial model with a shorter running time."}, {"label": 1, "content": "This paper proposes a novel model parameter identification based bus-bar protection principle. When an internal fault occurs on a bus, an inductance model can be developed. The unknown parameters, the inductance and resistance of the model, are identified and used to calculate the equivalent instantaneous impedance and dispersion of the parameter. This information is then utilized to distinguish between external and internal faults with different levels of CT saturation. A new criterion with a self-adaptive restraint characteristic for bus-bar protection is put forward based on this principle. \n\nCompared to traditional phasor based bus-bar protection principles, this new principle is suitable for non-periodic, fundamental, and harmonic components, making it more efficient. Additionally, the proposed principle is immune to the impact of fault current flowing out when a fault occurs in the protection zone of the breaker-and-a-half bus-bar and is insensitive to fault resistance. Simulation results show high sensitivity and reliability for the presented principle."}, {"label": 1, "content": "In the research field of line-commutated converter-based high voltage direct current (LCC-HVDC) sending-end power grid, the stability analysis and protection control play a crucial role in ensuring the proper functioning of the system. Therefore, it is important to assess and extract the spatiotemporal distribution features of the dynamic response of the power grid, especially under LCC-HVDC blocking. This paper presents a two-stage approach that combines local and global analyses to extract these features effectively.\n\nIn the first stage, the authors propose constructing local quantitative indices to measure various aspects of the dynamic response, including the response time delay, deviation magnitude, rate of change, and cumulative effects of dynamic trajectories. These indices provide a thorough assessment of the temporal dynamics of the power grid at each observation site. In the second stage, the authors globally compare the temporal features of different observation sites by creating an observation space and dissimilarity matrices that depict the spatial disparities of the entire region. This step allows for a comprehensive evaluation of the spatial and temporal responses of the power grid under LCC-HVDC blocking.\n\nBy extracting the spatiotemporal features, a typical dynamic response pattern of the sending-end power grid is identifiable, which is critical for detecting LCC-HVDC blocking. To demonstrate the proposed approach and the obtained features and pattern, the authors select the Sichuan power grid in China as a case study. The results indicate that the proposed approach provides valuable insights into the dynamic response of the power grid, which can directly benefit the stability analysis and protection control of the system.\n\nIn conclusion, the proposed two-stage approach effectively assesses and extracts the spatiotemporal distribution features of the dynamic response of the LCC-HVDC sending-end power grid under blocking. The approach provides a comprehensive evaluation of the system's temporal and spatial responses, which can aid in detecting LCC-HVDC blocking and improving the system's stability analysis and protection control."}, {"label": 0, "content": "In this paper, the design of the control chart when the variable of interest follows the gamma distribution under the neutrosophic statistical interval method (NSIM) is proposed. The average run length, probability in-control, and probability of out-of-control will be derived using the NSIM. The neutrosophic control chart coefficient will be determined by the algorithm under the NSIM. The neutrosophic average run length for various shifts and specified parameters will be determined. The efficiency of the proposed control chart is discussed using the simulation study and a real example."}, {"label": 0, "content": "In recent years, active distribution networks have gained in importance due to increased emphasis on renewable energy sources. The integration of these sources into the power system is intrinsically related to various types of uncertainties. In such cases, evaluating line loading and line active power is of immense importance in providing comprehensive details for use in power system planning. This paper presents two key (numerical and analytical) approaches for comparing computational times and accuracy for line loading and active power in an active distribution network. Monte Carlo and quasi-Monte Carlo simulations are categorized as numerical methods and combined cumulants and Gram-Charlier expansion is an analytical approach. The IEEE 13-bus test system was employed to conduct the required analysis. It was determined that the combined cumulants and Gram-Charlier expansion method is quite accurate and computationally efficient when compared to Monte Carlo and quasi-Monte Carlo methods."}, {"label": 0, "content": "Unbalanced structure of distribution network brings difficulties into its analysis and computation. With increasing integration of renewable distributed generation (DG) in distribution network, more accurate and efficient sensitivity analysis method is required to facilitate following control and optimization procedures. In this paper, a sensitivity analysis method for unbalanced distribution network is developed. The proposed method utilizes existing measurement data to construct linearized power flow model and sensitivity data is obtained directly via solving linear equations. The numerical tests show that the proposed method achieves higher accuracy than traditional methods."}, {"label": 1, "content": "In this paper, we investigate a Two-Way Relay (TWR) Visible Light Communication (VLC) system that comprises two users communicating through a relay. To improve communication efficiency, we introduce Network Coding (NC) into the VLC system and develop two energy-efficient NC-based strategies, namely Straightforward Network Coding over Finite-Alphabet Sets (SNCF) and Physical-Layer Network Coding over Finite-Alphabet Sets (PNCF). The former requires three time slots, whereas the latter requires two time slots to achieve communication. Through simulation results, we demonstrate that both the SNCF and PNCF schemes outperform the traditional four time-slot transmission scheme."}, {"label": 0, "content": "Wireless IoT is a promising area in which large quantities of data and information are collected, exchanged, and stored constantly. While these data and information are utilized for good purposes in wireless IoT, great risks arise in that attackers are seeking to exploit the merits of this new technology for their own benefits. In such a resource-constrained environment, devices are easily exposed to various attacks. Confidential private information is as valuable as it is vulnerable. It is difficult for end users to trust wireless IoT and to adopt related applications due to the lack of satisfactory privacy-preserving mechanisms, which has become the major obstacle for the adoption and popularization of wireless IoT. In this article, we first introduce the classical application scenarios of wireless IoT, along with related security and privacy attack models. We then present a brief overview of privacy-preserving schemes of wireless IoT. Based on the classification of application scenarios, we further present recent advances in privacy-preserving mechanisms in wireless IoT. Finally, we discuss open issues and future research directions for these application scenarios in wireless IoT."}, {"label": 0, "content": "A key challenge in renal diagnosis using digital pathology has been the scarcity of reliable annotated datasets that can act as a benchmark for histological investigations. This paper uses a novel medical image dataset, titled Glomeruli Classification Database (GCDB), consisting of renal glomeruli images bifurcated into binary classes of normal and abnormal morphology. Based on this dataset, we direct our pioneering efforts to explore suitable deep neural network techniques related to kidney tissue slide imaging so as to establish a state of the art in this relatively unexplored domain. The paper focuses on classifying normal and abnormal categories of glomeruli which are the vital blood filtration units of the kidney. The results obtained using publicly available transfer learning models are held in comparison with supervised classifiers configured with image features extracted from the last layers of pre-trained image classifiers. Contrary to popular belief, transfer learning models such as ResNet50 and InceptionV3 are empirically proved to under-perform for this particular task whereas the Logistic Regression model augmented with features from the InceptionResNetV2 show the most promising results on the GCDB dataset."}, {"label": 0, "content": "This paper presents a pre-processed faster region convolution neural network (faster RCNN) for the purpose of on-road vehicle detection. The system introduces a preprocessing pipeline on faster RCNN. The preprocessing method is for the improvement on training and detection speed of Faster RCNN. A preprocessing lane detection pipeline based on the Sobel edge operator and Hough Transform is used to detect lanes. A Rectangular region is then extracted from lane coordinates which is a reduced region of interest (ROI). Results show that the proposed method improves the training speed of faster RCNN when compared to faster RCNN without preprocessing."}, {"label": 1, "content": "The issue of maximizing influence in social networks has been of great concern to many individuals. This involves identifying a set of nodes that can help spread influence across a network. However, previous research primarily focused on a node's impact on its neighboring nodes, without considering time and cost constraints. Realistically, people often attempt to influence their friends multiple times over a specific time period. Additionally, spreading information may have a certain cost associated with it. \n\nThis paper explores the Time-sensitive Influence Maximization Problem and proposes a Time and Cost constrainted Influence model with users' Online patterns (TCIO model). The selection of seed nodes is restricted by budget, and each node may continuously impact its neighbors based on their online patterns with varying probabilities until a defined message expiry time is reached. Our research shows that the problem is NP-hard, and our model satisfies monotonicity and submodularity for influence spread. As a result, we develop a greedy algorithm for resolving the issue. \n\nTo reduce the computational complexity and optimize seed node selection with cost, we introduce an efficient approach, GMAI, for approximately assessing added influence using influence weight. Our experiments demonstrate that our model is both efficient and effective since it accounts for time factors, while GMAI is faster and more efficient than other evaluated algorithms."}, {"label": 0, "content": "Human heart is a vital organ therefore proper diagnosis of heart activities is essential. Various parameter estimation techniques have been developed to estimate heart parameters. In this work, we use Ensemble Kalman Filter (EnKF) and Particle Filter (PF) for dynamic assimilation of human heart parameters. EnKF and PF are modified filters specifically designed for state prediction of nonlinear systems with large data samples. A third order mathematical heart model is used to estimate three heart parameters that includes movements of heart muscle fiber, tension in heart muscle and electrochemical activity of the heart. EnKF and PF are applied to heart model and different case studies are performed to observe the prediction accuracy by comparing sum squared error values. Case studies are performed with variable state and measurement noise values. The proposed approach demonstrates promising results in accurately predicting the human heart parameters."}, {"label": 0, "content": "Compared to in-clinic balance training, in-home training is not as effective. This is, in part, due to the lack of feedback from physical therapists (PTs). In this paper, we analyze the feasibility of using trunk sway data and machine learning (ML) techniques to automatically evaluate balance, providing accurate assessments outside of the clinic. We recruited sixteen participants to perform standing balance exercises. For each exercise, we recorded trunk sway data and had a PT rate balance performance on a scale of 1-5. The rating scale was adapted from the Functional Independence Measure. From the trunk sway data, we extracted a 61-dimensional feature vector representing the performance of each exercise. Given these labeled data, we trained a multi-class support vector machine (SVM) to map trunk sway features to PT ratings. Evaluated in a leave-one-participant-out scheme, the model achieved a classification accuracy of 82%. Compared to participant self-assessment ratings, the SVM outputs were significantly closer to PT ratings. The results of this pilot study suggest that in the absence of PTs, ML techniques can provide accurate assessments during standing balance exercises. Such automated assessments could reduce PT consultation time and increase user compliance outside of the clinic."}, {"label": 0, "content": "This paper starts from the weight and sensitivity of the parameters of the identified elements, A method for parameter identification of synchronous generator based on rough set theory and particle swarm difference algorithm is proposed. Firstly, the output of synchronous generator is constructed by rough set theory, The attribute reduction of the decision table is made to obtain the attribute parameters which have great influence on the simulation results. Then, according to the dependence of the calculated parameters, the pruning dimension of the identification parameters was realized. Finally, A generator model is built on the Matlab/Simulink platform, The model parameters are identified by particle swarm difference algorithm and model input and output data. The simulation results show that the proposed method can effectively improve the efficiency and precision of parameters identification."}, {"label": 1, "content": "The Sparse Iterative Covariance-based Estimation (SPICE) method is a computationally efficient approach for estimating the Direction of Arrival (DOA), however it lacks resolution and noise immunity. To improve the performance of the SPICE method, this paper proposes an improved version based on the fourth-order cumulant. This method extends the array aperture and reduces Gaussian noise, resulting in higher resolution and better performance in low Signal-to-Noise Ratio (SNR) scenarios. Additionally, it is computationally more affordable as it distills the un-redundant data of uniform linear array. The proposed method is validated and evaluated through simulations."}, {"label": 1, "content": "Due to an excessive number of databases, unbalanced development, and lagging sensing infrastructures, distributed network data is plagued with inconsistency, missing data, large measurement errors, and other quality issues that hinder the advancement of smart distribution networks. In order to gain a deeper understanding of the more intricate underlying rules and provide more effective decision-making support for power systems, it's imperative to study data mining and analysis techniques that are appropriate for vast amounts of data under the current situation. \n\nThis paper focuses on identifying bad data for multi-temporal and multi-spatial data in distribution networks and presents a method for identifying bad data utilizing the likelihood-ratio test for 3D spatio-temporal data. To increase the rate of data processing, a 3D-LRT method is introduced based on multi-threading and Hadoop parallelization methods."}, {"label": 1, "content": "A distributed energy management system has been developed and tested for an interconnected multi-microgrid system. The system utilizes the alternating direction method of multipliers, and has been implemented using the CVX platform of the MATLAB environment. The microgrids are interconnected and communicate with each other to lower the operational cost of the system and maximize profits via energy exchanges with the main grid. The simulation results show that the proposed method is effective."}, {"label": 1, "content": "The behavior of APT attacks has been a hot topic in recent network security studies. Understanding the implementation principles of APT attacks is critical in safeguarding networks. In this paper, we analyze the behavior of APT attacks in the Ngay campaign from two perspectives: network traffic and code implementation. \n\nFirstly, we use traffic analysis to establish the attack chain. Subsequently, we detail the process of vulnerability exploitation via reverse code analysis. We then illustrate the building of a backdoor in the system. Finally, we discuss the obfuscation technology utilized in APT malware samples. \n\nBy studying the Ngay campaign, we have gained great insights into APT attack techniques. The knowledge we have acquired can be applied to strengthening the cybersecurity of networks, further ensuring the safety of critical data."}, {"label": 0, "content": "Internet of Things (IoT) development is a very challenging topic, and the debate about the actual implementation is still wide open. Various studies have been conducted in term of smart home system based on IoT technology. However, resources that concern on how to practically implement the particular energy-saving and resource efficient technology for smart home need to be improved. In this study, presented the field experiment results related to implementation of low-power node modules using sub 1 GHz LPWAN (low-power wide area network) connectivity. LPWAN technology has unique characteristics, such as a transmit power that in SRD class (short range device) but cellular like coverage range. LPWAN network support star topology by default, so it can overcome the problem of power usage inconsistency, network delay and the complexity of the routing management process found on conventional mesh-based sensor networks. Based on the field experiment result, the furthest distance under outdoor condition could reach up to 350 meters (RSSI -85 dBm) and 150 meters (RSSI -95 dBm) under indoor condition. Based on the power usage test, life-span estimation could reach up to 1 year or 17 years depend on the scenarios and battery types."}, {"label": 1, "content": "This paper presents a novel fault-tolerant adaptive control approach for nonlinearly parameterized systems with mismatched disturbances. The control scheme is developed by incorporating a power integrator technique, which enables the system to handle uncertain actuator faults, including actuator stuck. The proposed controller incorporates a disturbance observer to estimate the mismatched disturbances and ensures that the closed-loop system is input-to-state stable. The effectiveness of the developed control technique is demonstrated through simulation results for a robotic arm system."}, {"label": 1, "content": "Numerous machine learning techniques and social engineering methods have been developed and utilized in the fight against phishing threats. This paper proposes a unique hybrid deep learning model to identify phishing attacks. The model comprises two components: an autoencoder (AE) and a convolutional neural network (CNN). The AE is used to reconstruct features that enhance the relationship between the features. Experimental results show that the model can detect phishing attacks with an average accuracy of 97.68%. Moreover, the model exhibits strong generalization ability and can identify phishing attacks in a timely manner."}, {"label": 0, "content": "The ever-increasing number of wireless network systems brought a problem of spectrum congestion leading to slow data communications. All of the radio spectrums are allocated to different users, services and applications. Hence studies have shown that some of those spectrum bands are underutilized while others are congested. Cognitive radio concept has evolved to solve the problem of spectrum congestion by allowing cognitive users to opportunistically utilize the underutilized spectrum while minimizing interference with other users. Byzantine attack is one of the security issues which threaten the successful deployment of this technology. Byzantine attack is compromised cognitive radios which relay falsified data about the availability of the spectrum to other legitimate cognitive radios in the network leading interference. In this paper we are proposing a security measure to thwart the effect caused by these attacks and compared it to Attack-Proof Cooperative Spectrum Sensing."}, {"label": 0, "content": "Based on the background of DC normal education project in the Jinji Lake core area of Suzhou Industrial Park, this paper discusses the key technologies in the practice of multi-terminal DC power distribution technology engineering. Firstly, the basic situation of Suzhou four-terminal DC project was outlined, and the key electrical equipment and its selection principle were analyzed, including the converter valves, bridge arm reactors, link transformers, and DC monitoring and protection systems. Secondly, based on the RTLAB real-time simulation platform, the simulation model of Suzhou four-terminal DC distribution system was built. Finally, based on the RTLAB simulation model, single-ended converter startup, four-terminal stable operation, active power step and reactive power step were simulated. The simulation results show that the four-terminal DC grid model has good steady state performance and active power and reactive power step response performance. It can provide reference for the future DC distribution network construction."}, {"label": 0, "content": "Management and configuration of optical networks, implementing new policies to keep up with ever-changing network etc. have always been tedious tasks. Software-defined networking (SDN) has provided many network solutions in the electrical counterpart. SDN for optical networks can provide new opportunities to make the above mentioned tasks easier and faster. As a first step towards this goal, we develop an optical network description language (ONDL). We use it to describe various network components, and their configuration and run-time states, such as modulation schemes, wavelength and spectral-width of a transponder, switching matrix of an optical switch etc. The language is based on resource description framework (RDF). Furthermore, we develop a controller which understands and sends instructions in this language to different network devices to provide/change their states. We show the applicability of ONDL by simulating a network, controlling and managing its nodes using ONDL and developed controller."}, {"label": 0, "content": "Power system state estimation is a key component of real-time monitoring, enabling extensive analysis and decision making for grid security and efficiency. One challenge that has seen recent interest involves the monitoring and mitigation of geomagnetically induced currents (GICs). These quasi-dc currents are the result of solar activity and can cause additional reactive power losses in transformers. The subsequent loss of reactive power support may result in voltage deviation at many buses. In a traditional state estimator, these voltage deviations may be masked by or attributed to incorrect estimations of generator reactive power output. Alternatively, the voltage state estimate may accumulate additional error, due to trying to match measurements to equations that do not represent the actual physical system and condition. This paper presents a case study that shows the need for state estimation models that consider GIC effects and analyzes the required increase in GIC-related measurements and models incurred therein."}, {"label": 1, "content": "Improving the effectiveness of information system design is a key objective in the field of computer science. To address this issue, a network model of information system organization has been developed to deal with the challenges posed by uncertainty. The operational numerical characteristics and parameters of the network graph have been carefully analyzed, allowing for a more accurate estimation of the probability of completing complex tasks within a fixed timeframe. The study has used a range of methods, including network planning and management, to achieve these outcomes. Through these efforts, the study has successfully increased the effectiveness of information system design, providing a more robust and reliable approach to this important area of computer science."}, {"label": 0, "content": "Dynamic security assessment (DSA) is widely used in dispatching operation systems, and calculation speed is one of its most important performance indices. In this paper, a deep learning model called Siamese neural network is proposed aiming to predict the transient stability indicators of power system, for example critical clearing time (CCT). The method is much faster than the simulation and suitable for online analysis. Firstly, a simulation sample database is constructed based on historical online data; then a Siamese model is trained, which uses static state quantities as its inputs like active power of generators. While a new online power flow needs to be evaluated, the high level features of Siamese model are obtained and a k-NN is implemented to find the most familiar samples in the database using the chosen features; the final result will be determined comprehensively by the familiar samples. The validity of proposed method is verified by the simulation using online data of State Grid Corp of China (SGCC) and different key faults. It is proved that the method meets the requirements for speed and accuracy of online analysis system, especially for small sample set."}, {"label": 1, "content": "In the envisioned next/future generation networking (N/FGN) infrastructures, 5G wireless and optical backbone networks will serve as the main pillars for supporting IoT networks. This will ensure enhanced quality of service (QoS) and relatively higher bandwidth in both access and core network sections, which will enable effective device-to-device (D2D) communication. However, secure and effective group-based authentication and key agreement (AKA) protocols are necessary for D2D driven applications and services that involve the interaction of several machine type communication devices (MTCDs) in a group.\n\nTherefore, in this paper, we present a secure and efficient Group AKA (Gr-AKA) protocol for D2D communication that achieves efficacy in maintaining the group key unlink-ability while generating minimal signalling overhead. Our protocol is compared to existing similar protocols, and it is found to require lower computational as well as signalling overhead, which improves performance in terms of fulfilling D2D communication's security requirements. In conclusion, the Gr-AKA protocol will be essential in ensuring the security and efficiency of D2D type services and applications."}, {"label": 1, "content": "The proposed control system has the potential to be implemented in various industries for the development of remote diagnostic systems. Its objective is to identify the technical condition and proper functioning of complex objects, with a view to reducing emergency operating modes, relieving the load-bearing measuring channels, and enhancing the speed and reliability of the identification process. The system utilizes the principle of maximum approximation processing to obtain accurate measuring information, which is then used to solve problems of identification promptly and effectively. Additionally, the diagnosis can be carried out remotely, directly on the object without the need for physical presence."}, {"label": 1, "content": "Deploying new base stations is an effective solution to cater to the rapidly increasing mobile traffic. Nevertheless, this solution can be quite expensive due to temporal and spatial traffic fluctuations. To tackle this issue, mobile network operators are exploring the use of drone base stations which offer flexibility, easy deployment, and can be used in multiple locations. \n\nIn this paper, we investigate the impact of using drone base stations on the performance of a cellular network. Our study includes a heterogeneous architecture consisting of a macro-base station and small cells. We compare two scenarios for the small cells: the first with a fixed number of small base stations, and the second with a variable number of drone base stations. \n\nOur results indicate that a small number of drone base stations can replace a large number of fixed ones. Moreover, using a smaller number of drone base stations compared to fixed ones improves the average bit rate of users and capacity per unit of energy by up to 75 and 78 percentage points, respectively. \n\nIn conclusion, our findings suggest that deploying drone base stations can significantly minimize the operational cost of the network while providing higher bitrates compared to traditional approaches. Therefore, mobile network operators should consider using drone base stations as a viable alternative to conventional base stations, especially in areas where traffic fluctuates or coverage is sparse."}, {"label": 0, "content": "To solve the resource optimization issue for the related power-protection services under the background of Energy Internet, this paper proposes a two-layer resource-balanced optimization Model in the planning design of the power backbone communication network. Firstly, the communication requirements of the related power-protection service are analyzed based on the description of the power-protection system. Secondly, the proposed resource-balanced model is formulated as a mathematic optimization model based on Optical Transport Network layered architecture, which comprises the optimal resource-balanced funciton and related restraints considering both the routing hops and the Quality-of-Service requirements. Finally, the simulation experiment is implemented to analyze the resource-balanced performance of the proposed model using the typical power-protection communication network topology. Simulation Results illustrate that the proposed model is of effectiveness for the power-nrotection services."}, {"label": 0, "content": "The automated deployment of cloud applications is of vital importance. Therefore, several deployment automation technologies have been developed that enable automatically deploying applications by processing so-called deployment models, which describe the components and relationships an application consists of. However, the creation of such deployment models requires considerable expertise about the technologies and cloud providers used-especially for the technical realization of conceptual architectural decisions. Moreover, deployment models have to be adapted manually if architectural decisions change or technologies need to be replaced, which is time-consuming, error-prone, and requires even more expertise. In this paper, we tackle this issue. We introduce a meta-model for Pattern-based Deployment Models, which enables using cloud patterns as generic, vendor-, and technology-agnostic modeling elements directly in deployment models. Thus, instead of specifying concrete technologies, providers, and their configurations, our approach enables modeling only the abstract concepts represented by patterns that must be adhered to during the deployment. Moreover, we present how these models can be automatically refined to executable deployment models. To validate the practical feasibility of our approach, we present a prototype based on the TOSCA standard and a case study."}, {"label": 0, "content": "The distributed generator and non-linear loads will lead to deterioration of power quality at the point of common coupling between microgrid and low voltage distribution network. Tracking and compensating for the power quality problem of the converter is a means to ensure the safe and stable operation of microgrid. At present, the research on power quality problems basically adopts the strategy of pre-emptive and post-processing, which can not guarantee the continuous and stable operation. Based on this, an advanced converter with situation awareness and orientation function is proposed. It is mainly carried out in three aspects: awareness, decision-making and execution. The situation awareness of the elements related to power quality is realized through the establishment of the information management system and the optimization of the prediction algorithm. The decision model of grid-connected converter and the optimal control are used to realize the active decision of the problem. The situation orientation of power quality problem is realized by establishing a smooth switching model of adaptive control strategy. This paper provides a new idea for more effective management of power quality problems in microgrid."}, {"label": 0, "content": "The HF radio communication has long been a big problem in channel selection since the spectrum environment is dynamic. To verify the feasibility of detecting idle channels by spectrum prediction, the data in this paper are based on realworld measurements collected by USRP in different time periods. The received signal power is converted to continuous sequences through a new channel state model reflecting spectrum availability. We then develop a prediction algorithm using simplified frequent pattern mining which can predict channel availability based on past channel states with considerable accuracy. The experimental results show that the measured data are more fluctuant in the afternoon which increase the predicted difficulty, nevertheless, the proposed algorithm is superior to neural network and Markov model in this situation, and the larger samples the better prediction performance."}, {"label": 1, "content": "The integration of the space-ground information network requires the management of multiple security domains and security gateways to preserve the security domain boundaries. Dynamic adjustment of network security functions in response to various security threats and malicious attacks can be achieved through reconfiguration of the gateways. However, determining when and what to reconfigure can be challenging. To address this problem, we proposed a privilege transition graph for the internal security domain to visualize the vulnerabilities between hosts in the domain. An algorithm combining forward breadth search and depth backtracking was introduced to identify attack paths from one host to critical resources in the domain. We also presented a method to quantify the risk of attack paths, which was tested and proved effective in providing data for reconfiguration decisions."}, {"label": 1, "content": "In real-time applications, a fast and robust visual tracker must possess a feature representation that is not only efficient but also has good discriminative capability. Appearance modeling should be adaptable to the variations of foreground and backgrounds. However, most tracking algorithms are unsatisfactory in both aspects. In this paper, a novel and efficient visual tracker is proposed by harnessing the feature learning and classification capabilities of extreme learning machine (ELM), an emerging learning technique. The contributions of this work are twofold. First, an ELM-AE-based feature extraction model is presented, providing a compact and discriminative representation of the inputs. Second, an ELM-based appearance model is developed for feature classification, allowing rapid distinction of the object of interest from its surroundings. Online sequential ELM is used to incrementally update the appearance model to cope with visual changes of both the target and backgrounds. Experiments on challenging image sequences demonstrate the effectiveness and robustness of the proposed tracker."}, {"label": 0, "content": "Alloy supports reasoning about software designs in early development stages. It is composed of a modelling language and a tool that is able to find valid instances of the model. Alloy is able to produce graphical representations of analysis results, which is essential for their interpretation. In previous work we have improved the representations with the usage of layout managers. Here, we further extend that work by presenting the improvements on the approach, and by introducing a new case study to analyse the contribution of layout managers, and to support validation trough a user study."}, {"label": 1, "content": "This paper presents a new geometric approach for analyzing parameter identifiability in models of power systems dynamics. When comparing a power system model with measured data, it can be viewed as a mapping from parameter space into a prediction space. Model mappings can be seen as manifolds with the dimensionality equal to the number of structurally identifiable parameters. Typically, model mappings correspond to bounded manifolds. \n\nThe authors propose a new definition of practical identifiability, based on the topological definition of a manifold with boundary. Their proposed definition extends the properties of structural identifiability. Numerical approximations to geodesics on the model manifold are constructed, and insights from the equations' mathematical form are utilized to identify combinations of practically identifiable and unidentifiable parameters. The approach is applied to several examples of dynamic power systems models."}, {"label": 1, "content": "Nowadays, audio generation plays a crucial role in human-computer interactive applications. However, machine-generated audio lacks the expressiveness and complexity of natural sound. Conditional variational Auto-encoder (cVAE) has shown remarkable results in data generation but suffers from defects caused by KL divergence used in stochastic distribution measurement. This paper proposes the incorporation of Hellinger distance in the cVAE model. The experiment shows that using Hellinger distance effectively improves the limitations of KL divergence. The relationship between latent space parameters and generated music quality is analyzed, and experiments reveal the distribution centroid as the best generative parameter. Subjective evaluation of the generated music shows significant improvement over the original model."}, {"label": 0, "content": "Intelligent transportation systems (ITSs) will be a major component of tomorrow's smart cities. However, realizing the true potential of ITSs requires ultralow latency and reliable data analytics solutions that combine, in real time, a heterogeneous mix of data stemming from the ITS network and its environment. Such data analytics capabilities cannot be provided by conventional cloud-centric data processing techniques whose communication and computing latency can be high. Instead, edge-centric solutions that are tailored to the unique ITS environment must be developed. In this article, an edge analytics architecture for ITSs is introduced in which data is processed at the vehicle or roadside smart sensor level to overcome the ITS's latency and reliability challenges. With a higher capability of passengers' mobile devices and intravehicle processors, such a distributed edge computing architecture leverages deep-learning techniques for reliable mobile sensing in ITSs. In this context, the ITS mobile edge analytics challenges pertaining to heterogeneous data, autonomous control, vehicular platoon control, and cyberphysical security are investigated. Then, different deep-learning solutions for such challenges are revealed. The discussed deep-learning solutions enable ITS edge analytics by endowing the ITS devices with powerful computer vision and signal processing functions. Preliminary results show that the introduced edge analytics architecture, coupled with the power of deep-learning algorithms, provides a reliable, secure, and truly smart transportation environment."}, {"label": 1, "content": "In wireless sensor networks (WSN), a secure link in the key pre-distribution (KP) scheme can be compromised if sensor nodes are captured. To address this issue, the KP q-composite scheme has been proposed. However, this scheme has a computational overhead problem when the node identifiers have a significant difference. To tackle this problem, Bechkit et al. proposed a hash chain-based KP (HCKP) q-composite scheme, which, nevertheless, still involves too many hash operations for establishing a secure link between two nodes, especially when the value of q is large.\n\nTo further enhance network resiliency against node capture and reduce the computational overhead, this paper proposes a computational overhead invariant HCKP q-composite scheme by introducing one additional hashed value in sensor nodes. By doing so, the proposed scheme effectively reduces the number of hash operations required for establishing a secure link between nodes, while keeping the storage overhead insignificant."}, {"label": 0, "content": "Pilot contamination (PC) is one of the main obstacles that limit the performance of massive multiple-input multiple-output (MIMO) systems. In this paper, we propose the asynchronous scheduling which is based on the fractional pilot reuse so that the users can be free from the pilot contamination during the uplink transmission. According to the level of interference, the users are divided into two groups, which are referred as the center users, who suffer from the mild pilot contamination, and the edge users, who suffer from the severe pilot contamination. Based on this distinction, a cell-center pilot set is reused for all the center users in all cells, whereas a cell-edge pilot set is applied for the edge users in the adjacent cells. In this case, the pilots used by the cell-edge users are orthogonal to each other. So the edge users can transmit the pilots at any time. But the pilot set for the center users are reused for all the cells, the cell-center users send their pilots in the non-overlapped time periods in order to avoid the pilot contamination. With this scheduling, the cost of the orthogonal pilots for each cell is reduced obviously. And the base station (BS) can easily recover the estimation of the pilots as it knows there is no pilot contamination. Simulation results show that the proposed asynchronous fractional pilots scheduling (AFPS) outperforms the other conventional pilot assignment schemes."}, {"label": 1, "content": "In this paper, the authors propose the design of a control chart that accounts for when the variable of interest follows the gamma distribution using the neutrosophic statistical interval method (NSIM). The NSIM is used to derive important metrics such as the average run length, the probability in-control, and the probability of out-of-control. Furthermore, the proposed control chart takes into account the neutrosophic control chart coefficient which is determined by an algorithm under the NSIM. \n\nThe neutrosophic average run length is calculated for various shifts and specified parameters which provides insight into the control chart's efficiency. The authors discuss the effectiveness of the proposed control chart through the use of a simulation study and a real-life example."}, {"label": 1, "content": "Radar Cross Section (RCS) is a crucial metric for assessing the scattering effectiveness of an object, particularly in the development of stealth weapon systems. However, calculating the RCS for multiple angles of azimuth and elevation can be a complex and inefficient process. To address this issue, we have proposed a novel approach that utilizes multilayer Long Short Term Memory (LSTM) networks based on unsupervised learning. This mechanism operates akin to an autoencoder, with the encoder LSTM mapping the input RCS to a fixed-length representation, while the decoder LSTM decodes the representation to predict RCS. To evaluate the proposed method, we generated a dataset of RCS data for a 3D object using the electromagnetic simulation software FEKO. Upon testing, the proposed networks produced satisfactory predictions of RCS values. This work demonstrates the potential of LSTM networks utilizing unsupervised learning in predicting RCS values for objects."}, {"label": 1, "content": "With the rapid development of intelligent manufacturing, the collection and analysis of product data have become increasingly important. However, researchers are facing the challenge that data acquisition systems cannot be universally used. To address this issue, a configurable data acquisition system for automatic filling lines has been designed in this paper. The system can be implemented in other working lines without the need to change the hardware. \n\nThe system is based on ARM and comprises of three parts: data acquisition, server, and cloud platform. The data acquisition part is responsible for acquiring data and transmitting it to the host computer. The server part receives data from the host computer and saves it in the database. The cloud platform provides data analysis and serves users. \n\nThe hardware and software design of the acquisition board have been analyzed, and a communication protocol has been developed to ensure data transmission. The data acquisition has been successfully implemented in this system, and the whole process is running smoothly without any issues. \n\nIn conclusion, this configurable data acquisition system for automatic filling lines is a significant step towards universal data collection and analysis in the field of intelligent manufacturing. It has great potential for future advancements, and researchers will continue to develop it further to meet the growing demand for intelligent systems."}, {"label": 0, "content": "Engine-triggered tasks are real-time tasks that are released when the crankshaft in an engine completes a rotation, which depends on the angular speed and acceleration of the crankshaft itself. In addition, the execution time of an engine-triggered task depends on the speed of the crankshaft. Tasks whose execution times depend on a variable period are referred to as adaptive-variable rate (AVR) tasks. Existing techniques to calculate the worst-case demand of AVR tasks are either inexact or computationally intractable. In this paper, we transform the problem of finding the worst-case demand of AVR tasks over a given time interval into a variant of the knapsack problem to efficiently find the exact solution. We then propose a framework to systematically reduce the search space associated with finding the worst-case demand of AVR tasks. Experimental results reveal that our approach is at least 10 times faster, with an average runtime improvement of 146 times, for randomly generated tasksets when compared to the state-of-the-art technique."}, {"label": 0, "content": "Energy consumption constitutes a significant proportion of data centers' operational costs. Furthermore, the establishment of large scale Cloud data centers due to the fast growth of utility-based IT services made the energy usage of data centers a concern. Cloud data centers use load balancing algorithms to allocate their physical resources (CPU, memory, hard disk, network bandwidth) efficiently on demand and hence optimize their energy consumption. In the load balancing process, some Virtual Machines (VMs) are selected from over-or under-utilized physical hosts and these VMs are migrated, while live and running, to other hosts. This live migration can result in Service Level Agreement Violations (SLAVs) and consequently low Quality of Service (QoS). Thus, in this paper, we propose an energy aware VM selection policy to minimize the number of migrations and consequently decrease SLAVs. Load balancing has three stages: a) Detecting over-and under-utilized hosts; b) Selecting one or more VMs for migration from those hosts; c) Finding destination hosts for the selected VMs. The focus of this research is on the VM selection stage of CPU load balancing. Our proposed VM selection algorithm considers CPU utilization of the VMs on each host and any linear correlation between the CPU usage of the VMs. The algorithm was evaluated on two different real Cloud data sets provided by the CoMon project and Google. Its performance was compared to our benchmark policy that only considers minimum migration time for VM selection. The results showed that our proposed algorithm decreases SLAVs by 66%, ESV (SLAVs \u00d7 energy consumption) by 64% and the number of \"re over-utilized\" hosts by 81% when the CPU usage of VMs in a data set are highly correlated (e.g., as in the Google data set)."}, {"label": 1, "content": "Tool wear prediction has become increasingly important due to the growing demand for improved finished quality and productivity. To achieve this, it is essential to establish a well-designed monitoring system that can obtain the relationship between tool wear and cutting process. The Generalized Regression Neural Network (GRNN) is an ideal tool for handling non-linear problems, given its memory-based character. However, in the past several decades, GRNN was rarely used for tool wear prediction. \n\nTo address this issue, in this paper, we employed GRNN to predict tool wear. In addition, we used a newly proposed evolutionary algorithm called the Fruit Fly Optimization Algorithm (FOA) to tune the smooth parameter of the GRNN. We also presented an improved version of the FOA, called the Improved Fruit Fly Optimization Algorithm (IFOA), which introduced escaping and distance control parameters to prevent FOA from falling into local optimums and enhance its search ability. \n\nThrough two cutting experiments, we found that the IFOA-GRNN provided the same level of regression ability as the GRNN with Particle Swarm Optimization (PSO), the Least Squares Support Vector Machines (LS-SVM), and the BP Neural Network. Therefore, the IFOA-GRNN is a promising tool for predicting tool wear, and its use can help improve finished quality and productivity in various industries."}, {"label": 0, "content": "Dynamic spectrum assignment of cognitive heterogeneous wireless networks is a typical integer programming problem which is difficult to obtain optimal solutions within the limit computing time. In order to obtain optimal dynamic spectrum allocation scheme, a novel discrete optimization algorithm described as quantum harmony search algorithm (QHSA) is put forward. By means of the harmony search algorithm and quantum optimization theory, the quantum harmony and harmony of the quantum harmony algorithm are developed through designing of new quantum evolutionary equations. The proposed dynamic spectrum allocation method based upon QHSA has better convergent accuracy and speed. Computer simulation indicates that the proposed dynamic spectrum assignment method based upon QHSA is superior to the dynamic spectrum assignment methods based upon previous intelligence algorithms in different cognitive heterogeneous network environments."}, {"label": 0, "content": "This paper presents a learning-based solution to tackle the real-time gesture recognition of bimanual (two hands) gestures which is not well studied from the literature. To overcome the critical issue of hand-hand self-occlusion problem common in bimanual gestures, multiple cameras from diversified views are used. A tailored multi-camera system is constructed to acquire multi-views bimanual gesture data, and data from each view is then fed into a separate classifier for learning. Thus, to ensemble results from these classifiers, we proposed a weighted sum fusion scheme of results from different classifiers. The weightings are optimized according to how well the recognition performed of the particular view. Our experiments show multiple-view results outperform single-view results."}, {"label": 0, "content": "A central problem with distributed ledger technologies involves the latency that must be incurred in processing and verifying transactions to be accepted as permanent records in the ledger. In many applications, high latency is simply not a tolerable aspect of the governance of the ledger. To help reduce latency, we offer a distributed ledger architecture, Tango, that mimics the Iota-tangle design as articulated by Popov [1] in his seminal paper. A main idea is the introduction of a semi-synchronous transaction entry protocol layer. We model periodic pulsed injections into the evaluation layer from the entry layer."}, {"label": 0, "content": "Many machine learning techniques and social engineering methods have been adopted and devised to combat phishing threats. In this paper, a novel hybrid deep learning model is proposed to identify phishing attacks. It incorporates two components: an autoencoder (AE) and a convolutional neural network (CNN). The AE is adopted to reconstruct features that enhances correlation relationship among the features explicitly. The results from the experiments show that the model is able to detect phishing attacks with a mean accuracy over 97.68%, yet it has high generalization ability and can detect phishing attacks in the receivable time scale."}, {"label": 0, "content": "To solve the problem of joint channel estimation for two-way multiple-input multiple-output (MIMO) relay systems, we propose a low complexity algorithm in this paper. At each user, the proposed channel estimation algorithm uses a unified formulation of the received signal as a Tucker-2 model. A joint channel estimation process is derived out by resorting to the proposed low complexity iterative algorithm. The proposed algorithm can provide each user with full knowledge of all channel matrices in the considering communication system. Moreover, the proposed algorithm can estimate channel effectively even when the channel becomes strongly correlated. Simulation results demonstrate the effectiveness of the proposed algorithm."}, {"label": 1, "content": "The path towards wind power forecasting has led to great socio-economic benefits on a global scale. However, previous studies have focused mostly on improving deterministic forecasting, potentially overlooking the importance of probabilistic forecasting. In this paper, we present a novel forecasting system that can perform both deterministic and probabilistic forecasting of wind power simultaneously. This system is composed of four modules: feature selection, forecasting, system optimization, and system evaluation.\n\nTo determine the optimal system input, we propose a hybrid feature selection strategy in the feature selection module. Our approach outperforms traditional gradient descent algorithms. In the forecasting module, we develop a dynamic reservoir theory-based recurrent neural network that performs better than previous methods. In the system optimization module, an enhanced multi-objective optimization algorithm is proposed to provide an optimal scenario for system parameters. The objectives of this algorithm are accuracy and stability. The final module, system evaluation, validates the effectiveness and feasibility of the proposed system.\n\nWe also conduct a comprehensive performance analysis of the proposed system. Our experimental results demonstrate that our system has a significant advantage over the benchmarks considered, further verifying its immense potential to be used in practical wind power systems. \n\nIn conclusion, our novel forecasting system offers a comprehensive solution for both deterministic and probabilistic forecasting of wind power. The proposed modules work together to optimize system parameters and provide more accurate and stable forecasts. By incorporating both deterministic and probabilistic forecasting, our system has the potential to further improve the socio-economic benefits of wind power systems."}, {"label": 0, "content": "In general, distributed renewable energy, energy storage and DC load are connected to the traditional AC distribution network through multistage converters, which leads to low energy efficiency of the system. AC/DC power distribution technology can effectively reduce the intermediate link of AC/DC transformation, and improve the economy, reliability and operation flexibility of power distribution. This paper is aimed at the economic energy problems of efficient access for high-capacity distributed renewable energy and DC load, on the basis of research before, several aspects such as system structure design, key equipment support technology and operation control technology are explored, in order to provide some ideas and references for further research and application of AC/DC hybrid system."}, {"label": 0, "content": "Gas turbine distributed energy supply system (DESS) is a kind of important black-start unit in the future power system. This paper proposes a method of using Support Vector Machine (SVM) model for fast amplitude determination of transmission line switching overvoltage in the black-start plans based on Gas turbine distributed energy supply system. Black-start is the last line of defense for ensuring the reliability of power system. Hence black-start plays an important role both in the process of system recovery to ensure system security. During the process of making black-start plans of power system, it is necessary to verify the rationality of some technical issues by repeated modeling and simulation of different black-start plans, thus costing a lot of manpower and time. In recent years, distributed integrated energy supply system is greatly supported by government because of high efficiency and less pollution. Especially, gas turbine integrated energy supply system has excellent self-start and flexible adjustment ability, which can be considered as suitable black start unit. In this paper, firstly, the black-start scenarios are classified by the function and the type of the black-start units. Secondly, transmission line switching overvoltage involved in the process of black-start are modeled through PSCAD/EMTDC simulation software and analyzed by a large number of simulations. Thirdly, a support vector machine (SVM) model is established for fast amplitude determination of overvoltage in a black-start scenario. In this model, the selection of characteristic inputs in SVM method is analyzed in detail under the influence of important technical problems and the features of Gas turbine distributed energy supply system, and then the characteristic inputs are selected by orthogonal decomposition method. In the study case, artificial neural network (ANN) and support vector machine method are used for comparison, 200 samples are used in training set and more than 1400 samples are used in testing set, the error analysis shows that the support vector machine method is more effective than the artificial neural network method in the case of small training sample size. At last, an actual example analysis which considered the Guangzhou Higher Education Mega Center distributed energy station as black-start unit shows that the fast amplitude determination of switching overvoltage model can effectively reduce manpower and time."}, {"label": 0, "content": "In this paper, we derived the analytical expressions of the system performance (in term outage probability and throughput) of the power splitting half-duplex power beacon-assisted energy harvesting relay network in both amplify-and-forward and decode-and-forward modes. Moreover, the analytical results are also demonstrated and convinced by using Monte-Carlo simulation. The numerical results demonstrated and convinced the analytical and the simulation results are matched well with each other in connection with all possible system parameters."}, {"label": 0, "content": "Internet has brought about a tremendous increase in content of all forms and, in that, video content constitutes the major backbone of the total content being published as well as watched. Thus it becomes imperative for video recommendation engines to look for novel and innovative ways to recommend the newly added videos to their users. However, the problem with new videos is that they lack any sort of metadata and user interaction so as to be able to rate the videos for the consumers. To this effect, this paper introduces the several techniques we develop for the Content Based Video Relevance Prediction (CBVRP). We employ different architectures on the CBVRP dataset to make use of the provided frame and video level features and generate predictions of videos that are similar to the other videos. We also implement several ensemble strategies to explore complementarity between both the types of provided features. The obtained results are encouraging and will impel the boundaries of research for multimedia based video recommendation systems."}, {"label": 1, "content": "The article introduces a novel approach to data sharing through a two-level system of residual classes. This group method has been tested and analyzed in the context of data sharing with error correction. The paper presents several models for protecting both cryptographic and non-cryptographic information during data sharing. \n\nThe research demonstrates the efficacy of the proposed data sharing approach. The two-level system of residual classes established a powerful framework for data sharing, which is both efficient and secure. The group method of data sharing also offers a feasible solution to error correction, making it a promising candidate for future data-sharing applications.\n\nMoreover, the article provides various models for information protection that cater to different information requirements. These models include cryptographic methods for ensuring confidentiality and integrity, as well as non-cryptographic methods for maintaining data privacy. These models are relevant to a wide range of applications and can be adapted to the specific needs of a given scenario.\n\nOverall, the article presents a comprehensive approach to data sharing that addresses key issues such as efficiency, security, and error correction. The group method of data sharing has a significant potential application in various domains such as healthcare, smart cities, and Internet of Things."}, {"label": 0, "content": "Internet of Things (IoT) is playing an important role in our lives. It connects lots of embedded devices, which can deal with very complicated and difficult tasks to facilitate our work. However, the security of IoT faces many challenges due to the wireless broadcasting nature and the energy constraint of the physical objects. In order to provide a secure environment for IoT, in this paper, we investigate two opportunistic relay selection schemes to further enhance physical layer security, which are single relay selection(SRS) and multi relay selection(MRS) respectively. We analyze the outage probability (OP) and intercept probability (IP) as well as the system tradeoff performance (STP) for SRS and MRS under Nakagami-m fading channel. Simulation results show that the OP and IP of MRS are all better than that of SRS. Besides, the STP for MRS is also more perfect than that of SRS. What's more, the STP of these two opportunistic relay selection schemes improve with the increasing of relay numbers."}, {"label": 0, "content": "Based on the related theories and methods of complex networks, this paper proposes a method for evaluating the reliability of complex software based on weighted network model for large-scale complex software systems. This model is proposed to overcome the shortcomings of existing software networks in describing the dependencies of real software systems. First, the complex software system is abstracted into a complex software network, and the internal topological structure of a large-scale complex software system is viewed in the form of the network. Secondly, a weighted network model between nodes is established on the basis of this topology, and the actual dependencies of each structure within the software system are studied and analyzed. Finally, according to the weighted network model, the reliability analysis of a real software system is carried out, and the comparison with other similar technologies proves the validity of the method."}, {"label": 1, "content": "To solve the persistent issue of low voltage at the end of feeder lines in low-voltage distribution networks (LVDNs) composed of agriculture loads, such as oxygen and water pumps, a model for distributed reactive power compensation optimal allocation has been proposed. The model includes pump load characteristics and considers the investment cost of the compensation device and financial benefits resulting from the reduction of power loss in the LVDN. Constraints relevant to the normal operation and startup of pump loads are also included in the model. \n\nTo address the discontinuous sign function in the objective function, a sigmoid function is used as an approximation. Additionally, a nonlinear penalty function with a high curvature near the discrete value is utilized to account for the reactive power compensation capacity's discrete variables characteristics. The primal-dual interior-point algorithm is then used to solve the optimization model. \n\nReal-world testing of an LVDN confirms that the proposed model's optimal allocation scheme can effectively improve the LVDN's voltage quality, ensure normal pump load startup, and decrease the power loss in the LVDN."}, {"label": 1, "content": "In today's digital and networked society, one of the defining characteristics is the digitization of personal identity. This has led to a growing need for accurate and reliable identification and authentication, which has paved the way for biometric authentication technologies. Fingerprint, face, finger vein, iris, and DNA recognition are some of the biometric technologies that are gaining prominence in our society.\n\nWith the rapid advances in Internet and cloud computing technologies, cloud biometric authentication is becoming an increasingly important area for biometric research. Among these, iris recognition stands out as a crucial biometric technology. This paper proposes a method for cloud-based iris recognition, demonstrating the link between cloud computing/storage and recognition.\n\nAs society continues to digitize, biometric authentication technologies will continue to play a critical role in ensuring secure and reliable identification and authentication. Cloud biometric authentication technology, in particular, holds enormous potential to transform the field of biometric recognition and improve its accuracy and reliability."}, {"label": 0, "content": "In this paper, we consider the finite-state approximation of a discrete-time constrained Markov decision process (MDP) under the discounted and average cost criteria. Using the linear programming formulation of the constrained discounted cost problem, we prove the asymptotic convergence of the optimal value of the finite-state model to the optimal value of the original model. With further continuity condition on the transition probability, we also establish a method to compute approximately optimal policies. For the average cost, instead of using the finite-state linear programming approximation method, we use the original problem definition to establish the finite-state asymptotic approximation of the constrained problem and compute approximately optimal policies. Under Lipschitz-type regularity conditions on the components of the MDP, we also obtain explicit rate of convergence bounds quantifying how the approximation improves as the size of the approximating finite-state space increases."}, {"label": 1, "content": "Soft shell crab farming is a popular practice in Southeast Asian countries, including Indonesia. However, farmers face significant challenges in maintaining acceptable water quality levels in crab ponds, which can negatively impact crab survival rates and overall yield. To address this issue, we propose a water quality monitoring system for crab farming using IoT technology that enables farmers to monitor and maintain optimal water conditions.\n\nOur proposed system utilizes a LoRa-based wireless sensor network and a lightweight Message Queuing Telemetry Transport (MQTT) protocol for exchanging messages between small embedded devices, mobile devices, and sensors. The system includes sensor nodes as publishers, a Raspberry pi MQTT broker, and mobile client devices as subscribers. Each sensor node consists of small embedded devices, a LoRa wireless interface, and three water quality sensors: a water temperature sensor, pH sensor, and salinity sensor. The sensors collect data on water conditions and transmit this information wirelessly to the Raspberry pi MQTT broker, which then delivers the data to mobile client devices through the MQTT protocol.\n\nTo enable remote monitoring of water quality levels, we also set up a web-based monitoring application using a node-red dashboard. This application enables farmers to access real-time water quality data remotely, allowing them to adjust water conditions as needed to improve crab survival rates and increase overall yield.\n\nIn conclusion, our proposed water quality monitoring system for crab farming using IoT technology offers a practical solution for farmers to monitor and maintain optimal water conditions. By increasing crab survival rates and yield, this system can help to improve the economic viability of soft shell crab farming in Southeast Asia."}, {"label": 1, "content": "With the rapid expansion of ultra-high voltage ac and dc projects (UHVAC/DC), a large-scale UHVAC/DC hybrid network has formed in China, increasing the complexity of power grid operation. To maintain the stability and safety of such a complex power grid, it is highly necessary and significant to perform operation characteristics analysis. However, existing power grid simulation tools are not suitable for this challenging task due to requirements for high accuracy modeling, large-scale electro-magnetic transient simulation, and massive off-line calculation. In order to address these issues, a new generation UHVAC/DC power grid simulation platform (NGSP) architecture has been proposed in this paper, along with detailed design schemes. These designs allow for computing on a scale larger than 6000 nodes in digital-analog hybrid simulation, and a speed-up ratio of over 3000 in digital simulation, making it the most powerful power grid digital simulation platform in the world. Comparative results between simulation and engineering recorded data are presented to verify the correctness and effectiveness of NGSP. Additionally, the practicality of NGSP is demonstrated through application effects described in this paper."}, {"label": 1, "content": "The article discusses the challenges surrounding the development of predictive neural network models using the residual number system, particularly in enhancing the quality of automatic control systems by integrating a prognostic component. For astatic control objects, this system is particularly critical in offering an advanced level of control. \n\nThe article also highlights how neural network training algorithms can be effectively executed in the residual number system, which can substantially boost the performance of these training algorithms. Additionally, adding new features to automatic control systems in the shape of prognostic neural network models is more straightforward with this approach."}, {"label": 0, "content": "Systems based on the concept of `Internet of Things' (IoT) differ by multi-tiered architecture, a great number of used `things', the influence of new types of attacks, the incompleteness and ambiguity of their parameters. For these reasons, solving security management tasks in IoT networks, such as network traffic analysis, requires applying intelligent approaches and methods. The purpose of the paper consists in development and assessment of a new algorithm of the network traffic analysis in a real or near real time. The paper also considers various variants for implementation of intelligent agents intended for network traffic analysis in IoT networks in different cases: (1) high-performance computers, (2) embedded devices, and (3) systems-on-chip. The agents are based on the algorithm of pseudo-gradient anomaly detection and fuzzy logical inference. The suggested algorithm operates in real time. The experimental assessment of the approach shows that the gain can reach 50% in accuracy and 90% in speed."}, {"label": 1, "content": "Advanced communication and network technologies have revolutionized the human experience and have created significant impacts on all areas of people's lives. Wireless sensor networks rely on sensor nodes for support, and energy harvesting and transfer technology offers a solution for their long-term survival. However, the single collection scheme results in wasteful energy use, and efficient energy utilization and fast data collection remain significant challenges for these networks. \n\nTo address these challenges, an adaptive collection scheme based on matrix completion (ACMC) has been proposed. The ACMC scheme is designed to reduce delay and improve the energy utilization of the network. Compared to traditional data collection schemes, the ACMC scheme is dynamic, collecting large amounts of data when sufficient energy is available to obtain high-quality data-based applications. When energy is limited, selective collection based on previously collected data reduces the amount of data collected while still meeting application requirements, thus improving energy utilization. \n\nThe ACMC scheme also proposes a method to reduce delay by increasing the duty cycle of the nodes that are located far away from the CC. The increase in the transmission frequency increases transmission reliability, further decreasing network delay. Experimental results of the ACMC scheme on planar networks show that it outperforms traditional data collection schemes. It can improve the energy utilization of the network by 4.26% to 6.68% while reducing the maximum delay by 9.4%. \n\nIn summation, the ACMC scheme offers an efficient and dynamic solution for energy harvesting wireless sensor networks. With its adaptive data collection and delay reduction features, it enables the network to maximize energy utilization and deliver high-quality data-based applications."}, {"label": 0, "content": "The huge AC power girds of China have been all connected by dozens of HVDC links now. To understand the behavior of the complicated network and mechanism of interaction between AC and DC, there is a demand for a more precise simulation technology. Traditional power system analysis tools, such as electromechanical transient program and off-line electromagnetic transient program, the models of DC control and protections are simplified to some degree. Sometimes the simplification may not reflect the same act that grid may do as in the field. In order to get a detailed, panoramic view, it is believed that simulating the large-scale network model by electromagnetic transient software in real-time with actual DC control and protection devices linked will give a better performance. In this way, the simulation time step should be small enough so that the physical DC controllers can be connected to realize hybrid simulation. Normally, microsecond level is applied. However, running a tremendous AC/DC grid model of thousands of buses with a time step of microseconds is the precondition of digital-analog hybrid simulation, and also a great challenge to all experts in the field of power system real-time simulation of the world. Based on task mapping and parallel computing technology, power system real-time simulator HYPERSIM and supercomputer SGI UV300 are used in this paper to achieve the goal. Considering of the processor's computational capability, on account of naturally decoupling by long distributed parameter lines, this paper optimizes the method of large-scale grid decoupling, splits the sophisticated region grid model into distributed tasks reasonably. Exploring the automatic task mapping function in HYPERSIM, a synthetic method about multiple auto-assigned parameters optimization is proposed. Eventually, an electromagnetic transient network model of 17746 single phase buses can be operated in real-time, which is a breakthrough on scale expanding of real-time simulation. The precision of AC/DC grid simulation makes a step further. The realization of massive grid model real-time operation gives a new technical means for characteristic cognition, system planning, operating, decision-making and failure reproduction of Chinese future power system."}, {"label": 1, "content": "Radio frequency (RF) communication technology has found widespread application in data acquisition systems, particularly in low-voltage electric areas. However, co-channel interference, which is an inherent property of RF communication, can have a detrimental impact on communication performance. In order to improve acquisition efficiency, this paper proposes an RF channel selection method that is based on adaptive sensing. The method involves several key steps, including channel evaluation, adaptive sensing, channel negotiation, and power control. Simulation and experiments were conducted in the advanced metering infrastructure (AMI) domain to validate the efficacy of the proposed method. The results demonstrate that the method can significantly increase acquisition efficiency and mitigate the negative effects of co-channel interference."}, {"label": 1, "content": "Data infrastructure and quality are crucial components that influence the overall health of any organization. They are fundamental for creating and delivering business insights, and stakeholders expect a flawless experience, real-time solutions, and support. Unfortunately, these expectations are often too high for IT departments who are inundated with various users' requests. However, the Big Data era and its analytics have enabled organizations to predict and forecast users' needs to produce an excellent user experience. Achieving this relies on intuitive design, error-free coding, and quality performance. As organizations strive to establish a solid foundation by building out a data management ecosystem that delivers the required flexibility and performance, the demand for better business analytics is increasing. The ultimate aim is to generate the actionable insights needed to achieve academic business goals through leveraging better analytics to process the continually generated data from various sources. This paper takes a case study of a South African university, using semi-structured interviews, to explore how universities can use better analytics to achieve academic business objectives. The study found that data value and its use in academic management must be consistent with faculty business plans. By aligning business analytics strategically with faculty plans and overall institutional strategic objectives and targets, it will not be haphazardly employed but will be deliberate in addressing needs and in turn produce the desired results."}, {"label": 1, "content": "Currently, there is a lack of tools available for validating 5G scenarios. Due to the increasing traffic demand on 5G networks, network operators are in search of cost-effective solutions. One such solution being adopted is the multi-tenancy approach, which requires changes to the network architecture to accommodate user mobility. This shift towards more dynamic services necessitates the development of new tools that can provide these capabilities to facilitate the validation of all new developments. To meet this need, this work presents a novel experimentation framework for the emulation of 5G scenarios with real-time user mobility and multi-tenancy support. This framework has been validated through a series of experiments, establishing its functional viability."}, {"label": 0, "content": "To solve the problem of the low detection rate of minority samples in imbalanced datasets in network intrusion detection, a deep learning intrusion detection model based on optimized imbalanced data is proposed. Firstly, a hybrid sampling method is adopted in data processing. Synthetic Minority Over-sampling Technique (SMOTE) was used to increase the numbers of samples in minority categories and the majority of the samples were under-sampled by Neighborhood Cleaning Rule (NCL). Secondly, on the preprocessed balanced dataset, the high-dimensional data was reduced by Deep Belief Network (DBN) to obtain the lower low-dimensional representation of the preprocessed data. Finally, the classification work was completed by Probabilistic Neural Network (PNN). The experiment on NSL-KDD dataset showed that hybrid sampling can improve the detection rate and classification accuracy of minority categories. And the performance of DBN-PNN is obviously superior to the traditional method."}, {"label": 1, "content": "This paper proposes a fast and precise analytical model for calculating iron loss in inverter-fed induction machines. The model takes into account the impact of output voltage harmonics from the inverter and slot harmonic components on the motor's iron loss, using a piecewise variable coefficients method. The accuracy of the proposed model is demonstrated by comparing calculated core loss values with those measured from 5.5- and 55-kW inverter-fed induction motors, under different speeds and load conditions. The proposed model achieves desirable accuracy while significantly reducing computational burden, as compared to classical iron loss models and finite-element-based piecewise variable coefficient models."}, {"label": 1, "content": "5G, the upcoming wireless technology, is set to be launched in 2020. However, the increased exposure to electromagnetic radiations emitted by mobile phones has raised concerns about the potential health effects from exposure to Radio Frequency (RF) and Millimeter (mm) wave radiations on human brain tissue. Hence, this paper aims to investigate the effects of 5G radiations for different frequency candidates on the human brain through the use of Computer Simulation Technology (CST) software. We conducted simulations on Specific Anthropomorphic Mannequin (SAM), a model designed according to various international standards, to represent the average material properties of the head. The calculations obtained by comparing the Specific Absorption Rate (SAR) of the human brain with the safety limit of exposure to high frequency radiations set by different international standards have shown that the resulting exposure of 5G radiations is safe."}, {"label": 1, "content": "This article showcases a novel approach to human activity classification in handheld devices such as iPod Touch and smartphones, utilizing shape descriptor-based features. The signals acquired from the in-built accelerometer and gyroscope sensors are analyzed to identify various activities performed by the user. In order to capture discriminative information accurately, shape descriptor-based features are calculated, normalized, and consolidated into one feature vector. The k-nearest neighbor classifier is employed to identify the user's activity accurately. The proposed approach is evaluated using the publicly available dataset, Physical Activity Sensor Data. The results show a remarkable improvement in classification accuracy as compared to existing work, proving the effectiveness of the shape descriptors in activity classification."}, {"label": 1, "content": "To achieve effective fault diagnosis and remaining useful life prediction when system states and fault parameters operate on different time-scales, a double time-scale particle filter algorithm has been proposed that allows for the joint estimation of both the system states and possible fault parameters. A bond graph model has also been developed, which unifies the different energy forms and allows for fault diagnosis through fault detection and isolation via analytical redundancy relations. This allows for the calculation of residual system integrity. Fault estimation is then used to identify the parameters of the isolated fault set, determine the true fault, and predict remaining useful life. Due to the slow time-varying nature of fault parameters and the quick time-varying characteristics of the system states, the double time-scale particle filter has been designed to simultaneously estimate and predict the system states and parameters on two different time-scales, for diagnostic and prognostic purposes. Simulation results on a non-linear electromechanical system have demonstrated the effectiveness of the proposed double time-scale particle filter algorithm."}, {"label": 1, "content": "In this paper, we introduce DiFRuNNT - a real-time deep neural network architecture that tackles the challenge of disguised face verification. Our proposed model comprises two neural networks, with the first one being a convolutional neural network (CNN) that predicts the spatial locations of 20 distinctive facial key-points within the image. The second neural network then classifies the subject based on the calculated angles and ratios derived from the predicted points. \n\nOur experiments have shown accuracies of 67.4% and 74.8% for prediction and classification respectively. These results are compared with the state-of-the-art methodologies in the field, demonstrating the strong potential of our proposed approach."}, {"label": 1, "content": "Visible Light Communication (VLC) is a wireless data transmission technology that uses optical scintillation. However, the challenge of balancing illumination and communication remains. To address this, this paper proposes using dimming control to adjust brightness for human eyes and non-orthogonal multiple access (NOMA) to improve system throughput. \nFirst, a model combining signal power allocation with dimming control is established. Second, gain ratio power allocation (GRPA) and variable on-off keying dimming control are introduced to improve spectral efficiency. Third, the relationships between luminescent angles, user data rate, and luminous intensity are analyzed for indoor users. \nExperimental results show that the proposed GRPA scheme outperforms previous strategies in terms of user data rate at the same dimming factor. Additionally, optimal indoor VLC cell deployment of semi-angles and dimming factors is discussed."}, {"label": 0, "content": "Deploying new base stations is being used as an effective solution to satisfy the tremendous increase in mobile traffic. However, this solution is costly due to temporal and spatial variation of traffic, where mobile network operators need to deploy base stations that may work for limited periods of the day. Due to its high flexibility, low deployment time and possibility to be used in more than one location, the usage of drone base stations is a promising alternative that can reduce the operational cost of the network while providing higher bitrates compared to traditional approach. In this paper, we study the impact of using drone base stations on the performance of a cellular network. We consider a heterogeneous architecture composed of a macro-base station overlaid with base stations serving small cells. For the small cells, we compare two scenarios; the first corresponds to a specific number of fixed small base stations while the second corresponds to variable number of drone base stations serving small cells. Results show that a small number of drone base stations can replace large number of fixed ones. Moreover, using smaller number of drone base stations compared to fixed ones increases the average bit rate of the users and capacity per unit of energy up to 75 and 78 percentage points respectively."}, {"label": 0, "content": "The paper considers the method of forming a training sample for intelligent methods of predicting electricity loads based on artificial neural networks. The training sample is formed taking into account the criteria of informativeness and compactness. It is shown how much the accuracy of the forecast can be increased with the approach used."}, {"label": 0, "content": "Write Variation is an inevitable non-ideal effects for Resistive Random Access Memory(ReRAM) based On-chip Neural Networks training, which incurs significant accuracy drops and affects the system robustness. In this paper, we have proposed a Hierarchical Crossbar(HC) to achieve a software competitive accuracy and enhanced the system robustness to random effects. The accuracy on MNIST can be enhanced 10% for 2000 iterations on-chip fine-tuning comparing with conventional crossbar."}, {"label": 0, "content": "To solve the problems of the data reliability for NAND flash storages, a variable-node-based belief-propagation with message pre-processing (VNBP-MP) decoding algorithm for binary low-density parity-check (LDPC) codes is proposed. The major feature is that, by making use of the characteristics of the NAND flash channel, the proposed algorithm performs the message pre-processing (MP) scheme to effectively prevent the propagation of unreliable messages and speed up the propagation of reliable messages. To further speed up the decoding convergence, the treatment for oscillating variable nodes (VNs) is considered after the MP scheme being employed. Simulation results show that the proposed VNBP-MP algorithm has a noticeable improvement in convergence speed without compromising the error-correction performance, compared with the existing algorithms."}, {"label": 1, "content": "Variation is an inevitable non-ideal effect for resistive random access memory (ReRAM) based on-chip neural networks training. It results in significant accuracy drops and adversely affects the system's robustness. To address this issue, we propose a hierarchical crossbar (HC) architecture that helps achieve software competitive accuracy and enhances the system robustness to random effects. By leveraging the HC approach, we observed that the accuracy on the MNIST dataset could be improved by up to 10% for 2000 iterations compared to conventional crossbars. Overall, HC represents a promising solution for mitigating variation in ReRAM-based on-chip neural networks training."}, {"label": 1, "content": "High-dimensional and sparse (HiDS) matrices produced by recommender systems contain valuable information on various patterns such as users' potential preferences and community tendencies. Extracting knowledge from such HiDS matrices highly efficiently is possible through latent factor (LF) analysis. The stochastic gradient descent (SGD) algorithm is highly efficient in building an LF model, but present LF models mostly use a standard SGD algorithm. Can SGD be expanded from various angles to enhance the convergence rate and prediction accuracy of the resulting models for missing data? Are such SGD expansions compatible with an LF model? This paper examines eight expanded SGD algorithms to propose eight new LF models. Experimental results on two HiDS matrices produced by real recommender systems demonstrate that an LF model with expanded SGD algorithms can achieve higher prediction accuracy for missing data, faster convergence rates, and model diversity compared to an LF model using the standard SGD algorithm."}, {"label": 0, "content": "A statistical model for predicting the output power and energy of Solar Photovoltaics (PV) has been developed. The multiple input single output (MISO) system is based on Jackknife regression and generates PV power in kilo-watts in response to inputs that include irradiance, precipitation, ozone, ambient temperature and atmospheric aerosol components. The model is trained and tested on data from National Renewable Energy Laboratory and residual statistical tests are applied to validate the estimation results. An absolute error of less than 1 kW is observed for 90.6 % of the predicted values that corresponds to a percentage error of less than 8.33 % for the 12 kW system under study."}, {"label": 0, "content": "An emerging issue of GDPR (General Data Protection Regulation) and other like similar concept is facing now and future on all intellectual data base. The most secured situations are on standalone operations for data handling during the certain processing, however it is difficult to get relatively higher performance processing such as AI with deep learning. One development approach will present here for those purpose. Deep learning is consisted by the two functions which one is trainings by big data bases and other is making inferences for the objected needs. The inferences are provided by simple hardware for the processing that can make standalone module. On the other hand, training processing is structured by huge repeating calculations to require heavy hardware or cloud interface, consequent hard to produce mobile feature meant standalone module. Our development approach is with two novel functions to resolve heavy training processing. One problem would be how to access with high speed from SSD storage. Another problem is how to reduce power and calculation time by huge repeating processing. Our approach is based by lookup table (LUT) subsequent zero calculation to enable power reduction and shorten calculation-time. The architecture is with the dynamic reconfiguration by Memory Logic Conjugated System (MLCS) even easy realized for high speed execution that is a non-Neumann processor. In the conclusion, our trial demonstration module has the 1W of effective power and 400Mops of the processing performance in a commercial based right FPGA evaluation board. If it would be arranged by custom designed SoC, that would suppose to be 0.5W and 2Gops. Both cases are perfectly enough for the standalone modules for middle range of DL execution with seamless training and inference operations on real time sequences. The operation with dynamic reconfiguration is furthermore on the performance that has not been realization in any processing systems so far."}, {"label": 1, "content": "Mission-critical wireless sensor networks have the potential to obtain important information in complicated environments and to enable various mission-critical applications, such as industrial automation and security surveillance. Nevertheless, to exploit these networks for such applications, it is crucial to have agile and scalable network management systems that guarantee efficient data transfer and computing task implementation. In this paper, we present a software-defined mission-critical wireless sensor network (MC-SDWSN) that can address the complex issues in traditional WSNs, like resource utilization, data processing, and latency requirements. The architecture of the MC-SDWSN is based on the SDN architecture and incorporates hierarchical cloud and edge computing technologies. Additionally, we propose a centralized computation offload strategy that demonstrates the feasibility of the MC-SDWSN architecture in sensor network applications. The simulation results confirm that the MC-SDWSN architecture, along with the edge offloading strategy, could effectively support critical missions."}, {"label": 1, "content": "Alloy is a powerful technology that supports software design reasoning in the early stages of development. It provides a modelling language and a powerful tool that can generate valid instances of the model. The technology can create graphical representations of analysis results that are essential for interpreting the software design. Our previous work on this technology has improved the graphical representations using layout managers. Building on this work, we are pleased to introduce further improvements to our approach. We have developed a new case study that explores the contribution of layout managers to software design and supports the validation of our approach through a user study. The use of layout managers provides enhanced visualisation and interpretation of the analysis results, and our work demonstrates the significant value that this technology can bring to software design reasoning."}, {"label": 1, "content": "In recent years, clustering has emerged as a promising approach for facilitating data routing and data aggregation in Wireless Sensor Networks (WSNs). However, clustering-based routing approaches are not suitable for large-scale WSNs, as they suffer from the adverse effects of isolated nodes in the network and some coverage problems, as is the case in LEACH [1]. To address these challenges, we propose LEATCH-L, a Low Energy Adaptive Tier Clustering Hierarchy for Large-scale WSNs. \n\nThe proposed approach makes the major functions of LEACH applicable to large-scale WSNs whose dimension is much larger than the largest transmission radius of the sensor nodes. This imposes a dynamic decomposable structure on the network topology, resulting in a set of smaller subnetworks. This decomposition is implemented through a smart m-level hierarchical clustering process. Furthermore, the proposed approach involves a two-level data aggregation. \n\nEvaluation results demonstrate that the introduced approach is scalable, with significantly much better performance than state-of-the-art approaches. Therefore, LEATCH-L is a promising solution for large-scale WSNs that can enhance the efficiency and effectiveness of data routing and data aggregation in these networks."}, {"label": 0, "content": "The principal observed progressive swim types of sperm cells are linear mean and circular swim. Using motility characteristic parameters produced by CASA systems, we perform a parameter subset search to produce distinct clusters of the different swim types. For this task, the artificial bee colony algorithm (an iterative search algorithm modeled after the collective behavior of bees) and the well-studied k-means clustering algorithm were used on simulated and human sperm swim data. The result is distinct clusters with features of each types of swim. The clustering approach displays potential as a tool for automated sperm swim subpopulation analysis."}, {"label": 1, "content": "In this paper, a new algorithm is proposed for estimating parameters of multiple frequency-hopping (FH) signals. The algorithm is based on maximum energy difference and involves several steps. First, a time-frequency (TF) matrix is obtained using short-time Fourier transform (STFT) or smoothed pseudo wigner-ville distribution (SPWVD). The carrier frequency and the TF data with valid frequency are then determined based on the energy distribution. The number of signal segments and window length in each nonzero row of the TF data are obtained next. Finally, hopping time and hop cycle are estimated using the maximum energy difference.\n\nSimulation results show that the proposed algorithm outperforms the TF pattern modification method and the STFT-SPWVD method in terms of anti-noise performance. Moreover, the new method is suitable for both asynchronous and synchronous networks. Overall, the algorithm provides an effective approach for parameter estimation of multiple FH signals."}, {"label": 1, "content": "The principal has observed that sperm cells have two main types of swim \u2013 linear mean and circular. To differentiate between the different swim types, we have utilized motility characteristic parameters produced by CASA systems and performed a parameter subset search. By leveraging the artificial bee colony algorithm and the well-studied k-means clustering algorithm on both simulated and human sperm swim data, we were able to produce distinct clusters with features of each swim type. This clustering approach shows great potential as a tool for automated sperm swim subpopulation analysis."}, {"label": 1, "content": "Security of vehicular networks has often been overlooked since they were traditionally designed to be a closed system. However, an attack on such a network could have catastrophic effects, including human casualties or severe injuries. To tackle this issue, this paper proposes a novel algorithm that extracts the real-time model of the controller area network (CAN), which is then used to develop a specification-based intrusion detection system (IDS) that utilises anomaly-based supervised learning.\n\nThe proposed IDS has been evaluated using real CAN logs collected from a sedan car, with encouraging results. By utilising the real-time model of the CAN, the system is able to detect and respond to anomalies quickly, preventing potential attacks and protecting the lives of drivers and passengers.\n\nIn conclusion, the importance of vehicular network security cannot be overstated, and the proposed algorithm and IDS offer an effective way to enhance the security of these networks. The results suggest that this approach is a promising direction for future research in this field."}, {"label": 0, "content": "The proposed control can be applied at different enterprises of industry in the building of systems of remote diagnostics of technical condition and determining of the correct functioning of multivariable objects aimed at reducing the number of emergency operating modes of electric equipment, reduce the load measuring channels and improve the speed and reliability of solving the problems of identification of technical condition of Electromechanical systems. This is achieved through the implementation of the principle as maximum approximation processing of the measuring information to the places of its occurrence and solution of problems of identification directly on the remote object."}, {"label": 0, "content": "In this paper, we present a real-time deep neural network architecture (called DiFRuNNT) for disguised face verification. The proposed model consists of two neural networks, first one being a convolutional neural network (CNN) that predicts 20 facial key-points in the image and the second neural network classifies the subject based on the angles and ratios calculated from the predicted points. The accuracies are 67.4% and 74.8% for prediction and classification respectively and the results have been compared with the state-of-the-art methodologies also."}, {"label": 1, "content": "To achieve a more comprehensive and accurate estimation of 10kV distribution network line losses, a new evaluation method has been proposed. This method is based on the use of a BP neural network improved with particle swarm optimization (PSO). To ensure sufficient data resources, relevant information related to the 10kV distribution network line and line loss has been collected and integrated from the State Grid Corporation of China. This information includes power grid equipment data and operation data gathered during the process of power distribution and utilization.\n\nThe proposed method involves the selection and establishment of electrical characteristic indices to reflect the structure and operation state of the 10kV distribution network. The inertia weight and acceleration coefficient of PSO are dynamically adjusted to enable more effective search of the weights and biases of BPNN. Nonlinear relations between the electrical characteristic indices and line losses are fitted through the learning of training sample sets. This approach enables the prediction of line losses of test sample sets.\n\nThe effectiveness and appropriateness of the PSO-BPNN method has been demonstrated using actual 10kV distribution network sample data. This new approach offers an improved method for estimating 10kV distribution network line losses by making full use of available data resources to develop a more sophisticated and accurate model."}, {"label": 1, "content": "Transmission network extension is imperative due to the impact of transmission lines on power market businesses. However, the process is currently restricted due to various factors, including financial constraints, health concerns surrounding electromagnetic fields, and substantial contemplation. In light of the ever-growing demand for electric power, it is essential to utilize the available transmission network assets. One option that may prove fruitful in enhancing power flow capacity (PFC) within the existing transmission network is the implementation of flexible AC transmission systems (FACTS).\n\nThese devices diversify the tracks of exchanged power over transmission lines by changing the bus voltage angle and the reactance of transmission lines. This study focuses on a modified miniature of the Unified Power Flow Controller (UPFC) with Fuzzy Logic (FL) based shunt and serial controllers to improve power network stability. Fuzzy control is incorporated into the control scheme of UPFC and Pulse width modulation. The critical case study comprises a 3-phase fault occurrence in a power system, and simulations were performed using MATLAB/Simulink software.\n\nThe obtained results were compared with the presence and absence of UPFC, as well as with a PID controller, to present the effectiveness of the proposed controller. The findings indicate that the Fuzzy Logic-based UPFC performed better than the PID controller. Therefore, it can be concluded that the implementation of FACTS devices, specifically Fuzzy Logic-based UPFC, shows great potential to improve PFC capacity in the existing transmission network."}, {"label": 0, "content": "Supporting real-time communications over Wireless networks (WSNs) is a tough challenge, due to packet collisions and the non-determinism of common channel access schemes like CSMA/CA. Real-time WSN communication is even more problematic in the general case of multi-hop mesh networks. For this reason, many real-time WSN solutions are limited to simple topologies, such as star networks. We propose a real-time multi-hop WSN MAC protocol built atop the IEEE 802.15.4 physical layer. By relying on precise clock synchronization and constructive interference-based flooding, the proposed MAC builds a centralized TDMA schedule, supporting multi-hop mesh networks. The real-time multi-hop communication model is connection-oriented, using guaranteed time slots, ad enables point-to-point communications also with redundant paths. The protocol has been implemented in simulation using OMNeT++, and the performance has been verified in a real-world deployment using Wandstem WSN nodes."}, {"label": 0, "content": "Nutrient elements of NPK are macro nutrients that play an important role in the growth and development of plants, therefore it is necessary to measure NPK nutrient content to measure how well soil fertility condition before the land planting period, but NPK measurement through laboratory tests takes a relatively long time. This research develops a prototype of NPK nutrient measurement system based on a mobile application by using soil image for determining the textural characteristic, the textural characteristics are processed with local binary pattern and back-propagation neural network to accelerate the measurement process.Sample data in this research was taken on rice field land in the province of Yogyakarta Special Region by varying the distance at 30 cm to 110 cm with interval 20 cm and angle image capture at -30\u00b0 to 30\u00b0 with interval 10\u00b0. Datasets were being pre-processed to improve image quality and adjust image format. Preprocessed results are extracted using local binary pattern uniform to obtain texture features. The texture features were being inputted of the neural network model, that being trained with a back-propagation algorithm by varying parameters of the neural network model.The model tested to determine the effect of distance and angle of image capture, system processing speed, and effect of artificial neural network parameters. The best model is implemented on a smartphone application. The results obtained an average of computation time 0.65s, and the optimal result is obtained at distance capture of 50 cm and angle capture of 0\u00b0 with the measurement accuracy at each soil nutrient level of nitrogen 91.80%, while phosphorus 83.49%, and potassium 82.54%, therefore the average is 84.16%."}, {"label": 1, "content": "This article describes a pre-processed version of the faster region convolution neural network (faster RCNN) for the detection of on-road vehicles. The system incorporates a preprocessing pipeline to improve the training and detection speed of faster RCNN. A Sobel edge operator and Hough Transform based preprocessing lane detection pipeline is employed to detect lanes. From the lane coordinates, a rectangular region is extracted which represents a reduced region of interest. The results of the study indicate that the proposed pre-processing approach enhances the training speed of faster RCNN in comparison to the standard faster RCNN without any preprocessing."}, {"label": 0, "content": "While houses were simply connected to the electrical grid and contained a gas or oil heating system, for a long time energy flows were simple. With the integration of alternative energy systems such as photovoltaic (PV) systems, heat pumps, combined heat and power units (CHPs), home batteries and electrical cars into houses, energy flows become more complicated. This paper describes the implementation of a model to simulate the technical and economical outcome of a system with the previously mentioned components in single- and multi-family houses in Germany. As input for climate data and energy demand, literature data is used. The simulation is capable of economic and ecologic estimation and can be controlled by a web interface with the intention to provide a straightforward online tool for homeowners."}, {"label": 0, "content": "WTCM (Wind Turbine Condition Monitoring) system is important for wind farm operators to realize condition-based O &M (operation & maintenance), in the purpose of reducing O &M cost and improving wind turbine reliability. A WTCM method using only SCADA data based on data mining algorithm is proposed in this paper. Firstly, ARD (Automatic Relevance Determination) algorithm is adopted to determine the effective variables that are relevant to wind turbine condition. Feature vector is then extracted using the effective variables to represent the operation condition of wind turbine. Finally, the condition of a wind turbine is determined using outlier detection algorithm based on the extracted feature vector. Real-world dataset is used to validate the efficiency of the proposed method. Experiment results show that the proposed method can provide advanced failure alarm for wind turbines many days before failure happens. O &M cost can be reduced by condition-based O &M strategy using the result of our proposed WTCM method."}, {"label": 1, "content": "We focused on addressing the challenge of achieving balanced use of sensor nodes' battery power in order to maximise the overall lifetime of ad-hoc Wireless Sensor Networks. To optimise the network lifetime, it is important to utilise less attended sensor nodes as compared to heavily used ones. To achieve this, we proposed a joint optimisation problem that involves selecting a subset of active sensor nodes and a multi-hop routing structure that interconnects all selected sensor nodes, thereby enabling routing of the aggregated information to a querying node. \n\nHowever, our optimisation problem is non-convex over both the subset selection and the multi-hop routing paths selection, which makes it computationally demanding and one of the NP-hard problems. To overcome this, we relaxed one of the variables, thereby making the problem convex and easier to solve efficiently. Additionally, we proposed an iterative algorithm to solve the problem distributively. \n\nOur simulation results demonstrated that both the aforementioned approaches are effective in enhancing the overall network lifetime within a given power budget. Moreover, the distributed approach yields an optimal routing structure, outperforming the well-known shortest path tree based routing structure."}, {"label": 0, "content": "Just noticeable difference (JND) for stereoscopic 3D content reflects the maximum tolerable distortion; it corresponds to the visibility threshold of the asymmetric distortions in the left and right contents. The 3D-JND models can be used to improve the efficiency of the 3D compression or the 3D quality assessment. Compared to 2D-JND models, the 3D-JND models appeared recently and the related literature is rather limited. In this paper, we give a deep and comprehensive study of the pixel-based 3D-JND models. To our best knowledge, this is the first review on 3D-JND models. Each model is briefly described by giving its rationale and main components in addition to providing exhaustive information about the targeted application, the pros, and cons. Moreover, we present the characteristics of the human visual system presented in these models. In addition, we analyze and compare the 3D-JND models thoroughly using qualitative and quantitative performance evaluation based on Middlebury stereo datasets. Besides, we measure the JND thresholds of the asymmetric distortion based on psychophysical experiments and compare these experimental results to the estimates from the 3D-JND models in order to evaluate the accuracy of each model."}, {"label": 1, "content": "In this paper, we present a technique utilizing deep reinforcement learning (DRL) for unmanned aerial vehicles (UAVs) to execute navigation tasks in large-scale complex environments. This method has significant applications in remote surveillance and goods delivery. Our approach involves formulating the problem as a partially observable Markov decision process (POMDP) and solving it using an innovative online DRL algorithm. This algorithm is designed based on two policy gradient theorems which have been strictly proven within the actor-critic framework. \n\nUnlike traditional methods such as simultaneous localization and mapping-based or sensing and avoidance-based approaches, our technique directly maps the UAVs' sensory measurements into control signals for navigation. Through experimentation, we demonstrate that our method allows UAVs to perform autonomous navigation within virtual large-scale complex environments. Moreover, we prove that our approach can be generalized to more extensive, more complicated, and three-dimensional environments. \n\nOur proposed online DRL algorithm addressing POMDPs exceeds the state-of-the-art in terms of performance, making it an innovative solution for UAV navigation tasks in large-scale complex environments."}, {"label": 0, "content": "Route randomization is an important research focus for moving target defense which seeks to proactively and dynamically change the forwarding routes in the network. In this paper, the difficulties of implementing route randomization in traditional networks are analyzed. To solve these difficulties and achieve effective route randomization, a novel route randomization approach is proposed, which is implemented by adding a mapping layer between routers' physical interfaces and their corresponding logical addresses. The design ideas and the details of proposed approach are presented. The effectiveness and performance of proposed approach are verified and evaluated by corresponding experiments."}, {"label": 0, "content": "This article suggests an algorithm of formation a training set for artificial neural network in case of image segmentation. The distinctive feature of this algorithm is that it using only one image for segmentation. The segmentation performs using three-layer perceptron. The main method of the segmentation is a method of region growing. Neural network is using for get a decision to include pixel into an area or not. Impulse noise is using for generation of a training set. Pixels damaged by noise are not related to the same region. Suggested method has been tested with help of computer experiment in automatic and interactive modes."}, {"label": 1, "content": "Person re-identification has gained popularity in various fields, such as security, criminal investigation, and video analysis. The purpose of this paper is to develop a two-stage attribute-constraint network known as the TSAC-Net to learn a discriminate and robust spatial-temporal representation for video-based person re-identification. The use of pedestrian attributes containing high-level information can aid in re-identification tasks and is resilient to visual variations. In this paper, we manually annotated three video-based person re-identification datasets with four static appearance attributes and one dynamic appearance attribute, where each attribute serves as a constraint added to the deep network. In the TSAC-Net's first stage, we solved the re-identification problem by using a classification approach and adopted a multi-attribute classification loss to train the CNN model. In the second stage, two LSTM networks were trained under identity and dynamic appearance attribute constraints. With this approach, the two-stage network offers a spatial-temporal feature extractor for pedestrian sequences in videos. During the testing phase, inputting a sequence of images to the proposed TSAC-Net creates a spatial-temporal representation. Our method demonstrates an improvement in performance attained with attribute usage on several challenging person re-identification datasets such as PRID2011, iLIDS-VID, MARS, and VIPeR. Furthermore, our extensive experimentation has shown that our approach's state-of-the-art performance achieves results for three video-based benchmark datasets."}, {"label": 0, "content": "Iris segmentation is a critical part in iris recognition systems. It segments the acquired image into iris and non-iris parts. It is the foundation of subsequent processing. The errors in this stage are propagated to subsequent processing stages, which will affecting the recognition rate of the whole system. A majority of iris segmentation algorithms require a significant amount of user cooperation during image acquisition process to provide good segmentation performance. However, the quality of iris images can not be guaranteed. When an iris image is acquired under non-ideal conditions (e.g., bad illumination, uncooperative subject, occluded iris, etc.), segmentation becomes a challenging task. In this paper, we present a more robust iris segmentation method using fully convolutional network (FCN) with dilated convolutions. We reduce the downsampling factor of the FCN model, and use the dilated convolutions to extract the more global features, which makes our method better at dealing with details. Moreover, our model supports end to end prediction, it does not need any pre-processing, such as adjusting the image to a fixed size. We used three datasets for training and testing, including CASIA-iris-interval-v4, UBIRIS v2 and IITD Delhi datasets. Experiments show that our model greatly reduced the error rate of the current state-of-the-arts by 79%, 84% and 79% on the CASIA-iris-interval-v4, IITD Delhi and UBIRIS v2 datasets respectively."}, {"label": 0, "content": "At the substation job site, the use of intelligent video surveillance technology can greatly reduce the supervision burden on safety inspectors for irregular operations of operators. However, in the outdoor complex working environment, the identification accuracy of recognition algorithm based on the traditional radial basis function neural Network (RBFNN) is not high enough and the missing alarm rate is high. In order to solve this problem, this paper proposes the RBFNN robust algorithm for dress recognition based on classifier output sensitivity. The algorithm firstly extracts the shape and color feature vector of the helmet, the top and the bottom of the operator image. Then, the Monte Carlo method is used to randomly sample the points in the neighborhood of training samples to expand the number of samples and reduce the volatility of the classifier output. And then, the loss function that considers the sensitivity of the sample neighborhood is established. Finally, the weights from the hidden layer to the output layer are solved by Gauss-Newton method. The RBFNN classifier based on Gaussian function is established. The simulation results show that the recognition algorithm based on sensitivity RBFNN (S-RBFNN) can effectively reduce the missing alarm rate, which is more robust in practical applications."}, {"label": 1, "content": "China's power grids have accumulated a vast amount of dispatching and operating data at different voltage levels, providing a rich historical resource for system analysis. To ensure online safety and stability, a comprehensive system assessment is conducted every 15 minutes using calculation data and result data of around 1G. This paper proposes a system stability analysis method for a provincial power grid based on six-month online historical data, which considers the time-varying characteristics of the power system. An operation rule extracting method is also proposed, which selects corresponding features for each type on the hierarchical grid using correlation analysis methods. A power grid security risk assessment system based on load ratio and line limitation is established to automatically discover the characteristics and rules of grid dispatching operations. This study provides a theoretical basis for operational mode and scheduling decisions, improving the ability of large-scale grid dispatching and online safety analysis, and enhancing the ability of power grids to resist external factors for safe and stable operations. The practical application value of this study is significant."}, {"label": 0, "content": "Predicting the future states of things is an important performance form of intelligence and it is also of vital importance in real-time systems such as autonomous cars and robotics. This paper aims to tackle a video prediction task. Previous methods for future frame prediction are always subject to restrictions from environment, leading to poor accuracy and blurry prediction details. In this work, we present an unsupervised video prediction framework which iteratively anticipates the raw RGB pixel values in future video frames. Extensive experiments are implemented on advanced datasets \u2014 KTH and KITTI. The results demonstrate that our method achieves a good performance."}, {"label": 0, "content": "Coronary artery disease (CAD) is one of the leading causes of mortality and morbidity globally. Nowadays, it is spreading at an alarming rate. Recently, there is an increasing interest to develop simple and non-invasive automated methods for reliable diagnosis of CAD. Studies reported that the use of single-channel phonocardiogram (PCG) signal for detecting weak CAD murmurs caused by the stenosed coronary arteries due to turbulent blood flow. In this work, we introduce a new framework with multi-channel data acquisition system to classify CAD and normal subjects. The proposed method does not require any reference signal such as an electrocardiogram (ECG) signal for PCG signal segmentation as reported in the earlier studies. Subsequently, the study has used five different features, such as spectral moments, spectral entropy, moments of PSD function, autoregressive (AR) parameters, and instantaneous frequency derived from frequency representations of PCG signals. These features have captured the specific details related to the disease. We use an artificial neural network (ANN) for the classification task. Experimental results show that the AR features well-performed. We achieve an accuracy of 74.24% by using multi-channel recorded data where as the best performance obtained using single-channel signal is 69.69%."}, {"label": 0, "content": "Association rule mining is an important data mining technique that help discover interesting attribute relationships that are useful for decision making. Most association rule mining methods use item-set manipulation approach, whereby data type must be categorical in nature. When a dataset contains numerical attributes, they will need to be discretized before rule mining. At the moment, most unsupervised data discretization methods do not account for data distributions, and users have to try different methods and discretization settings in order to improve rule mining results. In this paper, we propose using TwoStep clustering for data discretization. Unlike simple discretization methods, TwoStep automatically determines the discretization intervals by taking into account the unique data distribution property of each attribute. In our experiments, we evaluated the performance of Apriori algorithm based on four datasets, whereby each dataset was pre-processed using TwoStep and three other commonly used discretization methods. Our results show that TwoStep produced the greatest number of high-quality rules, as compared to common discretization methods."}, {"label": 1, "content": "The sale of electric vehicles (EVs) is increasing rapidly worldwide due to their efficiency and energy security. Charging systems are a critical aspect of electric vehicles, and they can be classified into three levels according to the Society of Automatic Engineers (SAE). This paper presents the three types of charging systems' topologies and simulates them in the RT-Lab real-time simulator.\n\nThe charging systems for Level 1 and Level 2 have single-phase input ac. Each charging system consists of two diode bridge rectifiers, a power factor correction (PFC) boost circuit, a DC/AC converter, an LLC resonant converter, and a high-frequency transformer. The DC/AC converter uses Constant Current/Constant Voltage (CC/CV) control.\n\nIn contrast, the Level 3 charging system has a three-phase input source, and its bi-directional converter is equipped with reactive power and DC bus voltage control. To simulate the three types of charging systems' charging power levels, three testbeds were set up. A 10 kWh-battery was charged in each of the testbeds. The simulation results show the expected charging performance of the charging systems."}, {"label": 0, "content": "In terms of the accumulation of historical data of the power system, China's power grids have accumulated a great number of dispatching and operating data which contains different voltage levels. Online safety and stability analysis conduct a comprehensive system assessment every 15 minutes, including power flow, short circuit, static safety analysis, voltage stability, transient stability, small disturbance stability, limit transmission power, and scheduling assistance decision-making, and the calculation data and result data are about 1G. Based on the six-month online historical data accumulated by a provincial power grid, this paper studies the characteristics of the actual power grid security and stability, and proposes a system stability analysis method based on the online security analysis of historical resources. At the same time, Considering the time-varying characteristics of the power system, an operation rule extracting method was proposed. Firstly, based on the correlation analysis methods, the corresponding features are selected for each type on the hierarchical grid. Then, a power grid security risk assessment system is established based on load ratio, and line limitation. Finally, this article automatically discovers the characteristics and rules of grid dispatching operations from numerous online data, provides a theoretical basis for operational mode and scheduling decisions, further improves the ability of large-scale grid dispatching and online safety analysis, and enhances the ability of power grids to resist the influence of external factors on safe and stable operations ability and has important practical application value."}, {"label": 1, "content": "This manuscript presents a transmission technique that has the potential to save energy in telemetry networks embedded into IoT or stand-alone WSNs or DSC. The technique involves supportive transmission in the feedback channel and requires a central node that has significantly more energy than the supported node. In scenarios where the data collected by the central node and transmitted can be modeled as a Poisson distributed random variable, this technique could be a practical and efficient choice. In contrast to similar techniques presented in [1], the new method does not require the central node to have any prediction capabilities. Moreover, it allows for a tradeoff between the gain achieved in savings and the delay in data delivery. A proof of concept is provided by an example in which transmitting one value of the Poisson distributed random variable requires approximately 1 bit on average."}, {"label": 0, "content": "In order to predict insulator contamination degree on the transmission lines, this paper proposed an insulator contamination prediction model based on back-propagation (BP) neural network optimized by genetic algorithm. In view of the inadequacies of the slowness of convergence rate and the weakness of global optimization ability, the genetic algorithm was used to improve the performance of prediction result. This paper not only consider meteorological factor which includes temperature, wind, precipitation and relative humidity as input layer but also consider air quality index (AQI) which contains PM2.5, PM10 as input layer. Moreover, the equivalent salt deposit density (ESDD) and non-soluble deposit density (NSDD) were set as output variables. The results show that the optimized prediction model has better ability in searching optimization. Furthermore, the prediction result of proposed model is more accurate than that of the BP neural network."}, {"label": 1, "content": "We are considering a two user Gaussian multiple access channel that involves an additive Gaussian state process, where the past values of the state and received symbols are causally available to the encoders at each instant. Although the literature has solved the capacity region for the noiseless scenario without feedback, we are researching the model with both noise and feedback. Our proposed communication scheme makes use of feedback symbols and state information to enhance the achievable region. We effectively utilize the Wyner-Ziv binning on state information and Ozarow feedback scheme for the MAC. To achieve this, we use a suitable interleaving technique that results in an obtained region which is much better than the feedback capacity region without any state information."}, {"label": 1, "content": "The Internet of Things (IoT) is an essential technological application that has emerged as a result of the Industrial Revolution, also known as Industry 4.0. Today, smartphones are not only used for communication, but also as wireless smart devices to access, process, and send information at lightning-fast speeds through enhanced features such as cameras, GPS, and OTT apps. The remarkable developments in other devices, such as social media, video conferencing, video streaming, tracking, navigation, drones, remote, forecasts, monitoring, and payment, can be efficiently processed by sensor and actuator devices. With the development of cutting-edge technology, this smart capability can be applied to any device, and devices can interact with each other through the internet network.\n\nIn Industry 4.0, IoT is a crucial aspect that is broadly embodied in smart city (policy-driven), smart industry (business-driven), and smart life (experience-driven) solutions. Appropriately utilizing the capabilities of Industrial IoT can lead to appropriate development in Nusantara as a large archipelago and agrarian area that is rich in natural resources. This research investigates the concept of IoT and its use cases against various socio-economic and specific geographic challenges, and then evaluates them based on PESTLE strategic analysis for external and internal factors. The result is the Development Strategy and Technology for Developing Nations."}, {"label": 1, "content": "The ever-increasing proliferation of wireless network systems has created a problem of spectrum congestion, which results in slow data communication. As all radio spectrums are allocated to different services, users, and applications, some bands are often found to be underutilized while others are congested. Therefore, the cognitive radio concept has emerged as a solution to spectrum congestion by enabling cognitive users to opportunistically use the unused spectrum while minimizing interference with other users.\n\nHowever, the successful deployment of this technology is threatened by security issues such as Byzantine attacks. These attacks compromise cognitive radios and relay falsified data about the availability of the spectrum to other legitimate cognitive radios in the network, resulting in interference.\n\nTo address this issue, we propose a security measure to counter the effects of Byzantine attacks. We compare our proposal with Attack-Proof Cooperative Spectrum Sensing, which is used to detect attacks through consensus-based receiver sensing.\n\nIn conclusion, our proposed security measure provides an effective way to combat the effects of Byzantine attacks and improve the resilience of cognitive radio networks against security threats."}, {"label": 0, "content": "Classic DC power flow and Generalized Generation Distribution Factors (GGDF) are used for modeling the transmission network constraints in a DC optimal power flow (OPF). The first method is known for its simplicity, accuracy, and robustness, and the second for its ability to express transmission power flows as a function of the power generation with less equality constraints. This paper compares the two formulations by testing them on a PJM 5-bus system and an IEEE 57-bus system including transmission losses, using different commercial optimization solvers."}, {"label": 1, "content": "In this paper, a new progressive spectral mapping learning algorithm is proposed for throat microphone (TM) speech enhancement. Rather than using full-band spectra mapping algorithms, this new algorithm splits the mapping into two tasks: voice conversion and artificial bandwidth extension. Additionally, a Long short-term memory recurrent neural network (LSTM-RNN) is utilized as the mapping model. The objective evaluation results demonstrate a significant improvement in TM speech quality compared to conventional full-band spectra mapping framework and DNN-based mapping model."}, {"label": 0, "content": "Retinal vessel tortuosity is an early indicator of different retinopathies. Although various automated methods in determining retinal vessel tortuosity have been proposed in the literature, there are needs for further study. This study extracted three different features namely distance metric, normalized hybrid metric and non-normalized hybrid metric from the thinned vessels. The weights of vessel data samples were dynamically updated using the Adaboost with linear discriminant analysis (LDA) and the feature correlation was used to facilitate the selection of the best feature combination at each of the boosting iteration rather than a single feature that minimizes the weighted error at each of the iterations. Adaboost with LDA method is then used for the classification of the retinal vessels as either tortuous or normal using a majority voting method. The proposed method achieves the accuracy rate of 100% for the training sample sizes of 70%, 80% and 90%."}, {"label": 1, "content": "By renting cloud resources on a pay-as-you-go basis, scientific workflows that are data-intensive can prove to be costly, not just in terms of the time taken for execution, but also in terms of money due to the large amounts of data transfers required. Delays in these transfers can severely compromise workflow execution time, leading to prolonged resource rentals and ultimately exceeding budgets. To mitigate this issue, a new approach is proposed whereby some communication may be traded for computation during the scheduling production of workflows. This will involve duplicating computation of certain tasks to reduce the communication load between dependent tasks. The Heterogeneous Earliest Finish Time (HEFT) algorithm and the Lookahead variant of HEFT are leveraged for this purpose, and the approach is evaluated using simulations and synthetic data from four real-world scientific workflow applications. The results suggest that task duplication can effectively reduce data transfer size, leading to shorter rental durations of resources and minimising network traffic in the cloud."}, {"label": 1, "content": "Network modeling of high-dimensional time series data is a crucial task with widespread usage in various applications such as macroeconomics, finance, and neuroscience. Although the literature extensively investigates the problem of sparse modeling with Vector Autoregressive models (VAR), more complex network structures, which include group sparse components and low rank, have received considerably less attention despite their presence in data. Failure to account for low-rank structures could lead to spurious connectivity among the observed time series, causing practitioners to draw incorrect conclusions concerning critical scientific or policy issues.\n\nTo mitigate such challenges and accurately estimate a Granger causal interaction network after accounting for latent effects, we propose a novel approach to estimate low-rank and structured sparse high-dimensional VAR models. Our approach utilizes a regularized framework comprising of nuclear norm and lasso (or group lasso) penalties. Furthermore, we establish nonasymptotic probabilistic upper bounds on the estimation error rates of the low-rank and structured sparse components. Additionally, we introduce a fast estimation algorithm and demonstrate the performance of the proposed modeling framework over standard sparse VAR estimates through numerical experiments on synthetic and real datasets."}, {"label": 1, "content": "The article examines the use of artificial intelligence techniques in mobile robot control. To enhance the functional capabilities of the control system and its subsystems, a method for selecting the optimal artificial intelligence technology from a pool of available options is proposed. By ensuring maximum coverage of tasks and procedures accomplished by the robot control system, the method elevates the overall performance of the system. The technique involves presenting the initial data in the form of preference relations among a range of alternatives and adopting the most nondominant option to solve the issue of selecting the best information technology. Overcoming the challenge of choosing the ideal artificial intelligence technology for modelling traffic control and technical operation in mobile robot sprinkler systems, the proposed method was utilized to design the control system of the \"Fregat\" robot. This approach can be easily adapted for other robot control systems beyond mobile robotics."}, {"label": 1, "content": "A Physically Unclonable Function (PUF) extracts the manufacturing variations of integrated circuits to generate and authenticate keys, addressing security issues in traditional non-volatile memory (NVM)-based systems. However, machine learning-based modeling attacks have emerged as a new threat to Strong PUF-based authentication schemes. In this study, we propose a novel reconfigurable XOR Arbiter Physical Unclonable Function (R-XOR APUF) to withstand such attacks. The R-XOR APUF comprises multiplexers and inverters and generates two responses based on configured challenges, which are XORed to generate the final response. This eliminates uniformity in the model, making it resistant to machine learning-based modeling attacks. Experimental results show that the uniqueness of R-XOR APUFs is 42.15% (compared to an ideal value of 50%), and the prediction rate is reduced from 95% to 55% (compared to an ideal value of 50%), outperforming traditional APUFs."}, {"label": 0, "content": "5G wireless together with optical backbone networks are expected to be the main pillars of the envisaged next /future generation networking (N/FGN) infrastructures. This is an impetus to practical realization of an IoT network that will support and ensure relatively higher bandwidth as well as enhanced quality of service (QoS) in both access and core network sections. The high-speed wireless links at the network peripherals will serve as a conducive platform for device-to-device (D2D) communication. D2D driven applications and services can only be effective as well as secure assuming the associated machine type communication devices (MTCDs) have been successfully verified and authenticated. Typically, D2D type services and applications involve the interaction of several MTCDs in a group. As such, secure and effective D2D group-based authentication and key agreement (AKA) protocols are necessary. They need to inherently achieve efficacy in maintaining the group key unlink-ability as well as generate minimal signalling overheads that otherwise may lead to network congestion. In this paper we detail a secure and efficient Group AKA (Gr-AKA) protocol for D2D communication. Its performance is compared to that of existing similar protocols and is found to comparably lower both computational as well as signalling overhead requirements. Overall the analysis shows that the Gr-AKA protocol improves performance in terms of fulfilling D2D communication's security requirements."}, {"label": 1, "content": "This paper presents a study on time synchronization in wireless sensor networks (WSN) using a fully distributed algorithm that is formally modelled and analyzed through statistical model checking. The study is conducted using the Theatre framework which is based on actors and asynchronous message passing, and can be reduced to the Uppaal Statistical Model Checker (SMC). The paper discusses the chosen time synchronization algorithm, outlines the Theatre modelling features, and demonstrates its mapping onto Uppaal Smc. Furthermore, the study presents a Theatre model with an adaptation mechanism for energy-saving purposes, and this model is analyzed through simulations. Overall, the study provides a comprehensive approach to time synchronization in WSN that can be used to optimize energy consumption and enhance network performance."}, {"label": 0, "content": "The relational database service in cloud usually achieves energy efficiency by using virtualization technology, in which it consolidates multiple independent database systems into a single physical machine while enforcing the hardware-level isolation among them. However, the disk I/O performance is inevitably hurt due to the resource contention on the shared device. We propose VMSQL, a novel disk I/O model for the virtualized relational database management system (RDBMS). VMSQL has two innovations over the original disk model of virtualized database systems. First, VMSQL enforces the synchronous operation in guest operating system to handle with the transaction commitment. Due to its simplicity, a portion of CPU cycles is decoupled from I/O buffer management and then used to serve the upcoming requests, thereby improving their response times. Second, in host system, VMSQL asynchronizes the storage path of transactions which are committed from the different co-located guest databases. An obvious advantage of this procedure is that systems can apply host-level improvements into the disk I/O performance of virtualized RDBMS, relieving the random I/O and enhancing the throughput of whole system. We implement a prototype of this Sync-Async model in QEMU-KVM hypervisor, in which the InnoDB engine is deployed in the guest operating system. Extensive experiments are conducted to verify its advantages and the results are positive without any loss of ACID-compliance. In the meanwhile, VMSQL incurs moderate overhead at the hypervisor layer."}, {"label": 1, "content": "The Internet of Things (IoT) has become an integral part of our lives as it allows for the connection and coordination of numerous embedded devices to carry out complex tasks that make our work easier. However, with the wireless broadcast nature of IoT and the energy constraint of physical objects, its security poses significant challenges. In this study, we explore two opportunistic relay selection schemes, namely single relay selection (SRS) and multi relay selection (MRS), to improve the physical layer security of IoT. Our analysis considers the outage probability (OP), intercept probability (IP), and system tradeoff performance (STP) for both schemes under Nakagami-m fading channel. Our simulation results indicate that MRS outperforms SRS regarding both OP and IP. Additionally, MRS offers a more optimal STP compared to SRS. Moreover, the STP of both schemes also improves with the increase in relay numbers. Thus, our work offers a path towards achieving a secure and reliable IoT system."}, {"label": 0, "content": "Message passing model, represented by MPI (Message Passing Interface), is the main tool for parallel programming for distributed computer systems. The most of MPI-programs contain collective communications, which involve all the processes of a parallel program. Effectiveness of collective communications substantially effects on total time of program execution. In this work, we consider the problem of design of the algorithm of barrier synchronization, which refers to one of the most common types of collective communications. We developed adaptive algorithm of barrier synchronization, which suboptimally selects barrier synchronization scheme in parallel MPI-programs among such algorithms as Central Counter, Combining Tree and Dissemination Barrier. The adaptive algorithm chooses the barrier algorithm with the minimal evaluation of execution time in the model Lo g P. Model LogP considers performance of computational resources and interconnect for point-to-point communications. Proposed algorithm has been implemented for MPI. We give the results of experiments on cluster systems, analyze dependency of algorithm selection on LogP parameters values. In particular, for the number of processes less than 20 adaptive algorithm selects Combining Tree, while for a larger number of processes adaptive algorithm selects Dissemination Barrier. Developed algorithm minimizes average time of barrier synchronization by 4%, in comparison with the most common determined barrier algorithms."}, {"label": 1, "content": "The management of distribution networks (DN) includes various aspects such as fault analysis, early warning, and differentiated operation and maintenance. However, the data collected from these networks are often multi-time-scale and multi-spatial-temporal, making it challenging to analyze them accurately. To address this issue, this paper proposes the application of data mining and high-dimensional random matrix theory (RMT) to enhance the operation and maintenance management of DN.\n\nFirstly, the K-means clustering algorithm is used to extract one-dimensional fault features from fault information. Next, the Apriori algorithm is employed to mine association rules for different failure modes and establish key performance matrices. Spatial-temporal characteristics are analyzed using RMT, which enables the identification of fault patterns in high-dimensional data sets.\n\nAdditionally, the paper proposes the use of D-S evidence theory to combine one-dimensional and multi-dimensional fault features, which results in the development of fault diagnosis criteria for DN. Moreover, the health index and importance index of equipment are established, based on the operating state of DN and variations in power users. The implementation of these indices will significantly reduce the decision-making risk associated with DN operation and maintenance.\n\nThe proposed method is supported by simulation results, which demonstrate its effectiveness in enhancing the operation and maintenance management of DN. By combining data mining, high-dimensional random matrix theory, and D-S evidence theory, this approach offers a comprehensive and reliable solution for the complex operation and maintenance management of distribution networks."}, {"label": 1, "content": "Protozoa detection and identification are of great importance in various fields including parasitology, scientific research, biological treatment processes, and environmental quality evaluation. Unfortunately, traditional laboratory methods for protozoan identification are expensive, time-consuming, and require expert knowledge. The use of micrographs is one approach that can save time and reduce costs, but current methods can only identify protozoan species once they are already segmented using shape and size features.\n\nIn this study, we present a new approach named Segmentation-driven RetinaNet that can automatically detect, segment, and identify various species of protozoans in micrographs, including Giardia lamblia, Iodamoeba butschilii, Toxoplasma gondi, Cyclospora cayetanensis, Balantidium coli, Sarcocystis, Cystoisospora belli, and Acanthamoeba. These species have round shapes in common and can cause serious health issues in humans and animals.\n\nOur proposed method overcomes the lack of data issue by applying multiple techniques such as transfer learning and data augmentation techniques while dividing training samples into life-cycle stages of protozoans. Despite having at most five samples per life-cycle category in the training data, our proposed method still achieves promising results and outperforms the original RetinaNet on our protozoa dataset.\n\nIn conclusion, our Segmentation-driven RetinaNet approach is a reliable and efficient method for protozoa detection and identification that can significantly reduce costs and time while providing accurate and detailed results."}, {"label": 1, "content": "In accordance with the rules of interaction between the subjects of the Electricity Market and JSC ATS, the subjects of the Electricity Market are required to implement daily hourly forecasts in the \"days ahead\" mode. To ensure high-quality prediction of electricity loads, Electricity Market subjects need to prepare the regulatory base, develop techniques for creating electricity load predictions, and consider the risks associated with the accuracy of the models used.\n\nThe complexity of the problem being solved is characterized by the availability of data on supply points, as not all Electricity Market subjects have access to data on the consumption of individual power facilities in hourly mode. The introduction of commercial accounting systems can solve this problem with significant investments in the installation of automatic systems for commercial measurement of electricity loads (ascme), but Electricity Market subjects may not always be willing to invest in such long-term payback costs.\n\nThis work is useful for specialists of power sales companies who build forecast models, as well as specialists of Electricity Market entities who carry out forecasts for the day-ahead. The main aim of this study is to apply a methodology for forecasting using neural networks to build predictive models for LLC \"Omsk Energy Retail Company.\"\n\nThe study uses the Holt-Winters model, the ARIMA, neural networks, temperature, and wind index. It considers the methods of constructing predictive models and their evolution since the launch of the Electricity Market. A method for constructing the forecast for \"Omsk Energy Retail Company\" is developed using neural networks, taking into account the temperature and wind index and the allocation of common types of days by electricity load."}, {"label": 1, "content": "We are currently in the process of developing a novel natural language processing (NLP) method that will aid in the analysis of text corpora related to long-term recovery efforts. The main objective of our method is to provide a means for measuring the degree to which user-specified propositions regarding potential issues are present in the corpora, thereby serving as a proxy for gauging the progress of disaster recovery.\n\nOur method utilizes a statistical syntax-based semantic matching model which has been trained on a publicly available dataset. We applied this NLP method to a corpus of news stories detailing the recovery of Christchurch, New Zealand following the 2010-2011 Canterbury earthquake sequence. By measuring the semantic frequencies of various potential recovery issues represented in the news corpus from 2011 to 2016, we were able to evaluate the effectiveness of our method through a user study involving twenty professional emergency managers. \n\nThe results of the user study demonstrated that our NLP method was indeed effective when applied to disaster-related news corpora. Furthermore, 85% of study participants expressed interest in utilizing our method as a means of measuring recovery issue propositions in news or other corpora. Encouraged by these findings, we see the potential for our NLP method to be of great use in after-action learning, recovery decision making, and disaster research."}, {"label": 1, "content": "This paper aims to investigate an important issue relating to enhancing the efficiency of electric energy transportation in distribution networks with voltage range of 6-20 kV. The primary objective is to reduce transmission losses while ensuring the quality of electricity at consumer nodes according to the deviation criterion from nominal in modern Russian electric power industry. The paper presents a detailed description of the distribution network model, and the results of distribution network modeling with the mentioned voltage range. \n\nTo achieve the research objective, the use of intelligent control devices that rely on a thyristor regulator of booster voltage (TRBV) is explored. The paper discusses the high laboriousness involved in applying precise control algorithms for intelligent control devices to decrease transmission losses in networks with the mentioned voltage range. The study provides a comparison of the distribution electric networks' operation before and after the intelligent control devices' adjustment. \n\nOverall, this research contributes significantly to addressing the problem of enhancing the efficiency of electric energy transportation in distribution networks. The findings show that the use of intelligent control devices based on TRBV is an effective strategy to reduce transmission losses while ensuring the quality of electricity at consumer nodes in modern Russian electric power industry."}, {"label": 0, "content": "In this paper, an analytical framework is provided to analyze the uplink performance of device-to-device (D2D)-enabled millimeter-wave (mm-wave) cellular networks with clustered D2D user equipments (UEs). The locations of cellular UEs are modeled as a Poisson point process, while the locations of potential D2D UEs are modeled as a Poisson cluster process. Signal-to-interference-plus-noise ratio outage probabilities are derived for both cellular and D2D links using tools from stochastic geometry. The distinguishing features of mm-wave communications such as directional beamforming and having different path loss laws for the line-of-sight and non-line-of-sight links are incorporated into the outage analysis by employing a flexible mode selection scheme and Nakagami fading. Also, the effect of beamforming alignment errors on the outage probability is investigated to get insight into the performance in practical scenarios. Moreover, area spectral efficiency of the cellular and D2D networks is determined for both underlay and overlay types of sharing. Optimal spectrum partition factor is determined for overlay sharing by considering the optimal weighted proportional fair spectrum partition."}, {"label": 1, "content": "Facial expression and emotion recognition have captured the attention of game developers in recent years. Several gaming proposals now utilize facial detection to recognize players and load their profile automatically. Additionally, the recognition of players' facial expressions is used to change their avatar's expression. This paper aims to explore the potential of using facial expression recognition to modify game parameters, such as stimulating a bored player or increasing difficulty when a player is happy and engaged. \n\nTo achieve this goal, we propose a new approach for facial expression and emotion recognition that involves two classification stages. The first stage detects whether the emotion is positive, negative, or neutral. The second stage refines the classification to recognize specific emotions such as happiness or fear accurately. Our approach runs in real-time and has shown great accuracy when compared to other classification methods commonly used.\n\nTo test our approach, we developed a game that could adapt to a player's emotional state detected through facial recognition. The estimation of the emotional state was used to modify the game's parameters, increasing or decreasing difficulty in real-time. \n\nOverall, this study demonstrates the potential for facial expression and emotion recognition in improving gaming experiences. Moreover, our proposed approach for real-time detection of specific emotions has shown superior accuracy compared to other methods."}, {"label": 1, "content": "Spacecraft detection is a crucial issue in aerospace information processing and control, providing dynamic state information of the target to support decisions with regard to recognition, classification, and cataloging. However, existing spacecraft detection methods exhibit inadequate accuracy, fault tolerance, and real-time performance. Recent advances in deep learning algorithms have produced the regression-based convolutional neural network YOLOv2, which has demonstrated excellent detection performance, surpassing other state-of-the-art methods. In this study, we apply CNN to spacecraft detection for the first time, utilizing image annotation and data augmentation, and using our improved YOLOv2 model to detect spacecraft in images. Our algorithm reaches a test set detection rate of 97.8%, with an average detection time of each image at about 0.018s. This performance shows significantly lower time overhead and better robustness to changes in spacecraft rotation and illumination."}, {"label": 1, "content": "In this paper, we present a new technique for identifying adversarial examples by training a binary classifier with both the original data and the saliency data. For image classification models, saliency illustrates how a model makes decisions by identifying significant pixels in the prediction process. When a model produces incorrect classification output, it often learns incorrect features, resulting in inaccurate saliency data. Our method demonstrates excellent performance in detecting adversarial perturbations. We also evaluate the generalization ability of the detector, demonstrating that detectors trained using strong adversaries exhibit strong performance against weak adversaries as well."}, {"label": 0, "content": "In this article, a novel interferometric synthetic aperture radar (InSAR) baseline estimation method based on ground control points (GCPs) and partitioning is proposed. Instead of introducing the existing low-resolution digital elevation model (DEM) to correct the phase jumps between high and low coherence regions in the process of phase unwrapping, we use the high coherence regional block to calibrate the interferometric parameters. In the process of calibration, the GCPs can reference for low resolution DEM, also can be obtained through the filed measurement. Because the interferometric parameters calibration does not change the absolute phase, which avoids the local DEM restriction by low resolution DEM. In addition, the block based on the coherence map avoids DEM inversion error due to overall absolute phase deviation. Gaofen-3 InSAR data of Ningbo area are used to verify the effectiveness of the proposed method."}, {"label": 0, "content": "It is crucial to implement an effective and accurate fault diagnosis of a gearbox for mechanical systems. However, being composed of many mechanical parts, a gearbox has a variety of failure modes resulting in the difficulty of accurate fault diagnosis. Moreover, it is easy to obtain raw vibration signals from real gearbox applications, but it requires significant costs to label them, especially for multi-fault modes. These issues challenge the traditional supervised learning methods of fault diagnosis. To solve these problems, we develop an active learning strategy based on uncertainty and complexity. Therefore, a new diagnostic method for a gearbox is proposed based on the present active learning, empirical mode decomposition-singular value decomposition (EMD-SVD) and random forests (RF). First, the EMD-SVD is used to obtain feature vectors from raw signals. Second, the proposed active learning scheme selects the most valuable unlabeled samples, which are then labeled and added to the training data set. Finally, the RF, trained by the new training data, is employed to recognize the fault modes of a gearbox. Two cases are studied based on experimental gearbox fault diagnostic data, and a supervised learning method, as well as other active learning methods, are compared. The results show that the proposed method outperforms the two common types of methods, thus validating its effectiveness and superiority."}, {"label": 0, "content": "The development of high-resolution video mounts a serious challenge to the previous video coding standard. The appearance of the new generation standards greatly relieves the dilemma but increases the coding complexity dramatically. Motion estimation is considered as the module with a relatively high computational complexity. In this paper, a parallel motion estimation implementation is proposed, which includes pre-motion estimation, integer motion estimation, and fractional motion estimation. They are highly accelerated on GPU based on AVS2, which is one of the new generation standards. A rapid mapping table algorithm is introduced to improve the efficiency of data access. In addition, a quasi-integral-graph algorithm is designed to calculate SAD or SATD efficiently for blocks of different sizes. The two novel techniques can effectively improve the utilization and efficiency of threads and exploit the characteristics of GPU. The experimental results show that the proposed parallel method can effectively accelerate the motion estimation."}, {"label": 0, "content": "In this paper, a novel algorithm based on convolutional neural network (CNN) and support vector machine (SVM) for fire detection in infrared (IR) video surveillance is proposed. To improve the performance of IR fire detection, we develop a 9-layer convolutional neural network named IRCNN instead of traditional empirically handcrafted methods to extract IR image features. Then, a linear support vector machine is trained with extracted features to achieve fire detection. Our network adopts data augmentation technique and Adam optimization to deal with problems caused by the insufficient dataset, and accelerate the training process. Experimental results show that our method achieved both high precision (98.82%) and high recall (98.58%) on our IR flame dataset and real-time detection on the ordinary infrared surveillance cameras."}, {"label": 0, "content": "Diagnostics and monitoring are resource-intensive, but necessary processes for providing reliable services by cellular operators. The high cost of existing measuring systems and the need for constant participation in the measurement of highly qualified specialists imposes its limitations. Thus, the purpose of developing an alternative solution for diagnosis and monitoring tasks is relevant. The proposed solution is distributed automated information and measuring system based on a smartphone, using the passive method of monitoring. The proposed solution makes it possible to use the capabilities of modern smartphones and differs relatively simplicity of development, implementation and operation, as well as a high degree of flexibility and mobility."}, {"label": 0, "content": "The article is devoted to the description of approaches to the solution of the actual problem of optimization of neural network calculations using the non-position number system (residual number system) for design problems of neural network automatic control systems. The problems arising in neural network control systems of industrial objects characterized by astatic properties are identified and analyzed. The approach proposed to solve these problems is confirmed by an example of solving a real problem."}, {"label": 0, "content": "A new concept of approximate power flow (APF) is proposed in this paper, aiming to help deal with the non-convergence problem of power flow calculation. In the approximate power flow model, active and reactive power decoupling strategy is adopted, and a branch model with virtual midpoint is the key foundation of the whole research. Based on the branch model, the approximate power flow equation is constructed and its iterative solving method with good characteristics of convergence is also introduced. Active and reactive power automatic adjustment measures are also used to improve the algorithm robustness. The effectiveness and feasibility of this approximate power flow model is proved by the error and robustness analysis for practical examples. The APF program has been developed based on the proposed method, and can be applied to the actual large-scale power grid."}, {"label": 0, "content": "This paper is aimed at evaluating the reliability indices of a distribution network (DN) considering the rerouting of the interrupted customers during outages. The DN is represented by its circuit graph that is used to analyze the effect of the failure mode (FMEA method) and compute the reliability of the system. To have a more accurate estimation, the capacity of the feeder and the voltage deviation when the rerouting occurs, are also considered. After the occurrence of an outage, the problem is finding the optimum tree by interchanging the status of normally open (NO) and normally closed (NC) switches to restore as many as interrupted customers subject to defined constraints. The proposed algorithm presents a systematic method to find the optimum solution of the restoration problem, and by decreasing the search space is suitable for analyzing the big size DN. To obtain the impact of each NC switches in the reliability improvement, a ranking method is also introduced."}, {"label": 1, "content": "The aim of this study was to develop a voltage control strategy using the model of reactive power optimization including interval uncertainty (RPOIU). The strategy ensures that the interval state variables of the power grid such as load voltages and reactive power generation are kept within safe operating limits, even under interval active power generation and power load demand input data.\n\nTo solve the RPOIU model, this paper defined security limits, and the model was switched to two deterministic reactive power optimization models, whose constraints bound limits are the security limits. Using the interior point method, the deterministic models were solved and a voltage control strategy was obtained as the solution of the RPOIU model.\n\nThe proposed method was compared with the previously proposed linear approximation method, and simulation results and analysis demonstrated the advantages, effectiveness, and good applicability of the proposed method in comparison to the linear approximation method.\n\nOverall, the results of this study demonstrate the effectiveness of the proposed voltage control strategy in ensuring the safe operating limits of the power grid under interval uncertainty."}, {"label": 0, "content": "Recent topics of interest such as smart cities and autonomous driving are currently in focus of many research activities. In this context, simulations are used to evaluate new algorithms, performance of current technologies, or the impact of upcoming products. In particular, they allow finding errors and optimizing parameter sets prospectively, prior to a real-world implementation. Simulation models of many traffic problems need to handle large-scale scenarios, connect entities from different domains, and run in feasible time. In order to meet these challenges, an extendable multi-level traffic simulation approach is proposed in this paper. We briefly introduce existing traffic simulation techniques, name upcoming problems, available solution approaches, and topics regarding the development of our framework. As a first step, we coupled two different resolution levels of traffic simulation by using High Level Architecture (HLA) and evaluated this approach in light of simulation results and simulation performance."}, {"label": 0, "content": "In recent years, deep learning object detectors including Fast/Faster R-CNN, SSD, R-FCN and Mask R-CNN have shown significant performance for general object detection except for pedestrians. The Region Proposal Network (RPN) in Faster R-CNN works well yet lacks of adaptability. Therefore, we propose an adaptive real-time pedestrian detection and attribute identification scheme based on Caffe. The first contribution is the adaptive threshold adjustment (ATA) algorithm for intelligent monitoring, utilizing the pedestrian movement information to adjust the threshold. Moreover, to overcome the time-consuming defect, we analyze the influences of the number of layers, the size of convolution kernels and the number of feature maps to reduce redundant computation while maintaining satisfactory performance. By optimizing the neutral network structure, choosing model parameters and data augmentation, a stable and well-performed model with fast detection rate and high accuracy is obtained. Besides, pedestrian information can also be identified in our program, offering better service in security monitoring, intelligent robots and other fields. Extensive experimental results demonstrate that even in complex and athletic scenarios, our method can make an improvement in quality and speed over state-of-the-art."}, {"label": 1, "content": "Epilepsy is a chronic condition that results in sudden, unprovoked abnormal activity in the brain. Early prediction of epileptic seizures can be achieved by characterizing the electroencephalogram (EEG) signals of the patient. In this study, we propose a novel method called the entropy of visibility heights of hierarchical neighbors (EVHHN) for detecting seizures from EEG signals. Our method determines the visibility relationships of the three nearest neighbors based on a visibility criterion, and calculates the visibility heights of three nearest neighbors for each data point. We then compute four different types of entropy associated with neighbor visibility states to characterize the EEG signals, and validate these features with LS-SVM classifiers. Our results demonstrate an accuracy of 99.6% for classifying normal and ictal EEG signals, and 98.35% for distinguishing interictal and ictal EEG signals. Remarkably, the computational time for extracting features for each set is only 1.751 seconds, which is significantly shorter than other weighted visibility graph-based methods. In conclusion, the EVHHN has the potential to be an effective method for characterizing complex EEG signals and real-time detection of epileptic seizures."}, {"label": 1, "content": "As high-bandwidth and data-intensive applications continue to evolve rapidly, traditional electrical networks are no longer able to meet the increasing traffic requirements. Consequently, the hybrid electrical/optical architecture has emerged as a new research topic for improving data center network (DCN) communication performance. Moreover, traditional architectures are often too complex to manage efficiently. Therefore, Software Defined Network (SDN) has emerged as a new technology to address this issue.\n\nIn this paper, we propose a hybrid architecture based on SDN, where the control manager is used to monitor and allocate traffic, and then configure the network using SDN. The hybrid network platform is implemented under virtual machine migration. Our experiment shows that the proposed scheme reduces the total time required for virtual machine migration compared to running on an electrical architecture. Additionally, our scheme enables flexible network topology configuration and load balancing.\n\nOverall, our proposed hybrid SDN-based architecture presents a promising solution for improving DCN communication performance and addressing network complexity management."}, {"label": 1, "content": "Feature extraction is crucial for monitoring the states and evaluating the performance of mechanical electro-hydraulic systems (MEHS). However, MEHS poses a challenge as it has complex multi-domain energy conversion properties, especially during varying operation conditions, making it difficult to extract desired features effectively. Moreover, conventional signals are challenging to collect and analyze due to the mixed coupling of different kinds of information. To address this challenge, a power distribution analysis of MEHS led to identifying the change rate of kinetic energy (CRKE) as a suitable index for evaluating the system's performance. To characterize CRKE, a cooperation analysis method was proposed using internal and external features. The method selected kinetic energy stiffness (KES) and instantaneous speed fluctuation (ISF) as the internal and external features, respectively. A systematic approach was developed to obtain the magnitudes of KES and ISF using Lissajous figure-based information fusion and order tracking technology. By analyzing the complementary advantages and mutual relationship of KES and ISF, the system's performance under varying operation conditions was evaluated. The proposed method was verified through experiments on a real rig. The results showed that changes in KES and ISF effectively reflected the operational changes, and lowering KES loss improved the efficiency of the system while restraining the ISF."}, {"label": 0, "content": "Today, numerical controls (CNC) are the standard for the control of machine tools and industrial robots in production and enable highly flexible and efficient production, especially for frequently changing production tasks. A numerical control has discrete inputs and outputs. Within the NC channel, however, it is necessary to analytically describe curves for the calculation of the position setpoints and the jerk limitation. The resulting change between discrete and continuous description forms and the considerable restrictions in the parallelisation of the interpolation of continuous curves within the NC channel lead to a performance overhead that limits the performance of the NC channel with regard to the calculation of new position setpoints. This can lead to a drop in production speed and thus to longer production times. To solve this problem, we propose a new approach in this paper. This is based on the use of deep generative models and allows the direct generation of interpolated toolpaths without calculation of continuous curves and subsequent discretization. The generative models are being trained to create curves of certain types such as linear and parabolic curves or splines directly as discrete point sequences. This approach is very well feasible with regard to its parallelization and reduces the computing effort within the NC channel. First results with straight lines and parabolic curves show the feasibility of this new approach for the generation of CNC toolpaths."}, {"label": 1, "content": "A method for estimating the near-field signal source using a symmetric uniform linear array is proposed in this paper. This method employs instrumental sensors for parameter separation, decoupling, and estimation. It achieves this through reconstruction of the virtual array and steering vector transformation. The method has advantages in that it does not lose aperture during reconstruction, avoids multi-dimensional spectrum search, and is more practical. The proposed method improves computational efficiency, calibrates gain and phase error more accurately, and has higher parameter estimation accuracy when compared to traditional methods. Additionally, this cascading estimation technique enables real-time estimation of azimuth, range, and error parameters. Simulation experiments attest to the efficiency of the method."}, {"label": 1, "content": "Functional encryption is an innovative form of public-key cryptography that enables secret-key owners to decrypt only specific functions of the encrypted data. This has numerous applications in various fields. Despite the existence of general constructions of theoretical interest, practical functional encryption currently only allows for the evaluation of low-degree functions of the encrypted inputs. \n\nIn this paper, we explore how Inner-Product Functional Encryption (IPFE) may be utilized to develop a tax calculation system that incorporates privacy into its design. Our study concludes with the unveiling of performance results which demonstrate the viability of the approach on the issue of carbon tax calculations."}, {"label": 0, "content": "This paper presents a novel Viterbi method based on one-bit differential detection for Gaussian minimum shift keying (GMSK) used in the satellite based automatic identification system (AIS), which is easy to achieve in hardware. Because of the inherent intersymbol interference (ISI) of the GMSK, the bit error rate (BER) of hard decision based demodulation methods is always seriously exacerbated by Gaussian white noise especially under a small BT value. Correlation based traditional Viterbi method for GMSK will consume a mass of multiplier and adder resources and is not suitable for the hardware achievement. A phase rotation based differential Viterbi method is proposed in this paper, which just consumes few calculation resources and has better BER performance comparing with the hard decision method. The corresponding comparable result of the BER is also shown."}, {"label": 1, "content": "Sensor data can be utilized to swiftly detect any changes in the performance of a system, which may indicate a potential system fault. Nevertheless, to achieve this, robust and efficient algorithms are essential to detect such changes in the data streams. This paper focuses on the anomaly detection of sensor data from a marine diesel engine on an ocean-going ship, specifically emphasizing on the use of cluster-based approaches. The concept involves identifying clusters within the sensor data during normal operating conditions, and assessing whether new observations belong to any of the identified clusters. If new observations do not align with any of the clusters, it may indicate an anomaly and require further investigation. \n\nThe cluster-based methods used in this research are genuinely unsupervised, and the approach is applied to sensor data free of known faults. Since these methods are entirely unsupervised, they do not assume that every observation in the training data is faultless, provided that the amount of the flawed data is not large enough to form a separate cluster. Furthermore, the training data may be utilized to detect anomalies. The study explores various clustering techniques for an unsupervised approach to anomaly detection, such as K-means clustering, Mixture of Gaussian models, density-based clustering, self-organizing maps, and spectral clustering. \n\nThe study analyzes the performance of various methods and compares them to other approaches proposed for anomaly detection, such as auto-associative kernel regression and dynamical linear models. Overall, cluster-based methods are deemed to be promising candidates in online anomaly detection based on sensor data. This approach can serve as a productive initial screening stage before applying more detailed analysis to the suspicious parts of the data streams."}, {"label": 1, "content": "This article presents an imitation computer model for a synchronous-in-phase electric drive, which incorporates the impulse type of the control system. This feature enables the simulation of transient processes with remarkable precision when investigating effective methods of control and devices that implement those methods. Compared to models that do not consider the impulse type of control systems, the developed model exhibits a heightened level of accuracy - particularly when studying transient processes in the low-frequency rotation range of the electric drive. Additionally, the model allows researchers to analyze the electric drive in a low-frequency range where the nature of transient processes is heavily influenced by signal discretization."}, {"label": 0, "content": "The paper represents the model of AC drive control system. An analysis of conventional drive control system has been performed, its advantages and disadvantages, and preconditions for use of intelligent approaches for implementation of control systems for this kind of objects have been estimated. The intelligent system is supposed to be modeled with use of fuzzy logic controller and different inference algorithms for implementation of control laws in various control loops. Dynamic characteristics of the system both for conventional case and with use of controllers based on fuzzy set theory have been analyzed. The practicability of using the multi-cascade fuzzy systems while implementation of unified intelligent control module and different combinations of fuzzy inference algorithms has been demonstrated for complex drive systems. Application of this technology allows implementation of control system for the entire class of drive systems considering all special aspects and relationships between the coordinates in the complex control object. Additionally, modeling of spacial membership function using multi-cascade fuzzy controller will give a possibility to avoid a variety of quantitative and qualitative restrictions due to complex relations between the coordinate in such systems."}, {"label": 0, "content": "Machinery condition monitoring has entered the era of big data and some research has been done based on big data. Abnormal segments, such as missing segments and drift segments, are inevitable in big data acquired from harsh industrial environment due to temporary sensor failures, network segment transmission delays, or accidental loss of some collected data and so on. Being independent of the machinery condition, the abnormal segments not only reduce the quality of the data for condition monitoring and big data analysis, but also bring a heavy computation load. However, there are few reports to address abnormal segment detection for further data cleaning in the field of machinery condition monitoring. Therefore, an abnormal segment detection method is proposed to improve the quality of big data. First, a sliding window is used to separate the data into different segments. Then, 14 kinds of time-domain features are extracted from each segment and principle component analysis (PCA) is employed to extract the principle components from these features. In addition, local outlier factor (LOF) is calculated based on the principle components to evaluate the degree of being an outlier for each segment. Finally, the data, including a drift segment from a real wind turbine, are used to verify the effectiveness of the proposed method."}, {"label": 0, "content": "Computer networks are ubiquitous and growing exponentially, with a predicted 50 billion devices connected by 2050. This tremendous growth dramatically increases the attack surface of both private and public networks. These attacks often influence the behaviour of the system, leading to the detection of the attack. In this manuscript we model the path of an attack through the network by graphs. The model developed aims to better integer attackers intentions. Using the data produced by 5 honeypots, we apply our model. The preliminary results show that the approach is useful to rapidly detect anomalies in the experiment dataset."}, {"label": 1, "content": "Non-rigid registration plays a critical role in applications such as object recognition, motion tracking, and model retrieval. However, the initial position utilized in the registration step is crucial for the accuracy of these applications. In this paper, we propose a new method for Convex Hull Aided Coarse Registration refined by projected points algorithms. Our approach uses statistical methods to determine the optimal plane that represents each point cloud. Each cloud's points are then projected onto the corresponding planes, and two convex hulls are extracted from the projected point sets and matched optimally. We estimate the non-rigid transformation from the reference to the model through minimizing the distance between the matched point pairs of the two convex hulls. We refine this transformation estimation through two methods - Iterative Closest Point (ICP) and Normal Distribution Transform (NDT). Our experimental study on several clouds indicates that ICP provides better results than NDT in refining the coarse registration."}, {"label": 1, "content": "Wind power has been widely advocated as a solution to the global energy crisis due to its numerous benefits for both the economy and the environment. However, the unpredictable nature of wind patterns poses significant challenges for power system planning and operation. Addressing these uncertainties requires novel and sophisticated approaches.\n\nTo this end, we propose a novel deep learning-based combined approach to tackle the nonlinear and non-stationary uncertainties in wind power generation. Our approach incorporates a state-of-art point forecast technique, which uses wavelet transform and RNN-RBM deep learning to decompose the raw wind power data into different frequencies and extract critical nonlinear patterns to improve the forecast accuracy.\n\nMoreover, the non-parametric approach is utilized to statistically formulate the probabilistic distribution of wind power data. Our experimental results demonstrate that the proposed approach outperforms alternative techniques and achieves a higher forecast accuracy with an average increase of around 3.2% in MAPE.\n\nOverall, our combined approach provides a promising solution to the challenges posed by the uncertain nature of wind power generation. By leveraging the latest deep learning techniques, we can generate more accurate and reliable forecasts to support power system planning and operation."}, {"label": 1, "content": "In this paper, we address the downlink in a heterogeneous network with K tiers. We account for the impact of Nakagami-m fading and noise and develop closed-form approximations for the achievable average rate and coverage probability. To simplify the expressions, we utilize a piece-wise linear approximation. Our proposed results are validated through numerical simulations, which demonstrate that our approach provides a good approximation compared to existing work."}, {"label": 0, "content": "Tool wear prediction becomes increasingly important due to the growing demand for finished quality and the improvement of productivity. In this case, it is necessary to establish a well-designed monitoring system to obtain the relationship between tool wear and cutting process. Generalized regression neural network (GRNN) is able to handle non-linear problems with its memory-based character. However, it was rarely used for tool wear prediction in the past several decades. Therefore, in this paper, it was employed to tackle this problem. In addition, in order to tune the smooth parameter of the GRNN, a newly proposed evolutionary algorithm called fruit fly optimization algorithm (FOA) was adopted. Meanwhile, an improved fruit fly optimization algorithm (IFOA), in which escaping and distance control parameters were introduced to prevent FOA from falling into local optimum, was presented to enhance the search ability. Two cutting experiments showed that the IFOA-GRNN provided a comparable regression ability to the GRNN with particle swarm optimization(PSO), the least squares support vector machines(LS-SVM) and the BP neural network."}, {"label": 0, "content": "Load Monitoring (LM) is a fundamental step to implement effective energy management schemes. LM includes Intrusive LM (ILM) and Non-Intrusive LM (NILM). Compared with intrusive approaches, non-intrusive approaches enjoy low cost, easy installation, and promising scalable commercialization potentials. This paper provides a survey of effective NILM system framework and advanced load disaggregation algorithms, reviews load signature models, presents existing datasets and performance metrics, summarizes commercial applications such as demand response, highlights the challenges, and points out future research directions."}, {"label": 0, "content": "Transportation and locomotion mode recognition from multimodal smartphone sensors is useful for providing just-in-time context-aware assistance. However, the field is currently held back by the lack of standardized datasets, recognition tasks, and evaluation criteria. Currently, the recognition methods are often tested on the ad hoc datasets acquired for one-off recognition problems and with different choices of sensors. This prevents a systematic comparative evaluation of methods within and across research groups. Our goal is to address these issues by: 1) introducing a publicly available, large-scale dataset for transportation and locomotion mode recognition from multimodal smartphone sensors; 2) suggesting 12 reference recognition scenarios, which are a superset of the tasks we identified in the related work; 3) suggesting relevant combinations of sensors to use based on energy considerations among accelerometer, gyroscope, magnetometer, and global positioning system sensors; and 4) defining precise evaluation criteria, including training and testing sets, evaluation measures, and user-independent and sensor-placement independent evaluations. Based on this, we report a systematic study of the relevance of statistical and frequency features based on the information theoretical criteria to inform recognition systems. We then systematically report the reference performance obtained on all the identified recognition scenarios using a machine-learning recognition pipeline. The extent of this analysis and the clear definition of the recognition tasks enable future researchers to evaluate their own methods in a comparable manner, thus contributing to further advances in the field. The dataset and the code are available online. http://www.shl-dataset.org/."}, {"label": 1, "content": "Air pollution is a global concern, and the estimation of air quality can guide individual and industrial behaviors. However, traditional instrument-based methods are expensive and require maintenance, making them less accessible. This paper proposes a cheap photo-based method for estimating air quality in the case of particulate matter (PM2.5). The method extracts two types of features, namely gradient similarity and distribution shape of pixel values in the saturation map. The gradient similarity measures the structural information loss caused by PM2.5 attenuating light rays and distorting the structures in the photo. On the other hand, the saturation map quantifies the color information loss by fitting it with the Weibull distribution. Combining the two features results in a primary PM2.5 concentration estimator, which is mapped to real PM2.5 concentration via a nonlinear function. The proposed method's effectiveness and efficiency are demonstrated through experiments on real data captured by professional PM2.5 instruments. The results show high consistency with the real sensor's measures and low implementation time. Overall, the proposed photo-based approach offers a cost-effective and accessible alternative to traditional instrument-based methods for air quality estimation."}, {"label": 0, "content": "Research in the field of co-prime arrays and samplers has been mainly focused on reconstructing the autocorrelation and the spectral content of a signal at the Nyquist rate from sub-Nyquist data. This has found applications in power spectrum estimation, beamforming, direction-of-arrival estimation, and system identification. However, the use of coprime samplers for cross-correlation estimation has not received much attention. We describe cross-correlation estimation using co-prime samplers and consider two scenarios. In the first, both signals are acquired using co-prime samplers, while in the second scenario we assume one of the signals to be a known signal and thus available at the Nyquist rate, and the second signal is acquired using a co-prime sampler. We determine the number of contributors available for cross-correlation estimation at each difference value as this is a key parameter in determining the estimation accuracy. The work presented in this paper will have applications in time-delay, range, velocity, acceleration, and cross-spectrum estimation, which require cross-correlation estimation."}, {"label": 0, "content": "The increased power of the hardware and software complexes and the sharp improvement in the quality of television screens at the same time as the price of their television allowed to solve the problem of simulating visually observed terrain behind the simulator window for the preparation of the locomotive driver. Such a system is a imitator of the visual situation, standing from a computer image generator and 3D indicator. However, the availability of modeling does not imply the need for system analysis to select the main nodes of the 3D modeling system. When training on the simulator, the driver interacts not with real objects, but with their models. Since it is impossible to create a complete model, in studying the simulator complex it is necessary to take into account the negative consequences of the \u201cunreliability\u201d of the synthesis of visible 3D models located in the 3D space model, in comparison with real analogues. The comparison was carried out in the study of the peculiarities of the decision making by the locomotive driver, while controlling the real locomotive, using the information obtained by viewing the terrain through the cab window. Studies have shown that in order to carry out a comparative analysis, it is necessary to develop criteria for assessing the imitator of the visual situation. It was determined that at present the main criterion for the successful modeling of a three-dimensional image in the locomotive operator's integrated simulator is the requirement to train professional skills in determining the distance to visible 3D models. Earlier similar tasks were investigated and successfully solved in aviation simulators. Their solution showed that such a problem can be solved using pseudo-3D 3D imaging systems. Two classes of 3D modeling systems are used: one-channel and two-channel. Each class allows the trainee to see 3D images of sufficient quality so that he can train his visual apparatus and use it to visually distance the selected object. Each class of 3D imaging systems will allow a person to see a 3D image due to exposure to certain components of the person's vision. Single-channel, needleless 3D modeling systems affect the accommodation and convergence of human vision. Two-channel effects on the disparity of human vision. Each 3D image modeling class has its disadvantages and advantages. The choice of a specific class of modeling 3D images is determined by the tasks set in the TOR for the simulator. After selecting the 3D simulation modeling class, it is necessary to determine the composition of the main nodes of the imitator of the visual situation in order to perform the tasks set in the TOR for the simulator. The article shows one of the possible methods for assessing the main nodes of the visual environment simulator, which allows to determine the composition of the optical-software-hardware simulation complex of the required 3D image for the train simulator of the locomotive driver. The quality of the formation of the information space should ensure the formation of the trainee's professional skills in locomotive management."}, {"label": 1, "content": "Methods for estimating uncompensated errors in measurements caused by noises and interferences that do not follow statistical patterns are being investigated. The proposed model uses a multiparameter quasi-determined model function to represent the signal and a background function that is fixed over the measurement interval. The background function is described by an arbitrary set of functions whose range of variation is limited by the E-layer. This model enables the calculation of interval estimates of signal parameters for confidence values close to unity. The technique for calculating interval errors of parameters using the E-regions and nonlinear model function is described. This approach allows for interval estimations of parameters for a particular signal realization, given that the range of values of the background function is limited. The E-regions in the parameter space are shown to characterize the state of the control object. An approach using E-regions to identify abnormal situations is proposed. The results of the practical application of E-regions for monitoring power consumption and other energy resources are presented."}, {"label": 0, "content": "Base on the fourth-order cumulants, a direction finding algorithm for noncircular sources under unknown mutual coupling is proposed. Utilizing the symmetric Toeplitz structure of the mutual coupling matrix and the noncircularity of the sources, a closed form solution for the DOA estimation is obtained by constructing a series of cumulant matrices. According to the simulation results, the proposed algorithm is effective in the presence of unknown mutual coupling. Moreover, it enjoys better estimation performance via utilizing the noncircularity of the sources."}, {"label": 0, "content": "The article addresses the well-known \"knapsack problem\" in the context of the distribution of virtual servers within the cloud infrastructure. At the same time, the main focus in solving this problem is the distribution of virtual machines within the cloud, which includes resources with different access capabilities. The mathematical model proposed in the article allows conducting optimal allocation of resources not only taking into account such parameters as random access memory and processor power, but also parameters of data storage systems related to memory capacity and access speed. The resulting model was tested on a real network infrastructure. From a practical perspective, the method of virtual machines distribution under question allows using the low-power devices available at the data center."}, {"label": 1, "content": "In this paper, a novel algorithm for electromechanical-electromagnetic hybrid simulation is presented, based on the grouping and decoupling of boundary nodes. The backbone network for Ultra High Voltage AC/DC systems is modelled using electromagnetic transient models, while the remaining systems are modelled using electromechanical transient models, with power grids at low voltage being decoupled at their boundaries. To support electromagnetic transient parallel computing, a new partitioning method for the backbone network is proposed, based on heuristic rules. A hybrid parallel simulation algorithm is subsequently designed using subnet groups. The theoretical analysis and derivation of the algorithm demonstrate that it can significantly improve the efficiency of hybrid simulation with a large number of boundary nodes."}, {"label": 0, "content": "This paper proposes a method of identifying major power quality disturbance sources based on monitoring data correlation analysis among multiple grid nodes in a regional power grid. First, using correlation calculation between the voltage quality index in each node and the current quality index in each branch, strong-correlated branches which significantly affect voltage distortion in problematic nodes are extracted. Second, the contribution of each strong-correlated current branch to node voltage distortion is quantified through partial correlation analysis. Finally, the major disturbance sources are identified by means of contribution estimation. The proposed method is validated through a case study conducted on a regional power grid which includes multiple types of disturbance sources. The results evidenced that the method can identify correctly the major disturbance sources responsible for node voltage distortion, thus providing effective guidance for disturbance source governance."}, {"label": 1, "content": "Non-Intrusive Load Monitoring (NILM) is a method of monitoring electronic devices through a centralized capture of current and voltage. By analyzing the waveforms, each device can be distinguished by their unique turn-on and turn-off events. While most methods utilize the active power (P) and reactive power (Q) signals, the information content of these power signals is limited. \n\nTo address this issue, various machine learning and classification algorithms have been applied to different datasets. However, real comparability between these works is often difficult due to proprietary, unpublished datasets and limited scenarios. \n\nTo improve upon existing methods, the Home Equipment Laboratory Dataset 1 (HELD1) has introduced a new waveform called Frequency Invariant Transformation of Periodic Signals (FIT-PS). This current-voltage-based waveform allows for the use of standard machine learning algorithms like the k Nearest Neighbors Classifier, Support Vector Machine Classifier, and Naive Bayes Classifier. \n\nBy utilizing FIT-PS, over 90% of events can be accurately assigned to their respective devices. This new waveform allows for a more equitable comparison between existing NILM methods and the effectiveness of this new approach."}, {"label": 0, "content": "The increasingly popularity of vehicles embeded with high computational devices and resources has attracted great interests as a mean for sharing resources in a new or existing cloud infrastructure. Vehicles can be clustered together to ease the management, increase the effectiveness and capability of vehicular cloud. Clustering would allow these vehicles to pool their resources to serve the upper layers of the network infrastructure. This paper investigates the trends and state of the art of vehicular clustering in the past 5 years especially for parked vehicles in fog computing paradigm. It was observed that static and dynamic clustering are the two common methods, albeit dynamic clustering is more common. The challenges and issues are discussed."}, {"label": 1, "content": "A novel method for carrier tracking in coherent demodulation for satellite communications has been proposed. The method uses quadrature phase shift keying (QPSK) and addresses the problem of frequency ambiguity that limits the maximum pull-in range of traditional carrier tracking loops to one eighth of the symbol rate. This new approach employs a low-complexity, QPSK frequency/phase discriminator and a frequency estimation and compensation module, which can resolve phase jump and frequency ambiguity issues while keeping computation complexity low compared to conventional QPSK detectors. The design structure of the method is simplified by adopting a parallel FLL-assisted-PLL and module reuse, resulting in lower complexity, smaller variance, and a larger pull-in range when compared to current state-of-the-art techniques."}, {"label": 1, "content": "We propose a novel hybrid probabilistic interval prediction method for short-term load forecasting. This method integrates K-means clustering based feature selection and online Gaussian processes regression(OGPR) to improve the accuracy of load forecasting results. The K-means clustering algorithm contributes to relevant feature selection during a dynamical process to better capture the load characters along with time. OGPR, which includes both dynamically updating the hyper-parameters and the training sample sets, is used as a forecasting engine to perform load probability interval prediction. To validate our proposed model, we use load data from the Queensland market, Australia. The comparative results show that our approach outperforms others in terms of gaining higher quality prediction interval."}, {"label": 1, "content": "This study introduces a new measure for choosing MSER regions called Mean Intensity Difference (MID). The calculation of this metric is based on calculating the contrast between the pixels within an MSER region and its surrounding pixel set. The paper examines two types of surroundings - the pixels of the first contour layer and the complementary pixels within the bounding box of the region. To test the effectiveness of the proposed contrast metric, the authors perform a location retrieval task using SURF descriptors and a Bag-of-Words approach to generate global image signatures. The Devon Island dataset is used for evaluation, which is known for having a Mars-like environment and contains GPS ground-truth data. Additionally, the authors incorporate the contrast-based methods with the Grid Adaptation approach. The experimental results suggest that MID outperforms other state-of-the-art metrics, such as Perceptual Divergence, while yielding better performance than random region selection. The authors also evaluate computational complexity in their work."}, {"label": 0, "content": "The experimental research on the security and stability control system (referred to as SSCS) of UHVDC transmission project mainly involves the functional verification on information interaction between SSCS and UHVDC control and protection system, the fault criterion of DC converters, the calculation of DC power loss, and the coordinated control strategies such as generators/loads tripping or DC transmission power modulation after failures in DC system or N-2 faults in AC system, and other aspects. This paper analyzed the existing test methods for SSCS, then proposed a modular modeling method. This modeling method transformed the external system of the AC/DC hybrid network into REI nodes, and performed coherency-based dynamic equivalence on the sending-end generator groups. The modular design reduces the granularity of simulation calculations, improves the speedup ratio of parallel computing, and improves the efficiency of processors usage to meet demands for large-scale closed-loop testing on UHVDC system-level protection technologies including DC co-control and precisely machines tripping. Based on this modelling design, the function of DC control and protection system was simulated, the interface between RTDS and SSCS was realized, and the simulation and test platform for UHVDC SSCS was built on RTDS. The stability control strategies and the system function verification of Zhalute-Qingzhou \u00b1800 kV UHVDC SSCS were carried out on the platform. The simulation and test results verified the effectiveness of the typical test scheme.."}, {"label": 0, "content": "A custom designed distribution network communication systems can maintain the fast and reliable information transmission and lead to improved operation and management of a distribution automation system. In this paper, the distribution automation transformation scheme of a regional power grid is analyzed. Then a simulation model of three types units, including the intelligent distributed protection unit, the three-remote unit and the master station unit, is constructed based on the Optimized network engineering tool (OPNET) Modeler platform, which is the main novelty of this paper. Finally, an intelligent distributed protection system is constructed and simulated using the models. The simulation results show that the models can act an effective tool to analyze the network performance, which can provide a quantitative basis for the design of the distribution network communication systems."}, {"label": 0, "content": "In an Orthogonal Frequency Division Multiple Access (OFDMA) based multi-cellular WiMAX system, a suitable base station is obtained by cell search. In the cell search technique, in addition to timing and frequency synchronization, the detection of the frame start position is another basic task that the mobile terminal must successfully complete before establishing a communication link with the base station. In this work, we propose a dual correlation algorithm to improve the accuracy of frame detection synchronization. In order to reduce the influence of channel fading under low SNR, we also propose a scheme for joint blind channel estimation and equalization. Compared with the existing scheme, the obtained scheme can obtain better frame detection system performance at a low signal to noise ratio."}, {"label": 0, "content": "Despite importance higher education institution sustainable development paradigm and many publications in this area, there are at least two real problems that are restricted successful implementation of this paradigm: a) complexity the subject domain and b) lack of a methodological base to reuse some interesting already created models together with future models. This paper introduces an approach to reduce the subject domain complexity by substituting it with a set of abstract infrastructure formalized systems. Within this approach, the models' reusability problem can be reduced by using a simulation umbrella as a united methodological base of the paradigm implementation. To Illustrate how the proposed approach works the paper includes a simulation case-study associated with a USA young fast-growing higher education institution."}, {"label": 1, "content": "To create a ubiquitous wireless network environment, unmanned aerial vehicles (UAVs) are considered an excellent candidate for the next generation network communication infrastructure. The UAV wireless sensor network consists of low cost and power-constrained sensor nodes scattered over a spatial region, allowing users to transfer data using UAVs. The UAV's good maneuverability and wide range of coverage improve communication efficiency. Mobile Ad-hoc Networks (MANET) and Delay/Disruption Tolerant Networks (DTN) are considered supporters for UAVs networks, but the network's performance may change with the surrounding environment, such as UAV density and mobility. Therefore, a novel method is proposed to improve UAVs network capabilities. In this method, each source node selects the appropriate network architecture (MANET or DTN) based on the feature of the data that needs to be sent and the network environment. This architecture has been implemented and tested in the real world, with results indicating that the bandwidth of our proposed adaptive architecture is 150% better than DTN architecture. Furthermore, in a high latency and easily interrupted network, the adaptive architecture effectively transfers data compared to MANET."}, {"label": 1, "content": "The paper explores the technique of creating a training dataset for predictive electricity load models using artificial neural networks. The training dataset is designed to meet the criteria of being informative and compact. The research highlights the effectiveness of this approach, which significantly enhances the accuracy of forecasts compared to other conventional methods."}, {"label": 0, "content": "The feature information is usually corrupted by the irrelevant harmonics and background noise for bearing fault signal, which makes it difficult to identify the fault symptom in time. This paper proposes a new diagnosis method to identify the incipient periodic impulsive features of rolling bearings. Firstly, the Over-Complete Rational-Dilation Wavelet Transforms (ORDWT) is employed to decompose the original fault signal to obtain several sub-bands. Secondly, a periodic impulsive index which absorbs the advantages of ACFHNR, the kurtosis and Pearson's correlation coefficient index is proposed to adaptively track the best fault frequency band. Finally, the envelop spectrum of the best fault frequency band is gained for fault diagnosis. The simulation and experiment results demonstrate that the proposed adaptive fault frequency band detection (AFFBD) method is effective."}, {"label": 0, "content": "Video summarization (VSUMM) has become a popular method in processing massive video data. The key point of VSUMM is to select the key frames to represent the effective contents of a video sequence. The existing methods can only extract the static images of videos as the content summarization, but they ignore the representation of motion information. To cope with these issues, a novel framework for an efficient video content summarization as well as video motion summarization is proposed. Initially, Capsules Net is trained as a spatiotemporal information extractor, and an inter-frames motion curve is generated based on those spatiotemporal features. Subsequently, a transition effects detection method is proposed to automatically segment the video streams into shots. Finally, a self-attention model is introduced to select key-frames sequences inside the shots; thus, key static images are selected as video content summarization, and optical flows can be calculated as video motion summarization. The ultimate experimental results demonstrate that our method is competitive on VSUMM, TvSum, SumMe, and RAI datasets about shot segmentation and video content summarization, and can also represent a good motion summarization result."}, {"label": 0, "content": "This article presents the imitation computer model of a synchronous-in-phase electric drive that takes into account the impulse type of the control system, which allows simulating transient processes with high accuracy while studying effective methods of synchronous-in-phase electric drive control and devices implementing these methods. The modelling accuracy when using the developed model is especially increased (in comparison with the model that doesn't consider the impulse type of the control system) when studying transient processes in the low-frequency range of the electric drive rotation. The developed model allows to carry out researches of the electric drive in a range of low frequencies, where there is a strong influence of signal discretization on the transient processes type."}, {"label": 0, "content": "Rendering translucent materials is costly: light transport algorithms need to simulate a large number of scattering events inside the material before reaching convergence. The cost is especially high for materials with a large albedo or a small mean-free-path, where higher-order scattering effects dominate. We present a new method for fast computation of global illumination with participating media. Our method uses precomputed multiple scattering effects, stored in two compact tables. These precomputed multiple scattering tables are easy to integrate with any illumination simulation algorithm. We give examples for virtual ray lights (VRL), photon mapping with beams and paths (UPBP), Metropolis Light Transport with Manifold Exploration (MEMLT). The original algorithms are in charge of low-order scattering, combined with multiple scattering computed using our table. Our results show significant improvements in convergence speed and memory costs, with negligible impact on accuracy."}, {"label": 1, "content": "This paper presents a comparative analysis of performance enhancement techniques in a two-dimensional atmospheric optical code division multiple access (OCDMA) system, taking into consideration beam divergence, multiple access interference (MAI), noise and atmospheric turbulence. The effects of atmospheric turbulence are evaluated using lognormal and gamma-gamma probability density functions (pdfs). The study finds that double hard limiters, spatial diversity and error correcting code (ECC) can improve the performance of the 2-D atmospheric OCDMA system. However, performance improvement is more substantial with double hard limiters and ECC as compared to spatial diversity. Additionally, the study also finds that double hard limiters are more cost-effective than the other two techniques. Hence, the study concludes that double hard limiters are superior to other performance improvement techniques for 2-D atmospheric OCDMA systems."}, {"label": 1, "content": "Unified Communications (UC) is transforming the way businesses operate by enabling human voice and video to travel over existing packet data networks, along with UC services like video teleconferencing (VTC), unified messaging, and chat. UC represents the future of fully-converged, cloud-based architecture, but it also presents new forensic challenges to network investigators.\n\nThe adoption of cloud-based UC offerings, otherwise known as UC as a Service (UCaaS), is gaining momentum in the Information Technology field. However, this technology presents significant forensic challenges as digital evidence can be widely dispersed across the network.\n\nTo improve UC security, this research proposes two semi-formal patterns to create a Cloud Forensic Model. The patterns offer a structured approach to network forensic collection and analysis of digital evidence in UCaaS environments, allowing network investigators to specify, analyze and implement network forensic investigations for UC technologies.\n\nThis proposed cloud forensic framework will facilitate UC security investigations and help investigators to efficiently collect and analyze digital evidence. As UC continues to evolve, it is imperative that forensic techniques and security measures are in place to ensure the protection of sensitive information."}, {"label": 0, "content": "In the modern information warfare, the requirements for the reliability and real-time performance of the communication signal recognition technology are getting more and more strict. Although a great number of studies have been conducted in the reliability of communication signal recognition, few studies have been performed in the speed of communication signal recognition. The purpose of this study is to explore an improved feature extraction methods based on extreme learning machine (ELM) which has the advantage of higher speed in communication signal recognition. The results of simulations show that the approach in this paper not only improves the speed of recognition and ensures a high reliability, but also reach an ideal recognition accuracy at a low SNR."}, {"label": 0, "content": "The paper presents a method for representing the instruction set truth tables of microprocessors with High-Level Decision Diagrams (HLDD). A behavior level fault model is defined for the microprocessor control parts on the basis of instruction level truth tables (TT). Two methods are proposed for creating HLDDs from TTs with minimization of the edges on graphs: greedy algorithm, and branch and bound algorithm (B&B). A simple and fast computable lower bound is proposed to be used for pruning the search space of the B&B algorithm. Experimental data of using the fault model for several microprocessors and comparison data are provided to show the efficiency of the proposed high-level fault model over the gate-level Stuck-at-Fault (SAF) model."}, {"label": 0, "content": "In this paper the general aspects of energy consumption and environmental effects of cryptocurrency mining technology are considered. For the data mining equipment, the main technical specifications defining its energy efficiency are analyzed. The aggregation of separate units within data mining pools or installation of respective farms were shown to have the most prominent energy saving effect. Eventually some design and operation issues related to power supply of data mining pools are outlined. The sufficient amount of standards prescribing the power supply design for this new type of energy consumers is still missing. Considering the facility located in Moscow power supply of a data mining pool is studied and respective power quality survey is conducted. Finally, the impact of data mining equipment on the power factor and grid voltage variations is demonstrated."}, {"label": 0, "content": "The infrared thermal image is an importance in condition monitoring of electrical equipment. However, the infrared image edge is coarse, fuzzy, and with serious noise and brings great inconvenience to the infrared image processing. An adaptive optimal threshold edge extraction algorithm based on improved Sobel operator is proposed in this paper. In the algorithm, by eight-direction Sobel operators the infrared image edge is extracted in the high temperature area in noisy environment. At the same time, the wavelet coefficients in sub-band are described by general Gaussian distribution and the variance is estimated from the local neighborhood information of sub-band wavelet coefficients so that the adaptive optimal denoising threshold can be obtained. Finally, the edge extraction infrared image is denoised by combining with improved Sobel operator and the optimal threshold. Matlab simulation results show that the algorithm can effectively detect the edge of infrared image and greatly improve its anti-noise ability."}, {"label": 1, "content": "In partnership with ZTE, Intel, and Tencent video, China Unicom has embarked on a pilot project to construct an edge ecosystem incubation and service platform. The project aims to build an edge data center test bed in the university town of Tianjin, China, using over-the-top (OTT) business and edge computing technology. This paper provides an overview of the architecture and construction plan of edge virtual content delivery networks (vCDNs) and outlines the infrastructure and business flow. The vCDN test results reveal that deploying applications to the network edge reduces bandwidth pressure, latencies, and optimizes user experience while cutting content provider costs. This project marks the beginning of edge computing collaboration between China Unicom and OTT and holds significant guidance value for the overhaul of edge data center equipment rooms and incubation of 5G innovative services."}, {"label": 1, "content": "Hand detection is a crucial aspect of gesture recognition, especially for communicating control commands and conveying information between people and computers. The accuracy of hand detection from images significantly influences the success of these applications. This task primarily relies on the extraction of effective features that are discriminative, robust to variations, and easy to compute. \n\nThis study compared commonly used features for object detection- Haar-like features, a histogram of oriented gradient (HOG), and local binary pattern (LBP)- for hand detection. The adaptive boost (AdaBoost) cascade classification method was used to combine weak learners into a strong classifier. The classifier was trained on 300 positive images, i.e., images containing the hand's region of interest (ROI), and 10000 negative images, i.e., images without a hand.\n\nThe study evaluated the performance of the classifier, considering different parameter combinations, on 320 static test images using Haar, HOG, and LBP features. Results indicate that parameter combinations significantly affect the hand detection accuracy, and the different features have varying effects."}, {"label": 1, "content": "Distributed and real-time simulation is a highly effective method for managing complex modern systems. To support analysis, engineering, and training in various research and industrial domains, the IEEE 1516-2010 for Modeling and Simulation High Level Architecture (HLA) is an interoperability standard for distributed simulation.\n\nThe HLA allows simulation entities called Federates to interact with other Federates in a common simulation environment or Federation. These entities can publish and/or subscribe ObjectClasses and InteractionClasses, communicate data, and synchronize actions through the services provided by the Run-Time Infrastructure (RTI), which abstract and hide the details of the computing infrastructure.\n\nGiven the highly concurrent, distributed nature of modern HLA distributed simulation systems, managing the communications flow between Federates can be a challenge handled through HLA Callbacks. Reactive approaches, tools, and techniques can provide significant benefits, which is why this paper presents a solution for defining reactive HLA Federates. This solution uses the RxHLA software framework, which aims to define and build reactive, concurrent, and distributed time/event-driven simulation components (Federates) in a reactive fashion."}, {"label": 1, "content": "Currently, the field of computer vision widely uses the distance function to learn image pairs, with the Euclidean distance being the most commonly used method. However, the traditional Euclidean distance has limitations in accurately measuring feature similarity. To address this issue, we propose a new algorithm called weighted Pairwise Constrained Component Analysis (wPCCA) for person reidentification (Re-ID), which is based on the weighted Euclidean distance. The wPCCA algorithm builds upon the PCCA and improves its measurement of characteristics using the weighted Euclidean distance. We conducted experiments on two challenging datasets called i-LIDS and CAVIAR, and achieved promising results with our approach."}, {"label": 0, "content": "A computational benchmark suite is presented for quantifying the performance of modern RCS simulations. The suite contains a set of scattering problems that are organized along six dimensions and range from basic to challenging. It also includes reference solutions, performance metrics, and recommended studies that can be used to reveal the strengths and deficiencies of different simulation methods."}, {"label": 0, "content": "Traditional intelligent diagnosis methods and current popular deep learning based diagnosis methods basically adopt the approach of batch learning, which may waste time and computing resources since they need to discard the previous learned model and retrain a new model based on the newly added data and prior data. Moreover, manual feature extraction is often a necessary step for intelligent diagnosis, and such a process relies much on prior knowledge. To solve the above mentioned problems, this paper proposes a fault diagnosis method based on class incremental learning without manual feature extraction. Based on denoising autoencoder, the proposed method obtains the autoencoders using the raw data acquired for each health state. In the class incremental learning process, only the autoencoder of new health state need to be trained while the former trained autoencoders are retained. Test data is classified according to the minimal reconstruction error calculated through the autoencoders. At the end of this paper, the proposed method is validated through vibration data of rolling bearings for rail vehicle. The experimental results show that the presented method is effective."}, {"label": 0, "content": "The lie is very detrimental to the fraudulent acts of many people who were cheated. The lies are common in the general population. To be able to reveal a lie we can detect through some limbs that unconsciously will show a different reaction when someone is lying. Among them, through organs of our eyes can detect someone is lying or not.Lie detection discussed in this final Task is the eyes, namely with the object of eye tracking and eye pupil diameter changes by using method of Wavelet Transform to Gabor Image Processing process and afterwards perform the classification to determine the answer someone is lying or not by using a Decision Tree. The existence of this lie detector, is expected to be helpful for people who need to detect lies. With the final test results are accurate. This research has the precision value of 97%, 94%, and recall accuracy 95% of testing has been done."}, {"label": 0, "content": "High Efficiency Video Coding (HEVC) provides more compression than its predecessors. One of the modules that contributes to higher compression rates is the Motion Estimation module, which consists of Integer and Fractional pixel motion estimation. The Fractional Motion Estimation (FME) process performs interpolations to find sample values at fractional-pixel locations, which can be computationally demanding. In this paper, we propose an interpolation-free method for FME based on Artificial Neural Networks (ANNs). Our proposed method is implemented in HEVC reference software (HM-16.9). According to our results, ANNs can accomplish FME task with an average increase of 2.6% in BDRate and an average reduction of 0.09 dB in BD-PSNR."}, {"label": 0, "content": "PV power generation has such shortcomings as volatility, intermittency and so on. When it connects to the power grid, it will adversely affect the power system. So it is important to predict the power of PV power generation system accurately. The more similar the weather condition is, the more similar the generation of photovoltaic power generation system is. This paper proposes a new PV Generation Power Prediction model Based on GA-BP Neural Network with Artificial Classification of History Day. Firstly, we classify historical weather data artificially. Then, we establish different PV Generation Power Prediction model for each weather type using BP neural network and genetic algorithm (GA-BP neural network). At last, the power of the forecast day is predicted by the GA-BP neural network which is for the weather type of the forecast day."}, {"label": 0, "content": "The papers in this special section focus on cloud computing, fog computing, the Internet of Things, and Big Data analytics for the future healthcare industry, or Healthcare 4.0. Healthcare Industry 4.0 allows increasing flexibility in production, speeding up both manufacturing and market processes, increasing both the product quality and productivity, and changing business models modifying the interaction with value chain, competitors, and clients. Healthcare Industry 4.0 requires investments and mind-set change for cross-industry collaboration, agreements on data ownership, security, legal issue solving, product registration standards, new machine-to-machine communication protocols, and employment/skills development. Furthermore,Healthcare Industry 4.0 is revolutionizing the market of health service provisioning to patients and clinical operators. "}, {"label": 1, "content": "Electric vehicles (EVs) have the potential to provide valuable frequency regulation services as distributed storage devices, due to their ability to quickly adjust their charging and discharging power. With the support of policy incentives, it is now practical for EVs to participate in the regulation market through an aggregator. \n\nThis paper presents an optimal control strategy based on reinforcement learning (RL) for EVs in distributed networks. The overall goal of this strategy is to closely follow the regulation signals sent by the system operator in the real-time regulation market by customizing the charge and discharge strategy of the EVs. The RL algorithm utilizes this data to optimize the aggregator's allocation of regulation power and baseline charging power, resulting in the best possible regulation performance.\n\nComprehensive simulations were conducted, based on PJM electricity market data, and the results demonstrate the excellent regulation performance of this control strategy. This was observed in both traditional and dynamic regulation signals. With this improved control strategy, EVs can effectively participate in the regulation market, offering a new source of flexibility to power systems."}, {"label": 1, "content": "On battery-operated devices, energy and power consumption are important factors to consider. With the advancement of technology, mobile devices can now be integrated with traditional systems for more complex computations. These devices can even be used as part of computational networks, sharing their computational and memory resources. However, traditional simulation frameworks are not designed to work efficiently on these heterogeneous networks, mainly because of the limited computational resources available on mobile devices. \n\nThis paper proposes a new simulation framework called SEECSSim, which is specifically designed for mobile devices. SEECSSim is derived from the School of Electrical Engineering and Computer Science (SEECS) and includes state-of-the-art distributed synchronization algorithms that are implemented to run on mobile or embedded devices. \n\nTo test the proposed framework, the well-known PHOLD model is used, and performance results are reported in terms of execution time, CPU usage, memory, and energy consumption. The results show that SEECSSim can efficiently simulate complex models on mobile devices while consuming less energy and resources. \n\nOverall, the SEECSSim simulation framework is a promising solution for mobile devices and can be used in various applications, such as mobile gaming, augmented reality, and Internet of Things (IoT) devices."}, {"label": 0, "content": "A soft ring-shaped actuator is inspired by the human stomach. This actuator can contract inward, mimicking the movements of the human stomach. It can benefit the research field of medical science and food engineering. In this paper, we investigate the deformation of such actuator employing finite-element simulation and motion tracking. Simulations are carried out to examine the influence of different pressure on the performance of the actuator. Motion tracking system is applied to track the mid-points on the deformed surface when the actuator is under pressurisation. The results show that the actuator can achieve axisymmetric contraction when inflated. The principal movements of mid-points are on the horizontal plane, whereas the change in the axial direction is negligible. This investigation can benefit the understanding of the profile of the deformed membrane and the construction of the mathematical modelling."}, {"label": 1, "content": "As the internet protocol has become the standard for communication on the internet, even low-power devices with limited computing, memory and battery power capacity must be accessible through an IP-based network infrastructure. To achieve this, the use of 6LoWPAN can be applied to wireless devices.\n\nHowever, while wireless sensor networks are popular, sometimes a more structured, wire-based infrastructure may be more appropriate. To address this, basic 6LoWPAN concepts and ideas were transferred from an IEEE 802.15.4-based wireless to a wire-based RS485 network.\n\nIn this paper, the process of transforming to a wire-based infrastructure is outlined from a technical perspective. Key adoption steps and changes were highlighted to enable IPv6 network capability on RS485.\n\nContiki and GNU/Linux were used to implement IPv6 over RS485 approaches, which were verified for their functionality. This allows for the efficient integration of small devices into IPv6 networks without the need for expensive hardware components or gateways that require translation and transformation steps, leading to faster development times."}, {"label": 0, "content": "In recent years, cyber-attacks have become an imminent danger threatening the stable power supply. Data-driven approaches based on machine learning techniques are frequently used to detect cyber-attacks. To get satisfactory detection performances, these approaches have to establish an in-depth understanding of power system operating behaviors, not only under normal conditions but also under contingencies like short-circuit faults. Contingency data appears far more rarely than normal data in the historical measurement database. If normal and contingency data are uniformly sampled to generate training datasets, the included contingency examples may be too few for machine learning models to recognize the operating patterns thoroughly. In this paper, a data preparation method combining Random Matrix Theory (RMT) and Adaptive Synthetic Sampling (ADASYN) algorithm is proposed. RMT is applied to extract rare contingency data from the historical operating database and ADASYN is used to synthetically generate new contingency examples according to the existing ones. Case studies show that this method can facilitate the learning of the intrinsic statistical characteristics of power system operating data, and then enhance the detection of cyber-attacks."}, {"label": 0, "content": "Wireless channels are increasingly being used to transmit highly sensitive information in the Internet of Things. However, traditional cryptography methods cannot solve all security problems in IoT systems. Covert wireless communication can prevent an adversary from knowing the existence of a user's transmission; thus, it can provide stronger security protection for IoT devices. In this article, we consider the covert communication in a noisy wireless network, and find that the uncertainty of the aggregated interference experienced by the adversary is beneficial to the potential transmitters. From the network perspective, wireless communications can be hidden in the interference of the noisy wireless network, which the adversary observes as a \"shadow\" network to a certain extent. Furthermore, we discuss some new results on the effect of active eavesdropper. Square root law becomes invalid, and even jammer-assisted schemes have little effect on the covertness in the presence of active eavesdropper. Finally, we provide a vision for future research."}, {"label": 0, "content": "This paper presents a flex-rigid soft robot for flipping locomotion. The proposed robot is made into a strip shape and consists of three rigid limbs connected by two active flexible hinges. Its flipping locomotion is achieved by active folding and developing of the hinges. To validate its locomotion ability, experiment is conducted on level ground. The results show that the proposed flex-rigid robot can perform flipping locomotion with average velocity of 60 mm/s."}, {"label": 0, "content": "Advertisements are an integral part of internet economics and culture, and video ads are the most popular and arguably the most entertaining form of advertisements. With the recent growth in digital marketing, video ads have seen unprecedented growth and are growing in importance as an advertising means. Video ads are expensive to create and are not always effective. The effectiveness of a video ad is usually not known before its deployment, which is non-ideal for creators, advertisers, and ad platforms. In this paper, we outline an idea to provide feedback before an ad is placed on its effectiveness based on the video along with the historical data about the effectiveness of other video ads. We propose a multi-modal mixture based algorithm to predict the effectiveness automatically. Specifically, we exploit rich textual information often found with an advertisement as well as visual information to learn a finite mixture model. Our experiments on a publicly available dataset show that our approach can outperform other baseline approaches."}, {"label": 0, "content": "Automatic Modulation Recognition (AMR) is of great significance in civil and military applications. Cumulant-based recognition is one of the effective methods for AMR. However, different from that in conventional SISO systems., Cumulant-based AMR in space division multiplexing MIMO systems faces the problem of lower recognition rate since the statistical characteristics of signals are different in these systems. In order to solve this problem., we propose to adopt auto-encoding network (AEN) as the data dimension reduction algorithm and artificial neural network (ANN) as the classifier of several kinds of digital modulation signals in MIMO system. In our simulations., the proposed scheme is used to classify five types digital modulation signals., which are 2PSK., 4PSK., 8PSK., 16QAM., 32QAM. Simulation results indicate that the proposed method will realize substantially higher recognition rate compared with direct classification by cumulants."}, {"label": 0, "content": "Multi-hop broadcast communication is one of the main schemes for supporting message dissemination in vehicular ad hoc networks. In this paper, we propose an efficient receiver-oriented broadcast scheme in which the receiving vehicles' probability of forwarding is modeled by the symmetric volunteer's dilemma game. Based on this game, the vehicles that receive the broadcast message are players. At least one of the players should pay a cost and be a volunteer to rebroadcast the message, and then all will benefit from this volunteering. Utilizing fuzzy logic techniques and considering information from the network layer about local density and probability of transmission, the contention window size at the MAC layer will be adjusted. We develop ns-3 simulations to evaluate the performance of the proposed volunteer's dilemma inspired broadcast (VDIB) scheme in terms of reachability, number of rebroadcasts per covered vehicle, number of bytes sent per covered vehicle, and per-hop delay. VDIB performance is compared with that of the distance-to-mean broadcast method, broadcast component of contention-based forwarding, distribution-adaptive distance with channel quality, statistical location assisted broadcast, fuzzy logic-based broadcast, bandwidth efficient fuzzy logic assisted broadcast, and intelligent hybrid adaptive broadcast protocols in both highway and urban environments."}, {"label": 1, "content": "In this paper, we introduce a household Electric Vehicle (EV) scheduling scheme based on a deep neural network (DNN) demand forecast. We present a novel approach for Short Term Load Forecasting (STLF) using DNN with clustering methodology to forecast both household and EV demand. The forecasting was conducted on demand profiles for 200 households in the Midwest region of the United States. We utilized a Tensor-flow based deep learning platform to develop the DNN structure. The households were clustered based on their demand profiles and the clusters were used as forecasting parameters. The scheduling model was developed using the forecasted household and EV demand values to create a linear programming-based optimization model aimed at minimizing the electricity costs for consumers. The optimization model takes into account household and cluster constraints to prevent sudden surges in power demand during low-price periods."}, {"label": 1, "content": "The proliferation of wireless devices and appliances is rapidly facilitating the development of the Internet of Things (IoT). The implementation of state-of-the-art applications is being observed in various sectors such as smart cities, autonomous vehicles, and biocomputing. However, the popularization of IoT is also giving rise to new privacy concerns. This article aims to provide an overview of privacy constraints and primary attacks based on new features of IoT. \n\nTo begin with, we discuss some of the potential privacy constraints and attacks that can be targeted through IoT. Three case studies are presented to demonstrate the principal vulnerabilities and classify the existing protection schemes. Based on this analysis, we have identified three key challenges that need to be addressed in order to ensure the privacy and security of wireless IoT. \n\nFirstly, a lack of theoretical foundation seems to be hindering the development of effective privacy protection solutions in IoT. Secondly, there is a need to find the optimal trade-off between privacy and data utility. Lastly, the over-complexity and high scalability of the system bring in another challenge that needs to be addressed.\n\nIn conclusion, we have identified several potential solutions to tackle the emerging privacy challenges that come with wireless IoT scenarios. It is important for interested readers to investigate the unexplored facets of this promising domain to fuel the development of the IoT industry further."}, {"label": 1, "content": "We have classified the impact of VPNs into two different aspects: the affecting aspect and the affected aspect. The affecting aspect refers to factors such as security, algorithms, hardware, and software, whereas the affected aspect relates to network performance. Although VPNs have effectively enhanced security, one of the affecting aspects of their impact, they can also pose a potential threat to network performance. To examine the impact of VPNs on network performance, we used an NS-2 simulated test-bed for affordability reasons. We evaluated the performance metrics of throughput and delay, which are the most common network performance metrics, using average and percentage change theories. The results indicated a more significant quantitative impact on TCP/IP throughput than on its counterpart UDP/IP. Finally, we formulated an analytical equation to model the performance impact of VPNs."}, {"label": 0, "content": "Recently, Dispatching methods for DC distribution networks have been the focus of researches. However, there is not an evaluating index, which considers the intense coupling property of power and voltage in DC distribution networks, to judge the effectiveness of these methods. Aiming at DC distribution networks with complicated converters' control modes, this paper proposes an evaluating index utilizing Thevenin theorem. Then, an auto regulation of droop control is presented based on the analysis of droop characteristic. Besides, according to the aforementioned evaluating index, this paper designs an automatic generation control strategy for DC distribution networks with converters' multi-control, whose function is to optimize bus power, bus voltage and network losses with the modified genetic algorithm (GA). The auto regulation of droop control and automatic generation control strategy compose the two-stage dispatching method. Finally, a testing network is employed to demonstrate the practicality and applicability of the evaluating index and the two-stage dispatching method. Results show that the evaluating index can accurately reflect the operating conditions of DC distribution networks with different forms of converter control and the operating conditions can be effectively optimized with two-stage dispatching method."}, {"label": 0, "content": "The economics of high performance computing are rapidly changing. Commercial cloud offerings, private research clouds, and pressure on the budgets of institutions of higher education and federally-funded research organizations are all contributing factors. As such, it has become a necessity that all expenses and investments be analyzed and considered carefully. In this paper we will analyze the return on investment (ROI) for three different kinds of cyberinfrastructure resources: the eXtreme Science and Engineering Discovery Environment (XSEDE); the NSF-funded Jetstream cloud system; and the Indiana University (IU) Big Red II supercomputer, funded exclusively by IU for use of the IU community and collaborators. We determined the ROI for these three resources by assigning financial values to services by either comparison with commercially available services, or by surveys of value of these resources to their users. In all three cases, the ROI for these very different types of cyberinfrastructure resources was well greater than 1 - meaning that investors are getting more than $1 in returned value for every $1 invested. While there are many ways to measure the value and impact of investment in cyberinfrastructure resources, we are able to quantify the short-term ROI and show that it is a net positive for campuses and the federal government respectively."}, {"label": 0, "content": "Power distribution systems require continuous monitoring as the integration of renewable energy resources is increasing and the load demand is growing. The implementation of synchrophasors in distribution systems enhances the situational awareness of the system and provides a unique opportunity for developing new monitoring algorithms. This paper proposes a voltage monitoring algorithm based on the synchrophasor-based linear state estimation method. Particularly, the voltage monitoring algorithm combines a set of early warning indicators and the BDS independence test which can detect the voltage instability in a timely manner while avoiding false alarms when the system is still away from the stability boundary. Numerical study has been conducted in the Quebec test feeder to show the effectiveness of the method."}, {"label": 0, "content": "Desirable properties of extensions of non-negative matrix factorization (NMF) include robustness in the presence of noises and outliers, ease of implementation, the guarantee of convergence, operation in an automatic fashion that trades off the balance between data approximation and model simplicity well, and the capability to model the inherently sequential structure of time-series signals. The state-of-the-art methods typically have only a subset of these aforementioned properties and seldom simultaneously possess them all. In this paper, we propose a novel approach that provides all these desirable properties by extending the automatic relevance determination framework in NMF from Tan and F\u00e9votte. Starting from an objective function derived from the maximum a posterior estimation of a Bayesian model, we develop majorization-minimization algorithms that work effectively to determine the correct model order, regardless of the impact of noise and outliers. Subsequently, we give a rigorous convergence analysis of the proposed algorithms. Moreover, convolutive bases are also incorporated in the basic model so that it is able to capture the richness of temporal continuity. We perform experiments on both synthetic and real-world data sets to show the efficiency and robustness of our approach."}, {"label": 1, "content": "Despite the increasing popularity of deep neural networks (DNNs), they cannot be efficiently trained on existing platforms. Thus, there have been efforts to design dedicated hardware for DNNs. Our recent work has directly supported the stochastic gradient descent (SGD) algorithm by constructing synapses, the basic element of neural networks, using memristors, which are emerging technologies. Optimization algorithms are commonly used in DNN training due to the limited performance of SGD. Therefore, DNN accelerators that only support SGD might not meet DNN training requirements. In this paper, we present a memristor-based synapse that supports the commonly used momentum algorithm. Momentum significantly improves the convergence of SGD and facilitates the DNN training stage. We propose two design approaches to support momentum: 1) a hardware-friendly modification of the momentum algorithm using memory external to the synapse structure, and 2) updating each synapse with a built-in memory. Our simulations show that the proposed solutions for DNN training are as accurate as training on a GPU platform, while speeding up performance by 886x and decreasing energy consumption by 7x on average."}, {"label": 0, "content": "The presence of periodical impulses in vibration signals usually indicates the occurrence of faults in roller bearings. Unfortunately, in the complex working condition with the heavy noises, fault detection in mechanical systems is often difficult. To solve this problem, a hybrid method of ensemble empirical mode decomposition (EEMD) and L-Kurtosis clustering-based segmentation is proposed. EEMD is similar to empirical mode decomposition (EMD), which can express the intrinsic essence using simple and understandable algorithm to solve the mode mixing phenomenon. L-Kurtosis is the improved version of kurtosis to recognize the impulses without the influence of outliers. Furthermore, the L-Kurtosis value is employed as an indicator in the clustering-based segmentation method to extract the fault features from the background noises. To illustrate the feasibility of utilizing the EEMD and L-Kurtosis based clustering segmentation method, benchmark data simulations and experimental investigations are performed to detect faults in bearings. The results show that the proposed method enables the efficient recognition of faults in bearings."}, {"label": 1, "content": "The problem of detecting nontechnical losses (NTL) using pattern recognition methods has been studied extensively by different research groups. However, a comparison between the proposed methods is challenging due to the unavailability of a standardized database for testing NTL detection methods. This paper suggests four variations of a database based on the IEEE 123 Bus-Test Feeder to enable fair testing of NTL detection methods. The proposed models and hypotheses are explained, and we present an application of the Optimum-Path Forest classifier in NTL detection using the developed database. The database is built on the Matlab platform and is accessible on http://www.power.ufl.edu."}, {"label": 0, "content": "As we see the cyberspace evolve we also see a directly proportional growth of the people using the cyberspace for communication. As a result, the misuse of the cyberspace has given rise to negative issues such as cyberbullying, which is a form of harassing other people using information technology in a deliberate and continual manner. The detection and prevention of cyberbullying becomes critical for safe and health social media platforms. In this paper, a review of the cyberbullying content in Internet, the categories of cyberbullying, data sources containing cyberbullying data for research, and machine learning techniques for cyberbullying detection are overviewed. The main challenges of the cyberbullying detection are demonstrated, including the lack of multimedia content-based detection and availability of public accessible dataset. Suggestions are provided as the conclusion of the overview."}, {"label": 1, "content": "An effective technique has been developed for calculating loss probability in queueing systems with heavy tails. This technique is particularly useful for analyzing queueing systems with power-law distributions, which are commonly used to model network devices that operate under fractal traffic. The study investigates the form of the loss probability, as it relates to buffer capacity in these systems. Additionally, the effect of the channel number in queuing systems on the dependence is examined. Practically speaking, the newfound rapid technique and the results derived from its application can be utilized to solve engineering problems encountered in the analysis and design of contemporary computer networks."}, {"label": 0, "content": "Tomographic synthetic aperture radar (TomoSAR) inversion of urban areas is an inherently sparse reconstruction problem and, hence, can be solved using compressive sensing (CS) algorithms. This paper proposes solutions for two notorious problems in this field. First, TomoSAR requires a high number of data sets, which makes the technique expensive. However, it can be shown that the number of acquisitions and the signal-to-noise ratio (SNR) can be traded off against each other, because it is asymptotically only the product of the number of acquisitions and SNR that determines the reconstruction quality. We propose to increase SNR by integrating nonlocal (NL) estimation into the inversion and show that a reasonable reconstruction of buildings from only seven interferograms is feasible. Second, CS-based inversion is computationally expensive and therefore, barely suitable for large-scale applications. We introduce a new fast and accurate algorithm for solving the NL L1-L2-minimization problem, central to CS-based reconstruction algorithms. The applicability of the algorithm is demonstrated using simulated data and TerraSAR-X high-resolution spotlight images over an area in Munich, Germany."}, {"label": 1, "content": "In a wireless sensor network (WSN), coverage holes can significantly impact the efficiency of data collection and the quality of service of the network. Detecting these holes is vital to patching the network and to ensure network quality. A new paper proposes a distributed coverage hole detection algorithm based on hole boundary nodes (HPNs-CHD). This algorithm uses a sensing disk model to identify the HBN nodes in the WSN and then uses probabilistic messages to detect coverage holes. Simulation results demonstrate that this algorithm is more efficient in terms of energy consumption and detection time when compared to two other algorithms."}, {"label": 0, "content": "The consumer Internet of Things (IoT) platforms are gaining high popularity. However, due to the open nature of wireless communications, smart home platforms are facing many new challenges, especially in the aspect of security and privacy. In this article, we first introduce the architecture of current popular smart home platforms and elaborate the functions of each component. Then we discuss the security and privacy challenges arising from these platforms and review the state of the art of the proposed countermeasures. We give a comprehensive survey on several new attacks on the voice interface of smart home platforms, which aim to gain unauthorized access and execute over-privileged behaviors to compromise the user's privacy. To thwart these attacks, we propose a novel voice liveness detection system, which analyzes the wireless signals generated by IoT devices and the received voice samples to perform user authentication. We implement a real-world testbed on Samsung's SmartThings platform to evaluate the performance of the proposed system, and demonstrate its effectiveness."}, {"label": 1, "content": "The Wearable Instantaneous Ball Speed Estimator (WIBASE) is a prototype system developed to measure the speed of a cricketer's bowling during training. This equipment is vital for coaches as it helps them assess the performance of fast bowlers and their ability to bowl consistently. WIBASE comprises two primary hardware components- a computer and a wrist-worn electronic board with a 3-dimensional (3D) acceleration sensor.\n\nThe system tracks the three-axis acceleration generated by the arm's movement during the delivery of the ball and stores the data. Data from three different sensors- accelerometer, gyroscope, and magnetometer- is processed by a Digital Motion Processor (DMP) on the board in a Sensor Fusion process before being sent to the computer via Bluetooth. A Python script on the computer receives the filtered acceleration, consisting of both static and dynamic acceleration. Numerical integration using the Trapezoidal method is used to derive the speed of the bowler, and the data is logged into a file while displaying the speed on the computer. \n\nThe WIBASE has undergone three sets of experiments, and test results indicate that the equipment can effectively track 3D acceleration, derive the bowler's speed, and log all data into a file in real-time. The WIBASE is a reliable, accessible, and affordable solution to measure the speed of a cricketer's bowling during training."}, {"label": 1, "content": "The residential rooftop solar market in the U.S. has experienced rapid growth in recent years. However, this growth has created operational and reliability challenges for the electric power industry if left unaddressed. Specifically, the under-utilization of available energy, increase in costs, and reduction in environmental benefits are key concerns, as evidenced by the CAISO Duck Curve.\n\nTo mitigate these challenges, the authors of this paper have enhanced their previous model for calculating season-wise household-level residential energy consumption profiles using a synthetic population resource. Specifically, the updated model now includes the effects of increasing percentages of rooftop solar penetration on the residential energy demand profiles of various regions.\n\nThis development is particularly relevant to electric power utilities, as it provides them with critical information to better manage the surge in residential rooftop solar installations in their supply areas. By leveraging this data, utilities can efficiently allocate their resources and reduce costs, while also ensuring that solar energy is being utilized effectively and without compromising grid stability.\n\nIn summary, the authors' bottom-up approach for computing residential energy consumption profiles has been updated to account for the effects of increasing rooftop solar penetration. This provides electric power utilities with important insights they need to manage this rapidly-growing market effectively."}, {"label": 0, "content": "High speed railway communications has received wide attention in the world. The Doppler shift from the motion of train induces inter-carrier interference. Meanwhile the doubly selective channel increases the difficulty of training and beamforming. In this paper, we propose an angle domain channel tracking scheme. The base station is equipped with large-scale uniform linear antennas array (ULA) to provide high angular resolution. The spatial property was investigated to decompose channel into angular information and beam gain. The former is acquired by aligning beams towards the direction of signals, based on which the Doppler frequency offset (DFO) is compensated. The latter is tracked by using linear Kalman filter, which is optimal for minimizing the mean square error (MSE). By combing the angular information and beam gain, the CSI is recovered. Simulation results show the superiority of proposed scheme."}, {"label": 0, "content": "With the development of smart substation, information sharing and its redundant transmission of analog quantities in secondary system provide new research ideas for state estimation of power system. This paper proposes secondary system state estimation method based on information redundant of secondary system and gives its structure and content and builds a linear static model of state estimation used redundant relationship of analog quantities. Furthermore, a numerical example is designed to simulate and verify the proposed method. The experiment results show that the method can effectively reduce absolute error of analog quantities and improve uploading analog quantities reliability. The research method in secondary system state estimation has a broad range of applications and research space."}, {"label": 0, "content": "5G is the next wireless technology that is expected to be launched in 2020. Electromagnetic radiations emitted by mobile phones resulted in considerably higher brain tissue exposure than other radiation sources in the radiofrequency band, which led to concerns about the possible potential health effects from exposure to Radio Frequency (RF) and Millimeter (mm) wave radiations. This paper investigates the effects of 5G radiations for different frequency candidates on human brain. This has been achieved by using Computer Simulation Technology (CST) software by conducting simulations on Specific Anthropomorphic Mannequin (SAM); a model designed according to different international standards representing the average material properties of the head by calculating the Specific Absorption Rate (SAR, a quantitative measure of interaction mechanisms of radiofrequency radiation with the living systems and expressed in watt per kilogram (W/kg) in order to check whether the resulting exposure is safe or not by comparing it to the safety limit of exposure to high frequency radiations set by different international standards."}, {"label": 0, "content": "The statistical delay of a path is traditionally modeled as a Gaussian random variable assuming that the path is always sensitized by a test pattern. Its sensitization in various circuit instances varies among its test patterns and the pattern induced delay is non-Gaussian. It is modeled using probability mass functions. The defect coverage is improved by test pattern selection using machine learning. Experimental results demonstrate accuracy in defect coverage when comparing to existing methods."}, {"label": 0, "content": "Fog computing enables computation, storage, applications, and network services between the Internet of Things and the cloud servers by extending the Cloud Computing paradigm to the edge of the network. When protecting information security in Fog computing, advanced security with low latency, wide-spread geographical distribution support, and high flexibility should be taken in to considertion first, because of its huge number of nodes. In this paper, we propose a new cryptographic primitive, named CCA2 secure publicly-verifiable revocable large-universe multi-authority attribute-based encryption (CCA2-PV-R-LU-MA-ABE), to achieve flexible fine-grained access control in Fog computing. In this primitive, end nodes in fogs generate private keys from multiple authorities that might be differentiated by their geographical locations or functions, and their attributes can be denoted by any strings in the large universe, which meets diverse needs in practical Fog applications. In addition, the accessibility of nodes can be revoked efficiently even by resource-limited devices. To ensure the validity of ciphertext, this primitive supports public verification and only valid ciphertext can be stored or transmitted. Based on the primitive and the feature of Fog computing, we construct a concrete CCA2-PV-R-LU-MA-ABE scheme. We define the security model of this primitive, which is much more secure than the CPA-secure scheme. Finally, we compare the efficiency of the proposed concrete scheme with that of the existing CPA-secure scheme by both theoretical and experimental analysis, and the results show that the extra consumption of efficiency to improving CPA to CCA2 is considerably low. The proposed scheme is highly secure, flexible, and efficient enough to be deployed in practical Fog computing."}, {"label": 1, "content": "Degradation modeling plays a crucial role in analyzing accelerated degradation test data and condition-based maintenance. The degradation rate of a process is typically dependent on both the age of the unit and its state. In the literature, various age-state-dependent degradation models have been proposed, but one of the main limitations of such models is their lack of tractability.\n\nTo address this concern, this paper introduces an analytically tractable age-state-dependent degradation model. The model is defined in terms of the degradation rate, whose mean function is represented by a bivariate power-law model. Additionally, the degradation increment is modeled by a non-homogeneous gamma process, whose mean function has a closed-form expression and is dependent on both the age and degradation level of the unit. The model encompasses age- and state-dependent models as special cases.\n\nTo select the appropriate model for a particular situation, an approach is proposed based on the Akaike information criterion. The proposed model and approach are demonstrated using a real-world example, which highlights the flexibility and usefulness of the model."}, {"label": 0, "content": "The stochastic dependency exists in many complex systems, which cannot be ignored in reliability modeling. The stochastic dependency is analyzed from the degradation perspective in this paper. For a two-component system, a degradation interaction model is built to describe the dependency between the degradations of two components based on nonlinear Wiener process and Copula function. Then, the reliability functions of components and system are derived, and the parameter estimation approach of the developed models is given. Finally, a numerical example about fatigue crack development is presented to validate and illustrate the performance of the developed models."}, {"label": 0, "content": "In this paper, we proposed an effective method of facial expression recognition based on a G-2DPCA feature and Sparse Representation-based Classification (SRC). Gabor filters with five scales and eight directions are first employed for feature extraction. To address the high dimension of Gabor features, we select one out of forty Gabor filters with an optimal parameter pair of scales and directions to filter facial images. Two-dimensional principal component analysis (2DPCA) is utilized for image representation and data dimension reduction. It retains the 2D geometric structure of an image, and the image matrix does not need to transform into a vector, which reducing the computation time greatly. Finally, Gabor plus 2DPCA (G-2PCA) features are regarded as the atoms of dictionary in SRC. Experimental results demonstrate that the proposed method has better performance than the existing algorithms."}, {"label": 0, "content": "Energy security is vital to the national welfare and the livelihood of the masses. As the most important link in the energy transfer chain, the security of power system operation is of great significance. For a long time, how to improve the knowledge of power system--one of the most complicated artificial system--has always been the goal and motivation of electronic engineers. This paper firstly summarizes the cognitive method of general physical system and power system, teases out their inner link and mapping relationship. With the population of WAMS in power grid, a new idea about taking advantages of WAMS data for stability detection, data-driven research method is purposed. This paper reviews on the classification, development and status quo of all kinds of data-driven methods. In view of the ambiguity in the internal of data-driven methods, this paper summarizes all kinds of technique routes and a clear frame of data-driven method is sorted out."}, {"label": 1, "content": "Microscopic traffic simulation is known to have limited feasibility for evaluating large-scale traffic scenarios due to long runtimes. While CPUs, graphics processing units (GPUs), and fused CPU-GPU devices are all widely available and inexpensive, common traffic simulators still rely solely on CPU-based execution. This leaves considerable acceleration potential untapped. Several existing works have explored the use of accelerators for traffic simulations, but have relied on simplified models of road networks and driver behavior tailored to the given hardware platform. This makes it difficult for these approaches to benefit from the extensive body of research on the accuracy of common traffic simulation models. \n\nIn this paper, we introduce an approach that explores the performance benefits of heterogeneous hardware while relying on conventional traffic simulation models used in CPU-based simulators. Our proposed partial offloading approach utilizes either a dedicated GPU or a fused CPU-GPU device. We also present a fully GPU-based traffic simulation, discussing the challenges of this approach. Our results demonstrate that CPU-based parallelization is comparable to and can closely approximate the results of partial offloading. However, full offloading outperforms both approaches, achieving up to a 28.7\u00d7 speedup over sequential CPU execution."}, {"label": 0, "content": "With the rapid development of the Internet, the amount of network data was exponentially increased and based on a dynamic society the network at any time will produce many hot topics. According to the requirements of the Hidden Markov Model, the paper provided and modeled a general framework based on the statistical modeling of HMM for solving the problem of hot topics on networks, then assessed the probability of a network theme to be a hot topic. Experiments show that the framework is superior to the traditional algorithm of network hot topics discovery considering the convergence speed and accuracy."}, {"label": 1, "content": "The gaming industry has expanded its services to include live streaming, with platforms such as Twitch.tv and YouTubeGaming enabling players to stream their gameplay both live and on demand. However, current video quality assessment methods have proven to be unsatisfactory. In response to this, a new No Reference (NR) gaming video quality metric called NR-GVQM has been developed, with performance comparable to Full Reference (FR) metrics. NR-GVQM was designed by training a Support Vector Regression (SVR) with a Gaussian kernel, using nine frame-level indexes such as naturalness and blockiness as input features, and Video Multimethod Assessment Fusion (VMAF) scores as the ground truth. Results from a publicly available dataset of gaming videos show a correlation score of 0.98 with VMAF and 0.89 with MOS scores. Additionally, two approaches to reduce computational complexity have been presented."}, {"label": 1, "content": "Dynamic spectrum assignment in cognitive heterogeneous wireless networks is a difficult integer programming problem to solve optimally within a limited time. To obtain the most optimal dynamic spectrum allocation scheme, a novel discrete optimization algorithm - the Quantum Harmony Search Algorithm (QHSA) - has been introduced. By leveraging quantum optimization theory and harmony search algorithms, the QHSA improves the accuracy and speed of convergent optimization. The QHSA-based dynamic spectrum assignment method has been demonstrated to outperform other intelligence-based dynamic spectrum assignment methods in various cognitive heterogeneous network environments through computer simulations."}, {"label": 1, "content": "In a future scenario with multiple wireless network coverage, the choice of vertical handoff decision algorithm will be critical to ensure session continuity, user mobility, and seamless roaming across heterogeneous wireless networks. Hence, investigating vertical handover related algorithms is essential to the success of various wireless access networks in the future. In this regard, this paper proposes an optimized algorithm that combines two multiple attribute decision making (MADM) techniques, namely the Entropy and the improved Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS). The Entropy method is utilized to obtain objective weights, and the improved TOPSIS method is applied to rank the alternatives. Simulation results reveal that the proposed technique can achieve a more reasonable weight distribution and significantly reduce the number of handoffs."}, {"label": 0, "content": "In this paper, the improved face recognition method based on two-directional 2DPCA (two-dimensional principal component analysis) in each block of face images is proposed. Firstly, the face image is divided into several sub-images, and then the sub-image features of each corresponding block are extracted by two-directional 2DPCA according to the number of sub-images. Finally, using the support vector machine to improve the recognition rate. Experimental results on ORL face database and YALE face database show that the proposed method is superior to any other 2DPCA methods in face recognition rate."}, {"label": 0, "content": "In this paper, we study the physical layer security and transmission reliability problem where there is an active eavesdropper (AE) in the D2D underlaying cellular networks. We formulate the cooperation between the cellular user equipment (CUE) and the D2D user equipment (DUE), the completion between legitimate users and the AE to be a secrecy anti-jamming game. In the proposed game framework, DUE launches the cooperative relaying or the friendly jamming mode to help CUE to improve its anti-eavesdropping and anti-jamming performance. CUE gives different-level rewards for the assistance of the DUE. And AE shifts its attacking modes between actively jamming and passively eavesdropping to maximize the destruction for the D2D underlaying cellular networks. Under the perfect information, we prove the existence of the pure-strategy equilibrium of the proposed game. Under the imperfect information, we analyze the existence of the mixed-strategy equilibrium of the proposed game and propose a distributed Q-Iearning-based algorithm to converge to a mixed-strategy equilibrium. Simulation results show that the proposed algorithm is convergent and verify that average utilities of legitimate users are improved by the cooperation between CUE and DUE."}, {"label": 0, "content": "In this paper, the deformation details of a soft actuator of a soft surface manipulator was investigated. The relation between the pressure of the inflation and the object displacement was established. Finite Element Analysis was used to investigate the working principle of single soft actuator. The simulation results show how the soft actuators effect the movement of the object on the surface. The relation between inflation pressure and object displacement was established by applying different pressure to the Finite Element model. The soft surface manipulator was considered as a servo mechanism acting on the object. The kinematic model of the object was established to facilitate the development of trajectory tracking algorithm in the future work."}, {"label": 0, "content": "A novel method for semantic action recognition through learning a pose lexicon is presented in this paper. A pose lexicon comprises a set of semantic poses, a set of visual poses, and a probabilistic mapping between the visual and semantic poses. This paper assumes that both the visual poses and mapping are hidden and proposes a method to simultaneously learn a visual pose model that estimates the likelihood of an observed video frame being generated from hidden visual poses, and a pose lexicon model establishes the probabilistic mapping between the hidden visual poses and the semantic poses parsed from textual instructions. Specifically, the proposed method consists of two-level hidden Markov models. One level represents the alignment between the visual poses and semantic poses. The other level represents a visual pose sequence, and each visual pose is modeled as a Gaussian mixture. An expectation-maximization algorithm is developed to train a pose lexicon. With the learned lexicon, action classification is formulated as a problem of finding the maximum posterior probability of a given sequence of video frames that follows a given sequence of semantic poses, constrained by the most likely visual pose and the alignment sequences. The proposed method was evaluated on MSRC-12, WorkoutSU-10, WorkoutUOW-18, Combined-15, Combined-17, and Combined-50 action datasets using cross-subject, cross-dataset, zero-shot, and seen/unseen protocols."}, {"label": 0, "content": "Internet of Things (IoT) systems are slowly but steadily becoming part of different aspects of our lives, with their applications ranging from smart homes, to wearable devices, to healthcare, etc. Traditional cryptographic schemes might not be suitable to be implemented on resource limited IoT devices. The decision to utilize a certain cryptographic algorithm, is mainly based on a tradeoff between security and performance, i.e. power consumption. A benchmark of these different cryptographic algorithms on IoT platforms is a must for security architects while designing their protocols and schemes. This paper presents a benchmark of the most known cryptographic algorithms on the Raspberry Pi platform, with a comparison with Arduino benchmark results provided in the literature."}, {"label": 0, "content": "In operation, methods of estimating probability density distributions are considered, which are urgent in the solution of the filtering issues of the useful information on the background of external acoustic noise in the telecommunications systems. Parametric and non-parametric methods of estimating probability densities are discussed, methods for determining an empirical distribution function for the case of a limited sample volume. It is shown that the approximation of the probabilities empirical data can be performed by the method of nuclear evaluations. Within this method, the estimate may be represented by the convolution of the core and the empirical density. It derives from the fact that the nuclear score is a result of a histogram of the histogram evaluation. It has been shown that reconstruction of the distribution function as a polynomial in the system of functions is the question of finding coefficients, which is the task of linear regression, which is solved by minimisation of the quadratic function of the loss built on the basis of the use of the least-squares method and representing the discrepancy of the empirical data and the estimates obtained on their basis. The results of the experimental studies show the error of the reconstruction one-dimensional function of probability density for the case of audio signals and acoustic interferences, given different kinds and orders of polynomial approximation."}, {"label": 1, "content": "Neuromorphic systems are an interdisciplinary research topic that combines electronic, computer, and biological sciences to imitate the functionalities of the human brain. The aim is to create software/hardware systems that mimic biological concepts of the nervous system, with the potential to provide advantages such as lower power consumption, fault tolerance, and massive parallelism in next-generation computers. \n\nThis brief presents a neuromorphic system architecture and a neural computing hardware unit based on a modified leaky integrate and fire neuron model in a spiking neural network for pattern recognition tasks. The authors focus on digital implementation, targeting low-cost high-speed large-scale systems to achieve the goal of neuromorphic computing. \n\nResults from field-programmable gate array implementation are presented as proof of concept, showing that the neuron model and spiking network can achieve a maximum frequency of 412.371 MHz and 189.071 MHz, respectively. Overall, this research contributes to the development of neuromorphic computing technology with potential applications in various fields."}, {"label": 0, "content": "Most of existing traffic simulation methods have been focused on simulating vehicles on freeways or city-scale urban networks. However, relatively little research has been done to simulate intersectional traffic to date despite its broad potential applications. In this paper, we propose a novel deep learning-based framework to simulate and edit intersectional traffic. Specifically, based on an in-house collected intersectional traffic dataset, we employ the combination of convolution network (CNN) and recurrent network (RNN) to learn the patterns of vehicle trajectories in intersectional traffic. Besides simulating novel intersectional traffic, our method can be used to edit existing intersectional traffic. Through many experiments as well as comparative user studies, we demonstrate that the results by our method are visually indistinguishable from ground truth, and our method can outperform existing methods."}, {"label": 0, "content": "This paper proposes a distributed demand response algorithm that considers the uncertainty of resident behavior. This algorithm based on the alternating directions method of multipliers (ADMM) allows for distributing the optimization process across several servers/cores, which conserves users' privacy and reduces the computational complexity of demand response. At the same time, the robust optimization method is applied to deal with the uncertainty of the response process, reduce the impact of resident behavior uncertainties. Finally, the effectiveness of the algorithm has been verified through the simulation."}, {"label": 1, "content": "Supporting real-time communications over wireless networks (WSNs) is a challenge that has been extensively researched. The non-determinism of common channel access schemes like CSMA/CA and packet collisions makes it difficult to achieve real-time WSN communication. Multi-hop mesh networks are even more challenging in the general case due to the inherent complexity. Most real-time WSN solutions are limited to simple topologies like star networks. \n\nTo overcome these challenges, we propose a real-time multi-hop WSN MAC protocol that is built atop the IEEE 802.15.4 physical layer. The protocol relies on precise clock synchronization and constructive interference-based flooding to build a centralized TDMA schedule to support multi-hop mesh networks. Our proposed MAC protocol is connection-oriented, using guaranteed time slots and enables point-to-point communications with redundant paths. \n\nThe protocol has been implemented in simulation, using OMNeT++, and the performance has been verified in real-world deployment using Wandstem WSN nodes. The results demonstrate that the proposed protocol is effective in overcoming the challenges associated with real-time multi-hop WSN communication. \n\nIn conclusion, our proposed real-time multi-hop WSN MAC protocol is an effective solution for supporting real-time communication over wireless networks. It is built atop the IEEE 802.15.4 physical layer and relies on precise clock synchronization and constructive interference-based flooding to build a centralized TDMA schedule to support multi-hop mesh networks. The protocol is connection-oriented, and enables point-to-point communications with redundant paths. The results of our tests using Wandstem WSN nodes demonstrate that our proposed protocol is effective in achieving reliable real-time communication in complex WSN topologies."}, {"label": 0, "content": "This paper investigates the possibility of improving the stability of radio-frequency transfer in telecommunication dense wavelength division multiplexing fiber-optic networks. As it has been identified, the dispersion compensation fibers (DCFs), frequently used in these networks, cause substantial differential delay, whose temperature-induced fluctuations have the most significant impact on the deterioration of the stability of the frequency transfer. The authors present a method that allows achieving significant improvement in the long-term stability of the frequency transfer. The developed method is based on modeling the impact of DCFs with the help of remotely accessible temperature sensors factory installed by the manufactures in DCF modules. The effectiveness of the proposed solution has been tested on three different long-haul routes (up to 1550 km), set up in the operational Polish National Research and Education Network."}, {"label": 1, "content": "Applications such as autonomous driving and virtual reality (VR) require high definition (HD) video to be transferred with low latency. To address this, a new method of ultra-low-latency video coding has been proposed, which uses line-based processing. With optimized entropy coding, image-adaptive quantization and multiple line-based image-prediction methods, the proposed method has a latency of only 0.44\u03bcs at minimum for Full-HD video. This method achieves a compression rate of up to 39.0% data size and an image quality of 45.4 dB. The proposed basic algorithm also achieves a compression rate of 33%, whereas the optional 1D-DCT mode achieves 20%. Neither of these result in a significant visual degradation. These results are very similar to those of the H.264 Intra, despite being a one-thousandth of the latency of the proposed method. With the proposed video coding method, autonomous vehicles and VR devices can transfer HD video using just 20% of the bandwidth of the source video, without latency or visual degradation."}, {"label": 0, "content": "As a special form of Mobile Ad hoc NETwork (MANET), vehicles in the network can connect to other vehicles on the road and the Internet, and can provide stable and high-speed wireless data access services for vehicles with high velocity. VANET has become an effective technology and important means to guarantee vehicle safety, provide intelligent traffic management of high-speed data communication and vehicle entertainment. However, the typical characteristics of VANET, including rapidly changing network topology, highly dynamic channel condition and node competition in channel accessing, etc., raise difficulties and challenges on data transmission in VANET. To stress the problem, a cluster algorithm based on vehicle mobility for VANET is proposed, in the proposed algorithm, the object function of cluster head selection is formulated based on the vehicle mobility including position, velocity and packet forwarding capability, the process of the clustering algorithm is also presented. Simulations demonstrate that comparing to previously proposed algorithms, the proposed algorithm offers better performance in terms of packet delivery rate, average transmission delay and total throughput."}, {"label": 0, "content": "This paper aims at the fault-tolerant adaptive control for the nonlinearly parameterized systems with mismatched disturbances. A novel adaptive control scheme is developed in this paper by adding a power integrator technique. The mismatched disturbances are estimated by a designed disturbance observer. A new fault-tolerant control method is proposed to deal with the uncertain actuator faults including actuator stuck. The proposed adaptive controller ensures that the closed-loop system is input-to-state stable. Simulation results for a robotic arm system are given to present the effectiveness of the developed control technique."}, {"label": 0, "content": "For serving traffic in the inter data centers which provide services such as, duplication of data and migration of the virtual machines, it is requisited to transfer voluminous data for which, under guarantee of a finishing time within the stipulated (i.e., pre-set) deadline, specific latency is tolerable. In the current work, we propose offline routing and spectrum assignment (RSA) schemes in view of the transfer of deadline complying voluminous data demands in elastic optical networks. The proposed schemes, which jointly optimize time and frequency domains, are initially formulated as an integer linear program (ILP). Subsequently, in view of practicality, we propose scalable scheduling techniques which combine three methods of ordering demands, and two schemes of RSA. To evaluate the proposed ILP model and the scheduling methods, we conduct simulations considering realistic network parameters and topologies. The obtained results demonstrate that in comparison to ILP model, scalable methods obtain similar spectrum usage performance within reasonable times. Lastly, based on the results, we also provide a `rule-of-thumb' on the selection of appropriate scheduling technique."}, {"label": 0, "content": "Most software defect prediction models usually assume that enough historical training instances with labels are available. Additionally, the training data and the predicted instances should share the same features to ensure the prediction accuracy. However, in practice, there are many datasets with different granularities containing information in different dimensions. Therefore, it is valuable to effectively use the small scale and different dimensions of data as training instances to improve the prediction performance of the model. We propose a heterogeneous data orienting multiview transfer learning for software defect prediction, denoted as MTDP, which can achieve different dimensions and granularities features to automatically learn labels through neural network models. With this multiview transfer method, lots of training instances are provided for software defect prediction model to ensure the effectiveness of training labels. The proposed MTDP method has four main stages: 1) build heterogeneous transfer models; 2) transfer heterogeneous instances to generate quasi-real instances; 3) label quasi-real instances through co-training and then expand the training set; and (4) construct improved software defect prediction models. The experimental results show that the quasi-real instances have similar effects compared with real instances. Moreover, the software defect prediction performance can be improved by introducing the quasi-real instances into the training dataset."}, {"label": 0, "content": "Grinding in ball mills is a crucial technological and industrial process which is used for the reduction of the size of particles with variant physical, chemical and mechanical characteristics. The control performance of the ball mills' grinding process is of outmost importance as this will determine the profit, where the energy consumption, the product quality and time efficiency are commonly concerned. In this paper, nonlinear model predictive control for ball mill grinding process is implemented. Economic performance, time delays and the consumption of energy in the grinding process with the proposed control system are engaged using Discrete Element Method (DEM) software. The results from experimental tests indicate the proposed method to be effective."}, {"label": 1, "content": "This paper proposes the use of an artificial neural network approach to implement electric demand response (DR) programs, which aim to reduce or even avoid load curtailments. The approach involves the classification of system hourly loads based on customers' potential participation in DR programs and identifying the effective periods of participation. A mathematical model based on demand-price elasticity, incentives, and penalties is developed to calculate interruptible/curtailable loads for DR programs. \n\nThe paper emphasizes that the effectiveness of DR programs to reduce/avoid load curtailments depends on efficient and accurate identification of appropriate hourly loads for DR programs. The proposed method is demonstrated on the IEEE reliability test system and the results of the case study showed that the approach is indeed effective in identifying appropriate hourly loads for DR programs.\n\nOverall, this paper provides a valuable contribution to the field of electric demand response programs by proposing a neural network based approach that can improve the accuracy and efficiency of identifying appropriate hourly loads for DR programs."}, {"label": 0, "content": "This paper proposes a unified framework to design sliding-mode control for stabilization of delayed memristive neural networks (DMNNs) with external disturbances. Under the presented framework, finite-time stabilization, and fixed-time stabilization of the controlled DMNNs can be, respectively, obtained by choosing different values for a specific control parameter. It is proved that the system responses can be made reaching the designed sliding-mode surface in finite and fixed time, and then stay on it. Moreover, it also illustrates that the inevitable external disturbances can be rejected by the designed sliding-mode control. Finally, the efficiency and superiority of the obtained main results are verified by comparisons with related works and numerical simulations."}, {"label": 1, "content": "Intelligent techniques for estimating the truthfulness of unknown entities in distributed environments are crucial for achieving complex goals. Distributed Reputation Management Systems (RMSs) enable this task without relying on a central entity that may become a bottleneck and a single point of failure. However, the design of a distributed RMS is challenging due to numerous factors that can affect its performance. To aid researchers in evaluating RMS robustness against security attacks, we present a distributed simulation environment that models agent behavior and RMS logic. Additionally, we introduce an omniscient truth-holder process that possesses global knowledge of all involved entities. This allows for comparison of the distributed RMS's performance with that of an ideal RMS at simulation time. Through a series of experiments, the effectiveness of our platform was demonstrated in measuring the vulnerability of an RMS to a common set of security attacks."}, {"label": 0, "content": "The Internet of Things (IoT) is becoming truly ubiquitous in our everyday lives, but it also faces unique security challenges. Intrusion detection is critical for the security and safety of a wireless IoT network. This article discusses the human-in-theloop active learning approach for wireless intrusion detection. We first present the fundamental challenges against the design of a successful intrusion detection system for a wireless IoT network. We then briefly review the rudimentary concepts of active learning and propose its employment in the diverse applications of wireless intrusion detection. An experimental example is also presented to show the significant performance improvement of the active learning method over the traditional supervised learning approach. While machine learning (ML) techniques have been widely employed for intrusion detection, the application of human-in-the-loop ML that leverages both machine and human intelligence to intrusion detection of IoT is still in its infancy. We hope this article can assist readers in understanding the key concepts of active learning and spur further research in this area."}, {"label": 0, "content": "Indoor localization is often challenging due to the non-availability of GPS signals. Recently, various radio frequency fingerprinting techniques have been proposed to identify indoor locations using simply received signal strength (RSS) measurements. In general however, RSS measurements are time-varying and are difficult to model for complex environments. This paper proposes the use of dictionary learning (DL) to generate high quality fingerprints that depend also on the channel characteristics for each location. An enhanced DL algorithm is proposed that utilizes prior information about the channel distribution, and can generate the fingerprints in an online fashion. Simulation results demonstrate the efficacy of the proposed approach."}, {"label": 0, "content": "Annotation systems provide services that vary from adding simple information for signifying content of interest, to indicate patterns in documents for creating statistical models and applying machine learning techniques. In this paper, we argue that AI mechanisms should be part of the annotation process to collaborate with annotation systems' end users (i.e. annotators) as oppose to be just an outcome from annotators' work. Indeed, current systems do not delve into AI aspects to support the annotation process, lacking features that we argue as essential in annotation systems. That is, collaboration between annotators and AI, collaborative knowledge curation by extracting and structuring knowledge from annotations considering the context of annotation anchors (i.e. area that was selected to create an annotation and comprises the content of interest). To illustrate the gains of this approach, we present HAS, the Hyperknowledge Annotation System. HAS allows one to annotate multimedia content (e.g., text, image, and video) and its tight integration with AI-based services enables the extraction of additional semantic information from the annotated content. In our approach, both the annotation and the information extracted from the content are structured using the hyperknowledge conceptual model, which promotes the use of the spatiotemporal query capabilities of this model for retrieving annotations based on semantic queries. We argue that integrating AI-based services and using the hyperknowledge model for knowledge structuring leverage multimedia annotation systems, enabling the development of novel use cases."}, {"label": 0, "content": "In recent years, micro-phasor measurement unit $(\\mu \\mathrm {P}\\mathrm {M}\\mathrm {U})$ and advanced metering infrastructure (AMI) have been gradually applied to distribution network, providing a large amount of distribution network measurement data, which makes it possible for parameter estimation of distribution network containing T-connection lines. This paper proposes a parameter estimation method for T-connection line in distribution network based on $\\mu \\mathrm {P}\\mathrm {M}\\mathrm {U}$ and AMI. Firstly, the T-connection line model considering the actual installation location of the $\\mu \\mathrm {P}\\mathrm {M}\\mathrm {U}$ and AMI in distribution network is constructed, and the virtual measurement is determined with real-time measurement data of the $\\mu \\mathrm {P}\\mathrm {M}\\mathrm {U}$ and AMI. Secondly, the measurement function equation and Jacobi matrix based on augmented state estimation method are listed. Thirdly, a weighted least square model with voltage phasor and T-connected line parameters as its augmented state variables is established based on multi-interval $\\mu \\mathrm {P}\\mathrm {M}\\mathrm {U}$ and AMI measurement data. Finally, the effectiveness of the proposed algorithm is validated by the four T-connection line zones of the NEV test feeder, and the accuracy of estimation parameters of different lines is analyzed. The method overcomes the difficulty of estimating the parameters of T-connection lines under the condition of insufficient measurement data in traditional distribution network, laying the foundation for the real-time optimization operation of the distribution network."}, {"label": 0, "content": "In the simulations of voltage source converter (VSC) based DC grids, fast protection schemes for overhead lines, such as traveling wave protection unavoidably require a small time step when simulating high voltage direct current (HVDC) circuit breakers. In order to present protection processes accurately and realize hardware-in-the-loop (HIL) simulation, this paper proposes a modeling method of HVDC circuit breakers for small time-step real-time simulation. After using the transmission line modeling method(TLM) to solve the arrester and constant impedance model of the switch, the admittance matrix of the HVDC breaker will keep constant, which reduces computing time greatly. A simplified HVDC breaker is used to interpret the modeling method. And a test circuit is implemented on a field programmable gate array (FPGA) board, on which efficient and accurate simulation results are obtained."}, {"label": 0, "content": "Direction-based methods are the most powerful and popular palmprint recognition methods. However, there is no existing work that completely analyzes the essential differences among different direction-based methods and explores the most discriminant direction representation of a palmprint. In this paper, we attempt to establish the connection between the direction feature extraction model and the discriminability of direction features, and we propose a novel exponential and Gaussian fusion model (EGM) to characterize the discriminative power of different directions. The EGM can provide us with a new insight into the optimal direction feature selection of palmprints. Moreover, we propose a local discriminant direction binary pattern (LDDBP) to completely represent the direction features of a palmprint. Guided by the EGM, the most discriminant directions can be exploited to form the LDDBP-based descriptor for palmprint representation and recognition. Extensive experiment results conducted on four widely used palmprint databases demonstrate the superiority of the proposed LDDBP method over the state-of-the-art direction-based methods."}, {"label": 1, "content": "Although convolutional neural networks (CNNs) are highly effective for image recognition, they have not yet produced impressive results for action recognition in videos. Their inability to model long-range temporal structures, particularly those involving individual action stages, is a significant limiting factor for human action recognition. To address this limitation, we present a novel approach called ActionS-ST-VLAD that emphasizes action stages, aggregates informative deep features, and segments videos based on adaptive video feature segmentation and adaptive segment feature sampling (AVFS-ASFS). Our ActionS-ST-VLAD encoding method uses AVFS-ASFS to automatically split deep features into segments, with features in each segment corresponding to a temporally coherent action stage. We then use a flow-guided warping technique to detect and discard redundant feature maps and aggregate the informative ones using an exploited similarity weight. Additionally, we introduce an RGBF modality to capture motion salient regions in RGB images corresponding to action activity. Extensive experiments on the HMDB51, UCF101, Kinetics, and ActivityNet benchmarks demonstrate that our method effectively pools useful deep features spatiotemporally, resulting in state-of-the-art performance for video-based action recognition."}, {"label": 0, "content": "A robust MAC protocol is required to provide Vehicle to Vehicle (V2V) and Vehicle to Infrastructure (V2I) data traffic communication for Vehicular Ad Hoc Networks (VANETs). The data traffic in VANETs is classified as high priority safety messages, control messages and non-safety infotainment related messages. For this purpose, three regional standards have been proposed. However, their performance has been questioned because of their inability to ensure timely delivery of safety messages. Therefore, most of recent research work has been focused on designing an optimal MAC protocol. This paper surveys significant TDMA-based MAC protocols proposed for VANETs. The purpose of this survey is to outline the current status of research work in MAC for VANETs by evaluating the basic idea, operation and performance of these protocols. Furthermore, certain requirements which must be considered for future work in development of optimal TDMA-based MAC protocols for VANETs have also been proposed."}, {"label": 0, "content": "A thorough performance evaluation of protocols and algorithms for (wireless) networks requires simulation and real-system experiments, as both of them provide individual benefits. Usually, this calls for two separate implementations: One tailored to a discrete-event simulator and a second designed to run on real hardware. Therefore, significant effort is required to implement the same mechanisms or protocols twice. To avoid this overhead, we propose a comprehensive framework based on DPDK and OMNeT ++, allowing to run simulations and real-system experiments from the very same codebase. Hence the best of both worlds is available: scalable scenarios and reproducibility when simulating, and realistic behavior and real-world performance metrics when running real-system experiments. Our evaluation of several representative real-world networking scenarios analyzes similarities between simulation and real-system results and discusses the framework qualitatively. Quantitative results indicate that the approach performs well, i.e., it allows even for productive deployment using the codebase later on, and results from both worlds are comparable."}, {"label": 0, "content": "Career direction is a crucial matter not to be undermined in the development of a more efficient generation of the corporate workforce. In order to obtain accurate career direction, one would think of different ways of identifying attributes that would lead to an accurate classification of personality. In this paper, the goal is extracting personality from the use of language. The paper covers all aspects of this process in terms of Text Normalization Techniques, Feature Extraction, Feature Selection, Data Pre-Processing, Data Sampling, Training Predictive Models to predict personality types, validating the results on test data, and finally, and finally,compare the findings with other approaches to personality classification. After having a personality type classified, the process is as simple as matching career paths that are most likely suitable for the user. All these processes combined by experimenting with various approaches to each operation would result in personality attribute classifiers yielding an average of 96% accuracy."}, {"label": 0, "content": "Nowadays, more and more people have their own accounts in different social networks, and they might use the different email addresses or phone numbers in different networks, so how to identify the same person among different social networks become a vital problem, called network alignment. Users with different accounts are called anchor users, researches showed that using some known anchor users to predict the potential anchor links for the full network is an effective way. To predict more accurate anchor links, the paper proposes a new prediction framework ISS, based on a reality of partially aligned social networks, it applies supervised learning based on social feature extraction and strict stable matching, which improve the accuracy of the prediction result, what is more, we apply an iterative framework to refine known information and maximize the prediction results. Experiments have conducted in two realworld heterogeneous social networks, Foursquare and Twitter, and it demonstrates that ISS can predict anchor links among heterogeneous social networks very well and outperform other similar prediction methods."}, {"label": 1, "content": "The frequency-domain Kalman filter (FKF) is a highly effective tool in many audio signal processing applications due to its efficient convergence and robustness. Nonetheless, the performance of the FKF in under-modeling situations remains largely unexplored. In this letter, we present an analysis of the steady-state behavior of the widely used diagonalized FKF and demonstrate that it produces a biased solution in under-modeling scenarios. As a solution, we propose a highly effective improvement of the FKF that offers optimal steady-state behavior, at a minimal computational cost. We also analyze the convergence behavior of the algorithm and provide computer simulations to validate its performance."}, {"label": 1, "content": "A real-time evaluation method for road service level based on a traffic model-driven approach is proposed to address the problem of predicting road service levels for real-time traffic flow. Firstly, the basic features of the traffic flow model are analyzed, and a flow-time occupancy model is used as a reference for congestion assessment. The K-means clustering algorithm is then employed to complete traffic-based congestion definition. The BP neural network algorithm is used to build a congestion assessment model, and a real-time stream processing framework based on Spark Streaming is established to realize real-time evaluation for the road service level. \n\nThe experimental results indicate that the method can effectively describe the state of congestion and evaluate road service levels in real-time using traffic flow data. It provides decision support for an intelligent traffic control system to improve the service level."}, {"label": 0, "content": "The infrared images recorded by the inspection robot can accurately locate the heating defects of the equipment and have an important role in the fault diagnosis of the equipment. The infrared thermal image is completed by the robot in the shooting stage, while the processing and analysis of the later period still need artificial progress. In this paper, the infrared thermal image of the transformer bushing is processed by the method of image recognition and pattern recognition, which is concerned with the over-reliance on the artificial characteristics of the infrared thermal image processing and analysis process. Firstly, The Normalized Cross Correlation (NCC) template matching method and Otsu threshold segmentation method are used to get the region of the interests (ROI) of the bushing. Then the maximum temperature rise, temperature mean value, temperature variance, temperature gradient and other characteristics of the ROI are extracted. Finally, support vector machine is used to identify the status of bushing. The results show that the proposed model can reduce the manual interference of infrared thermal image and has high accuracy, which is suitable for engineering application."}, {"label": 0, "content": "Based on the analysis of the fault characteristics of the inverter interfaced distributed generation under PQ control and the influence of the distributed generation access on the traditional protection technology, an active distribution network protection scheme considering the capability of regional measurement is proposed. The protection scheme based on the direction of zone current is proposed for the region with enough capability of measurement. The distribution network is divided into zones, and the fault zone is judged by collecting the fault component of current at breakers. As for the region with poor capability of measurement, an reclosing scheme coordinated with DG re-grid timing is proposed. The proposed scheme effectively solves the influence of distributed generation access on traditional protection and reclosing of distribution network. The simulation results of the examples in PSCAD/EMTDC prove the effectiveness of the proposed scheme."}, {"label": 1, "content": "Previous studies have confirmed that the electrostatic sensing technique is a feasible means of monitoring tribo-contact under laboratory conditions. Monitoring the gearbox of a wind turbine via electrostatic application is a promising approach, given the variable operating conditions that such equipment typically experiences during operation.\n\nTo further enhance the efficacy of electrostatic monitoring signals with multiple sensors, this paper presents a new methodology called \"moving window local outlier factor\" (MWLOF). This approach detects early faults much earlier and has a higher sensitivity than conventional techniques when employed in conjunction with an information fusion method. Consequently, it holds great potential for facilitating the widespread adoption of electrostatic-based monitoring in relevant industrial contexts in the future."}, {"label": 0, "content": "With the development of the Internet of Things (IoT) technology, its application in the medical field becomes more and more extensive. However, with a dramatic increase in medical data obtained from the IoT-based health service system, labeling a large number of medical data requires high cost and relevant domain knowledge. Therefore, how to use a small number of labeled medical data reasonably to build an efficient and high-quality clinical decision support model in the IoT-based platform has been an urgent research topic. In this paper, we propose a novel semi-supervised learning approach in association with generative adversarial networks (GANs) for supporting clinical decision making in the IoT-based health service system. In our approach, GAN is adopted to not only increase the number of labeled data but also to compensate the imbalanced labeled classes with additional artificial data in order to improve the semi-supervised learning performance. Extensive evaluations on a collection of benchmarks and real-world medical datasets show that the proposed technique outperforms the others and provides a potential solution for practical applications."}, {"label": 0, "content": "In the Philippines, efforts are being made to address inaccessibility of Internet access by 42% of the population. One such service installs Wi-Fi hotspots in public areas and government buildings. To make the service more effective, data on Wi-Fi access point deployment is vital. While there exist platforms for crowdsourced data submission such as WiGLE, and methods such as dedicated wardriving, these can be costly, infrequent, and can easily become outdated. Regular data collection on Wi-Fi access points is necessary for better accuracy. For regular, inexpensive, and frequent data gathering, opportunistic wardriving is proposed, in which neighborhood public utility vehicles, such as the tricycle, are utilized to serve as vehicles for gathering network data. Leveraging public utility vehicles is inexpensive, and ensures regularity of data gathering because of their usual trips. Comparing this method with crowdsourced data, it is found that opportunistic wardriving is able to find at least 60% of the access points in an area. Opportunistic wardriving also ensures wide coverage of an area, accessing 83% of the chosen location, Brgy. Teachers Village East, in forty trips. It is hoped that this study will aid in understanding an inexpensive method for gathering information on Wi-Fi deployment in various areas."}, {"label": 0, "content": "The prospects of achieving a trillion connected internet of things (IoT) devices by 2020 has created the urgency for effective intrusion detection systems (IDS) for these devices. Although it has been argued that the most effective technique used in such systems is anomaly detection, there exist no mechanisms to determine their performance in real-life deployment. In this paper, we report the results of applying asymptotic analysis to evaluate the performance of an anomaly detection algorithm which is designed using logic reasoning through fuzzy logic methodologies. In order to achieve this, the IDS was included as part of intrusion detection software for ZigBee Wireless Sensor Networks (WSNs). In particular, the solution is targeted to address the ZigBee protocol's vulnerability to flood attacks during node discovery and association to the network. The intrusion detection software is hosted external to the WSNs in pursue of a light solution mindful of resource preservation in sensor nodes."}, {"label": 1, "content": "Desirable properties of extensions to Non-negative Matrix Factorization (NMF) include robustness in the presence of noise and outliers, ease of implementation, guaranteed convergence, automatic balancing of data approximation and model simplicity, and the ability to model sequential structures of time-series signals. Unfortunately, these desirable properties are rarely all present in a single NMF extension. This paper proposes a novel approach that possesses all of these desirable properties by extending the Automatic Relevance Determination (ARD) framework in NMF by Tan and F\u00e9votte. Using an objective function derived from maximum a posteriori estimation of a Bayesian model, the authors develop majorization-minimization algorithms that effectively determine the correct model order, despite the impact of noise and outliers. Additionally, the proposed algorithms are rigorously analyzed for convergence. The authors also incorporate convolutive bases in the model to capture the temporal continuity of data. The efficiency and robustness of this approach are demonstrated through experiments on both synthetic and real-world datasets."}, {"label": 1, "content": "With the continuous development of distributed energy and energy storage systems, the VSC-based DC and AC-DC hybrid distribution networks have witnessed a rapid growth in recent years. DC distribution networks hold an edge over traditional AC networks as they remove the power conversion links in power supply, storage, and end loads. They also reduce the complexity and cost of the access system while improving network efficiency and power quality.\n\nThis paper aims to introduce several VSC-based DC distribution network projects being implemented in China. The paper analyzes key technologies such as AC/DC conversion, control and protection, fault isolation and restore, and DC transformer, along with technical challenges that need to be addressed to ensure successful implementation of these projects. The development trends of several DC distribution network related subjects such as DC voltage level, key equipment development, rapid protection & isolating fault, and regional coordinated control etc. are also explained in the paper.\n\nIn conclusion, the paper provides valuable insights into the development of VSC-based DC distribution networks in China, highlighting their advantages over traditional AC distribution networks. With ongoing innovation in key technologies such as AC/DC conversion and control, these networks hold immense potential to transform the energy landscape in China, making it more efficient and sustainable in the long run."}, {"label": 1, "content": "To address the resource optimization challenges faced by power-protection services in the Energy Internet, this paper proposes a two-layer resource-balanced optimization model for the power backbone communication network planning and design. Firstly, the communication requirements of power-protection services are analyzed, based on the power-protection system description. Secondly, we formulate the proposed resource-balanced model as a mathematic optimization model, based on the Optical Transport Network layered architecture. It comprises an optimal resource-balanced function, and related constraints considering both routing hops and the Quality-of-Service requirements. Finally, a simulation experiment is conducted to analyze the resource-balanced performance of the proposed model, using typical power-protection communication network topology. The simulation results demonstrate the effectiveness of the proposed model for power-protection services."}, {"label": 1, "content": "In this paper, a Software Defined Network was created in Mininet using Python scripts. Additionally, an external interface was added in the form of an OpenDaylight controller to facilitate communication with the network outside of Mininet. This controller was hosted on the Amazon Web Services elastic computing node and used as a control plane device for the switch within Mininet. Thanks to the OpenDaylight controller, flows were created to enable communication between the hosts in Mininet and the webserver in the real-life network. To thoroughly test the network, a real-life network hosted on the Emulated Virtual Environment - Next Generation (EVE-NG) software was connected to Mininet."}, {"label": 1, "content": "With the proliferation of location-based services in vehicle networks, users can access such services by inputting search locations and points of interest. However, users may be concerned about their real locations and other private information being leaked when availing such services. Therefore, it is imperative to apply suitable location privacy protection measures. Traditional location privacy protection methods, such as K-anonymous, are not directly applicable in vehicle networks with high mobility. In order to address this challenge, this article proposes a novel approach that combines cache strategy with K-anonymous to enable users to access services at the lowest possible cost while protecting their location privacy. Specifically, cache strategy pre-deploys a portion of services on roadside units to maximize user satisfaction in service requests. Meanwhile, every user is assigned a K value to meet the requirements of K-anonymous. The balance between these two factors ensures both user services and location privacy. The experiments demonstrate that the strategy proposed in this article is superior to other approaches."}, {"label": 1, "content": "A vehicular ad-hoc network (VANET) has the potential to revolutionize the way we think about transportation. By enabling self-organizing data transmission capabilities between vehicles on the road, VANETs can facilitate intelligent transportation and provide convenient information services. However, issues such as identity validity and message reliability when vehicle nodes share data with other nodes can pose significant security risks. To address these challenges, we propose a data security sharing and storage system based on the consortium blockchain (DSSCB).\n\nThe DSSCB system utilizes a digital signature technique based on the nature of bilinear pairing for elliptic curves to ensure the reliability and integrity of data transmission to a node. The consortium blockchain technology provides a decentralized, secure, and reliable database that is maintained by the entire network node. Smart contracts are used to limit the triggering conditions for preselected nodes when transmitting and storing data, as well as to allocate data coins to vehicles that participate in the contribution of data.\n\nOur security analysis and performance evaluations demonstrate that the DSSCB solution is more secure and reliable with improved data sharing and storage. Compared to traditional blockchain systems, the DSSCB system reduces the time required to confirm the data block by nearly six times and improves the transmission efficiency by 83.33%. With this breakthrough technology, we can ensure the safety and security of vehicles on the road while improving transportation efficiency and providing convenient information services."}, {"label": 1, "content": "In this article, we present an innovative solution to the many IoT data acquisition and storage systems that exist today. Our approach involves the design and development of a prototype electronic circuit extension for Raspberry Pi development board that allows for sensor data collection. Additionally, we introduce a Pi4Java API based Java application that facilitates the collection and storage of the sensor data. To further support the storage of these large volumes of data, we set up an Apache Cassandra database cluster on low-cost servers that provide high availability. Finally, we also include the presentation of our web application that enables various data visualization tasks to be performed on the stored data. With our IoT data acquisition, storage and visualization solution, we present a complete system that is both efficient and cost-effective."}, {"label": 0, "content": "By applying low power wide area network (LPWAN) to communication between a machine and the cloud, it can be anticipated that communication costs and the amount of power consumed by the machine can both be reduced. However, considering the addition of functions to the machine, it is necessary to have a technology able to transfer a large amount of data, which cannot be transferred by LPWAN, to the machine from the cloud at low cost. In this paper, a novel large data download method by Wi-Fi from cloud to machine using the terminals of general users is proposed. In the proposed method, the cloud selects the delivery terminals satisfying the data arrival rate requirement by analyzing the correlation of the movement history between the terminals. It guarantees a data arrival rate with a higher degree of accuracy than the existing DTN and CC-DTN, and at the same time minimizes the number of delivery terminals. In addition, this paper shows an authentication procedure that prevents DoS attacks on LPWAN by a spoofing terminal, which cannot be prevented by existing network authentication technology. Also, we report the effectiveness of the proposed method, which is confirmed by numerical calculation."}, {"label": 1, "content": "Revised:\n\nThis paper proposes an image recognition method for identifying substation switch states using regular computers and industrial personal computers (IPC) that is both efficient and accurate. Traditional deep learning methods require high-performance servers and significant processing power, leading to higher costs and delays. By utilizing target detection based on the HSV color space, this method quickly pre-screens potential identifiers from large image samples before training a classification model using labeled, smaller samples. It then locates and identifies the target identifier, which can be used for recognizing switch states based on their relative positions. Experimental results demonstrate that the proposed method processes image samples efficiently and accurately based on robot vision, providing a lightweight and precise solution for identifying substation switch state."}, {"label": 1, "content": "Current anomaly detection systems (ADSs) utilize statistical and machine learning algorithms to identify previously unknown attacks, however they are susceptible to advanced persistent threat actors. In this study, we propose an adversarial statistical learning mechanism known as the outlier Dirichlet mixture-based ADS (ODM-ADS) for anomalous detection. The ODM-ADS has three novel capabilities. Firstly, it can self-adapt and protect against data poisoning attacks that inject malicious instances during the training phase to disrupt the learning process. Secondly, it establishes a statistical legitimate profile that considers variations from the profile's baseline as anomalies utilizing the proposed outlier function. Lastly, we suggest a framework for deploying the mechanism as Software as a Service in the fog nodes to handle dynamic and large-scale networks such as Internet of Things and cloud and fog computing. The fog allows the proposed mechanism to process streaming data concurrently at the edge of the network. The ODM-ADS mechanism was evaluated utilizing both NSL-KDD and UNSW-NB15 datasets. The findings suggest that ODM-ADS outperforms seven other peer algorithms based on accuracy, detection rates, false positive rates, and computational time."}, {"label": 1, "content": "Accurate load forecasting is crucial for power system operators to ensure economic and reliability benefits. However, cyberattacks can lead to inaccurate load forecasting, which may result in unsuitable operational decisions for electricity delivery. To address this issue, this study proposes a machine learning-based anomaly detection (MLAD) methodology to effectively and accurately detect cyberattacks. \n\nThe MLAD method involves using load forecasts provided by neural networks to reconstruct the benchmark and scaling data, followed by estimating the cyberattack template using naive Bayes classification based on the statistical features and cumulative distribution function of the scaling data. Dynamic programming is employed to calculate both the occurrence and parameters of one cyberattack on load forecasting data. Comparisons with a widely used symbolic aggregation approximation method reveal that the MLAD method is more effective in detecting cyberattacks for load forecasting data with relatively high accuracy. \n\nNumerical simulations conducted on publicly available load data confirm the robustness of the MLAD method, with thousands of attack scenarios based on Monte Carlo simulation showing its ability to withstand cyberattacks. Overall, the MLAD methodology provides an effective and efficient solution for detecting cyberattacks and ensuring the accuracy of load forecasting."}, {"label": 1, "content": "The global positioning system (GPS) is a crucial navigation tool for unmanned surface vehicles (USVs) in marine environments. However, GPS can be unreliable due to natural interference or malicious jamming attacks. In these situations, a marine radar can be used for localization in coastal areas. The radar can extract landmark features of the coastlines for relative navigation. However, radar coastline maps may be unavailable or unreliable in certain areas, such as those with high tide elevations. \n\nTo address this, a new approach to coastal navigation for USVs using simultaneous localization and mapping (SLAM) has been developed. This approach utilizes the B-spline parameterization of coastline features for efficient map management. Field experiments were conducted to test the proposed coastal SLAM algorithm, and the results are presented and discussed in this paper. \n\nOverall, this new approach offers an alternative navigation system for USVs when GPS is restricted or prohibited. By utilizing marine radar and efficient map management techniques, USVs can navigate coastal waters with greater reliability and accuracy."}, {"label": 0, "content": "We propose a method for designing software transactional memory that relies on the use of locking protocols to ensure that transactions will never be forced to retry. We discuss our approaches to implementing this method and tunable parameters that may be able to improve schedulability on an application-specific basis."}, {"label": 1, "content": "Most of the existing graph-based semisupervised hyperspectral image classification techniques use the cluster assumption to create a Laplacian regularizer. However, this approach sometimes fails due to the presence of mixed pixels, which have recorded spectra that are a combination of multiple materials. In this study, we propose a novel semisupervised classifier that employs a geometric low-rank Laplacian regularizer by analyzing both the global spectral geometric structure and the local spatial geometric structure of hyperspectral data. \n\nTo evaluate the spectral-spatial similarity of mixed pixels, we develop a new geometric regularized Laplacian low-rank representation (GLapLRR)-based graph. By exposing the overall low-rank and local spatial structure of images via GLapLRR, the resulting graph exhibits attributes like spatial-spectral geometry description, robustness, and low sparsity, enabling superior classification accuracy of mixed pixels. We experimentally test our method on three actual hyperspectral datasets, and the findings indicate that our technique outperforms its counterparts with only a few labeled instances."}, {"label": 1, "content": "Single image super-resolution (SISR) has made significant strides thanks to the advancement in convolutional neural network (CNN). However, the bulky parameters of the network pose a challenge when applied to real-world problems. Hence, this letter proposes a solution - the lightweight feature fusion network (LFFN), designed to reduce network parameters while maximizing SISR outcomes by exploring multi-scale contextual information. \n\nLFFN comprises spindle blocks and a softmax feature fusion module (SFFM). To elaborate, the spindle block includes a dimension extension unit, a feature exploration unit, and a feature refinement unit. The dimension extension unit expands low dimension to high dimension, which ultimately learns feature maps suitable for the subsequent unit. The feature exploration unit conducts linear and nonlinear explorations on different feature maps, while the feature refinement layer fuses and refines features. The SFFM, with its self-adaptive learning using a softmax function, fuses features from different modules, exploiting hierarchical information and with minimal parameter cost. \n\nQualitative and quantitative experiments conducted indicate that LFFN produces favorable results when compared with state-of-the-art methods having similar parameters on benchmark datasets. Thus, the LFFN aids in reducing the number of parameters while maintaining or even improving SISR results."}, {"label": 1, "content": "Rendering translucent materials can be quite costly in terms of computational resources. This is because light transport algorithms need to simulate a large number of scattering events inside the material before reaching convergence. The cost is particularly high for materials with large albedo or small mean-free-path, where the higher-order scattering effects dominate.\n\nTo address this challenge, we have developed a new method for fast computation of global illumination with participating media. Our approach involves precomputing multiple scattering effects and storing them in two compact tables. These precomputed tables can be easily integrated with any illumination simulation algorithm, such as virtual ray lights (VRL), photon mapping with beams and paths (UPBP), and Metropolis Light Transport with Manifold Exploration (MEMLT).\n\nOur method combines low-order scattering effects, which are handled by the original algorithm, with multiple scattering computed using our tables. As a result, we have been able to achieve significant improvements in convergence speed and memory costs, all while maintaining a high degree of accuracy. \n\nOverall, our new method for fast computation of global illumination with participating media presents a promising solution for rendering translucent materials more efficiently."}, {"label": 1, "content": "In this paper, we introduce LayerOS, a framework designed to enhance the scalability of wearable devices through proactive app scheduling. LayerOS is a strategy-based system that can generate scheduling policies, taking into account user behavior and device capability, and deploy them ahead of time. By doing so, LayerOS can dynamically load apps from the cloud or offload them to the cloud based on the predefined policies. With LayerOS, users can run any cloud-provided app directly without interrupting their normal operations or modifying installed apps. App state migration is used to ensure application view and state consistency across different mobile devices. Experimental results from a prototype system show that LayerOS enables users to run virtually unlimited applications directly, increasing the running fluency by 25.6% from the perspective of FPS, even when having limited space. Moreover, LayerOS can significantly reduce the waiting time when launching an application, with an average waiting time reduction up to 13.0% in the experiments."}, {"label": 0, "content": "With the most HVDC projects in operation and largest renewable energy generation capacity, State Grid has become one of the most complex power grids worldwide, which makes it difficult to simulate, analyze and control. To improve the characteristic cognition level of State Grid, more detailed simulation method such as hybrid simulation of TS and EMT is widely applied in routine operation, and meanwhile more scenarios and contingencies are considered. However, these enormously increase the computing loads, and the conventional simulation tools are too slow to support the actual daily operation. In this paper, a novel digital parallel simulation system of State Grid is proposed, which adopts supercomputing technology to deal with massive simulation cases in parallel, and then provides simulation services to remote users. The system architecture and subsystems are introduced in detail. Firstly, two levels of parallel computing are carried out to speed up the simulation, case parallelism and sub-grid parallelism. Secondly, relying on cloud simulation technology, remote access is realized to the national dispatching center and dozens of regional/provincial dispatching centers. Moreover, several key technologies of the system are explained, including TS-EMT hybrid simulation, multi-layer sub-grid parallel simulation, automatic initialization of EMT simulation, etc., which promote the system performance greatly. Since the system has been applied in State Grid to support routine operation simulation and analysis, several actual applications are introduced at the last, which demonstrates the great enhancement of power grid simulation efficiency."}, {"label": 1, "content": "By utilizing low power wide area network (LPWAN) technology for communication between machines and the cloud, both communication costs and power consumption can be minimized. However, as the functionality of machines evolves, it becomes necessary to be able to transfer large amounts of data to the machine from the cloud at a low cost. In this paper, a unique method for large data downloads by Wi-Fi from the cloud to machines, leveraging the terminals of general users, is proposed. In this method, the cloud utilizes movement history analysis to select delivery terminals that meet data arrival rate requirements, resulting in higher accuracy than existing DTN and CC-DTN methods, while also minimizing the number of delivery terminals required. An authentication procedure designed to prevent DoS attacks by spoofing terminals is also presented in this paper, addressing limitations of existing network authentication technologies. Numerical calculations verify the effectiveness of this proposed method."}, {"label": 0, "content": "As the internet protocol is the de-facto standard for communication on the internet even very constrained devices with low computing power, working memory and battery power capacity should be addressable via an IP-based network infrastructure. To fulfil this necessity communication with wireless devices could be realized by applying 6LoWPAN. Wireless sensor networks are a main topic nowadays, but sometimes a well structured, wire-based infrastructure is more suitable.Based on this assumption we transferred basic 6LoWPAN concepts, ideas, and two implementations from an IEEE 802.15.4-based wireless to a wire-based RS485 network. In this paper, we outline our main objectives and decision points of this transformation process from a technical point of view.We highlight our crucial adoption steps and main changes to bring IPv6 network capability to RS485. Contiki and GNU/Linux implementations for our IPv6 over RS485 approaches were realised to verify correct functionality.With this work we enable tiny devices to be integrated efficiently into IPv6 networks without the necessity of equipping them with Ethernet controller or other expensive hardware components, avoid translation and transformation steps in the gateways, and so speed up development."}, {"label": 1, "content": "Deep learning has been highly successful in learning joint representations between different data modalities. However, most research in this area has neglected cross-modal correlation learning, which is critical when considering temporal structures of different data modalities. This limitation inspired our effort to study cross-modal music video retrieval in terms of emotional similarity.\n\nOur approach employs a novel audio-visual embedding algorithm that uses Supervised Deep Canonical Correlation Analysis (S-DCCA) to project audio and video into a shared space. This process bridges the semantic gap between audio and video and preserves similarities among audio and visual content from different videos with the same class label and temporal structure. The contribution of our approach lies in two key aspects.\n\nFirst, we propose selecting top k audio chunks for analysis using an attention-based Long Short-Term Memory (LSTM) model. This analytical method produces reliable audio summarizations with local properties. Second, we propose an end-to-end deep model for cross-modal audio-visual learning, where S-DCCA is trained to learn the semantic correlation between audio and visual modalities.\n\nTo evaluate our approach, we constructed a 10K music video dataset from the YouTube 8M dataset. Our results show that our proposed model is highly applicable to music video retrieval. Promising results, such as MAP and precision-recall, suggest that our approach can be highly effective in this domain."}, {"label": 1, "content": "Recommendation systems for mobile phones play a crucial role in helping mobile operators achieve their desired profit targets. In a market where clients determine the number of contract users and phones, these systems are particularly important. With the increasing number of mobile cellular telephone contracts available, it has become imperative to develop recommender systems that help users find suitable contracts based on their usage patterns. To achieve this, a hybrid of both collaborative and content-based filtering was used in this study. A prototype of a mobile recommender system was developed and evaluated based on precision and recall. The results showed that the developed recommender system was able to successfully recommend packages to subscribers. This was evidenced by the precision-recall curve, which demonstrated the outstanding performance of the system. The study conclusively showed that a hybrid system was capable of providing effective recommendations to mobile subscribers."}, {"label": 0, "content": "The extension of Transmission network is highly important because of the power transfer capability impingement of transmission lines on power market business but at present moment the new extension of the transmission network is confined because of the substantial contemplation, financial issues and potential health trappings of the electromagnetic field. This consistently growing need for electric power has formed it necessary to employ the accessible transmission network assets. The implementation of flexible AC transmission system (FACTS) might be fruitful choice to enhance power flow capacity (PFC) in existing transmission network. FACTS devices diverse the tracks of exchanged power over transmission lines through changing bus voltage angle, the reactance of transmission lines. In this work, a miniature of Unified Power Flow Controller is modified with Fuzzy Logic (FL) based shunt and serial controllers to increase the power network stability. Fuzzy control is employed in the control scheme of UPFC and Pulse width modulation. A critical case of 3 phase fault occurrence in a power system has been studied. The results are compared with the presence and absence of UPFC. Simulations have been accomplished on MATLAB/Simulink software. To present the effectiveness of purposed controller, the obtained outcomes have been also compared with PID controller. It reveals that the Fuzzy Logic based UPFC has the best performance over PID Controller."}, {"label": 0, "content": "Presently, Biometric features are often used to identify suspects in law enforcement processes. One of these biometric features is Speaker Recognition. Speaker recognition is used to discriminate people by their voice. In this study, the problem that can be solved is how to classify audio sample that exist on the evidence with the voice of the suspect.In this final project is made a application's prototype that can be used to classify and in that case will be done speaker recognition technique (Speaker Recognition) to be able to classify the speaker's voice in the evidence and the voice of the suspect. The stages used to compare the sound is by extracting the sound features using the Mel-frequency Cepstral Coefficients (MFCC) method and using the Learning Vector Quantization Neural Network (JST-LVQ) method as the classification method of the voice extraction result.By using LVQ, the accuracy in recognition the speaker's voice is pretty good. The use of LVQ method produces best accuracy at 73,33% to recognize the speaker that with the same sentence, and 46,67% for different sentence. So the results obtained in accordance with the expected."}, {"label": 0, "content": "In Lebanon, traffic problems are a major concern for the population. The rising number of cars that exceeds the capacity of the roads, the inefficiency of public transportation infrastructures and the non-adaptive traffic light systems are contributors to the traffic crisis. Most roads in Lebanon suffer from traffic jams due to the traditional static green and red times allocations that are inconsiderate to the current state of the traffic. A solution to this problem is a system that adapts to the variations of the traffic dynamically and updates the traffic signal phases accordingly. In this paper, an adaptive traffic light system is implemented using reinforcement learning and tested using real data from Lebanese traffic. For training and testing the system, a software simulation tool is used. This tool can simulate the traffic intersection and allows the neural network to interact with it. Compared with the actual traffic light system, the proposed model displayed a reduction in average queue lengths by 62.82% and in average queuing time by 56.37%."}, {"label": 0, "content": "As a multicarrier modulation system, OFDM/OQAM system is especially sensitive to the system synchronization errors. In this paper, we first introduce the existing data-aided joint carrier frequency offset and time offset estimation methods for OFDM/OQAM systems in time domain, and point out their shortcomings. On this basis, combining the advantages of the existing methods and introducing an iterative link, an improved time-domain data-aided joint carrier frequency offset and time offset estimation method for OFDM/OQAM system is proposed. Simulation results show that this method can effectively overcome the shortcomings of the existing methods and enhance the time-frequency offset estimation performance for OFDM/OQAM systems, which is an effective time-frequency offset estimation method for OFDM/OQAM systems."}, {"label": 1, "content": "The development of city transportation systems enables us to travel more efficiently. However, one of the challenges that come with it is traffic congestion. Congestion can cause environmental pollution and increase the cost of travel. Current research focuses on predicting traffic volume and recommending routes. Nevertheless, selecting a less time-consuming route on a congested road is also important. Skilled taxi drivers are familiar with the traffic conditions and tend to choose the less crowded and peak congestion road segments. Inspired by this concept, we present a hybrid framework that combines urban traffic flow characteristics theory and machine learning techniques. Our approach captures a set of congestion road features such as traffic volume, road speed limit, route distance, traffic light, and weather features from GPS trajectories. We generate the most commonly used top-k candidate alternate routes based on historical data. Then, we feed the feature representations to train the deep learning model and select the best alternative route after the training process. The effectiveness of our methodology is demonstrated through extensive experiments on real datasets derived from realistic car services."}, {"label": 0, "content": "Internet users in Indonesia have increased in recent years. Many product service providers who provide internet access services in accordance with tariff options and their superiority. In this research, sentiment analysis on social media to some service data service operator to see the level of public satisfaction in using data service of telecommunication operator for internet access in Indonesia. In this research is sentiment analysis with several stages, namely the collection of sentiment data using API (Application Programming Interface) which is available on Twitter. The preprocessing stage is then processed to process raw initial data, then perform POS tagging and weighing the word with TF-IDF calculation and perform classification using the Naive Bayes Classifier (NBC) method. This study yields an average value of 94,5% precision rate, 93,3% Recall and 99,09% Accuracy."}, {"label": 0, "content": "Agriculture has been identified as one of the pathways to achieve the Zero hunger goal of the United Nation's Sustainable Development Goals. [1]. Pests are one of the biggest factors affecting agricultural yield. Besides causing loss of yield, using the wrong pesticide could lead to a loss in investment for farmers. Hence accurately identifying pests and treating them correctly will increase yield and reduce wastage. Deep learning is a subset of Artificial Intelligence, which has become popular in recent years to perform tasks like image classification, speech recognition, etc. Of all the deep learning architectures, Convolutional Neural Networks (CNNs) are the most widely used architectures to classify images. Accurate classification of images can have wide applications, for example identifying agricultural pests. In this paper, we analyze the effects of dataset size on the accuracy of a Convolutional Neural Network when used for agriculture. We trained VGG16, ResNet and Inception CNNs with a small custom image dataset and CIFAR10. Our results show that larger training data leads to higher classification accuracy. We plan to test using a larger agricultural data set and deploy for farmers in rural communities in India."}, {"label": 0, "content": "For linear systems in the observer canonical form, we introduce a state observer with time-varying gains that tend to infinity as time approaches a prescribed convergence time. The observer is shown to exhibit fixed-time stability with an arbitrary convergence time, which is prescribed by the user irrespective of initial conditions. The output estimation error injection terms are also shown to remain uniformly bounded and converge to zero at the prescribed time."}, {"label": 1, "content": "The most efficient solvers utilize composite procedures that adaptively rearrange computation algorithms to maximize simulation performance. Similarly, this concept can be incorporated into electronic circuit analysis to combine various algorithms, allowing for scalability of simulation performance. In this study, we propose a novel adaptive internal solver based on the Biconjugate Gradient Stabilized method for iteratively solving nonsymmetric linear systems supplemented with incomplete LU factorization. This approach serves as an efficient replacement for the direct solver implemented in program Spice, particularly when solving large-scale circuits. We outline the basic concepts of the electronic circuit simulation with nonlinear time-dependent devices and provide implementation examples of this approach. We also demonstrate the optimal settings of the method, its application in program Spice, and compare its performance to other modern iterative solvers for nonsymmetric linear systems."}, {"label": 1, "content": "The issue of HF radio communication has long been a challenge when it comes to selecting channels due to the ever-changing spectrum environment. In order to determine the possibility of identifying idle channels through spectrum prediction, real-world measurements were collected by USRP during different time periods. To reflect spectrum availability, the received signal power was converted into continuous sequences using a new channel state model. A prediction algorithm was then developed using simplified frequent pattern mining, which accurately predicts channel availability by analyzing past channel states. Although the experimental results showed that the data collected in the afternoon were more fluctuant, making the prediction process difficult, the proposed algorithm outperformed both neural network and Markov model algorithms in this situation. Moreover, the prediction performance improves as the number of collected samples increases."}, {"label": 0, "content": "Aimed at the road service level prediction problem for real-time traffic flow, a real-time evaluation method for road service level based on traffic model driven is proposed. Firstly analyze the basic feature model of traffic flow, using flow-time occupancy model as a reference model for congestion assessment, and based on K-means clustering algorithm to complete traffic-based congestion definition. Then use the BP neural network algorithm to build congestion assessment model, finally establish a real-time stream processing framework based on Spark Streaming to realize real-time evaluation for road service level. The experiment results show that the method could effectively describe the state of congestion, and be able to evaluate road service in real time based on traffic flow data, with decision support for intelligent traffic control system to improve the service level."}, {"label": 1, "content": "The complexity of modeling deformable materials and infinite degrees of freedom has led to a lack of transfer of rigid robot control techniques to soft robots. Most model-based control methods for soft robots and soft haptic interfaces are device-specific. In this letter, we propose a general approach for stiffness control of soft robots that is applicable to any robot geometry and various types of actuation. Building on previous work that uses finite element modeling for position control, we establish the relationship between end-effector and actuator compliance, including inherent device compliance, to determine the appropriate controlled actuator stiffness for a desired stiffness of the end-effector. This stiffness control, which is the first component of impedance control, enables compensation for the natural stiffness of the deformable device and facilitates control of the robot's interaction with the environment or a user. We verify this stiffness projection on a deformable robot and integrate it into a haptic control loop to create a virtual fixture."}, {"label": 1, "content": "This paper presents a fast depth selection algorithm for frame coding units (CTUs) using machine learning. The algorithm addresses the lack of depth discrimination in initial division of coding units and the inefficiencies of the classifier's input feature selection. Firstly, the paper designs an initial division depth prediction strategy based on texture complexity and quantization parameters to skip nonessential sizes of coding units. Secondly, the input characteristics of the classifier are determined based on the relationship between bit-rate and distortion, and a selection strategy for the termination depth of coding units is designed. Finally, the partition problem is modeled as a two-element classification problem, and the nearest neighbor classifier is used. The proposed algorithm decreases frame encoding time by 34.56%, while maintaining accuracy compared with HM-15.0."}, {"label": 1, "content": "Methods for estimating probability density distributions are crucial in solving filtering issues in telecommunications systems that involve external acoustic noise. In this article, we discuss parametric and non-parametric methods of estimating probability densities, as well as how to determine an empirical distribution function for a limited sample volume. One method for approximating the probabilities of empirical data is through the use of nuclear evaluations, where the estimate is given by the convolution of the core and the empirical density.\n\nThe reconstruction of the distribution function can be achieved through the use of a polynomial system of functions. Finding the coefficients for this polynomial requires solving a linear regression task that minimizes the quadratic function of the loss, which represents the discrepancy between the empirical data and the estimates obtained from them. Experimental results demonstrate the error of reconstructing a one-dimensional function of probability density for different kinds and orders of polynomial approximation when considering audio signals and acoustic interferences."}, {"label": 1, "content": "Most DACs exhibit a frequency response that rolls off in accordance to the sin(x)/x frequency-response envelope [1]. This paper outlines a method for deriving a FIR filter that is designed, using the minimax sense, to compensate for the frequency response of DACs by incorporating ripple constraints into its design criteria. A function for estimating the filter order is also presented in detail in this paper, which can effectively reduce design time and provide a precise reference for configuring resources in the top-level system design [2]. The simulation example clearly demonstrates the advantages of using a compensation filter designed in the minimax sense, and proves the accuracy of the filter order estimation function."}, {"label": 0, "content": "We focused on a problem where balanced use of sensor nodes' battery power is considered to maximize the overall lifetime of ad-hoc Wireless Sensor Networks. A process in which utilizing less attended sensor nodes compare to sensor nodes which are used more frequently enhances the overall network lifetime. To perform this process, we propose a joint optimization problem to select a subset of active sensor nodes and a multi-hop routing structure interconnecting all selected sensor nodes, which helps to route the aggregated information to a querying node. Our optimization problem becomes non-convex over the subset selection and the multi-hop routing paths selection, thus belonging to the class of NP-hard problems. We solve our problem by relaxing one of the variable so that optimization problem becomes convex over this variable, which can be solved efficiently. We also propose an iterative algorithm to solve this problem distributively. We demonstrate by extensive simulation that the above mentioned both the approaches increase the overall network lifetime for a given power budget. One another important result is that the distributed approach provides an optimal routing structure considering over the well known shortest path tree based routing structure."}, {"label": 1, "content": "Anonymous Voice over IP (VoIP) communication is a valuable tool that enables freedom of speech. To ensure both quality of service (QoS) and anonymity, connecting mixes by padded links is a promising approach. However, building suitable overlay topologies for establishing padded links is challenging. The existing approaches are either not scalable due to using full meshes or impractical because of the computational complexity of the algorithms used to create optimal reduced topologies. This limits their deployment as volunteer-based networks like Tor. \n\nIn this paper, we propose and investigate various heuristic strategies for reduced topology construction and path selection with low computational complexity. We assess the effectiveness of our approach using latency and bandwidth estimates from the Tor network. Our results show that we can achieve appropriate performance and anonymity metrics for VoIP. Moreover, the achieved features are similar to those of a recently proposed approach that computes optimal topologies for small networks."}, {"label": 0, "content": "Realistic knowledge of transport properties of the environment is necessary while modeling gas-dynamic processes that occur in the units of energy power systems based on the macro levels of heat and mass transfer. In this article we offer an approach based on the kinetic molecular theory of gases to determine a transport coefficient in the multicomponent gaseous environments. The authors offer a mathematical model of description of molecular interaction in the K-component gaseous environments in which their microstructure, in particular, elastic and geometric characteristics of interacting molecules are taken into consideration. This approach allows to determine transport properties of a multicomponent environment in the macrosystems at a qualitatively new level taking into account its microscopic properties."}, {"label": 0, "content": "Emergency evacuation simulation is an important measure to effectively avoid personnel deaths and injuries. In view of the safe evacuation in underground tunnels, an emergency evacuation simulation system for personnel in three-dimensional tunnel is put forward. According to the different conditions encountered by the Agent in the tunnel, the behavior of the Agent is set and visualized. Through the design of three-dimensional model, evacuation path generation, visualization simulation, evaluation and analysis, the experiment of the underground tunnel evacuation is carried out. The results show that the system can effectively simulate the evacuation of people in the tunnel, which can be used for emergency evacuation guidance during disasters and evacuation simulation drills before disaster."}, {"label": 0, "content": "A considerable disadvantage that comes with the downscaling of the CMOS technology is the ever-increasing susceptibility of Integrated Circuits (ICs) to soft errors. Therefore, the study of the radiation-induced transient faults in combinational logic has become one of the most challenging issues as the absence of appropriate error-protection mechanisms may lead to system malfunctions. This paper presents an efficient and accurate layout-based Soft Error Rate (SER) estimation analysis for ICs in the presence of both single and multiple transient faults, since the latter are more prevalent as technology downscales. The proposed tool, i.e. SER estimator, is based on Monte-Carlo simulations taking into account a detailed grid analysis of the circuit layout for the identification of the vulnerable areas of a circuit and, in addition, temperature as one of the factors that affect the generated pulse width. The widening of the fault pulses due to elevated temperature is reflected in increased SER according to our results. Finally, the comparison between the simulation results for some of the ISCAS'89 benchmark circuits obtained from the proposed framework and the respective ones obtained from SPICE indicates a fairly good correlation."}, {"label": 0, "content": "Although face recognition systems have achieved impressive performance in recent years, the low-resolution face recognition task remains challenging, especially when the low-resolution faces are captured under non-ideal conditions, which is widely prevalent in surveillance-based applications. Faces captured in such conditions are often contaminated by blur, non-uniform lighting, and non-frontal face pose. In this paper, we analyze the face recognition techniques using data captured under low-quality conditions in the wild. We provide a comprehensive analysis of the experimental results for two of the most important applications in real surveillance applications, and demonstrate practical approaches to handle both cases that show promising performance. The following three contributions are made: (i) we conduct experiments to evaluate super-resolution methods for low-resolution face recognition; (ii) we study face re-identification on various public face datasets, including real surveillance and low-resolution subsets of large-scale datasets, presenting a baseline result for several deep learning-based approaches, and improve them by introducing a generative adversarial network pre-training approach and fully convolutional architecture; and (iii) we explore the low-resolution face identification by employing a state-of-the-art supervised discriminative learning approach. The evaluations are conducted on challenging portions of the SCface and UCCSface datasets."}, {"label": 1, "content": "Energy costs and environmental sustainability pose a significant challenge to cloud computing practitioners and the development of next-generation data centers. Data centers consume a significant amount of energy due to inefficient resource management, which results in high energy consumption. However, VM placement is a promising technique that can help save energy and improve resource management.\n\nOne of the significant challenges facing VM placement algorithms is the ability to accurately forecast future resource demands. Cloud applications are dynamic, and predicting future demands is challenging. Additionally, there is a lack of literature on placement strategies based on co-located resource consumption, which has the potential to improve allocation decisions.\n\nTo address these challenges, we conducted a comparative study of the most widely used prediction models and introduced a novel predictive anti-correlated VM placement approach. Using real workload traces, we demonstrated that our approach reduces energy consumption by 18% while also reducing service violations by over 47% compared to some of the most commonly used placement policies.\n\nIn conclusion, our study highlights the importance of efficient resource management in reducing energy consumption and improving environmental sustainability. Furthermore, our proposed approach showcases the potential of predicting anti-correlated VM placement in improving allocation decisions and reducing energy consumption."}, {"label": 1, "content": "In-memory computing using nanoscale memristive devices like phase-change memory (PCM) is increasingly being considered as a viable solution for training deep neural networks (DNN). Such systems are based on using device conductance to represent synaptic weights. However, PCM devices tend to exhibit temporal evolution of conductance values, which is known as the conductance drift. This can pose a significant challenge in maintaining reliable synaptic weights.\n\nA study involving 10,000 GST-based PCM devices observed a dependency of drift coefficient on conductance value. Additionally, it was found that even partial SET pulses can re-initialize PCM drift and erase drift history, regardless of the extent of drift. With models that account for these observations, it was demonstrated that drift can have a negative impact on DNN training, although multi-PCM synaptic architecture can help improve drift resilience.\n\nTherefore, while in-memory computing with nanoscale memristive devices like PCM may provide an alternative to conventional von Neumann systems, the issue of conductance drift needs to be addressed in order to make these systems more robust and reliable for deep neural network training."}, {"label": 1, "content": "This paper proposes a method for channel estimation in Multi-input Multi-output/Orthogonal Frequency Division Multiplexing (MIMO/OFDM) systems operating in fast linear-time-varying (LTV) multi-path channels using a special frequency-division (FD) pilot. Unlike linear-time-invariant (LTI) channels, MIMO/OFDM systems operating in LTV channels may suffer from significant inter-carrier interference (ICI) caused by Doppler frequency shift resulting from the relative movement of transmitters and receivers. To address this problem, this paper presents a redesigned frequency-division pilot for LTV channel, which enables effective estimation of the channel tap of an intermediate instant of each OFDM symbol with relatively low ICI. Finally, a well-known basis expansion model (BEM) is used to fit the entire channel. Numerical results indicate that the proposed method can achieve high-precision channel estimation for fast LTV multi-path channels."}, {"label": 0, "content": "This paper investigate the accuracies of short-term forecast for MISO\u2019s locational marginal pricing (LMP) data sets. A collection of methods such as rolling average, autoregressive integrated moving average (ARIMA), and long short-term memory (LSTM) were applied to a three-year data sets and compared against MISO\u2019s forecasting approach. Our preliminary findings indicate that the use of recurrent neural networks (RNN) gains an additional 25% increase in forecasting accuracy compared to MISO\u2019s forecasting approach."}, {"label": 0, "content": "A concept of a hybrid expert system for robot milling conditions control presented. Based on KUKA KR300 operating data. Milling conditions, measurements data, negative factors involved are considered. Expert estimations of actual milling episodes are used as criterion of successful milling. All measurable milling quality parameters are examined as possible feedback for milling conditions control. The choice is made in favor of using only real-time parameters available. Development of hybrid expert system includes method for expert estimations formalizing, system structure design, signal processing. An example of data processing and distribution within expert system given to further application in robotic milling control. The introduction of a hybrid expert control system into the robotic milling make it possible to implement two important functions for the manufacturing system. First, accumulate knowledge about the manufacturing process in form of measured data, simple integral and qualitative expert estimations - all in a solid and compact data format. Secondly, generate control signals for selection and maintenance of milling conditions using a rule's logic output system."}, {"label": 1, "content": "The packet classification problem involves determining the behavior of incoming packets at network devices. A linear search classification algorithm assigns each packet based on its prior actions by comparing its header with classification rules until a match is found. However, processing latency can increase with the number of rules, resulting in communication delays. To address this issue, the Optimal Rule Ordering (ORO) problem aims to identify the rule ordering that minimizes delay caused by packet classification.\n\nConventional ORO prohibits the posterior rule from being placed higher than the prior rule when two different rules match a single packet. However, if the actions of those rules are the same, interchanging them does not violate policy. In this paper, we present the Relaxed Optimal Rule Ordering (RORO) problem, whereby rules can be interchanged if their actions are the same. Rule weights in RORO may also vary as they are interchanged, which we calculate using a zero-suppressed binary decision diagram.\n\nWe acknowledge the difficulty in estimating weights and propose an algorithm for RORO that generates a rule list with lower latency than conventional algorithms and accurately calculates latency. Our method's effectiveness is demonstrated by comparing it with previous models and reordering techniques."}, {"label": 0, "content": "With the increase of cyber-attacks, the risks of information leakage, falsification and forgery, and bypass control are intensified, and attackers can attack the primary station circuitously through the terminal, resulting in a wider range of security threats. Quantum cryptography has a higher level of security than traditional cryptography. In this paper, a quantum cryptosystem suitable for power communication access network is proposed, and various practical quantum communication devices are designed, and proposes a key reading mode between the service terminal and the quantum key mobile storage device, an interface protocol between the service terminal device and the quantum key mobile storage device, and a key management method applicable to the terminal for the service of the electric service. The application of quantum communication technology to power systems plays a vital role in ensuring the safe, stable and efficient operation of the power grid."}, {"label": 0, "content": "In Infrastructure-as-a-Service (IaaS) clouds, remote users access provided virtual machines (VMs) via the management server. The management server is managed by cloud operators, but not all the cloud operators are trusted in semi-trusted clouds. They can execute arbitrary management commands to users' VMs and redirect users' commands to malicious VMs, which is called the VM redirection attack. The root cause is that the binding of users to VMs is weak. In other words, it is difficult to enforce the execution of only users' management commands to their VMs. In this paper, we propose UVBond for strongly binding users to their VMs to solve this problem. UVBond boots user's VM by decrypting its encrypted disk inside the trusted hypervisor. Then it issues a VM descriptor to securely identify that VM. To bridge the semantic gap between high-level management commands and low-level hypercalls, UVBond uses hypercall automata, which accept the sequences of hypercalls issued by commands. We have implemented UVBond in Xen and confirmed that a VM descriptor and hypercall automata prevented attacks and that the overhead was not large."}, {"label": 0, "content": "Data collection and analysis of product are causing more and more attention with the rapid development of intelligent manufacturing. Faced with the problem that data acquisition system cannot be used universally, researchers have a long way to go. This paper designed a configurable data acquisition system for automatic filling lines, and the system can be used in other working lines without changing the hardware. The system is based on ARM and consists of three parts: data acquisition, server and cloud platform. Data acquisition part is responsible for acquiring data and sending data to host computer. Server part is used for receiving data from host computer and saving it in the database. Cloud platform serves for users and provides data analysis. The hardware design and software design of the acquisition board are also analyzed, meanwhile the communication protocol is made to ensure the data transmission. The data acquisition has been realized in this system and the whole process is running smoothly without problem."}, {"label": 0, "content": "As the density of sensing/computation/actuation nodes is increasing, it becomes more and more feasible and useful to think at an entire network of physical devices as a single, continuous space-time computing machine. The emergent behaviour of the whole software system is then induced by local computations deployed within each node and by the dynamics of the information diffusion. A relevant example of this distribution model is given by aggregate computing and its companion language field calculus, a minimal set of purely functional constructs used to manipulate distributed data structures evolving over space and time, and resulting in robustness to changes. In this paper, we study the convergence time of an archetypal and widely used component of distributed computations expressed in field calculus, called gradient: a fully-distributed estimation of distances over a metric space by a spanning tree. We provide an analytic result linking the quality of the output of a gradient to the amount of computing resources dedicated. The resulting error bounds are then exploited for network design, suggesting an optimal density value taking broadcast interferences into account. Finally, an empirical evaluation is performed validating the theoretical results."}, {"label": 0, "content": "The article is devoted to the discussion of the problems of development of predictive neural network models based on the residual number system and the use of modular arithmetic to improve the quality of automatic control systems by adding to the control algorithms a prognostic component, which is especially important for astatic control objects. The possibility of implementing neural network training algorithms in the residual number system is shown, which allows to significantly accelerate the work of these algorithms, which is especially important when adding new functionality to automatic control systems in the form of prognostic neural network models."}, {"label": 1, "content": "Unit commitment (UC) problems are critical for system operators to ensure secure and cost-effective operation in both day-ahead and real-time markets. Typically, UC problems are formulated using mixed-integer linear programming (MILP) or mixed-integer quadratic programming (MIQP) models with binary variables representing generators' ON/OFF statuses. These models are solved with Branch and bound (B&B) algorithms. \n\nIn recent years, researchers have explored using semidefinite programming (SDP) in power system applications, including UC. SDP relaxes integrality requirements and can provide better solutions than the traditional LP relaxation. This paper proposes a 2-order moment relaxation technique to reformulate UC problems as an SDP model, solved using an interior point method. \n\nTo reduce computational burden, the paper applies a variable reduction strategy and two refined moment relaxation-based UC models. The paper also studies the models' tightness and presents a sufficient condition for an exact solution. \n\nNumerical studies demonstrate the effectiveness of the proposed models and discuss their potential applicability."}, {"label": 0, "content": "A mixed integer linear programming model, which can be used in microgrid (MG) intra-day scheduling, is put forward in order to conciliate further the desired attributes of accuracy and computational performance of solving existing model. The proposed model is the minimization of MG economic operation cost considering reactive power capability of distributed generation (DG). Compared with the literature of microgrid economical dispatching, the linearized constraint model, such as power flow with voltage amplitude limitation, DG reactive power characteristic and branch capability based on analytic geometry, exchange power and power factor of point of common couple, and et al, is included in the paper. The performance of the proposed model and accuracy of the results are verified by using an experimental microgrid for engineering practical purposes. Finally, the effects of different segments on the feasible region coverage of DG operating point are discussed."}, {"label": 1, "content": "Fog computing extends Cloud Computing to the edge of the network, enabling computation, storage, applications, and network services between the Internet of Things and cloud servers. However, security must be a priority in Fog computing because of the huge number of nodes involved. Therefore, we propose a new cryptographic primitive called CCA2 secure publicly-verifiable revocable large-universe multi-authority attribute-based encryption (CCA2-PV-R-LU-MA-ABE) for fine-grained access control in Fog computing. \n\nIn this primitive, end nodes generate private keys from multiple authorities differentiated by geographical locations or functions. These nodes' attributes can be denoted by any strings in the large universe, fulfilling diverse needs in practical Fog applications. Also, accessibility to nodes can be efficiently revoked even by resource-limited devices. The primitive's validity is ensured by supporting public verification, meaning that only valid ciphertext can be stored or transmitted. \n\nWe construct a concrete CCA2-PV-R-LU-MA-ABE scheme based on the primitive and the feature of Fog computing. We also define the security model of this primitive, which is much more secure than the CPA-secure scheme. Finally, we compare the efficiency of the proposed concrete scheme with that of the existing CPA-secure scheme through theoretical and experimental analysis, and the results show that the extra efficiency consumption in improving CPA to CCA2 is low. The proposed scheme is secure, flexible, and efficient enough to be practical in Fog computing."}, {"label": 0, "content": "Evolutionary circuit design, is a time-intensive process, especially for large scale circuits. In this paper, we design a distributed computation framework for evolutionary circuit design via parallel genetic algorithm, which actualizes the tasks-distributing and tasks-collecting with the employment of communication network structure using client-sever mode. A series of experiments are conducted and the results show that the framework presented performs better on seeking for the circuits that satisfy designer specified performance goals with high efficie-ncy, great flexibility, strong fault tolerance."}, {"label": 1, "content": "The Internet of Things (IoT) has gained widespread attention for its ability to monitor and control the environment. IoT enables data-driven decision-making through the use of everyday devices that have been equipped with sensing, processing, and communication capabilities. A key aspect of any IoT device is its communication capability, which enables it to transfer and share data with other devices. Wireless communication technologies are commonly used in IoT systems. \n\nThe industry and research community have proposed numerous communication technologies for IoT systems. In this study, the authors present the findings of an in-depth analysis of the benefits and limitations of these technologies."}, {"label": 0, "content": "The proposed framework employs discriminative analysis for gaze estimation using kernel discriminative multiple canonical correlation analysis (K-DMCCA), which represents different feature vectors that account for variations of head pose, illumination and occlusion. The feature extraction component of the framework includes spatial indexing, statistical and geometrical elements. Gaze estimation is constructed by feature aggregation and transforming features into a higher dimensional space using the RBF kernel \u03b3 and spread factor. The output of fused features through K-DMCCA is robust to illumination, occlusion and is calibration free. Our algorithm is validated on MPII, CAVE, ACS and EYEDIAP datasets. The two main contributions of the framework are the following: Enhancing the performance of DMCCA with the kernel and introducing quadtree as an iris region descriptor. Spatial indexing using quadtree is a robust method for detecting which quadrant the iris is situated, detecting the iris boundary and it is inclusive of statistical and geometrical indexing that are calibration free. Our method achieved an accurate gaze estimation of 4.8\u00b0 using Cave, 4.6\u00b0 using MPII, 5.1\u00b0 using ACS and 5.9\u00b0 using EYEDIAP datasets respectively. The proposed framework provides insight into the methodology of multi-feature fusion for gaze estimation."}, {"label": 1, "content": "Worst-case timing analysis plays a critical role in designing safe real-time systems based on manycore architectures, specifically Networks-on-Chip (NoCs). To address bursty traffic, such as real-time audio and video streams, this paper proposes potential extensions to a previously-published buffer-aware worst-case timing analysis approach. One promising lead involves improving the algorithm analyzing backpressure patterns, capturing the consecutive-packet queueing effect while still considering flow dependencies. Additionally, the improved algorithm can decrease the inherent complexity of computing the indirect blocking latency due to backpressure. These enhancements can help to ensure real-time systems are safe and meet performance requirements."}, {"label": 0, "content": "Consensus is fundamental for distributed systems since it underpins key functionalities of such systems ranging from distributed information fusion, decision making, to decentralized control. In order to reach an agreement, existing consensus algorithms require each agent to exchange explicit state information with its neighbors. This leads to the disclosure of private state information, which is undesirable in cases where privacy is of concern. In this paper, we propose a novel approach for undirected networks, which can enable secure and privacy-preserving average consensus in a decentralized architecture in the absence of an aggregator or third party. By leveraging partial homomorphic cryptography to embed secrecy in pairwise interaction dynamics, our approach can guarantee convergence to the consensus value (subject to a quantization error) in a deterministic manner without disclosing a node's state to its neighbors. We provide a new privacy definition for dynamical systems, and give a new framework to rigorously prove that a node's privacy can be protected as long as it has at least one legitimate neighbor, which follows the consensus protocol faithfully without attempts to infer other nodes' states. In addition to enabling resilience to passive attackers aiming to steal state information, the approach also allows easy incorporation of defending mechanisms against active attackers who try to alter the content of exchanged messages. Furthermore, in contrast to existing noise-injection-based privacy-preserving mechanisms that have to reconfigure the entire network when the topology or number of nodes varies, our approach is applicable to dynamic environments with time-varying coupling topologies. This secure and privacy-preserving approach is also applicable to weighted average consensus as well as maximum/minimum consensus under a new update rule. Numerical simulations and comparison with existing approaches confirm the theoretical results. Experimental results on a Raspberry-Pi board based microcontroller network are also presented to verify the effectiveness and efficiency of the approach."}, {"label": 0, "content": "Mechanical equipment fault signals are mostly nonlinear and non-stationary signals. Processing these signals by Fourier transform and wavelet transform usually cannot obtain desired fault diagnosis results. In this paper, a machine fault diagnosis method based on Industrial Internet of Things (IIoT), industrial wireless sensor networks (IWSNs), Hilbert-Huang transform (HHT), and support vector machine (SVM) is proposed, in which HHT and SVM are used for fault feature extraction and fault diagnosis respectively. The proposed fault diagnosis approach by SVM is implemented and tested on the IWSN sensor node, while the fault feature extraction method using HHT is verified by MATLAB simulation. The effectiveness of the presented approach is evaluated by a set of experiments using bearing vibration data. The result indicates that the fault diagnosis accuracy of the presented method reaches 100% for four machine working conditions and 92% for five working conditions."}, {"label": 0, "content": "Non-rigid registration is a crucial step for many applications such as motion tracking, model retrieval, and object recognition. The accuracy of these applications is highly dependent on the initial position used in registration step. In this paper we propose a novel Convex Hull Aided Coarse Registration refined by two algorithms applied on projected points. Firstly, the proposed approach uses a statistical method to find the best plane that represents each point cloud. Secondly, all the points of each cloud are projected onto the corresponding planes. Then, two convex hulls are extracted from the two projected point sets and then matched optimally. Next, the non-rigid transformation from the reference to the model is robustly estimated through minimizing the distance between the matched point's pairs of the two convex hulls. Finally, this transformation estimation is refined by two methods. The first one is the refinement of coarse registration by Iterative Closest Point (ICP). The second one consists of the refinement of coarse registration by the Normal Distribution Transform (NDT). An experimental study, carried out on several clouds, shows that the refinement of coarse registration with ICP gives, in the most cases, a better result than refinement with NDT."}, {"label": 0, "content": "Multiple Signal Classification (MUSIC), Steered Response Power-PHAse Transform (SRP-PHAT) and Generalized Cross Correlation (GCC) are the well known techniques for Direction of Arrival (DoA) estimation, using microphone array. However, in real time scenarios, these techniques encounter limitations such as computational complexity and thresholding difficulties. In this paper, a novel and robust method is introduced in which DoA is estimated using the concept of subarray decomposition to provide better performance with effective thresholding and minimal computational complexity."}, {"label": 0, "content": "In the field of Cyber Security there has been a transition from the stage of Cyber Criminality to the stage of Cyber War over the last few years. According to the new challenges, the expert community has two main approaches: to adopt the philosophy and methods of Military Intelligence, and to use Artificial Intelligence methods for counteraction of Cyber Attacks. This paper describes some of the results obtained at Technical University of Sofia in the implementation of project related to the application of intelligent methods for increasing the security in computer networks. The analysis of the feasibility of various Artificial Intelligence methods has shown that a method that is equally effective for all stages of the Cyber Intelligence cannot be identified. While for Tactical Cyber Threats Intelligence has been selected and experimented a Multi-Agent System, the Recurrent Neural Networks are offered for the needs of Operational Cyber Threats Intelligence."}, {"label": 1, "content": "The holomorphically embedding method is a class of nonlinear equation solvers that are known to be highly effective in solving power-flow problems. This method is guaranteed to find an operable solution to the power-flow problem, as long as certain conditions are met. However, all of the published approaches to this method have used a Gauss-Seidel-based fixed-point form as the starting point for the embedding.\n\nIn this paper, we present an alternative fixed-point form that is based on a Newton-Raphson scheme. This new approach has several advantages, particularly when attempting to find the saddle-node bifurcation point (SNBP). By using a Newton-Raphson scheme, we can exploit the faster convergence of this method, which allows us to find the SNBP more efficiently and with greater accuracy.\n\nWe have demonstrated the effectiveness of this new approach through simulations on various power systems. Our results indicate that the Newton-Raphson-based fixed-point form is indeed a promising alternative to the Gauss-Seidel-based approach for the holomorphically embedding method. Overall, we believe that this new approach can help to further improve the performance and accuracy of this important class of nonlinear equation solvers."}, {"label": 1, "content": "The relevance of this research lies in the need to enhance electromechanical systems utilizing linear electromagnetic engines and accumulator food. These systems are crucial for many applications, including striking operations, construction technologies, and technical research. One of the main challenges in this field is to effectively utilize the power stock of accumulators, thereby increasing the operation period of the system without requiring recharging of the source.\n\nThe key objective of this study is to assess the feasibility and potential of a system that can provide automatic correction of the electric power consumed from accumulators when the properties of loading change. Our research shows that systems equipped with sensors of limit provisions of an anchor and operated on the coordinate of a moving anchor are preferable to other technical embodiments. Furthermore, we propose an advanced controlling system that uses a programmable logic controller to provide automatic correction of the consumed energy from accumulators, without complicating the device of feedback.\n\nWith our findings, we hope to contribute to the development of electromechanical systems with enhanced efficiency and longer operation periods, benefiting a wide range of industries and applications."}, {"label": 0, "content": "The introduction of distributed generators (DG) and other emerging technologies such as electric vehicle charging (EV) to distribution networks have influenced the philosophy of operating the distribution networks. With the presence of these technologies, the nature of the distribution networks are changing from being passive networks to active networks and in order to accommodate these changes, the way of controlling and operating distribution systems must be reconfigured and distribution system state estimation based real-time model is needed for a secure control and protection in distribution systems. The objective of this paper is to present a comparison between the branch-current based distribution system state estimation in polar and rectangular coordinates. Moreover, the inclusion of the synchronized measurements, obtained from Micro-PMU is discussed. The methods are conducted on the IEEE-13 bus distribution test feeder and results are discussed."}, {"label": 1, "content": "Text analytics has become increasingly popular in various fields due to its ability to uncover valuable insights from unstructured data. However, when it comes to power dispatching, dispatchers often struggle to remember and comprehend the vast amount of unstructured data contained within manuals. To overcome these challenges, this paper proposes the use of text analytics, specifically, natural language processing technology. This includes key techniques such as data structure transformation, word segmentation tools for Chinese, and Word2Vec calculation, which can greatly assist dispatchers in handling the dispatching manual."}, {"label": 0, "content": "Forecasting of consumer electricity usages plays an important role to make total smart grid system more reliable. As the activities of individual residential consumers has many uncertain variables, it is hard to accurately forecast the residential load levels. For planning of the electrical resources and to balance demand and supply, accurate forecasting tasks are critical. This paper presents Deep Neural Network (DNN) based short term load forecasting for Residential consumers. In this work, we compare the Mean Absolute Percentage Error (MAPE) value for residential electricity dataset using different types recurrent neural network (RNN). Our preliminary results indicate that Long short-term memory (LSTM) based RNN performed better compared with simple RNN and gated recurrent unit (GRU) RNN for a single user with 1-minute resolution based on one year of historical data sets."}, {"label": 0, "content": "Various security threats have prompted covert timing channels to become an important alternative for the transmission of confidential information in an untrusted Internet of Things (IoT). This article aims to demonstrate the susceptibility of IoT to covert timing channels over mobile networks. It presents the system model of a covert timing channel for IoT and then analyzes whether the traditional covert timing channels based on inter-packet delays apply to IoT over 4G/5G networks. Given that there are so many covert timing channels proposed for computer networks, we investigate different kinds of construction approaches of covert timing channels to illustrate the feasibility of building covert timing channels for IoT, including packet-reordering-based, rateswitching- based, packet-loss-based, retransmission- based, and scheduling-based covert timing channels. Furthermore, this article also discusses several detection methods of revealing and preventing covert timing channels for IoT."}, {"label": 0, "content": "IoT (Internet of Things) is the most important technical applications of engineering advancement in the world today, through Industrial Revolution - Industry 4.0. Since people uses phone for more than just daily communication devices, but as wireless smart devices in accessing / processing / sending information through fast telecommunication networks with enhancements embedded in it like cameras, GPS and OTT apps. Various development capabilities on other devices that enable a person to do, social media, video conferencing, video streaming, tracking, navigation, drone, remote, forecast, monitoring, payment and all other things that may be computationally proceed by sensor and actuator devices. The development of cutting-edge technology makes this smart capability to be applied to any device, as well as the ability of devices to interact with each other's through internet network. IoT as a vital aspect in Industry 4.0, is broadly embodied in smart city (policy driven), smart industry (business driven), and smart life (experience driven) solution. Utilizing the capabilities of Industrial IoT rightly can lead Nusantara appropriate development as a large archipelago and agrarian area that rich of natural resources. This research investigates the concept and IoT's use cases against variety of socioeconomic and specific geographic challenges, then evaluate based on PESTLE strategic analysis for the external and internal. The result are Development Strategy and Technology for Developing Nations."}, {"label": 1, "content": "As the density of sensing, computation, and actuation nodes increases, it is becoming more feasible and beneficial to view a network of physical devices as a single, continuous space-time computing machine. The overall behavior of the software system is induced by local computations within each node and the dynamics of information diffusion. Aggregate computing and its companion language field calculus provide a relevant example of this distribution model, using functional constructs to manipulate distributed data structures evolving over space and time for robustness.\n\nThis paper studies the convergence time of a widely-used component of distributed computations in field calculus called gradient, which estimates distances over a metric space with a fully-distributed spanning tree. An analytic result linking the quality of the gradient output to the computing resources dedicated is provided, and error bounds are used for network design, indicating an optimal density value that considers broadcast interferences. Finally, empirical evaluation validates the theoretical results."}, {"label": 0, "content": "The minimum mean square error-residual inter-symbol interference cancellation(MMSE-RISIC) equalization algorithm for single carrier frequency domain equalization(SC-FDE) has estimated and removed the residual inter-symbol interference(RISI) of the minimum mean square error(MMSE) equalization, but the noise interference is still present in the decision data, what's more, the estimation deviation of RISI will bring additional disturbances, which reduced the accuracy of equalization. So an improved MMSE-RISIC equalization algorithm is presented and will be extended to the unique word(UW) based system of space-time block coded-single carrier frequency domain equalization(STBC-SC-FDE). This algorithm utilizes the correlation between the estimated noise in UW and the estimated noise in date, the noise in data can be predicted by the estimated noise in UW and will be removed before data decision, the estimation accuracy of RISI will also be improved. Simulation results have confirmed the significant performance gain the improved MMSE-RISIC equalization could achieve compared with the MMSE-RISIC equalization."}, {"label": 1, "content": "Recent research has focused on dispatching methods for DC distribution networks. However, there is currently no evaluating index that takes into account the intense coupling of power and voltage in these networks, making it difficult to judge the effectiveness of these methods. To address this issue, this paper proposes an evaluating index for DC distribution networks with complex converter control modes utilizing Thevenin theorem. As a result, a two-stage dispatching method is presented that features an auto regulation of droop control and an automatic generation control strategy. The former optimizes the droop characteristic, while the latter employs a modified genetic algorithm (GA) to optimize bus power, bus voltage, and network losses. Testing on an employed network demonstrated that the evaluating index provides accurate operating conditions for networks with different converter controls and that the two-stage dispatching method is an effective means of optimizing these networks."}, {"label": 0, "content": "Wind power curve is a key tool to characterize wind power output feature, and is also the basis of wind power planning and operation research. The wind power curve is a high dimension matrix data with local property. So it's a vital task to find an effective method to reduce dimension of the curve. In this paper, the latest techniques of artificial intelligence and deep learning are introduced to probe a new method for reducing the dimension of wind power curve. The convolutional autoencoder of typical deep learning framework is redesigned, and it learns feature representation from massive history data. The experiment result shows that the proposed autoencoder is better fit the wind power curve dimensionality reduction study."}, {"label": 1, "content": "Depression is a leading cause of suicide worldwide, along with other health issues, making it a major concern for public health. In the field of computer vision and signal processing, depression has been tackled in various ways. This paper presents a classification model for detecting depression using local binary pattern (LBP) texture features, which is an image processing approach for pattern recognition on images. The study used video recordings from the SEMAINE database, where the face image is cropped from a video and Uniformed LBP features are extracted from each frame. The classification process involves implementing PCA eigenvalues from the original features to assess the effects. The accuracy of the SVM using RBF kernel classifier was 81% when detecting Depressed to Not Depressed Behavior in a captured motion picture."}, {"label": 1, "content": "Research on data confidentiality, integrity, and availability is gaining momentum in the ICT community. The Internet is intrinsically insecure, making secure communication protocols essential for protecting confidentiality and preventing eavesdropping. However, distributed simulations generally do not consider these issues, as many real-world simulators rely on monolithic, offline approaches that do not require them. With the increasing complexity of systems to be simulated and the rise of distributed and cloud-based simulation, secure simulation architectures are becoming necessary. \n\nThis paper presents a solution to ensure both anonymity and confidentiality in distributed simulations. The solution addresses the issues of privacy and security by introducing techniques to safeguard anonymity and confidentiality. The performance evaluation of an anonymized distributed simulator is used for quantifying the performance penalty for being anonymous. The results of the study conclusively demonstrate that adopting secure simulation architectures is a viable solution for ensuring data confidentiality, integrity, and availability in distributed systems. \n\nIn conclusion, it is essential to consider data confidentiality, integrity, and availability in distributed simulations to ensure secure communication protocols. The solution presented in this paper addresses the challenges associated with privacy and security by introducing techniques to safeguard anonymity and confidentiality. The performance evaluation underscores the feasibility of this approach and highlights the need for secure simulation architectures in increasingly complex distributed systems."}, {"label": 1, "content": "A procedure for designing waveforms to improve the estimation of Doppler frequencies in active remote sensing applications has been presented. The accuracy of frequency estimation has been analyzed using a continuous waveform, and the optimal waveform has been derived. Various waveform designs have been analyzed and it has been demonstrated that a dual-pulse waveform that is close to optimal can offer better estimation accuracy than a single-pulse waveform that has the same signal energy."}, {"label": 0, "content": "The global positioning system (GPS) has become an indispensable navigation sensor for field operations with unmanned surface vehicles (USVs) in marine environments. However, GPS may not always be available, even in open outdoor areas, because it is vulnerable to natural interference and malicious jamming attacks. Thus, an alternative navigation system is required when the use of GPS is restricted or prohibited. In such circumstances, a marine radar, which is a standard sensor in a marine vehicle including USV, can be used for localization in coastal areas. The marine radar can extract landmark features of the surrounding coastlines. These features can be utilized for relative navigation with respect to the detected coastlines. However, coastline maps based on radar signatures may be unavailable in unexplored areas, and they may be unreliable in coastal areas with high tidal elevations. In this study, the relative navigation with respect to the surrounding coastlines is performed in the framework of simultaneous localization and mapping (SLAM) for a USV operation in coastal waters. In particular, coastline features are parameterized by using B-splines for efficient map management, instead of the conventional point cloud representation. To verify and demonstrate the performance of the proposed coastal SLAM algorithm, field experiments were conducted in actual coastal environments. The results are presented and discussed in this paper."}, {"label": 1, "content": "Local binary descriptors, including the well-known local binary pattern (LBP) and its variants, are extensively used in texture and dynamic texture analysis owing to their remarkable attributes such as grayscale invariance, low computational complexity, and good discriminability. Most of the existing local binary feature extraction techniques extract spatio-temporal features from three orthogonal planes of a spatio-temporal volume by visualizing a dynamic texture in 3D space. However, in the process of extracting local binary features for a particular pixel in a video, a portion of its surrounding pixels is excluded. It is argued that these overlooked pixels contain valuable discriminatory information that should be exploited.\n\nA new approach to fully utilize the information contained in all pixels within a local neighborhood is proposed. This approach involves extracting local binary features from the spatio-temporal domain using 3D filters that are unsupervisedly learned, enabling the capture of discriminative features along both the spatial and temporal dimensions simultaneously. The proposed method comprises three essential components: 1) 3D filtering; 2) binary hashing; and 3) joint histogramming. Firstly, densely sampled 3D blocks of a dynamic texture are normalized to have zero mean and filtered by 3D filters learned in advance. To retain the structure information, the filter response vectors are decomposed into two complementary components: the signs and magnitudes, which are then separately encoded into binary codes. Additionally, the local mean pixels of the 3D blocks are converted into binary codes. Finally, three binary codes, namely the sign binary code, magnitude binary code, and mean binary code, are combined using joint/hybrid histograms to obtain the final feature representation.\n\nThe proposed method is evaluated on three commonly used dynamic texture databases: UCLA, DynTex, and YUVL. Extensive experiments reveal that the proposed approach can produce results that meet or outperform the performance of many state-of-the-art methods."}, {"label": 0, "content": "In this paper, for the defects which the basic genetic algorithm can get local optimum easily and converge slowly, the genetic algorithm is improved by constructing a suitable fitness function and improving genetic operators. Meanwhile, the improved genetic algorithm is applied to select optimal access network in integrated heterogeneous wireless networks. According to Quality of Service (QoS) requirements of different business types, compared with combined the analytic hierarchy process (AHP) and the technique for order preference by similarity to an ideal solution (TOPSIS) and the basic genetic algorithm, the simulation results indicate that the improved genetic algorithm can find a network with higher fitness and better meet different QoS."}, {"label": 0, "content": "We study a communication scheduling and remote estimation problem within a worst-case scenario that involves a strategic adversary. Specially, a remote sensing system consisting of a sensor, an encoder and a decoder is configured to observe, transmit, and recover a discrete time stochastic process. At each time step, the sensor makes an observation on the state variable of the stochastic process. The sensor is constrained by the number of transmissions over the time horizon, and thus it needs to decide whether to transmit its observation or not after making each measurement. If the sensor decides to transmit, it sends the observation to the encoder, who then encodes and transmits the observation to the decoder. Otherwise, the sensor and the encoder maintain silence. The decoder is required to generate a real-time estimate on the state variable. The sensor, the encoder, and the decoder collaborate to minimize the sum of the communication cost for the sensor, the encoding cost for the encoder, and the estimation error for the decoder. There is also a jammer interfering with the communication between the encoder and the decoder, by injecting an additive channel noise to the communication channel. The jammer is charged for the jamming power and is rewarded for the estimation error generated by the decoder, and it aims to minimize its net cost. We consider a feedback Stackelberg game with the sensor, the encoder, and the decoder as the composite leader, and the jammer as the follower. Under some technical assumptions, we obtain a feedback Stackelberg solution, which is threshold based for the scheduler, and piecewise affine for the encoder and the decoder. We also generate numerical results to demonstrate the performance of the remote sensing system under the feedback Stackelberg solution."}, {"label": 0, "content": "We focus on two problems related to rumor detection. First, stance classification with respect to a rumor and, second, rumor veracity prediction. In the stance classification task, we aim to identify the users' stance toward the underlying rumor in a Twitter conversational thread, whereas, in the second problem, i.e., veracity prediction, we aim to verify the authenticity of a rumorous tweet (i.e., source tweet) in a conversational thread. We propose an MLP-based feature-driven model for veracity prediction and a hierarchical LSTM-based approach for detecting stances toward a rumor in a conversation thread. Evaluations show that our proposed system attained better performance in comparison with the various state-of-the-art systems on both the tasks."}, {"label": 0, "content": "A novel quadrature phase shift keying (QPSK) carrier tracking method used in coherent demodulation is proposed for satellite communications. Frequency ambiguity is always ignored by the traditional ones which results in that the maximum pull-in range of the carrier tracking loop is limited to one eighth of the symbol rate. A novel low complexity QPSK frequency/phase discriminator and a frequency estimation and compensation (FEC) module are presented in this paper to enlarge the frequency pull-in range, which can resolve the phase jump and frequency ambiguity problem in carrier tracking process and also has the few calculation complexity comparing with the traditional QPSK detector. A parallel FLL-assisted-PLL is adopted to simplify the design structure through the module reuse. Comparing with the state-of-the-arts, this method has lower complexity, smaller variance and larger pull-in range."}, {"label": 1, "content": "Jamming identification is a critical step in implementing effective anti-jamming measures, as it plays a crucial role in boosting the electronic information system's adaptability to the electromagnetic environment. While the most commonly used method of jamming recognition relies on expert knowledge-based feature extraction, the varied patterns and parameters of jamming can make it difficult to determine the correct feature set. As such, this paper proposes the use of deep learning techniques to extract features automatically from raw data for jamming recognition in electronic information systems. To demonstrate the feasibility of this approach, the identification of noise jamming factors in superheterodyne receivers is presented as a case study."}, {"label": 1, "content": "This research proposes an innovative strategy to significantly reduce the processing time required to solve the concurrent AC multistage transmission network expansion and reactive power planning problem with security constraints. The strategy involves modeling the concurrent planning problem as a mixed-integer linear programming (MILP) problem, using an AC branch flow formulation to represent the steady-state operation of the transmission network. Then, a stage-by-stage solution pool of the MILP model is obtained as a static problem, to identify the significant candidate lines. The insignificant lines are not considered as candidates in the multistage problem, thus reducing the search space. Using the updated database, the multistage MILP problem can be solved efficiently. The evaluation of the proposed methodology is done using the IEEE 24- and 118-bus test systems, demonstrating its effectiveness in solving the concurrent planning problem with security constraints."}, {"label": 0, "content": "High penetration of volatile renewable energy produces uncertainties in power system and poses severe challenges to transmission network planning (TNP). Usually, only one type of mathematical model of different uncertainties was considered in every single TNP method. However, in the process of TNP, different uncertainties may show different mathematical features and need to be represented by various types of mathematical models. Aiming at this problem, a TNP model taking into account interval uncertainty model of renewable energy generation and fuzzy uncertainty model of predicted load is proposed based on the expanded fuzzy chance constrained programming. In accordance with the features of the constructed planning model, the model is transferred to a robust TNP model considering interval uncertainty model of renewable energy generation and predicted load. Thus, the calculation burden for solving the planning model decreases. The analyses on the modified IEEE RTS 24-bus system and a 231-bus system verify the effectiveness and adaptability."}, {"label": 1, "content": "Video summarization (VSUMM) has emerged as a popular approach for processing large video datasets. Its main objective is to identify key frames that encapsulate the essence of a video sequence. Although current techniques can extract static frames as content summaries, they tend to ignore motion information. To address this limitation, a new framework has been proposed for efficient video content and motion summarization.\n\nTo begin with, Capsules Net is used to learn spatiotemporal features which are then used to generate inter-frame motion curves. Next, a transition effects detection method is employed to segment video streams into shots. Finally, a self-attention model is used to identify key-frame sequences within shots. This leads to the selection of static images as video content summaries, while the calculation of optical flows enables motion summarization.\n\nResults from experimental tests demonstrate that the proposed framework is competitive across VSUMM, TvSum, SumMe, and RAI datasets in shot segmentation and video content summarization. Furthermore, it produces effective motion summarization which enhances the quality of summarized video content."}, {"label": 1, "content": "To address the issues of size and energy consumption imbalance in the LEACH protocol cluster, this study introduces an energy-equalized unequal clustering routing protocol. The proposed approach utilizes the energy decision factor and node density decision factor to improve the candidate cluster head threshold calculation formula. With this improvement, nodes with more residual energy and more neighbor nodes are more likely to become candidate nodes. The candidate cluster heads are then elected as real cluster heads based on a certain competition radius which takes into account the number of cluster heads and the distance between nodes and the base station.\n\nThe algorithm divides nodes into clusters of different sizes to keep the size of the cluster close to the base station smaller, effectively resolving the \u201chot spots\u201d problem. Additionally, the number of cluster heads and distribution are more reasonable with this approach. Simulation results indicate that the proposed algorithm effectively balances network consumption and prolongs the network life cycle."}, {"label": 1, "content": "State Grid, with the most HVDC projects in operation and the largest renewable energy generation capacity, has become one of the most complex power grids globally, leading to difficulties in simulation, analysis, and control. To improve the understanding of State Grid, a more detailed simulation method, such as hybrid simulation of TS and EMT, is widely applied. However, this has raised computational loads, and conventional simulation tools are too slow to support day-to-day operation. This paper proposes a novel digital parallel simulation system of State Grid, adopting supercomputing technology to handle massive simulation cases in parallel and provide simulation services to remote users. The system architecture and subsystems are introduced in detail, with two levels of parallel computing carried out to speed up the simulation, case parallelism and sub-grid parallelism. The system relies on cloud simulation technology for remote access to the national dispatching center and dozens of regional/provincial dispatching centers. Key technologies of the system, including TS-EMT hybrid simulation, multi-layer sub-grid parallel simulation, and automatic initialization of EMT simulation are explained, promoting the system performance. The system has been applied in State Grid, supporting routine operation simulation and analysis, with several applications that demonstrate great enhancement of power grid simulation efficiency."}, {"label": 0, "content": "Single image super-resolution (SISR) has witnessed great progress as convolutional neural network (CNN) gets deeper and wider. However, enormous parameters hinder its application to real world problems. In this letter, We propose a lightweight feature fusion network (LFFN) that can fully explore multi-scale contextual information and greatly reduce network parameters while maximizing SISR results. LFFN is built on spindle blocks and a softmax feature fusion module (SFFM). Specifically, a spindle block is composed of a dimension extension unit, a feature exploration unit. and a feature refinement unit. The dimension extension layer expands low dimension to high dimension and implicitly learns the feature maps which are suitable for the next unit. The feature exploration unit performs linear and nonlinear feature exploration aimed at different feature maps. The feature refinement layer is used to fuse and refine features. SFFM fuses the features from different modules in a self-adaptive learning manner with softmax function, making full use of hierarchical information with a small amount of parameter cost. Both qualitative and quantitative experiments on benchmark datasets show that LFFN achieves favorable performance against state-of-the-art methods with similar parameters."}, {"label": 0, "content": "The path towards wind power forecasting has yielded huge socio-economic benefits at a global scale. However, most of the previous studies tend to emphasize the improvement of deterministic forecasting, usually losing sight of the significance of probabilistic forecasting. In this paper, a novel forecasting system that can perform deterministic and probabilistic forecasting of wind power simultaneously, composed by the modules of feature selection, forecasting, system optimization, and system evaluation is presented to further supplement the existing studies in this field. Concretely, a hybrid feature selection strategy is proposed in the feature selection module to determine optimal system input; superior to traditional gradient descent algorithm, a dynamic reservoir theory-based recurrent neural network is developed in the forecasting module; an enhanced multi-objective optimization algorithm with the objectives of accuracy and stability is proposed in the system optimization module to provide an optimal scenario for system parameters; the effectiveness and feasibility of the proposed system is then validated in the evaluation module. Moreover, the comprehensive performance analysis of the proposed system is investigated in depth. Finally, the experimental results demonstrate that the proposed system has a significant advantage over the benchmarks considered, further verifying its tremendous potential to be used in a practical wind power system."}, {"label": 0, "content": "Aim of this study is to develop an accurate and reliable method for estimating propagation characteristics inside and outside aircraft cabin so as to advance wireless link design for Wireless Avionics Intra-Communication (WAIC) system. This paper estimates propagation characteristics from inside cabin to exterior mounted antenna of WAIC system installed on a passenger aircraft (Airbus 320-200 model). EMF distributions excited by a 4.4 GHz wireless transmitter inside cabin are analyzed and fundamental characteristics for the WAIC system are derived from the analysis results."}, {"label": 0, "content": "IP source spoofing is a consequence of lack of packet level authentication in the Internet which allows attackers to carry out Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks. Source address validation filtering is one of the most important scheme that is deployed in the Internet to deter such attacks by filtering the spoofed IP packets. In this paper, we propose a novel scheme to study the deployment of source address validation-filtering by using some special path backscatter messages that are generated by the spoofed traffic. We use the long term absence of such messages from an Autonomous System (AS) to classify it as non-spoofer AS. We use Caida's backscatter dataset for our study. We provide the list of spoofer and non-spoofer ASes from the given dataset. We also provide detailed mathematical analysis for calculating the amount of time we need to wait before declaring an AS as a non-spoofer. Besides, we use the normal approximation of binomial distribution to calculate confidence interval for the proportion of ASes allowing spoofing and to test the hypothesis regarding the spoofing activity in the Internet."}, {"label": 0, "content": "Jamming identification is the precondition of taking targeted anti-jamming measures, and it is very important to improve the adaptability of electronic information system to electromagnetic environment. The traditional recognition method of jamming is based on the feature extraction based on expert knowledge, but due to the jamming pattern diversity and different parameter, in practice it is difficult to determine the appropriate feature set. Therefore, this paper introduces a deep learning approach, which automatically extracts features from the original data to identify the jamming factors of electronic information system. In order to demonstrate the effectiveness and practicability of this approach, the noise jamming factor identification of the superheterodyne receiver is introduced."}, {"label": 1, "content": "The existing traffic light control system has many drawbacks, including long delays and energy waste. To enhance efficiency in traffic signal control, it is crucial to take real-time traffic information as input and adjust traffic light durations dynamically. Current works either divide the traffic signal into equal parts or only use limited traffic information. In this paper, we propose a deep reinforcement learning model that uses data collected from various sensors to determine traffic signal durations. We represent the complex traffic scenario as states by gathering traffic data and dividing the intersection into small grids. We model the duration changes of a traffic light as a high-dimensional Markov decision process, where rewards are determined by the cumulative waiting time difference between two cycles. To solve the model, we use a convolutional neural network to map states to rewards. Our model incorporates various optimization techniques, such as dueling network, target network, double Q-learning network, and prioritized experience replay, to improve performance. We evaluate our model through simulation on the Simulation of Urban Mobility simulator, and the results show that our model efficiently controls traffic lights."}, {"label": 1, "content": "Driving assistance has seen a variety of advancements, with emergency braking being a key aspect. Researchers have analyzed emergency braking and proposed different approaches to identify them. One important scenario to consider is the mistaken acceleration during emergency braking when the accelerator pedal is wrongly pressed instead of the brake pedal. The objective of this study is to develop a classifier using evolutionary computation to identify mistaken pedal pressing based on pedal behavior. To collect the data, a driving simulator was used, and genetic programming was used for the evolution process."}, {"label": 0, "content": "A novel penetration capability of distribution generation methodology to voltage constraint is developed. The design goal of the methodology is to decide the maximum DGs' capacity and the best integrated location without voltage violation. Instead of dealing with the combinatorial nature of the proposed problem, the proposed methodology employs a two-stage methodology to decide the solutions. The first stage is a linear method to estimate the capabilities under all the possible DGs' locations. The second stage is a detailed calculation stage which employs a nonlinear optimization method to calculate the exact maximum penetration capability under the best location determined in stage 1. The maximum penetration capability and the corresponding location can be easily calculated by the proposed methodology. Finally, several examples under single and multiple DGs in IEEE 33-bus power system are used to verify the proposed methodology. The numerical studies show that the effectiveness of the proposed methodology in calculating the maximum penetration capability and determining the best location for DGs in distribution network systems."}, {"label": 1, "content": "Device Fingerprinting (DFP) is a technique used to identify a device with the help of the packets which the devices use to communicate over the network. In this paper, we propose a novel idea of DFP using Inter Arrival Time (IAT) to identify the device. IAT is the time interval between two consecutive packets received, and it is unique for a device due to the different hardware and software used. \n\nPrevious research on DFP uses statistical techniques to analyze IAT and further generate information for the identification of the device uniquely. In contrast, we plot graphs of IAT for packets, where each graph plots 100 IATs and subsequently process the resulting graphs for device identification. This approach is more efficient in identifying a device's DFP due to the deep learning libraries' benchmark in image processing. \n\nTo test our method, we configured a Raspberry Pi to work as a router and installed our packet sniffer application on it. We connected two Apple devices, iPad4 and iPhone 7 Plus, to the router and created IAT graphs for these two devices. We used Convolution Neural Network (CNN) to identify the devices and achieved an accuracy of 86.7%.\n\nOverall, our proposed method improves the efficiency of DFP using IAT and provides a more reliable solution for device identification on a network."}, {"label": 1, "content": "Component sizing is a crucial aspect of designing small off-grid electrical systems that rely on renewable energy sources and battery storage, also known as minigrids. To determine the appropriate sizes for these components, stochastic time-series simulations are commonly used. However, to improve computational efficiency, these simulations often avoid electrical system models and instead use power-balance methods that capture net energy flows without modeling current and voltage.\n\nUnfortunately, power-balance models can overestimate performance or fail to simulate critical supervisory control states necessary for control system development. To address these shortcomings, we propose a new modeling method that calculates required currents and voltages while retaining computational efficiency. Our method emphasizes an efficient representation of battery terminal voltage as a function of state-of-charge and battery current since microgrid components often rely on battery voltage as a key control input.\n\nWe demonstrate that critical battery parameters can be extracted from simple pulsed discharge tests which can be conducted in field conditions. When applied to a typical microgrid, our method can identify control instabilities that would be missed by power-balance methods, all while maintaining acceptable speeds. Overall, our approach provides a more accurate and efficient way to size components for microgrids, ultimately leading to better performance and economic outcomes."}, {"label": 0, "content": "The multiple security domains management and the security gateways that protect the security domain boundary are applied in space-ground integration information network, so we can adjust network security functions in the gateways dynamically by reconfiguration as the response to various security threats and malicious attacks. To solve the decision making problem about when to reconfigure and what to reconfigure, we presented privilege transition graph of internal security domain that can show us the vulnerabilities utilization between hosts in the domain. The algorithm combining forward breadth search and depth backtracking was proposed to get the attack paths from one host to key resources in the domain. And a quantification method for the risk of attack paths was presented. The method was tested which could provide effective data for recoufiguration decision."}, {"label": 0, "content": "It is an effective way to identify substation switch state using deep learning directly based on massive large image samples, which requires high-performance servers for off-line model training and high-quality industrial personal computer (IPC) for running models efficiently. The processing cost and delay will considerably increase by this means and the identify speed of robots for potential defects in the field reduces accordingly. Therefore, an image recognition method using only regular computers and IPC is proposed in this paper. Through target detection based on HSV (i.e. Hue, Saturation and Value) color space, this method firstly prefetch and preliminary screen the potential identifiers from large image samples, and subsequently trains a classification model with artificial neural network utilizing smaller samples labeled. Finally, target identifier can be located and identified through target detection, and then be used for recognizing switch state according to their relative positions. The experimental results show that with limit hardware resources, this method can process image samples efficiently and accurately based on robot vision. It is demonstrated to be a lightweight solution for precisely recognizing substation switch state."}, {"label": 0, "content": "In this paper, a Software Defined Network was created in Mininet using Python script. An external interface was added in the form of an OpenDaylight controller to enable communication with the network outside of Mininet. The OpenDaylight controller was hosted on the Amazon Web Services elastic computing node. This controller is used as a control plane device for the switch within Mininet. The OpenDaylight controller was able to create the flows to facilitate communication between the hosts in Mininet and the webserver in the real-life network. In order to test the network, a real life network in the form of a webserver hosted on the Emulated Virtual Environment - Next Generation (EVE-NG) software was connected to Mininet."}, {"label": 1, "content": "Ensuring the safe and stable operation of the power grid relies heavily on the reliable operation of relay protection equipment and its secondary circuit. Improving the operation and maintenance efficiency of this equipment is therefore crucial. This paper presents the design of a relay protection intelligent mobile operation and maintenance management system based on power wireless virtual private network (VPN). The system architecture and communication network deployment mode are discussed in detail. \n\nFurthermore, the paper covers information synchronization methods between the system and relay protection statistical analysis and operation management modules, the condition-based maintenance assistant decision-making module, and other system data modules. By enabling the collection, collation, interaction, and sharing of intelligent information in substations, the system achieves mobile, paperless, and standardized management and control of equipment information. This improves the convenience and efficiency of on-site operation and maintenance work, as well as management level, ultimately ensuring the safety and stable operation of the power grid. \n\nIn practice, the system has been successfully applied in substation operations and has yielded positive results."}, {"label": 1, "content": "In recent years, there has been a significant rise in mobile data consumption, leading to an increased demand for capacity from mobile network operators (MNOs). To meet this demand, MNOs have been deploying more base stations and allocating more spectrum layers, including small cells, both indoor and outdoor. However, this scaling up of the network may lead to an increase in the energy consumption of the Radio Access Network (RAN). Therefore, there is a need to optimize the network, reducing the overall power consumption through cloud-based models and the deployment of power-efficient radio nodes.\n\nThis paper analyzes the network's evolution towards a Cloud-based Radio Access Network (CRAN) for heterogeneous base stations with Macro RRUs, Micro RRUs, and Pico radio units. The authors derive the computational complexity using a flexible, future-proof power model and apply it to the network. They also compare the computation complexity for various UE channel conditions, different sub-components within the base station type, and present the results.\n\nFurther, the authors use the Bin-Packing algorithm to determine the number of base station cloud servers needed for the network and the power consumption of the base station cloud. They evaluate whether newer cloud servers with higher CPU cores are more power-efficient for a given load. Their simulations show that the current base station cloud servers have more capacity and are more power-efficient than baseline Compute Node servers used with the earlier power model.\n\nIn conclusion, this paper highlights the need for CRAN, which can lead to better network optimization and reduced energy consumption in the RAN. The authors provide a comprehensive analysis of the computational complexity and power consumption of base station clouds, which can be used by MNOs in their future network planning."}, {"label": 1, "content": "This paper explores methods for enhancing the stability of radio-frequency transfer in telecommunication dense wavelength division multiplexing fiber-optic networks. The study focuses on the significant differential delay caused by dispersion compensation fibers (DCFs) often utilized in these networks. Temperature-induced fluctuations exacerbate destabilization of the frequency transfer, and the authors propose a strategy for addressing this issue. The proposed technique entails modeling the impact of DCFs using temperature sensors installed by manufacturers in DCF modules that are remotely accessible. The research team tested the efficacy of this method on three distinct long-haul routes spanning up to 1550 kilometers set up in the operational Polish National Research and Education Network. The results demonstrate a significant enhancement in the long-term stability of the frequency transfer."}, {"label": 1, "content": "DC voltage control for a modular multilevel converter (MMC) not only involves controlling the total DC voltage but also controlling the capacitor voltage balancing of each sub-module (SM). The conventional capacitor voltage balancing control method sorting-based algorithm is simple to implement but requires sorting the capacitor voltages of all SMs of the valve arm, resulting in low calculation efficiency. In this study, a dynamic tiered sorting-based novel capacitor voltage balancing control algorithm is proposed. This algorithm layers all SMs on the valve arm dynamically, which significantly reduces the number of SMs participating in sorting. \nThe proposed algorithm is tested in the electromagnetic transient program of ADPSS (Advanced Digital Power System Simulator). The back-to-back MMC-HVDC simulation case verifies the correctness and effectiveness of the proposed algorithm. The results indicate that the proposed algorithm can achieve the same control effect as the traditional algorithm, maintaining stability, and improve computational efficiency significantly."}, {"label": 1, "content": "Pilot contamination is a major challenge that limits the performance of massive MIMO systems. In this study, we propose an asynchronous scheduling technique based on fractional pilot reuse to address the issue of pilot contamination during uplink transmission. Users are categorized into two groups based on the level of interference they face: center users, who experience mild pilot contamination, and edge users, who experience severe pilot contamination. A cell-center pilot set is reused for all center users across all cells, while a cell-edge pilot set is used for edge users in adjacent cells. The pilots used by cell-edge users are orthogonal to each other, which enables them to transmit pilots at any time. On the other hand, center users use a non-overlapping pilot transmission strategy to avoid pilot contamination. With this scheme, the cost of orthogonal pilots is significantly reduced, and the base station can easily recover pilot estimation, as there is no pilot contamination. Simulation results show that the proposed asynchronous fractional pilots scheduling (AFPS) outperforms conventional pilot assignment methods."}, {"label": 1, "content": "Retinal vessel tortuosity is an important indicator of various retinopathies, and several automated methods for determining retinal vessel tortuosity have been proposed in the literature. However, there is still a need for further study in this area. In this study, three different features were extracted from thinned vessels, including distance metric, normalized hybrid metric, and non-normalized hybrid metric. The weights of vessel data samples were dynamically updated using Adaboost with linear discriminant analysis (LDA), and feature correlation was used to facilitate the selection of the best feature combination at each boosting iteration.\n\nInstead of selecting a single feature that minimizes the weighted error, the proposed method selects the best feature combination at each iteration. Adaboost with LDA method was used for the classification of retinal vessels as either tortuous or normal through a majority voting method. The proposed method achieved an accuracy rate of 100% for the training sample sizes of 70%, 80%, and 90%."}, {"label": 0, "content": "Based on instrumental sensors, a method of cascade estimation of the near-field signal source using the symmetric uniform linear array is proposed. The method realizes the separation, decoupling and estimation of parameters by reconstructing the virtual array and steering vector transformation. The method has no loss of aperture in reconstructing the virtual array, avoids the multi-dimensional spectrum search and is more practical. Compared with the traditional methods, the method is less computationally intensive, the gain and phase error calibration accuracy is higher, and the parameter estimation accuracy is higher. Simultaneously, cascading estimation enables real-time estimation of the azimuth, range, and error parameters, and simulation experiments show the performance of the proposed algorithm in this paper."}, {"label": 0, "content": "Distributed and Real-Time Simulation represents a solid and effective approach to manage the ever-increasing complexity of modern systems. The IEEE 1516-2010 for Modeling and Simulation (M&S) High Level Architecture (HLA) is an interoperability standard for Distributed Simulation (DS) used to support analysis, engineering and training in different research and industrial domains. Using HLA, simulation entities (called Federates) can interact (that is, to publish and/or subscribe ObjectClasses and InteractionClasses, to communicate data, and to synchronize actions) with other Federates in a common simulation environment (called Federation) through the services provided by the Run-Time Infrastructure (RTI) that abstract and hide the details of the computing infrastructures making them communicable. In today's HLA-DS systems, in which simulation entities are highly concurrent, distributed, and the interactions among them are asynchronous, great benefits can derive from the exploitation of reactive approaches, tools and techniques so as to reactively manage the Federates' communication flow instead of handling it through HLA Callbacks. In this context, the paper presents a solution for defining reactive HLA Federates along with the RxHLA software framework that aims at defining and building reactive, concurrent, and distributed time/event-driven simulation components (Federates) in a r.eactive fashion."}, {"label": 1, "content": "Edge computing is an emergent paradigm that extends cloud technologies to the logical extremes of the network, providing on-demand and delay-sensitive services. Nonetheless, the placement of services on edge-enabling resources presents a new challenge: how to process enormous volumes of streaming data to provide query-driven analytics while still satisfying the delay-critical servicing requirements. To overcome this challenge, we propose StreamSight, a framework for edge-enabled IoT services that offers a rich and declarative query model abstraction for expressing complex analytics on monitoring data streams. StreamSight then dynamically compiles these queries into stream processing jobs for continuous execution on distributed processing engines. \n\nTo compensate for the resource restrictions in edge computing deployments, StreamSight outputs the query execution plan, enabling the reuse of intermediate results and avoiding their continuous recomputation. Furthermore, StreamSight facilitates optimization strategies such as approximate answers, query prioritization, and error bounds, allowing users to express delay-sensitive requirements relevant to their deployment without violating accuracy guarantees. \n\nIn this study, we evaluate our framework on Apache Spark with real-world workloads and show that using StreamSight can significantly increase performance by 4x while still meeting accuracy guarantees."}, {"label": 1, "content": "The field of Unmanned Aerial Vehicles (UAV) has recently seen a surge in the use of accurate target tracking. This paper focuses on the real-time detection and tracking of a walking pedestrian from a moving platform, amidst many interferences. To achieve this, we propose a scheme that employs CNN model (YOLO-V2) to detect pedestrian and matches the walking pedestrian with both postprocessing and feature queue, as well as Locality constrained Linear Coding algorithm. The ground station then receives and analyzes the video stream from the Parrot and sends back commands to control the motion of the UAV. At the onset of the tracking process, the UAV hovers while one pedestrian is selected as the designated target. We rely solely on visual information obtained via a front camera without assistant sensors. For the experiment, we utilized a Parrot Bebop 2, which allowed us to conduct outdoor experiments. The experimental results validate the effectiveness of our proposed solution."}, {"label": 0, "content": "A model of neurons for biometric authentication, capable of efficient processing of highly dependent features, based on the agreement criteria (Gini, Cramer-von-Mises, Kolmogorov-Smirnov, the maximum of intersection areas of probability densities) is proposed. An experiment was performed on comparing the efficiency of neurons based on the proposed model and neurons on the basis of difference and hyperbolic Bayesian functionals capable of processing highly dependent biometric data. Variants of construction of hybrid neural networks, that can be trained on a small number of examples of a biometric pattern (about 20), are suggested. An experiment was conducted to collect dynamic biometric patterns, in the experiment 90 people entered handwritten and voice patterns during a month. Intermediate results on recognition of subjects based on hybrid neural networks were obtained. Number of errors in verification of a signature (handwritten password) was less than 2%, verification of a speaker by a fixed passphrase was less than 6%. The testing was carried out on biometric samples, obtained after some time period after the formation of training sample."}, {"label": 1, "content": "Direction-based methods have become a popular choice for palmprint recognition due to their impressive results. However, there has been a lack of research analyzing the discrepancies among different direction-based models and determining the most significant direction feature extraction for palmprints. In this study, we aim to establish a connection between the discriminability of direction features and the direction feature extraction model. We propose a new model, the exponential and Gaussian fusion model (EGM), which captures the discriminative power of various directions. This model offers crucial insights into picking the optimal direction feature selection of palmprints. Furthermore, we introduce the local discriminant direction binary pattern (LDDBP) to represent the direction features of a palmprint fully. LDDBP-based descriptors can be formed by utilizing the most discriminant directions determined by the EGM for palmprint recognition. The LDDBP model performs better than other state-of-the-art direction-based models on four widespread palmprint databases, as shown by our extensive experimental results."}, {"label": 1, "content": "As the global population rapidly increases, the demand for Food, Energy, and Water (FEW) resources is expected to more than double by 2050. Meeting this demand will be a major challenge, requiring the use of resources with a smaller ecological footprint. To address this challenge, this paper proposes an automated smart irrigation system optimized for water, energy, and fertilizer use in agricultural crops.\n\nThe system uses real-time data from wireless sensor networks to schedule irrigation, with sensors monitoring soil moisture, temperature, solar radiation, humidity, and fertilizers embedded in the root area of the crops and around the test-bed. An algorithm based on temperature and soil moisture threshold values is used to control irrigation time, with wireless sensor data transmission and acquisition managed by an Access Point using ZigBee protocol.\n\nThe system's energy demand is completely supplied by a solar photo-voltaic panel, supplemented with an energy storage unit. The experimental data obtained from this prototype will be modeled and optimized to investigate food production profiles as a function of energy and water consumption, with an attempt to understand the effect of extreme weather conditions on food production.\n\nThis holistic approach explores the nexus between water and energy resources and crop yield for several essential crops, in an attempt to design a more sustainable method to meet the forecasted surge in demand. Overall, the proposed system offers a potential solution to the challenge of meeting the growing demand for FEW resources in a sustainable and efficient way."}, {"label": 1, "content": "With the increasing reliance on network communication in the intelligent substation, protection settings have become more critical to ensure the correct operation of protection devices, and overall efficiency in the power system. However, the frequent need to adjust these settings due to changing operation modes and climate conditions presents a challenge that can result in missing or omitted protection settings. To address this issue, this paper proposes an online management system for protection devices that leverages the data attributes and layered network structure of the intelligent substation. The system aims to eliminate potential danger caused by missing or omitted protection settings, while also ensuring the efficient and reliable operation of the power system."}, {"label": 0, "content": "The Space-Ground Integrated Network (SGIN) plays an important role for the future development of the country. The cyber-attacks against it are the focus of the research. In this paper, an association analysis algorithm based on knowledge graph of cyber security attack events is proposed to present the attack scenario for the Space-Ground Integrated Network. The construction of knowledge graph and association analysis can show the scene of cyber-attacks in the form of graphs. During the build process, the construction of an event ontology is an important part of it. Event ontology is used to represent various relationships in the network attack procedure. At last, we present a space-ground integration network security analysis system based on the knowledge graph of cyber security attack events, and uses the association analysis algorithm to analyze the attack scenario."}, {"label": 0, "content": "In this paper we review the registration of a binary element in a discrete channel with erasure using a nonlinear scale constructed on the basis of a fuzzy membership function, the concept of fuzzy sets theory. The source of information loss of a binary element is shown when it is recorded in a traditional way. The mechanism of compensation of information losses on the basis of a nonlinear scale is given."}, {"label": 1, "content": "The deployment of distributed energy resources (DERs) can add complexity to the distribution system and pose challenges for system operators. To address these challenges, advanced state estimators for distribution systems with high penetration of DERs can be developed. In this paper, an object-oriented Distributed Quasi-Dynamic State Estimator (DQDSE) is presented, which employs three-phase detailed models to extract the operating state and model of the system in real time. The DQDSE uses data from sensors and performs quasi-dynamic state estimation based on network-wise measurements. The proposed DQDSE offers accurate results, even for unbalanced and asymmetric systems, and its distributed architecture enables fast data processing. An illustrative example is presented to demonstrate the effectiveness of the method."}, {"label": 0, "content": "A joint communication and state estimation problem in a Gaussian multiple access channel with common additive state is considered. The state process is assumed to be IID Gaussian, and known non-causally at both the transmitters. The receiver not only has to decode the messages from the transmitters, but also needs to estimate the state process to within some prescribed squared error distortion. We provide a complete characterization of the optimal sum-rate versus distortion performance."}, {"label": 0, "content": "This paper presents a novel contrast measure for MSER region selection, termed Mean Intensity Difference (MID). The proposed metric is computed between the pixels of an MSER region and its surrounding pixel set. In this work we consider the complementary pixels within the bounding box of a region and alternatively the pixels of the first contour layer as surroundings. To evaluate the proposed contrast metric, a location retrieval task is performed. To this end, SURF descriptors are computed and the Bag-of-Words representation is used as global signature for each image. For the evaluation we use the Devon Island dataset, which is said to have one of the most Mars-like environments on Earth and which comes with GPS ground-truth data. We further integrate the contrast-based methods with the approach of Grid Adaptation. The experimental results show that our contrast metric outperforms state-of-the-art metrics, such as Perceptual Divergence, and yields better performance compared to random region selection. In this work we also evaluate the computational complexity of the methods."}, {"label": 1, "content": "Neuroimaging studies have demonstrated that multiple brain regions activate during cognitive tasks. Real-time functional magnetic resonance imaging neurofeedback (rtfMRI-NF) can assist individuals in self-regulating brain activity. However, the neural mechanisms of rtfMRI-NF are unclear. To investigate this problem, we combined graph theory with resting state fMRI to explore the topological properties of functional brain networks. Subjects were provided with ongoing functional connectivity information related to emotion regulation. Our study found that rtfMRI-NF training altered small-world properties and nodal degree in the temporal lobe, frontal lobe, and limbic system. These results suggest that rtfMRI-NF training is associated with changes in the topological properties of functional brain networks."}, {"label": 0, "content": "A refined phase estimation based parallel carrier recovery algorithm for high speed wireless communication systems is proposed in this paper. This parallel algorithm is based on a serial DPLL (digital phase locked loop) carrier recovery feedback architecture and a novel refined phase estimation module. To archive high speed communication, parallelization of serial DPLL carrier recovery algorithm is presented; to guarantee high accuracy, a refined phase estimation design is proposed. A 32 parallel baseband simulation model of 16QAM modulation is performed on MATLAB platform to validate the proposed algorithm. Simulation results demonstrate that the performance loss of EVM (Error Vector Magnitude) introduced by the proposed algorithm is less than 0.3%, which is only half of the traditional coarse compensation algorithm."}, {"label": 1, "content": "Career direction is a crucial factor in the development of efficient corporate workforce. It is essential to identify the attributes that lead to an accurate classification of personality. This paper aims to extract personality from language. The process involves various techniques such as Text Normalization, Feature Extraction, Feature Selection, Data Pre-Processing, Data Sampling, and Training Predictive Models.\n\nOnce the personality types have been classified, suitable career paths can be matched accordingly. By experimenting with various approaches to each operation, personality attribute classifiers can yield an average accuracy of 96%. The results can then be compared with other approaches to personality classification.\n\nOverall, the process of extracting personality from language is an effective way to obtain accurate career direction. It provides a reliable and efficient solution for identifying suitable career paths for the workforce."}, {"label": 0, "content": "To meet network security requirements, all components of network are designed to be able to withstand contingencies, which results in significant cost to network development. Hence, network pricing should take network security into consideration. In the original long-run incremental cost(LRIC) pricing method, security factor is defined to charge for network security. It assumes that security factor is constant before and after the nodal injection. This paper proposes an improved LRIC pricing method, considering the fact that security factor of each component will be changed with nodal injections. This paper analyzes the factors that affect the security factor and illustrates the improved LRIC method with dynamic security factor. To demonstrate the difference between the improved LRIC pricing method and the original LRIC pricing method, simulation studies are conducted on a three-nodes system and the IEEE 14-nodes system, respectively; and the results show that the improved LRIC method can more accurately reflect the impact on network costs from users and ensure more reasonable allocation of network costs."}, {"label": 0, "content": "Moving-object tracking (estimating position and velocity of moving objects) is a key technology for autonomous driving systems and driving assistance systems in mobile robotics and vehicle automation domains. To predict and avoid collisions, the tracking system has to recognize objects as accurately as possible. This paper presents a method for recognizing vehicles (cars and bicyclists) and pedestrians using multilayer lidar (3D lidar). Lidar data are clustered, and eight-dimensional features are extracted from each of clustered lidar data, such as distance from the lidar, velocity, object size, number of lidar-measurement points, and distribution of reflection intensities. A multiclass support vector machine is applied to classify cars, bicyclists, and pedestrians from these features. Experiments using \u201cThe Stanford Track Collection\u201d data set allow us to compare the proposed method with a method based on the random forest algorithm and a conventional 26-dimensional feature-based method. The comparison shows that the proposed method improves recognition accuracy and processing time over the other methods. Therefore, the proposed method can work well under low computational environments."}, {"label": 0, "content": "Many audio forensic applications would benefit from the ability to classify audio recordings, based on characteristics of the originating device, particularly in social media platforms where an enormous amount of data is posted every day. This paper utilizes passive signatures associated with the recording devices, as extracted from recorded audio itself, in the absence of any extrinsic security mechanism such as digital watermarking, to identify the source cell-phone of recorded audio. It uses device-specific information present in low as well as high-frequency regions of the recorded audio. On the only publicly available dataset in this field, MOBIPHONE, the proposed system gives a closed set accuracy of 97.2% which matches the state of art accuracy reported for this dataset. On audio recordings which have undergone double compression, as typically happens for a recording posted on social media, the proposed system outperforms the existing methods (4% improvement in average accuracy)."}, {"label": 1, "content": "Advancements in robotics and cloud computing have resulted in the emergence of cloud robotics, which enables robots to benefit from remote processing, greater memory and computational power, and massive data storage. However, integrating robotics and cloud computing has been considered complex due to the various components involved in such systems. To address this issue, several studies have attempted to develop cloud robotic architectures to simplify representation into different blocks or components. Nevertheless, limited research has been conducted to critically evaluate and compare these architectures. Therefore, this paper analyzes and compares existing cloud robotic architectures to identify key constraints and recommend future improvements. In this study, seven architectures were analyzed and compared, and the results indicate that there is limited evaluation of existing architectures due to security concerns."}, {"label": 0, "content": "Air pollution has become a worldwide concerned issue and automatical estimation of air quality can provide a positive guidance to both individual and industrial behaviors. Given that the traditional instrument-based method requires high economic, labor costs on instrument purchase and maintenance, this paper proposes an effective, efficient, and cheap photo-based method for the air quality estimation in the case of particulate matter (PM2.5). The success of the proposed method lies in extracting two categories of features (including the gradient similarity and distribution shape of pixel values in the saturation map) by observing the photo appearances captured under different PM2.5 concentrations. Specifically, the gradient similarity is extracted to measure the structural information loss with the consideration that PM2.5 attenuates the light rays emitted from the objects and accordingly distorts the structures of the formed photo. Meanwhile, the saturation map is fit by the Weibull distribution to quantify the color information loss. By combining two features, a primary PM2.5 concentration estimator is obtained. Next, a nonlinear function is adopted to map the primary one to the real PM2.5 concentration. Sufficient experiments on real data captured by professional PM2.5 instrument demonstrate the effectiveness and efficiency of the proposed method. Specifically, it is highly consistent with real sensor's measures and requires low implementation time."}, {"label": 0, "content": "Processing and analyzing big data sets updated in real time in an increasing number of applications such as severe weather prediction and particle-physics experiments require the computational power of extreme-scale high-performance computing (HPC) systems. To address the scheduling of massive task/thread sets on these extreme-scale systems, current strategies rely on improving centralized, distributed, and parallel scheduling algorithms as well as virtualization developed for HPC systems which aim to reduce the makespan and balance the load among the computing nodes in these systems. However, these HPC schedulers provide no guarantees on meeting timing constraints such as deadlines that are required in an increasing number of these real-time science workflows. This paper describes a new project which departs from this established trend of best-effort scheduling of large-scale HPC Message Passing Interface (MPI) tasks and ensemble workloads found in fine-grain many-task computing (MTC) applications. The new approach brings real-time scheduling to address the demands of real-time science workloads. This new framework abstracts information about the tasks or threads, and continuously dispatch this workload to meet deadlines and other timing constraints associated with individual tasks or groups of tasks in extreme-scale HPC systems to reduce execution time and energy consumption. This paper introduces deadline-based scheduling in the tasking programming model."}, {"label": 0, "content": "Social media have become increasingly popular components of our everyday lives in today's globalizing society. They provide a context where people across the world can communicate, exchange messages, share knowledge, and interact with each other regardless of the distance that separates them. This research trend, extraction of events for specific domain from these social media is emerging speedily ranging from business intelligence to nation security field. The short length of Twitter messages and frequent use of informal and ungrammatical language challenge many long standing approaches for automatically detecting and categorizing events using streamed data in Event Message Identification system. A semi-supervised approach with Support Vector Machine (SVM) in combination with the corpus to identify the events from twitter for targeted domain in specific location is proposed in this paper. The experimental results show that the proposed semi-supervised SVM model is more efficient than a strong state-of-the-art semi-supervised classification model of Logic Regression, Naive Bayes and Decision Tree."}, {"label": 1, "content": "This paper presents an iterative approach to jointly estimate the states of combined heat and power systems (CHPS). The objective is to achieve dynamic state estimation (DSE) in the district heating system (DHS) using the node method to address the temperature quasi-dynamics. An alternating estimation strategy is utilized to effectively account for the time-delay constraints of temperature in the computation of DSE. Two case studies are conducted on CHPS test systems to validate the proposed DSE and alternating approach. The simulation results indicate that the DSE in CHPS is more accurate than the static state estimation and separate state estimation in individual energy systems."}, {"label": 1, "content": "We examine the performance of a dual-hop downlink cellular cooperative system in the presence of channel estimation errors and RF impairments, using multiple base station antennas and single antennas at relays and mobile stations. Our study yields both approximate and exact closed-form expressions for outage probability and expected spectral efficiency, as well as simple asymptotic expressions for high SNR regimes. Simulation results validate our analytical expressions and indicate that full diversity order requires ideal RF hardware, whereas hardware imperfections inevitably reduce diversity order. We also examine the impact of key parameters such as antenna and user count, relay count, and RF impairment levels on system performance."}, {"label": 1, "content": "The controller synthesis problem for a SISO automatic extremum-seeking system with a plant model consisting of a nonlinear dynamic component and a static quality function is analyzed. The proposed approach comprises of organizing two separate cascades with varying process rates for the individual plant components. Controller synthesis procedures for each loop are presented, with the initial step being the stabilization of the inner contour processes using a controller based on the localization method or sliding mode. These controllers suppress perturbations and nonlinear characteristics of the plant dynamic part while providing required properties to the inner cascade. For extremum seeking in the outer control loop, a typical I-controller is recommended, with correspondences to a first-order linear differential equation. The type of extremum system is dependent on the inner loop controller type, and this is demonstrated through numerical simulations in MatLab, which illustrate the basic properties of the two system types."}, {"label": 0, "content": "Visible light communication (VLC) transmits the wireless data through optical scintillation. However, the joint problem of both illumination and communication remains the tradeoff challenge. For both illumination and communication, this paper adopts dimming control to flexibly adjust the brightness for human eyes, and introduces non-orthogonal multiple access (NOMA) to improve the system throughput. First, we establish a model combining signal power allocation with dimming control. Second, for the enhancement of spectral efficiency, we introduce gain ratio power allocation (GRPA) of NOMA and variable on-off keying dimming control. Third, in the indoor user scenario, we analyse the relationship of luminescent angles, user data rate, and luminous intensity. Finally, experimental results indicate that the proposed GRPA scheme achieves the better performance than the previous strategy in user data rate for the same dimming factor. Furthermore, the optimal indoor VLC cell deployment of semi-angles and dimming factors is discussed."}, {"label": 0, "content": "Mauritius suffers from chronic water shortages that can severely impact its economy and the well-being of its population. Both surface and groundwater availability are determined by rainfall, which is in turn influenced by large-scale circulation patterns such as the El Ni\u00f1o Southern Oscillation (ENSO) and the Indian Ocean Dipole (IOD). Here we report on the influence of these two teleconnection patterns and present the result of a simple neural network for precipitation forecasting, based on the state of ENSO and IOD. Data from the Vacaos station, for the period 1961 to 2012 is used. We found statistically significant correlation between average winter rainfall and ENSO and IOD indices. The correlation for summer was negligible. The prediction of summer precipitation was less accurate than that of winter precipitation. The findings from this study can help in more efficient planning and management of water resources on the island."}, {"label": 1, "content": "Utilizing intelligent transportation infrastructure can significantly improve the throughput of intersections for Connected Autonomous Vehicles (CAV). However, Intersection Managers (IM) are vulnerable to model mismatches and external disturbances, which degrades the throughput and reduces the speed of CAVs that intend to make a turn at the intersection. In this paper, we propose a new technique called RIM, which assigns a safe Time and Velocity of Arrival (TOA and VOA) to approaching CAVs, ensuring safe trajectories before and inside the intersection, while compensating for model mismatches and external disturbances. CAVs track a position trajectory, allowing them to determine and track an optimal trajectory at the assigned TOA and VOA, without the need for reduced speed before entering the intersection. Results from experiments show that RIM reduces position error at the expected TOA by 18 times on average, in the presence of up to 10% model mismatch and an external disturbance with an amplitude of 5% of max range. Our technique achieves 2.7 times better throughput on average compared to velocity assignment methods."}, {"label": 1, "content": "This paper presents a model for the optimal allocation of Automatic Switching Devices (ASDs) and Distributed Generation (DG) in distribution networks. The model considers the use of ASDs for protection and post-fault restoration purposes, as well as the role of DG units in the restoration process. The System Average Interruption Frequency Index (SAIFI) is used as a metric index to obtain the best placement of ASDs and DG units, taking into account both their interdependency with system reliability. A Goal Programming approach is used to establish the optimal trade-off between the placement of ASDs and DG units and the improvement of SAIFI. The developed model aims to minimize the number of interrupted consumers due to protection operation, maximize the number of consumers restored by automatic network reconfiguration, and minimize costs associated with the allocation of ASDs and DG units. Restoration is ensured by a set of linear power flow equations. The model's solutions are obtained through a Branch and Bound-based technique, and results are presented using the IEEE 123-bus system."}, {"label": 1, "content": "Estimating energy costs for industrial processes can be both lengthy and complex. This complexity arises from the multiple appliances attached to the workflow, along with the different optimization setpoints required to achieve the necessary operational cost objectives. Additionally, data collection from multiple monitoring sensors can be geographically distributed, further complicating the task. As a result, the computational infrastructure required for these tasks increases in complexity, including meeting execution performance targets per processing unit, within a specific deadline.\n\nTo meet these demanding requirements, an ensemble-based edge processing technique is proposed as a solution. By leveraging distributed edge nodes rather than relying on a single edge node, the ensemble approach is developed to overcome processing and data privacy/security challenges, while simultaneously enhancing overall reliability. The proposed ensemble-based network processing model facilitates the distributed execution of energy simulations tasks within an industrial process, meaning that the computational load can be effectively distributed across the network.\n\nTo illustrate the effectiveness of this approach, a scenario is provided where the energy profiling in a fisheries plant is utilized as a case study. However, this approach can be applied to a wide range of domains with similar requirements. By widening the applicability of the proposed solution, it is possible to develop a standardized methodology applicable across a range of different industries."}, {"label": 0, "content": "Automatic disease detection using visible symptoms on leaves is becoming more and more important. Here we describe an algorithm, which uses machine learning to detect diseases in a wide variety of plants and diseases. High accuracy (>93%) was obtained with very noisy images, different backgrounds and different disease coverage. The algorithm is able to train itself, which means that the accuracy can increase with usage. It can run on a variety of platforms including smartphones and can thus aid non-expert farmers manage diseases effectively."}, {"label": 1, "content": "In order to address the issues associated with recognizing the state of substation hard platens, this paper proposes an intelligent recognition system based on machine learning that does not require modifying the secondary device. To enable image acquisition, a manual patrol inspection car has been designed. The proposed system uses a hard platen state recognition algorithm based on machine learning, which has been used to develop a hard platen intelligent-patrolling application. The current status of the hard platens can be easily obtained by inputting the collected hard platen images. Additionally, a patrol database has been established, and the hard platen table is maintained to manage patrol and maintenance tasks. The system has been experimentally validated at a substation in Guizhou, demonstrating that it can quickly and accurately recognize the state of hard platens, thereby enabling intelligent hard platen inspection work."}, {"label": 1, "content": "In this article, we propose a two-frequency subtraction technique that can effectively reduce energy leakage in Fourier spectrum. The approach involves determining the two strongest frequency components by calculating the periodogram over an interval where they will not interfere with each other. With the main components accurately subtracted from the original signal, identification of weaker components becomes easier and more precise. By minimizing energy leakage from the main components, statistical error is reduced to a significant extent compared to the FFT method. Furthermore, the subtraction technique is proven to be robust for signals with fluctuating amplitude or frequency."}, {"label": 0, "content": "Estimation of pulse rate from photoplethysmogram (PPG) signals has led to a breakthrough in a smart wristband technology. This study introduces a method for estimating heart rate recovery (HRR) using a custom-made wrist-worn device, capable of acquiring instantaneous pulse rate, as well as a consumer smart wristband, which provides pulse rate at intervals of 5 s or longer. The feasibility to estimate HRR parameters using the PPG-based devices was assessed by comparing to the synchronously acquired reference electrocardiogram. Three HRR parameters were studied on pulse rate data, obtained from 22 healthy participants, instructed to perform standardized stair climbing test. Study findings show that HRR parameters, estimated using the wrist-worn device, are associated with twice lower absolute error compared to the consumer smart wristband, emphasizing the importance of an instantaneous pulse rate to ensure a sufficiently accurate parameter estimation."}, {"label": 1, "content": "Improving optimization efficiency in multi-objective multi-variable community microgrid optimization has always been a challenge due to poor efficiency and long running time of existent algorithms. In this regard, a new layered optimization algorithm based on NSGA-II is introduced. The algorithm adopts the structural feature of community microgrids and a multi-agent system concept in the optimization process to decompose complex microgrid optimization into several household optimizations of a smaller scale and one central microgrid optimization. \n\nThe household optimization is optimized first, and then the central microgrid optimization is approached next based on the Pareto solution set of household operation problems to obtain the optimal operation mode. The results of the simulation show that the proposed strategy is effective in improving optimization efficiency."}, {"label": 1, "content": "Despite the impressive progress made in face recognition systems, low-resolution face recognition remains a challenging task. This is especially true when the faces are captured under non-ideal conditions, such as in surveillance-based applications, where low quality images are prevalent due to factors such as blur, non-uniform lighting, and non-frontal face pose. This paper analyzes the techniques used in face recognition and presents a comprehensive analysis of the experimental results for two important applications in real surveillance scenarios. The study provides practical methods to handle low-quality face images, and shows promising results in both cases. \n\nThe contributions of this study are threefold. First, super-resolution methods are evaluated for low-resolution face recognition, which is a critical issue in image restoration. Second, face re-identification is studied on various public face datasets, including real surveillance and low-resolution subsets of large-scale datasets. Baseline results for several deep learning-based approaches are presented, and these results are improved by introducing a generative adversarial network pre-training approach and fully convolutional architecture. Finally, a state-of-the-art supervised discriminative learning approach is used to explore low-resolution face identification. The evaluations are conducted on challenging portions of the SCface and UCCSface datasets, demonstrating the effectiveness of the proposed methods.\n\nIn conclusion, this study addresses the challenging task of low-resolution face recognition in wild conditions, focusing on practical solutions for real surveillance applications. The results illustrate the importance of exploring various techniques, such as super-resolution and deep learning-based approaches, to yield promising performance in this domain. The findings pave the way for more research in this area, with the ultimate goal of improving the accuracy and reliability of low-resolution face recognition."}, {"label": 0, "content": "Local matching approaches are still common tools in real-time applications. Mismatch is a common situation in stereo vision, especially in local approaches. In this paper, we propose a truncated majority voting method (TMVM) to discriminate and reduce mismatches for local matching approaches in stereo. Experiments on Middlebury benchmark show that mismatches can be discriminated and reduced correctly by deploying the proposed method without losing real-time properties."}, {"label": 0, "content": "In this paper, we propose a progressive spectral mapping learning algorithm for throat microphone (TM) speech enhancement. Unlike previous full-band spectra mapping algorithms, this algorithm divides the spectra mapping from TM speech to Air-conducted (AC) speech into two tasks, one is the voice conversion task, and the other is the artificial bandwidth extension task. Long short-term memory recurrent neural network (LSTM-RNN) is further deployed as the mapping model. Objective evaluation results show that the TM speech quality is improved when compared with conventional full-band spectra mapping framework and DNN-based mapping model."}, {"label": 0, "content": "The model of reactive power optimization including interval uncertainty (RPOIU) is used to make a voltage control strategy for ensuring the interval state variables (including load voltages and reactive power generation) of the power grid within their safe operating limits, under interval active power generation and power load demand input data. To solve the RPOIU model, this paper defined security limits, and then switches the RPOIU model to two deterministic reactive power optimization models, whose constraints bound limits are the security limits. By solving the deterministic models through the interior point method, a voltage control strategy is obtained as the solution of the RPOIU model. The results obtained by the proposed method are compared with the linear approximation method, which is a previously proposed effective method for solving the RPOIU model, and simulation results and analysis demonstrate the advantages, effectiveness, and good applicability of the proposed method."}, {"label": 0, "content": "This paper presents a dynamic identification method for abnormal state detection in low signal-to-noise ratio (SNR) environment based on spiked population model which inspires from random matrix theory for developing the theory and method of grid situation awareness based on big data technology. It firstly constructs a data source matrix and obtains a moving split-window matrix and its standard matrix, then acquiring the sample covariance matrix. The global SNR estimation is performed using the classical spectral estimation method corrected by the Kaiser window function, from which a corresponding dynamic threshold is derived. In this way, the situation awareness and early warning for interconnected power systems could be achieved by calculating maximum eigenvalue of the sample covariance matrix and the dynamic threshold to check violation. Utilizing MATLAB\u00ae software, the case studies have been carried on an IEEE 50-machine system, involving two main working conditions such as abnormal load mutation and short circuit fault. The results show that the proposed methodology has the advantage of higher noise resistance in comparison with the traditional mean spectral radius based method and preliminarily verifies that it would be robust under incomplete information."}, {"label": 0, "content": "This paper presents an iterative approach to jointly estimate the states in combined heat and power systems (CHPS). The node method is used to address the temperature quasi-dynamics in the district heating system (DHS), resulting in a dynamic state estimation (DSE) model. An alternating estimation strategy is employed to effectively handle the complicated time-delay constraints of temperature in the computation of DSE. Case studies are conducted on two CHPS test systems to verify the effectiveness of the DSE and the alternating approach. Simulation results show that the DSE in CHPS outperforms the static state estimation and the separate state estimation in individual energy systems in terms of accuracy."}, {"label": 1, "content": "The high penetration of volatile renewable energy sources creates uncertainties in the power system and poses significant challenges to transmission network planning (TNP). Traditionally, TNP methods only consider one type of mathematical model for dealing with different uncertainties. However, different uncertainties may have varying mathematical features that require distinct mathematical models to represent them.\n\nTo address this issue, a TNP model is proposed that takes into account the interval uncertainty model of renewable energy generation and the fuzzy uncertainty model of predicted load. This model is based on expanded fuzzy chance constrained programming. By using the features of the planning model, it can be transferred to a robust TNP model that considers the interval uncertainty model of renewable energy generation and predicted load. This reduces the calculation burden for solving the planning model.\n\nThe effectiveness and adaptability of the proposed TNP model were verified through analyses on the modified IEEE RTS 24-bus system and a 231-bus system. The results of the analyses demonstrate the efficacy of the proposed model in dealing with uncertainties in the power system and its potential to improve transmission network planning."}, {"label": 1, "content": "We are considering a problem where communication and state estimation must be jointly solved on a Gaussian multiple access channel. The state process is assumed to be IID Gaussian and is known to both transmitters without causality. The receiver must decode messages from the transmitters and estimate the state process within a prescribed squared error distortion.\n\nOur objective is to determine the optimal sum-rate versus distortion performance, and we provide a comprehensive characterization of this performance."}, {"label": 0, "content": "Stochastic dynamics is a research topic for railway vehicles involving a wide range of randomness or uncertainty. However, the modeling and calculation of stochastic dynamic systems are often high-cost and low-efficiency. Neural network is an effective machine learning tool driven by data; this paper devotes to bridge the gap between neural networks and stochastic dynamics and to attain proper uses of this technique in railway vehicles. The mapping capability of neural networks for various stochastic suspension dynamics is validated by the proposed random repetition scheme. And this powerful computational tool is applied to predict the dynamic performance of high-speed trains in service instead of dynamics calculations; a typical case is analyzed to emphasize the advantage of the dynamic performance evaluation considering the coupling of various factors that it can enhance the security and reliability by attaining prognostic and health management and condition-based maintenance."}, {"label": 0, "content": "As the global population soars from today's 7.3 billion to an estimated 10 billion by 2050, the demand for Food, Energy and Water (FEW) is expected to more than double. Such an increase in population and consequently, in the demand for FEW resources will undoubtedly be a great challenge for humankind. A challenge that will be exacerbated by the need for humankind to meet the greater demand for resources with a smaller ecological footprint. This paper is proposing a system developed to optimize the use of water, energy, fertilizers for agricultural crops as a solution to this great challenge. It is an automated smart irrigation system that uses real time data from wireless sensor networks to schedule an irrigation. The test-bed consists of a wireless network monitoring soil moisture, temperature, solar radiation, humidity, and fertilizer sensors embedded in the root area of the crops and around the test-bed. Wireless sensor data transmission and acquisition is managed by an Access Point (AP) using ZigBee protocol. An algorithm was established based on threshold values of temperature and soil moisture automated into a programmable micro-controller to control irrigation time. The system's energy demand is completely supplied by a solar Photo-voltaic (PV) panel supplemented with an energy storage unit. The experimental data obtained from this prototype will be modeled and optimized to investigate food production profile as a function of energy and water consumption. It will also attempt to understand the effect of extreme weather conditions on food production. This holistic approach will explore the nexus between water and energy resources, and crop yield for several essential crops in an attempt to design a more sustainable method to meet the forecasted surge in demand."}, {"label": 1, "content": "Stochastic dependency is a prevalent feature in many complex systems, and ignoring it during reliability modeling can lead to inaccurate results. In this paper, we aim to analyze the stochastic dependency of degradation in such systems. For a two-component system, a degradation interaction model is formulated, taking into account the non-linear Wiener process and Copula function. Through this model, we describe the dependence between the degradations of the two components. With this analysis, we are able to derive the reliability functions of the components and the entire system. To estimate the parameters of the model, we use a suitable approach. We validate and illustrate our methodology with a numerical example involving fatigue crack development."}, {"label": 0, "content": "This paper proposes a simplified simulation and modelling tech for microgrid. This method can save simulation time and computational memory, compared with the simulation of the detail model. Firstly, the grid-tied inverter, the integrated load, and the nonlinear load in microgrid are analyzed and modeled. Next, their models are simplified as Th\u00e9venin-Norton equivalent models. Moreover, the consistency between hardware equivalent model and control equivalent model of power electronics devices is proved. Finally, the correctness of the simplified simulation and modelling tech is confirmed during voltage sag in the improved IEEE 13 Node Test Feeder system in MATLAB/SIMULINK. The results show that the outputs characteristics of the simplified models are consistent with those of their detailed models."}, {"label": 0, "content": "The Unified Power Flow Controller (UPFC) is the most complex and powerful FACTS device, which can provide full or independent control of line transmission parameters, such as voltage, line impedance and phase angle. In this paper, steady-state model of unified power flow controller (UPFC) and power flow algorithm of power system with UPFC were studied. The parallel and series sides of UPFC were equivalent to separate power injections, and an alternating iteration algorithm of power flow for power system with UPFC was presented. In order to improve the convergence of the algorithm, this paper analyzes the impact of UPFC's various control strategies on the convergence of power flow calculations, and proroses a control strategy for converting constant power control to constant variable control during the iterative process. Based on the actual project of Nanjing Western UPFC, the correctness and effectiveness of the model and algorithm are verified."}, {"label": 0, "content": "In this paper, we propose a fast and efficient multitemporal despeckling method. The key idea of the proposed approach is the use of the ratio image, provided by the ratio between an image and the temporal mean of the stack. This ratio image is easier to denoise than a single image thanks to its improved stationarity. Besides, temporally stable thin structures are well preserved thanks to the multitemporal mean. The proposed approach can be divided into three steps: 1) estimation of a \u201csuperimage\u201d by temporal averaging and possibly spatial denoising; 2) denoising of the ratio between the noisy image of interest and the \u201csuperimage\u201d; and 3) computation of the denoised image by remultiplying the denoised ratio by the \u201csuperimage.\u201d Because of the improved spatial stationarity of the ratio images, denoising these ratio images with a speckle-reduction method is more effective than denoising images from the original multitemporal stack. The amount of data that is jointly processed is also reduced compared to other methods through the use of the \u201csuperimage\u201d that sums up the temporal stack. The comparison with several state-of-the-art reference methods shows better results numerically (peak signal-noise-ratio and structure similarity index) as well as visually on simulated and synthetic aperture radar (SAR) time series. The proposed ratio-based denoising framework successfully extends single-image SAR denoising methods to time series by exploiting the persistence of many geometrical structures."}, {"label": 0, "content": "The security of inertial navigation system (INS) is the key factor to determine the safety of air vehicle. Due to the INS contains some components of dynamic logic gate, which makes its work process presenting a series of dynamic characteristics. When using dynamic fault tree to analyze the security of INS, most of the traditional approaches are based on Markov chain and Monte-Carlo simulation. However, those methods have curse of dimensionality problem and/or require long computation time particularly when results with a high degree of accuracy are desired. In this paper, an analytical method is proposed based on sequential binary decision diagrams (SBDD) for combinatorial reliability analysis of dynamic systems. Firstly, the dynamic fault tree (DFT) model of collapse of inertial platform is built based on safety analysis used DFT. Secondly, the DFT model is modularized into independent static sub-trees and dynamic sub-trees, and then the exact expression of the occurrence probability for each minimal cut-set is determined by the static fault tree analysis method or SBDD method. Finally, the possibility of collapse of inertial platform in all its life is obtained after comprehensive analysis, and the result is compared with that based on Markov chain and Monte-Carlo simulation. The result shows that the proposed approach can generate exact possibility of collapse of inertial platform. In addition, the system model of sequential binary decision diagrams and possibility evaluation expression are reusable for the reliability analysis with different component failure parameters, the method is more applicable and more effective, and the method of calculating occurrence probability of top event for a dynamic fault tree is expanded."}, {"label": 0, "content": "Understanding the dynamics of a power system requires that information be presented in a meaningful way. Large-scale modal results are presented for a large interconnected power system using visualization methods to reveal the underlying oscillations in the system. Visualization tools are used to capture the quality of mode estimation among several bus signals, identify different modal interactions existing in the system and visualize modal power flows for tracking sources of grid oscillations. The use of wide-area visualization in a synthetic large interconnected power grid is used to uncover critical information about the dynamic state of the system, which would have otherwise, not been captured from a graphical plot of the time-varying signals."}, {"label": 1, "content": "Beamforming using conventional array processing techniques involves linear and additive processing to combine signals from different array elements. However, a recent study proposed a multiplicative processing technique for super-resolution beamforming using sensor arrays. This technique, derived from linear array processing concepts, was found to replicate the performance of larger aperture arrays.\n\nIn this paper, the proposed method for direction-of-arrival estimation was validated using experimental measurements from an acoustic sensor array. The results were compared with linear processing of measurements from much larger aperture arrays. Interestingly, the analysis showed that the multiplicative processing of measurements from smaller apertures performed as well as linear processing of measurements from larger apertures.\n\nOverall, the study suggests that the multiplicative processing technique can lead to improved beamforming performance and may be particularly useful for applications where physical constraints limit the size of the array."}, {"label": 1, "content": "Covert timing channels have emerged as a viable alternative for transmitting confidential information in an untrusted IoT environment, owing to various security threats. This article aims to investigate the susceptibility of IoT to covert timing channels over mobile networks. A system model for a covert timing channel designed for IoT is presented and the applicability of traditional covert timing channels based on inter-packet delays to 4G/5G networks is analyzed. Multiple construction approaches for covert timing channels, such as packet-reordering-based, rateswitching-based, packet-loss-based, retransmission-based, and scheduling-based timing channels, are examined to showcase the feasibility of building such channels for IoT. Additionally, several methods for detecting and preventing covert timing channels in IoT are discussed in this article."}, {"label": 1, "content": "Emergency evacuation simulations are crucial to minimize the risk of personnel deaths and injuries. To ensure the safe evacuation of people in underground tunnels, a three-dimensional emergency evacuation simulation system has been proposed. The system is designed to consider various scenarios encountered by people in the tunnels and visualize their behavior accordingly. The experiment involves the development of a three-dimensional model for generating evacuation paths, visualizing simulation outputs, evaluating the results and performing a detailed analysis. The experimental results demonstrate that this system is effective in simulating tunnel evacuation and can be successfully used for emergency evacuation guidance during disasters as well as evacuation simulation drills before a disaster."}, {"label": 1, "content": "This paper presents a novel approach to track a walking person and automatically capture a frontal photo using an unmanned aerial vehicle (UAV). The proposed method consists of three main parts, namely person detection and recognition, face detection and feature points localization, and vision-based UAV control. The person tracking module employs the YOLOv3 deep neural network for detecting the target person, and utilizes the Locality-constrained Linear Coding (LLC) algorithm for matching the target person. The facial recognition module relies on the Multi-task Cascaded Convolutional Neural Networks (MTCNN) for detecting frontal faces. The UAV control is based on the information gathered from the two previous modules, which allows the UAV to fly around the target person and obtain their frontal face image. The results of outdoor experiments using a Parrot Bebop2 drone show the effectiveness and practicality of this approach."}, {"label": 1, "content": "The automated deployment of cloud applications is crucial in today's fast-paced technological landscape. As a result, several deployment automation technologies have been developed, which allow applications to be deployed automatically by processing deployment models. However, creating such deployment models requires a great deal of expertise about the technologies and cloud providers that are being used. This is especially true for the technical implementation of conceptual architectural decisions.\n\nFurthermore, deployment models have to be adapted manually if there are changes in architectural decisions or if technologies need to be replaced. This process is not only time-consuming but can also be prone to errors and requires additional expertise. To address this issue, we propose a meta-model for Pattern-based Deployment Models.\n\nOur meta-model allows users to use cloud patterns as generic modeling elements directly in deployment models, which makes them vendor- and technology-agnostic. This means that instead of specifying concrete technologies, providers, and configurations, users can model only the abstract concepts represented by patterns that must be followed during deployment.\n\nMoreover, our approach enables models to be automatically refined to executable deployment models. To validate the practical feasibility of our approach, we present a prototype based on the TOSCA standard and a case study.\n\nIn conclusion, our meta-model for Pattern-based Deployment Models presents a promising approach for simplifying and automating the deployment of cloud applications. By using cloud patterns as generic modeling elements, our approach enables users to create deployment models that are vendor- and technology-agnostic. This helps to reduce the time and expertise required to create and adapt deployment models, ultimately facilitating more efficient and error-free application deployments."}, {"label": 1, "content": "The purpose of this paper is to introduce an intelligent event-driven Electrocardiogram (ECG) processing module that provides a computationally efficient solution for diagnosing cardiac diseases. The method proposed in this paper includes an event-driven A/D converter (EDADC) that acquires the signal. The output of EDADC is then passed on to the activity selection and interpolation blocks, which help to focus on important signal parts and resample the signal uniformly by using the Simplified Linear Interpolator. The signal is then de-noised using the autoregressive (AR) method to extract classifiable features from the signal. Afterward, the output is classified using robust classification techniques such as support vector machines (SVMs), K-Nearest Neighbor (KNN), and Artificial Neural Network (ANN). \n\nThe event-driven feature of this system enables the system processing load to adapt to signal temporal variations. Such a feature allows for a significant reduction in processing activity, leading to lower power consumption compared to traditional systems. A comparison of classifier performance in terms of accuracy is also made. The results indicate that the proposed system has potential for automatic diagnosis of cardiac diseases."}, {"label": 0, "content": "We are developing a new natural language processing (NLP) method to facilitate analysis of text corpora that describe long-term recovery. The aim of the method is to allow users to measure the degree that user-specified propositions about potential issues are embodied within the corpora, serving as a proxy for the disaster recovery process. The presented method employs a statistical syntax-based semantic matching model and was trained on a standard, publicly available training dataset. We applied the NLP method to a news story corpus that describes the recovery of Christchurch, New Zealand after the 2010-2011 Canterbury earthquake sequence. We used the model to compute semantic measurements of multiple potential recovery issues as expressed in the Christchurch news corpus that span 2011 to 2016. We evaluated method outputs through a user study involving twenty professional emergency managers. User study results show that the model can be effective when applied to a disaster-related news corpus. 85% of study participants were interested in a way to measure recovery issue propositions in news or other corpora. We are encouraged by the potential for future applications of our NLP method for after-action learning, recovery decision making, and disaster research."}, {"label": 0, "content": "This paper focuses on target localization problem in a multi-station redundancy system which finds broad applications in sonar, radar, wireless sensor networks, and location based service. Previous solutions can only be applied to the minimum system, such as TOA method with three sensors or need matrix inversion. To solve this problem, a simple closed-form solution for a multi-station redundancy localization system is proposed by using the estimation variance as the weighting coefficient to compute an average of each group's localization result. The proposed method, with simple algebraic solution, requires no matrix inversion and can be used for low-cost hardware devices. The method is derived in TOA solution. It can also be extended to other locating technologies. Numerical examples are provided to illustrate the performance of the proposed method in root-mean-square error. The positioning accuracy of the proposed method is close to Cram\u00e9r-Rao low bound."}, {"label": 0, "content": "Currently, movie trailers are edited using various methods. However, the length of each trailer is at most several minutes, and the scenes used for editing and the types of effects are limited because a trailer is created for a certain target audience. Therefore, it is difficult to edit a trailer that caters to the different preferences of various users. Moreover, potential audience may be lost if the trailer is not enticing enough. To solve this problem, we define seven editing biases that occur when movies are summarized and edited into trailers. We investigate whether these biases can be used to generate a movie trailer catering to various viewer preference."}, {"label": 0, "content": "There are many fuzzy problems in the power system, such as fault diagnosis, condition assessment, etc. Despite the rapid development of science and technology, uncertain and incomplete information still exists to varying degrees. These problems of ten involve multiple factors. It is very important to choose a suitable method to deal with such problems. In this paper, a new uncertainty algorithm is proposed based on fuzzy theory, rough set theory and evidence theory to solve the problem of multi-index uncertainty in power system. This algorithm can process inaccurate data and then analyze it. Taking the transformer state evaluation as an example, the application of this algorithm is illustrated."}, {"label": 0, "content": "Currently, there is a lack of tools for real validation of 5G scenarios. The increasing traffic demand of 5G networks is pushing network operators to find new cost-efficient solutions. The selected solution is a multi-tenancy approach that, together with user mobility will impose some architectural changes. This approach increases service dynamism making it necessary to have tools that provide these new capabilities to be able to validate each development. This work presents a novel experimentation framework for the emulation of 5G scenarios providing them with real-time user mobility and multi-tenancy. The functionality of this novel framework has been validated through different experiments."}, {"label": 0, "content": "Usually, MMS packets are used to encapsulate the information of protection settings and TCP is the suite of protocols used to transmit the MMS packets in smart substations. Since TCP's own mechanism, such as vulnerability to Head-of-Line (HoL) blocking attacks, the protection settings of TCP transmission often fail to meet the high reliability requirements of important power information. A new method of SCTP protection setting and transmission based on IEC61850 intelligent substation information structure is proposed in this paper. To realize seamless handover of double MMS network during failure time, the SCTP's multi host function is optimized. To avoid HoL blocking attacks, the multi stream mechanism is used. This paper further studies the realization method and implementation process of SCTP transmission with protection setting in detail."}, {"label": 0, "content": "At present, the improved social network analysis methods are mainly based on homogeneous networks. However, the actual social networks are heterogeneous in nature. Heterogeneous social networks can better reflect the composition of the system and the associated relationships than the homogeneous social network model. The previously proposed ranking clustering provides a new idea, however, the algorithm only completes the clustering results of specific target types and cannot cover the complete heterogeneous network type. By introducing collaborative clustering algorithm and combining it with ranking, we propose a ranking collaborative clustering algorithm. Firstly, based on ranking clustering, the ranking distribution matrix is obtained, and then collaborative clustering is used to complete different types of clustering, and the relationship between different types and the same types is fully utilized. The experimental results on the real twitter and foursquare datasets show that the ranking collaborative clustering algorithm has better performance on the modularity compared to the ranking clustering."}, {"label": 0, "content": "Autism spectrum disorder(ASD) is a kind of developmental disorder which attracted a lot of attention of researchers for its urgency and pervasiveness. The diagnosis and intervention of ASD is still complicated and hard to handle. The rapid development of technology has brought new methods to the auxiliary diagnosis of ASD, such as face detection, gaze estimation, action recognition, etc. The paper proposed a preliminary visual system for assistant diagnosis of ASD in a core clinical testing scenario-response to name. The eye center localization and gaze estimation were applied to measure the responses of the subject. The purpose of this paper is analyzing the feasibility of this system, and optimizing the sensing structure and the evaluation indicator."}, {"label": 0, "content": "Sparse iterative covariance-based estimation (SPICE) method is a computational efficient sparse method for direction of arrival (DOA) estimation but has a poor performance in resolution and noise immunity. The high-order cumulant can extend the array aperture and reduce the Gaussian noise. Therefore, this paper proposed an improved SPICE based on fourth-order cumulant, which shares the same features of SPICE but has higher resolution and outperforms in low SNR case. Moreover, its computational cost is comparatively low by distilling the un-redundant data of uniform linear array. Simulations were conducted to validate and evaluate the proposed method."}, {"label": 1, "content": "We propose sparse-complementary convolution (SC-Conv) to optimize the utilization of convolution neural networks (CNNs). Using SC-Conv, CNN models achieve higher accuracy compared to regular convolution while using similar computations and parameters. SC-Conv applies two deterministic sparse kernels, where one of the kernels complements the other in either spatial or channel domain or both. The deterministic sparsity increases computational speed theoretically and practically. Moreover, with the complementary characteristic, SC-Conv maintains the same receptive field as with conventional convolution. Our approach reuses modern network architectures such as ResNet and DenseNet. During testing, SC-Conv improves top-1 classification accuracy on ImageNet by 0.6 points for ResNet-101, and we maintain the same model complexity. Additionally, SC-Conv outperforms recent sparse networks by 1.3 points at top-1 accuracy on ImageNet. After integrating SC-Conv with the sparse network, we further enhance accuracy by 1.8 points at similar FLOPs and parameters. Overall, SC-Conv provides a straightforward yet insightful approach for CNN model optimization."}, {"label": 1, "content": "This study focuses on the problem of diagnosing and controlling the technical state (TS) of various industrial equipment. The significance of this problem is driven by the increasing demand for a reliable industrial facility, the aging of existing equipment, and technological complexities that make it challenging to systematize diagnostic information used for making decisions. Therefore, there is a need to develop a universal automated information system based on a method for diagnosing the TS of different industrial objects. Numerous factors affect the TS of equipment, prompting the use of a technical state index (TSI) as a complex indicator whose calculation method relies on the mathematical model of fuzzy sets. The TS diagnosis depends on a system of indicators whose configuration is linked to the equipment's structure. This work highlights the development of a decision support system used for managing complex technical and software objects. It also presents a block diagram outlining the system's algorithm and describes the functionality and interface of the software developed."}, {"label": 1, "content": "This article outlines an innovating approach to combine traditional image processing techniques with deep learning to enhance melanoma classification accuracy. By utilizing three biologically inspired image processing modules and clinical information module, the conventional image processing arm can detect lesion features comparable to clinical dermoscopy information, such as atypical pigment network, color distribution, and blood vessels, while providing patient age, gender, lesion location, size, and patient history as clinical information. In parallel, the deep learning arm transfers knowledge using a repurposed ResNet-50 network to predict melanoma classification probability. \n\nThe classification scores of each individual module from both processing arms are then fused using logistic regression, resulting in an overall melanoma probability classification. Using cross-validation, the fusion technique demonstrates a classification accuracy of 0.94, which is higher than the deep learning-based classifier alone, with an AUC of 0.87, and the conventional image processing-based classifier, with an AUC of 0.90. \n\nFurther exploration and investigation into the fusion of traditional image processing techniques and deep learning is necessary to strengthen the accuracy of melanoma classification."}, {"label": 0, "content": "Gaming as a popular system has recently expanded the associated services, by stepping into live streaming services. Live gaming video streaming is not only limited to cloud gaming services, such as Geforce Now, but also include passive streaming, where the players' gameplay is streamed both live and ondemand over services such as Twitch.tv and YouTubeGaming. So far, in terms of gaming video quality assessment, typical video quality assessment methods have been used. However, their performance remains quite unsatisfactory. In this paper, we present a new No Reference (NR) gaming video quality metric called NR-GVQM with performance comparable to state-of-the-art Full Reference (FR) metrics. NR-GVQM is designed by training a Support Vector Regression (SVR) with the Gaussian kernel using nine frame-level indexes such as naturalness and blockiness as input features and Video Multimethod Assessment Fusion (VMAF) scores as the ground truth. Our results based on a publicly available dataset of gaming videos are shown to have a correlation score of 0.98 with VMAF and 0.89 with MOS scores. We further present two approaches to reduce computational complexity."}, {"label": 1, "content": "Industrial robots must accurately manipulate non-rigid objects, such as cables, which present considerable challenges due to their nonlinear, timedependent and location-dependent behavior. Controlling these objects requires real-time capable simulations that can run in cycle-time and highly precise simulations for path planning. To address these requirements, we present a common simulation environment wrapped in a container that includes all dependencies and is easy to set up and orchestrate. Automated code generation is utilized to establish standardized communication and virtualization within a Docker container, simplifying the simulation setup process for the user. Containerization is shown to be advantageous compared to running a simulation natively, as demonstrated with a sample robot simulation under heavy CPU load."}, {"label": 1, "content": "This paper aims to enhance the precision of Human Action Recognition (HAR) by fusing depth and inertial sensor data. The proposed approach consists of transforming depth data into Sequential Front view Images (SFI) and training a pre-trained AlexNet on these images. Subsequently, inertial data is converted to Signal Images (SI), and a separate convolutional neural network (CNN) is trained on these images. Finally, the learned features from both CNNs are fused to create a shared feature layer, which is fed to the classifier. Two classifiers, namely Support Vector Machines (SVM) and softmax classifier were experimented, and their performances were compared. Furthermore, the accuracies of each modality, depth data alone, and sensor data alone were calculated and compared with fusion-based accuracies to demonstrate the superiority of multimodal fusion methods. The experimental results on UTD-MHAD and Kinect 2D datasets demonstrate that the proposed method outperforms other recently proposed visual-inertial action recognition methods and yields state-of-the-art results."}, {"label": 0, "content": "Humans seamlessly perceive a massive amount of information while observing a scene. Though humans recognize real-world scenes easily and accurately but its not the same for computers due to scene images variability, ambiguity, and diverse illumination and scale conditions. Scene classification is a fundamental problem which provides contextual information to guide other processes, such as browsing, content-based image retrieval and object recognition. A baseline model based on traditional bag of words model is built to better evaluate the proposed solution. Model based on the idea of fine to coarse category mappings is proposed, whose information is combined with the fusion of feature descriptors resulting in a single feature representation. This additional information enhances performance by exploiting hierarchical relationship among the scene categories. Effectiveness of the proposed approach is validated using different evaluation metrics. Proposed model performs considerably better compared to the given baseline as well as several state-of-the-art methods. Proposed framework ensures appropriate balance between time and accuracy of the model"}, {"label": 0, "content": "With the improvement of industrialization, people's requirements for power supply quality also increase year by year. Therefore, how to ensure the normal supply of electricity has become a very concerned issue for power maintenance personnel. In this paper, we use the image captured by the cable tunnel inspection system to locate the cable connector in the image and map it to the infrared image based on the convolution neural network method. Based on the Analysis of the temperature of the cable connector in time, the monitoring system is able to make an alarm on the abnormal situation which can maintain the safety of power supply and extend the service life of the cable. Aiming at the small number of sample images collected, a transfer learning method is adopted, which reduces the training intensity and ensures a better positioning and recognition effect."}, {"label": 0, "content": "The revolution in information technology has lead to the availability of vast and varied collections of music on the digital platform. With the widespread use of smartphones and other personal digital devices, there has been a growing interest in accessing music, based on its various characteristics using information retrieval technologies. But the unavailability of meta-tags or annotations has lead to the need for developing technologies to automatically extract relevant properties of music from the audio. Automatically identifying meta-data from audio like, artist information - especially instrument artists - is a very tough task, even for humans. In this paper, automatic identification of percussion artist is attempted on mridangam audio from Carnatic music concert using probabilistic models. Unlike speaker identification where the voice of the speaker is unique, the timbre of the percussion instruments will be more or less the same across instruments. The distinctive characteristics of a musician can be found in the style of him/her playing the instrument. A single Gaussian mixture model (GMM) is built across all musician data using tonic normalized cent-filterbank-cepstral-coefficients (CFCC) features. Each artist's percussion audio is converted to a sequence of GMM tokens. Sub-string matching between train and test data is used to identify the musician. The performance is evaluated on a dataset of 10 mridangam artist and could identify the artist with an accuracy of 72.5%."}, {"label": 1, "content": "Moving-object tracking is essential for autonomous driving systems and driving assistance systems in the field of mobile robotics and vehicle automation domains. Accurate recognition of objects is crucial for predicting and avoiding collisions. In this study, we propose a method for recognizing vehicles (cars and bicyclists) and pedestrians using multilayer lidar (3D lidar). \n\nThe lidar data is first clustered, and an eight-dimensional feature set is extracted from each of the clustered lidar data points. The features include distance from the lidar, velocity, object size, number of lidar-measurement points, and distribution of reflection intensities. A multiclass support vector machine is then applied to classify the objects into cars, bicyclists, and pedestrians based on these features. \n\nWe compared the proposed method with a method based on a random forest algorithm and a conventional 26-dimensional feature-based method using \u201cThe Stanford Track Collection\u201d dataset. Our results demonstrate that the proposed method offers improved recognition accuracy and processing time over the other methods, making it suitable for use in low computational environments. \n\nOverall, our study shows the potential of multilayer lidar in recognizing moving objects accurately, thereby enhancing the safety and efficacy of autonomous driving systems and driving assistance systems."}, {"label": 0, "content": "In our modern world, almost every aspect of our lives is directly affected by the revolution of digitalization. Everything we rely on nowadays has a computer chip and a software controlling it whether it is a mobile phone, a car, a computer, or any other electronic device. These individual life affecting technologies are composing of cyber physical systems communicating via the Internet of Things (IoT). This paper addresses the development of Intrusion Detection Schemes for IoT. The increasing risks in IoT infrastructure compromise mainly against IPv6 over Low Power Wireless Personal Area Networks (6LowPAN) are a concern. The data breaches and data manipulation are discussed by identifying the most recent hacker methodologies and tools and analyzing what the breach will affect inside an IoT network. After discussing the \"Problems\", the paper will discuss the existing intrusion detection schemes with their limitations and will propose a more effective solution."}, {"label": 0, "content": "Recently, the Internet of Things (IoT) has penetrated many aspects of the physical world to realize different applications. Through IoT, these applications generate, exchange, aggregate, and analyze a vast amount of security-critical and privacy- sensitive data, which makes them attractive targets of attacks. Therefore, it is rather necessary for IoT systems to be equipped with the ability to resist security and privacy risks when fulfilling the desired functional requirements and services. To achieve these goals, there are many new challenges for IoT to implement privacy preserving data manipulation. First, data analysts need to process privacy-sensitive data to extract the expected information without privacy disclosure. In addition, many privacy related factors, including privacy valuation and risk assessment, affect sensitive and private data trading between data owners and requesters. Moreover, the data owners' security behavior also plays an important role in privacy protection in IoT applications. Concerning these issues, this article introduces and surveys privacy preserving techniques in the processes of data aggregation, trading, and analysis: the balance between data analysis and privacy preservation from the data analysts' perspective, secure data trading from the perspective of data owners and requesters, and secure private data aggregation from the data owners' perspective."}, {"label": 0, "content": "The environmental problems and associated issues had been a source of worry for the world. Emerge of IoT (Internet of things) and the step towards the smart approach such as smart cities, smart buildings, and smart grid have really posed the successful implementation of IoT. The success is only possible in the real sense when the problematic issues can be addressed. This paper proposes an Internet of Things Technology based protection and monitoring of environment of a poultry house. The proposed software based hardware is capable of monitoring the environment related parameters such as air temperature, air humidity, O2, CO2 level of concentration and NH3 concentration. The wireless sensor is responsible for the effective data collection of the described parameters and also these are source coordination and control. The hardware is implemented successfully at different sites within the poultry shed. The experimental setup was found very effective and accurate. This scheme will earn a safe environment and profit to the poultry industry."}, {"label": 0, "content": "The handling of non-rigid objects, such as cables, with industrial robots is characterized by nonlinear, timedependent and location-dependent equations for the object's behavior. To manipulate them in a desired way, real-time capable simulations for control that are able to run in cycle-time are required as well as highly accurate and detailed simulations for path planning. We introduce a common simulation environment that is wrapped in a container with the advantages of including all dependencies and being easy to set up and orchestrate. Automated code generation is used to set up a standardized communication and virtualization in a Docker container such that only the simulation itself has to be provided by a user. The advantage of containerization, compared to the simulation running natively, is finally demonstrated with a sample robot simulation under heavy CPU load."}, {"label": 1, "content": "We propose a top-down strategy to facilitate the formation control of multiagent systems, which includes heterogeneous agents. The methodology is based on eigenstructure assignment, whereby a state feedback control is generated to assign closed-loop eigenvectors and eigenvalues to achieve scalable formations on the plane. The relationship between the inter-agent communication topology and eigenvalues/eigenvectors is well characterized, and we design sparse topologies for local implementation of the synthesized control by individual agents. A hierarchical synthesis procedure is developed to enhance computational efficiency. Additionally, we expand the proposed methodology to attain fixed-size formation and circular motion, and simulation examples are provided to demonstrate the efficacy of the approach."}, {"label": 1, "content": "This article presents an algorithm for creating a training set for artificial neural networks in the context of image segmentation. One unique aspect of this algorithm is that it only requires one image for segmentation. The segmentation is accomplished using a three-layer perceptron and the main method utilized is region growing. The neural network makes a determination as to whether a pixel should be included in a specific region or not. To generate the training set, impulse noise is incorporated, as pixels affected by noise are not part of the same region. This suggested approach has been successfully tested through both automatic and interactive computer experiments."}, {"label": 1, "content": "This paper presents a novel method to detect equipment anomalies in low-light conditions inside cable tunnels using convolution neural network (CNN). The proposed methodology involves incorporating Sobel operator on the gray image to enhance edge-preprocessing effect, followed by training the CNN. The convergence criterion involves the Loss Function that considers Weight Parameter W and Bias Parameter b, and the convergence method adopted is Backpropagation, which updates the parameters iteratively to reduce the loss. The full connection layer with fast operating speed provides direct classification results for equipment status in images. Experimental analysis on internal images of Zhuhai tunnel reveals that the proposed method is suitable for the chaotic and low-light environment of tunnels, with high recognition rate for image segmentation of lighting equipment and high accuracy for classifying abnormal situation of images."}, {"label": 0, "content": "We consider networks in which each individual link is characterized by two delay parameters: a (usually very conservative) guaranteed upper bound on the worst-case delay, and an estimate of the delay that is typically encountered, across the link. Given a source and destination node on such a network and an upper bound on the end-to-end delay that can be tolerated, the objective is to determine routes they typically experience a small delay, while guaranteeing to respect the specified end-to-end upper bound under all circumstances. We formalize the problem of determining such routes as a shortest-paths problem on graphs, and derive algorithms for solving this problem optimally."}, {"label": 0, "content": "Recommender systems are engines that recommend new items to users by analyzing their preferences. The web contains a large amount of information in the form of ratings, reviews, feedback on items and other unstructured data. These details are extracted to get meaningful information of users. Collaborative filtering and content-based filtering are two common approaches being used to make recommendations. The paper aims to introduce a hybrid recommendation technique for Big Data Systems. The approach combines collaborative and content-based filtering techniques to recommend items that a user would most likely prefer. It additionally uses items ranking and classification technique for recommending the items. Moreover, social media opinion mining is added as a top-up to derive user sentiments from user's posts and become knowledgeable about users' tastes hidden within social media. A prototype has been implemented and evaluated based on the recommendation techniques."}, {"label": 1, "content": "Apache Mesos is a resource scheduler designed to provide resource sharing among multiple users in a clustered environment. Its Dominant Resource Fairness (DRF) policy ensures that resources are distributed fairly among users based on their current usage. However, multiple frameworks can cause fairness imbalances in a multi-user environment, leading to poor performance for some users. To address this issue, we developed Tromino, a policy-driven queue manager that takes into account each framework's overall resource demands and current consumption. By incorporating Dominant Share and demand awareness into scheduling, Tromino can reduce the impact of unfairness due to framework-specific configurations and unfair waiting times due to high resource demand in a pending task queue. With the proposed Demand-DRF aware policy, Tromino can significantly reduce the average waiting time of a framework, making it a highly effective solution for managing resource allocation in a multi-tenant environment."}, {"label": 1, "content": "Floods are a common natural disaster in Indonesia, making a system for predicting their arrival highly valuable for those living in areas affected by river flow. To accurately predict floods, parameters such as water level and rainfall around the river can be used. This study proposes the use of an artificial neural network, specifically the Radial Basis Function, to analyze flood prediction ability. The Radial Basis Function contains an input layer, hidden layer, and output layer. Data from water level and rainfall in Dayeuhkolot in 2015 were used for the training and testing process. The prediction results had MAPE values of 0.047% and 1.05% for water level data and 4.97% and 29.1% for rainfall data with a combination of hidden node = 35, learning rate = 0.2, and spread constant = 1.1. The maximum termination of 5000 epoch was set as the target. A highly accurate flood prediction system can be produced using this method of artificial neural network analysis."}, {"label": 0, "content": "Despite outstanding performance in image recognition, convolutional neural networks (CNNs) do not yet achieve the same impressive results on action recognition in videos. This is partially due to the inability of CNN for modeling long-range temporal structures especially those involving individual action stages that are critical to human action recognition. In this paper, we propose a novel action-stage (ActionS) emphasized spatiotemporal vector of locally aggregated descriptors (ActionS-ST-VLAD) method to aggregate informative deep features across the entire video according to adaptive video feature segmentation and adaptive segment feature sampling (AVFS-ASFS). In our ActionS-ST-VLAD encoding approach, by using AVFS-ASFS, the keyframe features are chosen and the corresponding deep features are automatically split into segments with the features in each segment belonging to a temporally coherent ActionS. Then, based on the extracted keyframe feature in each segment, a flow-guided warping technique is introduced to detect and discard redundant feature maps, while the informative ones are aggregated by using our exploited similarity weight. Furthermore, we exploit an RGBF modality to capture motion salient regions in the RGB images corresponding to action activity. Extensive experiments are conducted on four public benchmarks-HMDB51, UCF101, Kinetics, and ActivityNet for evaluation. Results show that our method is able to effectively pool useful deep features spatiotemporally, leading to the state-of-the-art performance for video-based action recognition."}, {"label": 1, "content": "The increasing popularity of vehicles equipped with high computational resources has garnered significant attention as a means of resource sharing within a new or existing cloud infrastructure. Vehicles can be consolidated into clusters to simplify management, improving the effectiveness and overall capability of a vehicular cloud. In pooling their resources, these vehicles can serve the upper layers of the network infrastructure. This paper explores the trends and state-of-the-art developments of vehicular clustering over the past five years, especially for parked vehicles in the fog computing paradigm. The findings indicate that there are two common methods of clustering - static and dynamic - with the latter being more prevalent. Challenges and issues surrounding vehicular clustering are also discussed."}, {"label": 1, "content": "Recently, many variational optical flow methods have been developed. However, these methods often lack robustness when it comes to illumination variance, and typically only consider local image relationships with respect to illumination. In order to address these limitations, a new efficient illumination-invariance total variation optical flow method has been proposed in this paper. This method is called the weighted regularization transform and optimizes the Weber's Law.\n\nOur method uses unequal probability as the weight, which provides non-local information to estimate stable optical flow even when illumination changes. To mitigate the influence of illumination, the weighted regularization transform employs a coarse-to-fine pyramid model. An energy optimization procedure also serves to minimize the data term with non-local regularization. Experimental testing has been performed on three optical flow datasets and a face liveness detection database, all of which contain challenging illumination variations. The results demonstrate that the proposed method is highly robust in the presence of illumination variance."}, {"label": 0, "content": "A total transfer capability (TTC) calculation model based on stacked denoising autoencoder (SDAE) is proposed in this paper, considering static security, static voltage stability and transient stability constraints. The TTC calculation model consists of feature pre-screening, SDAE and the regression layer. Fast correlation-based filter (FCBF) is used to eliminate irrelevant and redundant features to improve the training efficiency of SDAE. SDAE takes advantage of the deep structure to extract high-order features relevant to TTC from original features. The regression layer is utilized to create the mapping between high-order features and the TTC value. Experiment results of a real power system demonstrate that the proposed TTC calculation model has higher computational accuracy than shallow machine learning models and feature pre-screening decreases the training time of the TTC calculation model obviously."}, {"label": 1, "content": "With the widespread adoption of large-scale distributed generators (DGs) in active distribution networks (ADNs), conventional load flow convergence failures caused by heavy power transmission have become a common issue. To combat this challenge, the Holomorphic Embedding Load Flow Method (HELM) has emerged as a more robust solution when compared to the traditional Newton-Raphson method. Furthermore, it also offers lower sensitivity to initial points. It is important to note that HELM was primarily designed for balanced transmission networks. Hence, our study aimed to develop a three-phase HELM model that caters to ADNs with DGs, delta connection loads, and ZIP loads. Using modified unbalanced IEEE 13, 34, 37, and 123 test feeders, our proposed method was demonstrated to provide better performance and effectiveness, especially under heavy load situations."}, {"label": 0, "content": "With the ever-increasing number of diagnosed cases of Parkinson's Disease in the Philippines, there is a need for Ambient Assisted Living systems that will help improve the quality of life and independent living of patients with Parkinson's Disease. Currently, there are a lot of existing Ambient Assisted Living systems, such as the RAReFall Detection system, which incorporates various sensors, such as wearables, external sensors, and smartphone sensors to detect and recognize human activities. However, these existing systems are not easily accessible due to the costly and complex nature of the equipment being used. To address this problem, this project aims to create a cost-efficient, state of the art, accessible and user-friendly smartphone based Ambient Assisted Living system which incorporates the use of embedded smartphone accelerometer and gyroscope sensors in order to detect and categorize the daily activities and falls of patients with Parkinson's Disease, and at the same time employing new techniques to be able to provide a means for immediate response and to give appropriate advice in order to prolong the independent and active participation of patients in their communities."}, {"label": 1, "content": "To achieve a transient and accident analysis of the reactor system and verify the system control scheme, advanced computer technology has been utilized to develop a graphical modeling tool for the TRANTH code. This tool includes a human-machine interaction interface and integration research between software and digital reactor design and verification platform. As a result, a design and simulation system for the digital reactor fluid instruction control system has been established.\n\nThe nuclear power plant ACP1000 was selected as the research object, and typical operating conditions were executed. The results obtained from this research indicate that the design and simulation system can effectively execute the modeling, calculation, interactive manipulation, and display of the reactor fluid instruction control system. The system enables a multi-professional joint simulation of the reactor fluid instruction control system, which provides the necessary technological support for generating, evaluating, verifying, and optimizing design schemes related to the reactor fluid instruction control system."}, {"label": 0, "content": "Formal verification of arithmetic circuits checks whether or not a gate-level circuit correctly implements a given specification model. In cases where this equivalence check fails - the presence of a bug is detected - it is required to: i) debug the circuit, ii) identify a set of nets (signals) where the circuit might be rectified, and iii) compute the corresponding rectification functions at those locations. This paper addresses the problem of post-verification debugging and correction (rectification) of finite field arithmetic circuits. The specification model and the circuit implementation may differ at any number of inputs. We present techniques that determine whether the circuit can be rectified at one particular net (gate output) - i.e. we address single-fix rectification.Starting from an equivalence checking setup modeled as a polynomial ideal membership test, we analyze the ideal membership residue to identify potential single-fix rectification locations. Subsequently, we use Nullstellensatz principles to ascertain if indeed a single-fix rectification can be applied at any of these locations. If a single-fix rectification exists, we derive a rectification function by modeling it as the synthesis of an unknown component problem. Our approach is based upon the Gr\u00f6bner basis algorithm, which we use both as a decision procedure (for rectification test) as well as a quantification procedure (for computing a rectification function). Experiments are performed over various finite field arithmetic circuits that demonstrate the efficacy of our approach, whereas SAT-based approaches are infeasible."}, {"label": 1, "content": "Passive acoustic monitoring is seen as a promising solution to the urgent global need for new biodiversity assessment methods. There is growing recognition of the ecological significance of the soundscape, and the affordability of reliable hardware for remote audio recording is generating worldwide interest in its potential for acoustic biodiversity monitoring. However, given the huge scale of the data involved, automated methods are necessary. The development of acoustic sensor networks capable of monitoring soundscapes and relaying data presents a significant technical challenge, with power management being a core issue. The recording and transmission of large quantities of audio data are energy-intensive, making long-term deployment in remote, off-grid locations difficult. In this paper, we propose a solution for remote acoustic monitoring and in situ analysis using a low-cost and energy-efficient wireless acoustic sensor network integrated with an edge computing structure. This model involves the recording and computation of acoustic indices directly on edge devices using low noise primo condenser microphones and Teensy microcontrollers, employing internal FFT hardware support. The resulting indices are transmitted over a ZigBee-based wireless mesh network to a destination server. Benchmark tests revealed acoustic equivalence and significant power savings compared to existing solutions."}, {"label": 1, "content": "We present a novel approach to designing software transactional memory by using locking protocols to prevent the need for transaction retries. Our method involves implementing specific techniques to optimize the performance and reliability of the system. We also explore adjustable parameters that can be customized to increase the efficiency of the system for specific applications."}, {"label": 0, "content": "The Prevention of cardiovascular disease requires continuous monitoring of cross-clock ECG signals along with the activity status. Traditional ECG Holter has numerous electrodes connected to the chest, which is heavy, so it is very difficult to carry by the patient, so ECG monitoring usually requires the patient to stay in the hospital for a long time. This paper presented a small ECG Holter device that was developed to detect arrhythmias in real-time based on Android mobile application. The ECG signals are obtained directly through ECG's three-electrode sensor then transmitted through a Bluetooth module to Android smartphone. Prepossessing ECG signal algorithm is implemented on Arduino Device. Android mobile application analysis and classify patient's ECG data to detect abnormal signs. Data used in testing and training was 303 cases acquired from El-Monofia University, 162 cases were normal, and 141 cases were abnormally divided into 57 cases were Coronary Artery Disease, 36 cases were Old Anterior Myocardial Infarction, and 48 cases were Sinus tachycardia. The experimental results show that the presented system's performance has been improved in the accuracy of diagnosis of arrhythmias and the identification of the most widely recognized anomalies in various activities."}, {"label": 0, "content": "Malware detection is more challenging due to the increase in android malicious programs and the current problems of android malicious detection. This paper proposes an android mobile malware detection system based on deep neural network, a novel malware detection method which uses optimized deep Convolutional Neural Network to learn from opcode sequences. In the proposed detection system, the optimized Convolutional Neural Network is trained multiple times by the raw operation code sequence extracted from the decompiled android file, so that the feature information can be effectively learned and the malicious program can be detected more accurately. More critically, the k-max pooling method with better results was adopted in the pooling operation phase, and which improves the detection effect of the proposed method. The experimental results show that the detection system achieved the accuracy of 99%, which is 2%-11 % higher than the accuracy of the machine learning detection algorithms when using the same dataset. It also ensures that the indicators such as Fl-score, Recall and Precision are maintained above 97%."}, {"label": 1, "content": "Cloud computing is widely used to provide a virtual platform, enabling various services to run, including real-time applications such as video streaming. However, to meet real-time application requirements, CPU resources are usually allocated for the worst-case scenario, leading to system under-utilization or overpaying the cloud provider, especially under a pay-as-you-go model. To address this issue, we have developed Pacer, a framework that offers application developers a platform to implement customized virtual machine-level resource allocation algorithms that make use of real-time application-specific feedback from virtual machines. Additionally, we have presented two exemplary resource allocation algorithms for Pacer. These algorithms are based on additive-increase-multiplicative-decrease and self-tuning PID control. We have implemented Pacer to video stream object detection applications, demonstrating how it can save more than 50% CPU utilization and make more efficient use of CPU resources, while continuing to meet the real-time needs of the applications."}, {"label": 1, "content": "Energy security is crucial for the well-being of nations and the general public. The power system is the primary component in the energy transfer chain, and ensuring its security is immensely significant. For electronic engineers, enhancing the understanding of the power system has been a long-standing objective and motivation in the field. \n\nThis paper begins by summarizing the cognitive approach used to understand both physical systems and power systems. It highlights the interlinkage and mapping relationship between them. Moreover, the introduction of Wide-Area Measurement Systems (WAMS) in power grids inspires new ideas for stability detection and data-driven research methods. \n\nTo this end, this paper surveys the classification, development, and current state of various data-driven methods. It recognizes that there is ambiguity within these methods and develops a clear framework for data-driven technique routes. The use of WAMS data for stability detection is outlined, emphasizing the potential advantages and applications of the data-driven research method."}, {"label": 0, "content": "A lot of people have been concerned about the problem of maximizing influence in social networks, which is aimed to find a set of nodes to get the influence spread maximized. However, the existing reasearches mainly focus on that a node influences its neighbors once without considering time and cost constraints. But in real world, people often try to influence their friends repeatedly during a time interval. Sometimes, the spread of information will cost a certain price as well. In this paper, we study the Time-sensitive Influence Maximization Problem and propose a Time and Cost constrainted Influence model with users' Online patterns (TCIO model). In TCIO model, the selection of seed nodes is limited to the budget and each node can influence its neighbors repeatedly according to their online patterns with different probability until a given message expire time is reached. We then show that the problem is NP-hard and our model satisfies monotonicity and submodularity for influence spread. Based on this, we develop a greedy algorithm to solve the problem. To reduce the computation complexity and optimize seed node selection with cost, we propose an efficient method GMAI for approximately calculating added influence using influence weight. Our experiments show that our model is effective and practical since it takes into account time factors, and GMAI faster and more effecient than other evaluated algorithms."}, {"label": 0, "content": "In a deregulated electricity market, power system operator should systematically identify the optimal schedule of renewable distributed generation (DG) units to not only optimize the market profits but also improve the network conditions. This paper proposes a parallel computation-based methodology using fuzzy logic designed in the structure of a genetic algorithm (GA). Due to the efficient communication among the processors during the optimization, the proposed fuzzy-based parallel computation GA (FPCGA) addresses the shortcoming of the classic GA in convergence speed and quality of results. The proposed optimization algorithm is utilized in this paper to identify the optimal daily schedule for the system operator including the energy purchased from 1) the power grid, 2) each wind turbine DG, and 3) each photovoltaic DG. The efficiency of the proposed method is verified by its implementation on a 136-bus distribution system and its effectiveness is compared with similar methods."}, {"label": 0, "content": "IoT is leading a digital revolution in both academia and industry. It brings convenience to people's daily lives; however, the issues of security and privacy of IoT become challenges. Blockchain, a decentralized database based on cryptographic techniques, is promising for IoT security, which may influence a variety of areas including manufacture, finance, and trading. The blockchain framework in an IoT system is an intriguing alternative to the traditional centralized model, which is struggling to meet some specified demands in IoT. In this article, we investigate typical security and privacy issues in IoT and develop a framework to integrate blockchain with IoT, which can provide great assurance for IoT data and various functionalities and desirable scalability including authentication, decentralized payment, and so on. We also suggest some possible solutions to these security and privacy issues in IoT based on blockchain and Ethereum to show how blockchain contributes to IoT."}, {"label": 0, "content": "We analyze the performance of a dual-hop downlink cellular amplify-and-forward cooperative system in the presence of both channel estimation errors and radio frequency (RF) impairments, where multiple antennas are deployed only at the base station (BS) and single-antenna at relays and mobile stations (MS). Specifically, we derive approximate as well as exact closed-form expressions for the outage probability and expected spectral efficiency. In addition, simple asymptotic expressions at the high signal-to-noise ratio (SNR) regime are obtained, which facilitate the characterization of the achievable diversity order of the system. To validate the derived analytical expressions, we presented simulation results which are sufficiently tight across the entire range of SNRs. Findings of the paper suggest that full diversity order can be achieved only when the RF front-end hardware is assumed to be perfect, while in practice the imperfections in hardware are inevitable and reduce diversity order of the system. Moreover, the influence of key parameters such as the number of antennas, users, and relays on the system performance has been presented with the influence of the level of RF impairments."}, {"label": 0, "content": "An electromechanical-electromagnetic hybrid simulation algorithm based on boundary nodes grouping and decoupling is proposed in this paper. The UHV $AC/DC$ backbone network is described as electromagnetic transient models and the others are described as electromechanical transient models and power grids at low voltage side are decoupled at their boundary. A new partitioning method of backbone network based on heuristic rules is proposed to support electromagnetic transient parallel computing and a hybrid parallel simulation algorithm based on subnet groups is designed. The theoretical analysis and derivation of the algorithm shows that the algorithm can improve the efficiency of hybrid simulation with a large number of boundary nodes."}, {"label": 1, "content": "With the rise of the Internet, social bots have become a growing concern on social media platforms. Detecting these accounts that pose a threat to social networks has become an essential task that requires an effective detection algorithm. This paper presents a social bots detection model, called DeBD, based on a deep learning algorithm. The model is composed of three layers. The first layer is the joint content feature extraction layer, which focuses on the extraction of features from tweets' content and their relationships. The second layer is the tweet metadata temporal feature extraction layer, which uses tweet metadata as temporal information to extract user social activity temporal features through LSTM. The third layer is the feature fusion layer, which integrates the extracted joint content features with the temporal features to detect social bots. We evaluated the proposed DeBD model on three different types of social bot data sets from the real world, and the experimental results demonstrate the model's effectiveness."}, {"label": 1, "content": "State estimation is a crucial element of real-time power system monitoring, enabling comprehensive analysis and decision-making in enhancing grid security and efficiency. However, one of the present interests in the field of state estimation lies in addressing the monitoring and mitigation of geomagnetically induced currents (GICs).\n\nGICs are quasi-dc currents that originate from solar activity and may cause additional losses of reactive power in transformers, leading to voltage deviations in many buses. In a traditional state estimator, these voltage deviations may remain undetected, as they may be attributed to incorrect estimates of generator reactive power output. Moreover, the voltage state estimate may accumulate additional errors, resulting from the use of equations that fail to represent the actual physical system and condition.\n\nTherefore, there is a need for state estimation models that consider the effects of GICs on the grid. This paper presents a case study that highlights the importance of GIC-related measurements and models in enhancing state estimation accuracy. By analyzing the required increase in GIC-related measurements and models, the study demonstrates the potential benefits of integrating GICs into the state estimation process."}, {"label": 0, "content": "At present, the injection of false data (FDI) in power system network brings a direct challenge to state estimation and reduces the reliability of the system. The data collected by WAMS system is of high frequency and accuracy, which can effectively prevent FDI attacks. This paper focuses on the problem of PMU optimization configuration considering false data injection. On the premise of ensuring the overall observability of the system and taking the zero injection node into account, the effect of false data injection on the power grid is effectively reduced through the optimized configuration of PMU, and the accuracy of state estimation data is improved to the maximum extent. Taking IEEE14 IEEE30 and IEEE57 standard nodes as examples, the data accuracy was improved to the maximum extent with the minimum number of PMU, and the feasibility of the method was verified."}, {"label": 1, "content": "This paper presents a technique for recognizing abnormal human activities in surveillance videos. The approach involves using a combination of Bayes Classifier and Convolutional Neural Network to identify four types of activities: walking, running, punching, and tripping. To accomplish this, the KTH dataset is utilized as the input for the Bayes Classifier and Convolutional Neural Network. A Kalman filter is employed to identify the movement of the human target in each frame, and three features of the target image, namely, length-width ratio, entropy, and Hu invariant moment, are extracted. Additionally, a convolutional neural network is created and trained to recognize abnormal human activities. Experimental findings indicate that the Bayes Classifier achieves an accuracy of 88%, 92%, 92%, and 100% for each of the activities, while the Convolutional Neural Network attains an accuracy of 92%, 96%, 100%, and 100% for each of the activities."}, {"label": 0, "content": "Despite the increasing popularity of deep neural networks (DNNs), they cannot be trained efficiently on existing platforms, and efforts have thus been devoted to designing dedicated hardware for DNNs. In our recent work, we have provided direct support for the stochastic gradient descent (SGD) training algorithm by constructing the basic element of neural networks, the synapse, using emerging technologies, namely memristors. Due to the limited performance of SGD, optimization algorithms are commonly employed in DNN training. Therefore, DNN accelerators that only support SGD might not meet DNN training requirements. In this paper, we present a memristorbased synapse that supports the commonly used momentum algorithm. Momentum significantly improves the convergence of SGD and facilitates the DNN training stage. We propose two design approaches to support momentum: 1) a hardware friendly modification of the momentum algorithm using memory external to the synapse structure, and 2) updating each synapse with a built-in memory. Our simulations show that the proposed DNN training solutions are as accurate as training on a GPU platform while speeding up the performance by 886\u00d7 and decreasing energy consumption by 7\u00d7, on average."}, {"label": 1, "content": "Recent research activities have centered around topics such as smart cities and autonomous driving, making simulations a critical tool for evaluating new algorithms, technologies, and products. These simulations help identify errors and optimize parameter sets before real-world implementation. However, simulation models for traffic problems must handle large-scale scenarios, connect entities from different domains, and run in a feasible time. To address these challenges, this paper proposes an extendable multi-level traffic simulation approach. We discuss existing simulation techniques, upcoming problems, available solution approaches, and development topics for our framework. As our first step, we coupled two different resolution levels of traffic simulation using High-Level Architecture (HLA) and evaluated the approach based on simulation results and performance."}, {"label": 0, "content": "High-speed permanent magnetic synchronous machine (PMSM) drive systems offer several advantages compared to low and medium speed drive systems, such as higher power density, capability of directly driving high speed loads. Due to reliability and rotor dynamic balance problem, sensorless drive is preferred in high-speed drive system. This paper proposed a model reference adaptive system based sensorless control method of a high-speed permanent magnetic machine with 1- F startup strategy. The proposed method can guarantee reliable startup ability and achieve smooth transition between the open-loop and closed-loop control scheme. Also, more accurate rotor position estimation is obtained with the compensation of voltage in high frequency."}, {"label": 1, "content": "By conducting sparse coding and classifier training together, supervised sparse coding has proven to be effective in various recognition tasks. However, current methods often only consider linear classification, limiting their ability to handle highly nonlinear data. This letter proposes a new supervised sparse coding model that incorporates decision tree classifiers. Decision trees are ideal for dealing with non-linear properties of data, and the combination with sparse coding significantly improves its discrimination capabilities. Additionally, sparse coding can produce sparse de-correlated features that decision trees prefer. To further improve performance, an ensemble framework is used to close the loop of sparse coding and decision tree learning. Alternating between learning a dictionary for sparse coding and a decision tree for classification, a series of decision trees and dictionaries are produced to construct a decision forest for classification. Experiments with face recognition and scene classification demonstrate the superiority of this method compared to recent supervised sparse coding approaches."}, {"label": 1, "content": "This paper presents the design of a remote monitoring system for a multifunctional direct drinking machine based on the Internet of Things (IoT). The local control system detects the status data of water purifiers through the temperature sensor, flowmeter, and water level gauge. The collected parameters are then sent to the cloud-based management system using General Packet Radio Service (GPRS) communication network or Wireless Fidelity (WIFI), in real-time. \n\nThe developed prototype has a user-friendly interface and is easy to operate. It displays the real-time working states of the direct drinking machine, manages big data, and supports remote equipment operation. In addition, Radio Frequency Identification (RFID) technology and mobile payment technology are used for automatic user identification, water purchase, and information maintenance functions. \n\nOverall, the comprehensive monitoring and control system connects users and multifunctional direct drinking machines in the IoT, which has great potential for application."}, {"label": 0, "content": "Safe and affordable surgery is not accessible for five billion people when they need it. Multiple surgical capacity studies have shown that hospitals in low-and-middle income countries do not have complete coverage of basic surgical equipment such as, theatre lights, anesthesia machines and electro surgical units. Currently, almost all equipment is designed and manufactured with a main focus on the context in high income countries. The context in low-and-middle income countries in which surgical equipment is used, differs from high income countries, especially in terms of financial resources and access to maintenance, spare parts and consumables. The aim of this study is to present a roadmap for design of surgical equipment for worldwide use. The roadmap consists of four phases: before the start of a design project a clear need for certain surgical equipment should be identified (Phase 0). During Phase 1 the context should be researched thoroughly by determining the barriers encountered by patients to surgical care, the structure of the health care system and if the aspects required for safe surgery are in place. In Phase 2 the implementation strategy and design requirements should be determined and in phase 3 prototyping starts in close interaction with local end-users. We believe that designers should strive for design that is of the same quality and complies with the same safety regulations as equipment designed for HICs. In this way user and patient safety can be assured in any setting worldwide. And we advocate for surgical equipment that fits the context optimally and that will be applicable in comparable settings globally."}, {"label": 1, "content": "In this paper, we provide a comprehensive review on the task of enhancing the dynamic features of the vacuum arc furnace power controller. Our proposed solution to this task utilizes a fuzzy logic controller based on Sugeno algorithm, which is implemented within a speed loop of an electrode positioning drive. We also introduce the application of a compensation method for dead zone along the arc length using a fuzzy logic device also based on the Sugeno algorithm. \n\nThe results of our study on the dynamic characteristics of vacuum arc furnace power control system with classical and fuzzy controllers are presented. It is demonstrated that the fuzzy logic controller based on Sugeno algorithm provides improved performance over the classical controller. Our findings suggest that the fuzzy logic controller is an effective approach to addressing the problem of improving the dynamic characteristics of the vacuum arc furnace power controller."}, {"label": 1, "content": "Sounds are a constant presence in our daily routines, ranging from the sound of transportation to conversations between individuals. Therefore, it's easy to collect and categorize these soundtracks into different groups, allowing us to recognize various settings by using these assets. Acoustic scene classification entails training machines to perform such tasks and can be installed on devices like smartphones, increasing convenience and improving our lives. Our primary goal is to maximize validation rates while optimizing hardware usage, which we achieve by utilizing the dataset from IEEE Detection and Classification of Acoustic Scenes and Events (DCASE) to train our machine.\n\nWe employ two signal processing techniques, Log Mel and HPSS, in our work. We also modify the MobileNet structure to train our dataset, utilizing fine-tuning and late fusion to enhance our results' accuracy and improve performance. With the aforementioned approach, we achieve a validation rate of 75.99%, the seventh highest performing group at DCASE Challenge 2017. Moreover, our method has less computational complexity than others with higher accuracy, making it a worthwhile trade-off."}, {"label": 1, "content": "This paper presents the results of research work funded by the Salt River Project Agricultural Improvement and Power District (SRP) that aimed to maximize the economic benefits for customers who installed residential rooftop PV systems in SRP territory. The primary objective was to optimize the discharge of battery power to minimize the demand charge paid by customers. To achieve this goal, the study employed Machine Learning algorithms to improve load forecasting techniques used in the industry. The improved battery discharge algorithm was found to be effective in reducing battery charge-discharge cycles, which extended battery life. Tests were conducted in Arizona, focusing on a residential rooftop grid-tied PV with storage system installed at the Tempe campus of Arizona State University."}, {"label": 0, "content": "This paper addresses for the first time the multilabel classification of High-Voltage (HV) discharges captured using the Electromagnetic Interference (EMI) method for HV machines. The approach involves feature extraction from EMI time signals, emitted during the discharge events, by means of 1D-Local Binary Pattern (LBP) and 1D-Histogram of Oriented Gradients (HOG) techniques. Their combination provides a feature vector that is implemented in a naive Bayes classifier designed to identify the labels of two or more discharge sources contained within a single signal. The performance of this novel approach is measured using various metrics including average precision, accuracy, specificity, hamming loss etc. Results demonstrate a successful performance that is in line with similar application to other fields such as biology and image processing. This first attempt of multi-label classification of EMI discharge sources opens a new research topic in HV condition monitoring."}, {"label": 0, "content": "The accurate estimation of power system frequency and amplitude is essential for power system monitoring, stability, control, and protection. This work proposes a novel approach for power system frequency and amplitude estimation based on variational mode decomposition (VMD) algorithm and Cheb-function (Chebfun) approximation system. In this work, the spectral information of power signals is extracted using VMD as sub-signals or modes. Each mode is further interpolated by Chebyshev polynomials in continuous domain using Chebfun system. The instantaneous frequency and amplitude are estimated based on zero crossings and local extrema locations of the continuous function. The robustness of the approach is evaluated on various power system scenarios and the results are compared with other existing methods. The promising results suggest that the proposed approach can be used as an efficient candidate for power system frequency and amplitude estimation."}, {"label": 0, "content": "Recent years, EMG has attracted much attention as a tool of human interface. In hand motion recognition and personal authentication using wrist EMG, we have obtained good results. However, there has been no way to establish them at the same time. Therefore, in this paper we measure EMG by attaching dry type sensors to wrist, and carry out hand motion recognition and personal authentication. The conventional method used EMG of movement Japanese Janken. We use a multi-input and multi-output model of a Convolutional Neural Network (CNN). The average accuracy of hand motion recognition is 94.5%. The average accuracy of personal authentication is 94.6%. In the conventional method, personal authentication was classified into two classes. However, we carry out multi-class classification in the proposed method. In feature extraction, we obtain 128\u00d78 input data from the measuring unit. Then, a filter size of the convolution layers is 3 \u00d7 3. CNN does not contain pooling layers in this paper. In the proposed method, the average accuracy of hand motion recognition is 94.6%. The average accuracy of personal authentication is 95.0%."}, {"label": 0, "content": "This paper investigates the controller design problem of networked control systems subject to cyber attacks. A hybrid-triggering communication strategy is employed to save the limited communication resources. State measurements are transmitted over a communication network and may be corrupted by cyber attacks. The aim of this paper is to design a controller for a new closed-loop system model with consideration of randomly occurring cyber attacks and the hybrid-triggering scheme. A stability criterion is obtained for the system stabilization by employing Lyapunov stability theory and stochastic analysis techniques. Moreover, the desired controller gain is derived by resorting to some matrix inequalities. Finally, a numerical example is exploited to demonstrate the usefulness of the proposed scheme."}, {"label": 1, "content": "In this paper, a new algorithm for fire detection in infrared (IR) video surveillance is proposed. The algorithm is based on a combination of convolutional neural network (CNN) and support vector machine (SVM) techniques. To enhance IR fire detection, a 9-layer CNN model called IRCNN is developed to extract IR image features. This approach replaces traditional handcrafted methods, and therefore improves the overall performance of the detection system. \n\nThe extracted features are then used to train a linear SVM, which is capable of accurately identifying fires in IR video footage. To address issues arising from insufficient datasets and speed up the training process, our network adopts data augmentation techniques and Adam optimization. \n\nExperimental results demonstrate that our method achieves high precision (98.82%) and high recall (98.58%) when tested on our IR flame dataset. The system also achieved real-time detection capabilities when tested on ordinary infrared surveillance cameras. Overall, our proposed algorithm delivers efficient and effective fire detection for IR video surveillance systems."}, {"label": 0, "content": "Classroom observations are a key component of professional development programs for teachers. While there are many classroom observation systems, these systems are costly to implement and may suffer from biased feedback and Hawthorne effect. Automation of classroom observation processes can potentially help obviate these challenges. This paper presents the design and implementation of an automated classroom observation system based on audio data collected during a class session using an App on the teacher's smart phone. The App automatically labels classroom activities into Stallings-type class observation categories like lecture, classwork, classroom management, practice, question/answer, and reading aloud. Based on the teacher's use of different teaching activities and student performance, the app can provide teachers with intelligent recommendations on how to best allocate class time to various activities. The App used machine learning techniques and was trained on classroom observation data collected from semi-rural primary schools in Pakistan. A variety of machine learning algorithms were evaluated, and using 10-fold cross-validation, the Random Forest algorithm yielded the best accuracy of about 69%. The results show that this approach is a viable and a much cheaper limited alternative to physical classroom observations especially in low-resource contexts of the developing world."}, {"label": 0, "content": "Lip reading, the ability to recognize text information from the movement of a speaker's mouth, is a difficult and challenging task. Recently, the end-to-end model that maps a variable-length sequence of video frames to text performs poorly in real life situation where people unintentionally move the lips instead of speaking. The goal of this work is to improve the performance of lip reading task in real life. The model proposed in this article consists of two networks that are visual to audio feature network and audio feature to text network. Our experiments showed that the model proposed in this article can achieve 92.76% accuracy in lip reading task on the dataset that the unintentional lips movement was added."}, {"label": 0, "content": "Poor efficiency and long running time of existent optimization algorithms in dealing with multi-objective multi-variable community microgrid optimization have always been a concern. To address this issue, a novel layered optimization algorithm based on NSGA-II is proposed. The proposed algorithm integrates the structural feature of community microgrid and the concept of multi-agent system into the optimization process and decomposes the complex microgrid optimization into several household optimizations of smaller scale and one central microgrid optimization. The household operation is optimized first and the central microgrid optimization is solved subsequently based on the Pareto solution set of household operation problems to obtain the optimal operation mode. Simulation results demonstrate that the proposed strategy is effective in improving optimization efficiency."}, {"label": 1, "content": "Image registration is a crucial step for image fusion in both synthetic aperture radar (SAR) and optical remote sensing. It holds significant theoretical and practical importance. There are different methods for image registration, classified based on feature, gray-scale, and others. This article specifically focuses on feature-based registration techniques for optical and SAR remote sensing images. It systematically reviews and categorizes various registration methods, highlighting their strengths and weaknesses. Furthermore, the article forecasts the potential advancements in the field."}, {"label": 0, "content": "With the growing usage of credit card transactions, financial fraud crimes have also been drastically increased leading to the loss of huge amounts in the finance industry. Having an efficient fraud detection method has become a necessity for all banks in order to minimize such losses. In fact, credit card fraud detection system involves a major challenge: the credit card fraud data sets are highly imbalanced since the number of fraudulent transactions is much smaller than the legitimate ones. Thus, many of traditional classifiers often fail to detect minority class objects for these skewed data sets. This paper aims first: to enhance classified performance of the minority of credit card fraud instances in the imbalanced data set, for that we propose a sampling method based on the K-means clustering and the genetic algorithm. We used K-means algorithm to cluster and group the minority kind of sample, and in each cluster we use the genetic algorithm to gain the new samples and construct an accurate fraud detection classifier."}, {"label": 0, "content": "Recently, numerous remote sensing applications highly depend on the hyperspectral image (HSI). HSI classification, as a fundamental issue, has attracted increasing attention and become a hot topic in the remote sensing community. We implemented a regularized convolutional neural network (CNN), which adopted dropout and regularization strategies to address the overfitting problem of limited training samples. Although many kinds of the literature have confirmed that it is an effective way for HSI classification to integrate spectrum with spatial context, the scaling issue is not fully exploited. In this paper, we propose a high efficient deep feature extraction and the classification method for the spectral-spatial HSI, which can make full use of multiscale spatial feature obtained by guided filter. The proposed approach is the first attempt to lean a CNN for spectral and multiscale spatial features. Compared to its counterparts, experimental results show that the proposed method can achieve 3% improvement in accuracy, according to various datasets such as Indian Pines, Pavia University, and Salinas."}, {"label": 1, "content": "Recommender systems play a crucial role in suggesting new items to users based on their preferences. With the vast amount of information available on the web such as reviews, ratings, and feedback on items, it becomes essential to extract meaningful information from this unstructured data to comprehend user choices. Two common techniques employed for making recommendations are collaborative filtering and content-based filtering. However, in Big Data Systems, where the data volume is extensive, a technique that combines both these approaches can prove to be more efficient.\n\nTo address this, a hybrid recommendation technique has been proposed in this paper that combines collaborative and content-based filtering techniques to suggest items that a user is most likely to prefer. The system also uses an item ranking and classification technique to further refine the recommendations. To take user preferences from social media into account, a social media opinion mining technique has also been added to comprehend users' tastes from their social media posts.\n\nA prototype system has been implemented and evaluated based on the proposed hybrid recommendation technique. The results suggest that the combination of collaborative and content-based filtering techniques with social media opinion mining can improve the accuracy of recommendations, especially for users with diverse interests. Overall, the hybrid recommendation technique proposed in this paper can help achieve more personalized and accurate recommendations in Big Data Systems."}, {"label": 1, "content": "With Convolutional Neural Networks (CNN) achieving success in computer vision tasks, Steganalysis is moving towards Network Engineering from Feature Engineering. To capture weak embedded signals in low Signal-to-Noise (SNR) scenarios, deep neural networks are being used. This paper proposes a new Convolutional Neural Network that uses aggregated residual transformations to generate stronger image representations for steganalysis. The CNN has fewer hyperparameters and focuses on increasing classification accuracy while keeping the depth and number of parameters constant. Residual skip connections help preserve and improve weak embedded signals and gradient flow. The proposed CNN was evaluated on BOSSbase against S-UNIWARD and HILL steganographic algorithms with different payloads. Compared to Deep Residual Learning based on Residual Learning and the SRM plus Ensemble, our proposed CNN yields better classification accuracy."}, {"label": 1, "content": "The paper presents a model for controlling AC drives, analyzing both conventional drive control systems and intelligent approaches for implementing control systems. The authors estimate the advantages and disadvantages of traditional systems and identify the preconditions for intelligent approaches. They propose modeling the intelligent system using a fuzzy logic controller and various inference algorithms for different control loops. The paper includes an analysis of the system's dynamic characteristics with and without fuzzy set theory controllers. The authors demonstrate the practicability of using multi-cascade fuzzy systems and different combinations of fuzzy inference algorithms when implementing a unified intelligent control module for complex drive systems. By using this technology, the authors show that it is possible to implement a control system for the entire class of drive systems, considering all the special aspects and relationships between the coordinates in the complex control object. Furthermore, modeling a spacial membership function using a multi-cascade fuzzy controller can help avoid the quantitative and qualitative restrictions imposed by complex relations between the coordinates in such systems."}, {"label": 0, "content": "Depression is one of the causes of suicide in the world next to other health issues that makes up an alarming point of mortality living in this lifetime. Melancholy that in the field of computer vision and signal processing has been tackled in various ways. Thus, this paper presents the classification model of detecting depression based on local binary pattern (LBP) texture features an image processing approach for pattern recognition on images. The study used the video recording from the SEMAINE database. The face image is cropped from a video and extracting Uniformed LBP features in every single frame. Part of the classification is to implement PCA eigenvalues from the original features to see the effects. The result of the accuracy was 81% of the SVM using RBF kernel classifier when detecting Depressed to Not Depressed Behavior on a captured motion picture."}, {"label": 0, "content": "With the diversification of location-based services in vehicle networks, users can obtain such services through submitting searching locations and points of interest. However, users may worry that their real locations and other privacy information will leak out when they get such services, so appropriate location privacy protection measures are necessary. Traditional location privacy protection, such as K-anonymous, cannot be carried out directly because of the characteristics of vehicle networks, such as high mobility. In order to solve such problems, this article proposes a strategy combining cache strategy with K-anonymous that can not only satisfy users' demand on obtaining required services with lowest cost, but also protect the location privacy of users. Specifically, on one hand, cache strategy that deploys part of services on roadside units in advance is used to maximize the satisfaction of users' service requests. On the other hand, each user is set with a K value to meet the need of K-anonymous. The trade-off of these two aspects guarantees users' services and the location privacy. The experiments show that the strategy proposed in this article is better than other strategies."}, {"label": 1, "content": "We examined the challenge of synthesizing algorithms for adaptive nonlinear signal processing through the use of feed-forward blocks of nonlinear transformation in the presence of non-Gaussian noise of unknown density. In order to achieve this, we found that it is necessary to employ algorithms for estimating the parameters of the linear model of probability density function for noise. Such a model takes the form of a generalized polynomial that is decomposed into a series of linearly independent functions, as well as nonlinear models, such as the generalized Gaussian distribution and abnormally cluttered distribution. By utilizing these methods, we were able to successfully plot the adaptive feed-forward blocks of nonlinear transformation."}, {"label": 0, "content": "From positions of the principles of training selection decomposition and collective estimation the synthesis technique of multilevel nonparametric systems of pattern recognition for the multialternate classification problem is offered. Their application provides high computing performance of information processing of big dimension. Two approaches are considered. Poorly dependent feature sets of the classified objects are in case of the former used. Considering the assumption of independence of feature sets the generalized decisive rule of maximum likelihood is under construction. The basis of the second method is made by a dichotomy method. At each its stage we form the family of the private decision functions corresponding to various feature sets of the classified objects with the subsequent their integration in the non-linear decisive rule by means of methods of nonparametric statistics. At the same time formation of the generalized decision on situation belonging to this or that class is carried out in space of values of private decision functions. The offered technique allows to use technology of parallel calculations."}, {"label": 0, "content": "The paper studies an important problem how to increase the efficiency of electric energy transportation in distribution networks with the voltage range of 6-20 kV by reducing losses while transmitting electricity and ensuring its quality at the consumer nodes in accordance with the deviation criterion from nominal in the network in modern Russian electric power industry. A description of the distribution network model is given. The results of distribution networks modeling with the voltage range of 6-20 kV and the use of intelligent control devices based on the thyristor regulator of booster voltage (TRBV) are presented. The high laboriousness of applying precise control algorithms for intelligent control devices for reducing transmission losses in networks with the voltage range of 6-20 kV is shown. The comparative characteristics of the operation of the distribution electric networks before and after the adjustment of intelligent control devices are given."}, {"label": 1, "content": "Vehicular Ad-hoc Network (VANET) is a subcategory of Mobile ad-hoc Network (MANET), which utilizes wireless communication to facilitate data exchange in a vehicular environment. Its primary objective is to improve the service quality of Intelligent Transportation Systems (ITS), including road safety, logistics, environmental sustainability, and information sharing. However, VANET applications are currently facing significant challenges in terms of performance and efficiency, primarily due to the way messages are disseminated between the nodes. In this regard, finding a viable routing protocol for dynamic VANET systems remains a major hurdle.\n\nThis paper proposes the use of an Ant Colony Hybrid Routing Protocol (ACOHRP) to tackle the challenge of improving the service quality of ITS through enhanced efficiency and reliability of vehicle traffic information message transmission. ACOHRP achieves high efficiency by improving the packet delivery ratio, reducing end-to-end delay, and optimizing node-to-node communication. To evaluate the effectiveness of ACOHRP, a comparative study was conducted between this protocol and the existing Dynamic Source Routing (DSR) protocol in a realistic VANET architecture scenario. The simulation tests were carried out using Matlab.\n\nThe results showed that ACOHRP performed better compared to DSR in dynamic VANET environments, delivering superior packet delivery ratio, reduced end-to-end delay, minimized routing overhead, and improved throughput efficiency. This makes ACOHRP an ideal choice for improving the performance of VANET applications, allowing for better and more efficient information sharing in the vehicular environment.\n\nIn conclusion, ACOHRP is a promising solution to the challenges facing VANET applications. Its superior performance compared to other standard protocols makes it an excellent choice for improving the efficiency and reliability of ITS, thus enhancing road safety, environmental sustainability, and general transportation systems."}, {"label": 1, "content": "A new algorithm for multi-frame image super resolution (SR) has been proposed, which utilizes Bayesian modeling with natural image prior modeled by fields of experts (FoE). Multi-frame SR can be utilized to achieve a high resolution (HR) image from a series of degraded low resolution (LR) images without necessitating any hardware device modification. However, SR is a well-known ill-posed problem. Thus, current state-of-the-art solutions formulate the problem with Bayesian modeling techniques, which infer the HR image based on both the LR input images and prior knowledge about the HR image. Nonetheless, majority of the Bayesian SR approaches utilize simple prior models such as L1 norm, TV prior, and Laplacian prior, which do not exploit the statistics of natural scenes accurately. This paper presents a Bayesian multi-frame image SR approach, which utilizes FOE model as the prior for natural images. The Maximum a Posteriori (MAP) framework is employed to estimate the HR image. The proposed method can not only capture the statistics of natural images effectively, but also necessitates less computational power than other Bayesian modeling techniques like Sampling methods and Approximate inference. The proposed method delivers results that are either superior or comparable to state-of-art multi-frame SR methods."}, {"label": 0, "content": "In this paper, the parameters of the derivative of blood pressure signals for the waveform classification of the wrist pulse have been analyzed. The method used here shows the relationships between pulse waveforms and the amplitudes of the characteristic points of the first derivative of blood pressure signals. The algorithm considered in the paper can be used for the computerization of pulse diagnostics."}, {"label": 0, "content": "Unmanned aerial vehicle mounted base stations (UAV - BSs) can provide wireless cellular service to ground users in a variety of scenarios. The efficient deployment of such UAV-BSs while optimizing the coverage area is one of the key challenges. This work investigates the 3D UAV -BS placement that maximizes the numbers of covered users with different Quality-of-Service (QoS) requirements using the minimum power. In this paper, we first highlight the properties of the 3D placement problem and we model the problem as a multiple concentric circles placement problem with the objective of maximizing the numbers of covered users. We decouple the UAV-BS deployment problem in the vertical and horizontal dimensions without any loss of optimality, after some mathematical manipulations, we formulate a Mixed Integer Second Order Cone Problem (MISOCP) and propose an improved Multi-Population Genetic Algorithm (MPGA) for horizontal dimensions placement problem. Numerical simulations are presented showing that improved MPGA can obtain better performance compare to Standard Genetic Algorithm (SGA) in this problem."}, {"label": 1, "content": "Virtual Machine (VM) live migration is a strategic approach that can significantly reduce energy consumption while increasing the utilization of a single computer in large computing infrastructures. However, the application of virtualization in High Throughput Computing (HTC) has not yet been extensively studied in literature. \n\nTherefore, in this paper, we propose an extension of an existing trace-driven simulation that incorporates virtualization. Additionally, we have implemented the pre-copy live migration algorithm and created a test environment for job live migration in the HTC system. Our simulation provides the total number of migrations, their overall time, and the energy consumption during its runtime. \n\nFurthermore, we have proposed two methods to perform live migration in the HTC system. Our responsive migration approach was able to significantly save up to 75% of the system's wasted energy. Overall, this paper highlights the benefits of live migration for HTC systems and provides insights into the potential energy savings that can be achieved through effective use of virtualization."}, {"label": 1, "content": "The rapid advancement of technology is disrupting traditional job markets, leading job seekers to develop new digital skills suitable for the digital economy. This phenomenon has a significant impact on the economy, particularly in developing countries. Consequently, it is crucial for higher education institutions to align their offerings with industry requirements across all disciplines to sustain and improve the economy.\n\nTo address this issue, this paper presents a framework designed to determine the alignment of the digital skills acquired by students in higher education to the digital skills required by the industry. This alignment will guide universities in improving the digital skills of their graduates, ultimately sustaining the digital economy. As the required digital skills may vary in each sector, the proposed framework is not discipline-specific and may be employed to establish an alignment between any discipline in higher education and the respective industry that its graduates feed into.\n\nTo develop this framework, authors reviewed relevant articles to determine the factors influencing digital skills preparedness for graduates entering the industry. Based on this analysis, a digital skills preparedness model was developed. By utilizing this model, higher education institutions can more effectively prepare graduates with the necessary digital skills to succeed in the rapidly transforming job market."}, {"label": 0, "content": "Accurate load forecasting can create both economic and reliability benefits for power system operators. However, the cyberattack on load forecasting may mislead operators to make unsuitable operational decisions for the electricity delivery. To effectively and accurately detect these cyberattacks, this paper develops a machine learning-based anomaly detection (MLAD) methodology. First, load forecasts provided by neural networks are used to reconstruct the benchmark and scaling data by using the k-means clustering. Second, the cyberattack template is estimated by the naive Bayes classification based on the cumulative distribution function and statistical features of the scaling data. Finally, the dynamic programming is utilized to calculate both the occurrence and parameter of one cyberattack on load forecasting data. A widely used symbolic aggregation approximation method is compared with the developed MLAD method. Numerical simulations on the publicly load data show that the MLAD method can effectively detect cyberattacks for load forecasting data with relatively high accuracy. Also, the robustness of MLAD is verified by thousands of attack scenarios based on Monte Carlo simulation."}, {"label": 0, "content": "Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness."}, {"label": 0, "content": "An accurate estimation of a power system's state is a major requirement in the modern-day power system. An interconnected and highly nonlinear system requires a reliable and efficient algorithm for monitoring of the system's status in order to have a secure operation. The presence of wrong measurements has made the estimation process a challenging one. An efficient and reliable state estimator should have the ability to detect and eliminate the effects of bad-data during the estimation process. Least Measurement Rejected (LMR) estimator is one of such robust estimators with higher computational efficiency and better reliability. The performance of LMR estimator mainly depends upon the tolerance value of loaded measurements and tolerance is a constant value assigned to each of the measurement. This paper presents an efficient method of tolerance value selection for LMR estimator. Such selection of tolerance value will ensure the robustness of the estimator in terms of estimation accuracy and will provide better computational efficiency. The estimation accuracy and computational time of the proposed approach has been compared with Weighted Least Square (WLS) and Weighted Least Absolute Value (WLAV) estimator. The IEEE 30-bus system has been used to demonstrate the performance of the proposed estimator under different sets of bad measurement (single and multiple) scenarios."}, {"label": 1, "content": "Spectrum is an extremely valuable resource in a battlefield as it allows for communication between nodes and base stations. With multiple frequencies available, it is important to assign them in a way that meets the Signal-to-Interference-Noise Ratio(SINR) and throughput requirements of all users, while also ensuring fairness. \n\nPrevious attempts at frequency assignment have ignored the aforementioned considerations and simply placed constraints on the distance between interfering base stations. However, in this work, the optimal frequency assignment is found through the use of a genetic algorithm, which is referred to as the efficient rate and SINR matcher with hyperbolic cost (ERSMHC). \n\nTwo algorithms, namely a high SINR initialization algorithm (HSIA) and a high throughput greedy rate matching algorithm (HTGRM), are proposed to quickly initialize the system. However, the ERSMHC algorithm is shown to outperform both HSIA and HTGRM in meeting the rate and throughput requirements of more users. \n\nOverall, this work emphasizes the importance of considering SINR and throughput requirements when assigning frequencies in a battlefield, and highlights the effectiveness of using genetic algorithms to optimize frequency assignments."}, {"label": 0, "content": "Human activity measurement and classification has been hot research topic for several years. Most of the solutions are based on the mobile phones, however there are also some wearable device implementations that have very specific functionality. The aim of this paper is to propose a human activity recognition and fall detection solution that provides extra safety for people working in challenging conditions. The system is integrated with the monitoring solution that provides real-time information about all workers and raises automatically an alarm in case of accidents or abnormal conditions."}, {"label": 1, "content": "Power System Simulator for Engineering (PSS/E) is a widely used analytical toolset in the world of electrical engineering. One of its distinguishing features is its powerful user-defined function. This paper aims to study the user-defined modeling function in PSS/E by modeling a VSC-HVDC transmission system.\n\nIn this paper, the user-defined modeling (UDM) function of PSS/E is introduced in detail. A modeling method for VSC-HVDC is proposed, and the simulation curves of the user-defined VSC-HVDC in PSS/E are compared with those simulated by PSCAD/EMTDC. Through this comparison, the accuracy of the VSC-HVDC user-defined model is validated, and the feasibility and practicality of PSS/E's user-defined function are confirmed.\n\nThe modeling method presented in this paper can serve as a guide for the modeling of other complex components in PSS/E for simulations. Overall, this study demonstrates the usefulness and versatility of PSS/E's user-defined function and serves as a valuable reference for researchers and practitioners in the field of electrical engineering."}, {"label": 1, "content": "Forecasting consumer electricity usage is a crucial component of a reliable smart grid system. However, due to the multiple variables associated with individual residential consumers, predicting residential load levels remains a challenging task. Accurate forecasting is vital for the effective planning of electrical resources in order to balance demand and supply. This study proposes a Deep Neural Network (DNN) based short-term load forecasting approach for residential consumers. The performance of different types of recurrent neural networks (RNNs) is compared using the Mean Absolute Percentage Error (MAPE) value for residential electricity datasets. The preliminary results indicate that Long short-term memory (LSTM) based RNNs perform better than simple RNNs and gated recurrent unit (GRU) RNNs for a single user with 1-minute resolution based on one year of historical data sets."}, {"label": 1, "content": "The safety of air vehicles depends heavily on the security of the inertial navigation system (INS), which exhibits dynamic characteristics due to its dynamic logic gate components. Traditional approaches to analyzing the security of INS using dynamic fault trees rely on Markov chain and Monte-Carlo simulation methods, which often suffer from the curse of dimensionality and long computation times. In this study, we propose an analytical approach to combinatorial reliability analysis of dynamic systems using sequential binary decision diagrams (SBDD).\n\nOur approach involves building a dynamic fault tree (DFT) model of the collapse of the inertial platform based on safety analysis using DFT. We then modularize the DFT model into independent static sub-trees and dynamic sub-trees, determining the exact expression of the occurrence probability for each minimal cut-set using either the static fault tree analysis method or SBDD method. Finally, we analyze the possibility of collapse of the inertial platform throughout its lifetime and compare our results with those based on Markov chain and Monte-Carlo simulation. Our approach generates precise possibilities of collapse of the inertial platform and can be applied to reliability analysis with varying component failure parameters, making it a more effective and applicable method. Our study expands the method of calculating the occurrence probability of top event for a dynamic fault tree."}, {"label": 1, "content": "The internet has led to a massive surge in the amount of content being produced, with video content being a major component of this. This means that video recommendation engines need to find new and innovative ways to suggest newly added videos to their users. However, new videos often lack metadata and user interaction that could be used to rate them, making it difficult to recommend them effectively. In order to address this, we have developed a range of techniques for Content Based Video Relevance Prediction (CBVRP). By employing various architectures on the CBVRP dataset and using the provided frame and video level features, we have been able to generate predictions of videos that are similar to others. We have also implemented ensemble strategies that explore the complementary nature of these two types of features. The results are promising and will help to push the boundaries of multimedia-based video recommendation systems."}, {"label": 0, "content": "The aim of our research was to elaborate the current concept of the IoT based on the scientific papers up to now and to draw up the potential legal and security risks, which may affect the users or the state. We categorized the challenges of the IoT usage. The main problems in the aspect of data protection, are traceability and confidentiality issues."}, {"label": 1, "content": "Just Noticeable Difference (JND) is a measure of the maximum tolerable distortion in stereoscopic 3D content. It indicates the visibility threshold of asymmetric distortions in the left and right contents, and can be used to enhance the efficiency of 3D compression or improve 3D quality assessment. While 2D-JND models have been around for a while, 3D-JND models are relatively new, and the literature around them is limited. This paper presents a comprehensive review of pixel-based 3D-JND models, with each model described in terms of rationale, main components, targeted application, pros and cons. The paper also explores the characteristics of the human visual system presented in these models, and provides a thorough analysis and comparison of the 3D-JND models using qualitative and quantitative performance evaluation based on Middlebury stereo datasets. In addition, the paper measures JND thresholds of asymmetric distortion through psychophysical experiments and compares the experimental results with the estimates from 3D-JND models to assess the accuracy of each model."}, {"label": 1, "content": "With the advancement of renewable energy technology, it is now possible to power base stations (BSs) with renewable energy. This study focuses on reducing on-grid energy consumption in heterogeneous cellular networks that employ hybrid energy supplies. An energy-aware user association algorithm is proposed to achieve this objective. The algorithm allows for adaptive biasing factor adjustments for each small BS (SBS) based on the availability of renewable energy storage. This facilitates more utilization of renewable energy and minimizes on-grid energy consumption. Our proposed algorithm was compared with the max-RSRP algorithm and the traditional cell range expansion (TCRE) algorithm. The simulation results demonstrate that our algorithm significantly reduces on-grid energy consumption and improves the utilization of renewable energy."}, {"label": 1, "content": "Phoneme lattices have been shown to be a reliable method for encoding alternative decoding hypotheses from a speech recognition system. However, the search space of the decoder can become too large if the optimal phoneme sequence is produced by tracing all the phoneme identities in the lattice. This can lead to false substitutions or insertion errors in the final phoneme sequence. \n\nTo solve this problem, a new approach called split lattice structures is introduced in this paper. The split lattice structures are generated by splitting the speech frames based on the manner of articulation, which is detected using the spectral flatness measure (SFM). The sonorants (including vowels, semivowels, and nasals) and non-sonorants (including fricatives, stops, and closures) are separated into different split lattices. \n\nThe split lattices are modified based on the manner of articulation in each split to eliminate irrelevant phoneme identities in the lattice. For example, non-sonorant phoneme identities are excluded from the sonorant lattice, reducing false substitutions or insertion errors. \n\nThe proposed split lattice structure based on sonority detection demonstrated a 0.9% decrease in phone error rates compared to conventional decoding using state-of-the-art Deep Neural Networks (DNN) when evaluated on the core TIMIT test corpus."}, {"label": 1, "content": "This paper presents a stochastic solution for the problem of vehicular localization on roads, which utilizes vehicle-to-vehicle communication and GPS readings. The proposed methodology also utilizes the locations of stationary RoadSide Units (RSUs) as fixed reference points. The distance between vehicles is estimated using the beacons broadcasted periodically by neighboring vehicles. It is demonstrated that the additional position measurements received from neighboring vehicles can significantly improve the accuracy of the position estimate of the target vehicle. An analytical framework is established using particle filters to formulate the target vehicle's position estimate problem. Extensive simulations using a combination of Network Simulator (NS-3) and Simulation for Urban MObility (SUMO) were conducted to evaluate the validity, reliability and accuracy of the proposed methodology. Realistic vehicular mobility traces were used during the simulations."}, {"label": 0, "content": "Clinics with limited resources rely on paper patient records because they are easy to use, reliable, and can be supported by the clinics' financial and technical resources. Electronic health record (EHR) systems provide benefits in patient information management and reporting; however, they often require financial and technical resources that exceed those available to the clinics. This paper hypothesizes that limited-resource clinics could successfully install and sustain a patient-record automation system if it did not require resources beyond the reach of those clinics-if such a system was available. Because no system was found, the piClinic Console was developed to test this hypothesis. The piClinic Console is a Raspberry-Pi-based, patient-record automation system that provides essential patient-record automation functions and runs on hardware that costs less than $300 USD per clinic. This paper describes the features that provide the most benefit to the clinics and that run on a low-cost system as determined through end-user observation, participatory design, and iterative user testing. Preliminary testing shows that the piClinic Console can provide immediate benefits to clinic information processing and can prepare the clinic for a smoother transition to more complete EHR system when the resources to sustain one become available. The piClinic Console system is in its early stages of field testing and this paper describes the design and development process, the results of performance and user testing, and the plans for future research and development."}, {"label": 0, "content": "The article presents the results of the research, as well as a software module that allows you to configure the fuzzy logic controller to three possible ways of including a fuzzy logic controller in the control object. The evaluation of the quality of regulation and their comparative characteristics."}, {"label": 0, "content": "The performance of automatic speaker identification (ASI) systems on Voice over Internet Protocol (VoIP) speech varies with the type of codec used in the VoIP communication. The type of codec used depends on the service provider of the user. Thus there is a need for the codec-independent ASI systems to identify the speaker. Three modeling approaches based on UBM-GMM framework and i-vector framework are proposed to identify the speaker independent of codec used. These frameworks are also evaluated for the mismatch conditions with respect to the codec used in training and testing. The proposed approaches are evaluated on VoIP speech from four codecs with different bit rates along with uncoded speech."}, {"label": 1, "content": "The identification of fault signals in rolling bearings can be challenging due to the presence of irrelevant harmonics and background noise that distort the feature information. In this paper, we propose a novel diagnostic approach to identify incipient periodic impulsive features in rolling bearings. To achieve this, we employ the Over-Complete Rational-Dilation Wavelet Transforms (ORDWT) to decompose the original fault signal into multiple sub-bands. \n\nNext, we introduce a periodic impulsive index that combines the advantages of ACFHNR, kurtosis, and Pearson's correlation coefficient index to adaptively track the most suitable fault frequency band. Finally, we obtain the envelope spectrum of the best fault frequency band for fault diagnosis. \n\nOur simulation and experiment results demonstrate the effectiveness of our adaptive fault frequency band detection (AFFBD) method. With this approach, it becomes possible to identify the fault symptom in time and prevent significant damage to the bearing."}, {"label": 1, "content": "Livestock plays a crucial role in Indonesia, a country with a large agricultural sector in the Asia Pacific region. To increase livestock production, implementing Smart Livestock Monitoring System using internet of things (IoT) technologies such as Low Power Wide Area (LPWA) and Lora would be beneficial. Lora technology provides multiple benefits, including an extended transmission range, extended node battery life, and can support a massive number of nodes per gateway. However, the crowded sub 1 GHz unlicensed spectrum can create challenges in implementation. To mitigate this challenge, the proposed solution is to use mobile Lora gateways.\n\nThis paper addresses this challenge by comparing the deployment of the Lora technology using one mobile gateway with one and multiple static gateways using simulation. The results showed that for narrow livestock areas, one static gateway is ideal because of lower network energy consumption (NEC) and sufficient data extraction rate (DER) values for data transmission. Still, for vast livestock areas, one mobile gateway is better due to its smaller deployment cost and more than enough DER values.\n\nIn summary, implementing a Smart Livestock Monitoring System using Lora technology can increase livestock production in Indonesia. However, the crowded unlicensed spectrum can impede implementation, and using a mobile Lora gateway can mitigate this challenge. Through simulation, it was established that the number of gateways used depended on the size of the livestock area."}, {"label": 0, "content": "Synthetic Aperture Radar(SAR) and optical remote sensing image registration is the prerequisite for image fusion and it is of important theoretical significance and practical value. The image registration methods are mainly divided into the methods based on feature, the methods based on Gray-scale and others. This article systematically sorts out feature-based optical and SAR remote sensing image registration techniques, summarizes all types of image registration, points out their advantages and disadvantages and predicts the prospects of their future."}, {"label": 0, "content": "Spectrum is a scarce and precious resource in a battlefield. Multiple frequencies spread over different bands are available for usage in a battlefield. The frequencies need to be assigned to the nodes/Base Stations(BSs) in a battlefield in such a way as to meet the Signal-to-Interference-Noise Ratio(SINR) and throughput requirements of all the users. Previous attempts for frequency assignment abstract away the notion of SINR and throughput and usually constrain the distance between the frequencies required between pairs of interfering BSs. In this work, the frequency assignment that optimizes the SINR and throughput of all the users in a system while ensuring fairness among the users is found. A genetic algorithm is used to perform the unconstrained optimization and this method is termed as the efficient rate and SINR matcher with hyperbolic cost (ERSMHC). A high SINR initialization algorithm (HSIA) and a high throughput greedy rate matching algorithm (HTGRM) that can be used for quickly initializing the system are also presented. The ERSMHC algorithm is shown to aid in meeting the stipulated rate and throughput requirements of more number of users as compared to HSIA and HTGRM."}, {"label": 0, "content": "A remote monitoring system of multifunctional direct drinking machine based on Internet of things (IoT) is designed in this paper. The local control system detected the status data of water purifiers according to the temperature sensor, the flowmeter and the water level gauge, then sent the collected parameters to the backstage management system in the cloud by General Packet Radio Service (GPRS) communication network or Wireless Fidelity (WIFI) in real time. Prototype design and applications show that the management system has friendly user interface and is easy to be operated. It may display the working states of the direct drinking machine in real time, manage big data, and support remote operation of the equipment. Radio frequency identification (RFID) technology and mobile payment technology are employed to realize automatic user identification, water purchase, information maintenance and other functions. The comprehensive monitoring and controlling system has realized the function of connecting users and multifunctional direct drinking machines in the Internet of things, which has good application potential."}, {"label": 1, "content": "One of the main issues with distributed ledger technologies is the processing and verifying time required for transactions to be accepted as permanent records in the ledger. In some situations, such high latency cannot be accepted as a part of ledger governance. To address this, we propose an architecture for distributed ledger \u2013 Tango. This architecture mimics the Iota-tangle design by Popov, as described in his seminal paper. \n\nOur proposed architecture introduces a semi-synchronous transaction entry protocol layer. Here, we have periodic pulsed injections from the entry layer into the evaluation layer. This approach is expected to reduce latency, making it more acceptable in various applications."}, {"label": 1, "content": "The problem of detecting and classifying events in continuous video streams for information and telecommunication security systems has been successfully addressed. The proposed solution involves using a deep neural network ensemble which consists of feedback networks and convolutional networks. This approach is able to effectively recognize and classify abnormal emergency situations, based on the algorithmic framework.\n\nTo train and test various architectures of deep neural networks, a dataset of abnormal situations was collected. IndRNN layers were utilized during the training process, which resulted in up to 70% recognition rate for multi-class events in the video stream.\n\nIn order to improve the accuracy of classification estimation for video segments, the software package also includes extracting keywords from automatic annotations. This new feature has significantly enhanced the performance of the system.\n\nThe developed software package can be readily implemented in an integrated security system, allowing for real-time recognition of abnormal situations. Overall, this solution provides an efficient and reliable approach to event detection and classification in continuous video streams."}, {"label": 0, "content": "definition of the function of instrumental contact establishing for short-range radio detection devices is introduced. Analytical expression for the function of distance change between object and the detection device is defined. Analytical dependences of the function of instrumental contact establishing in two-dimensional and three-dimensional coordinate system are obtained provided that object (or objects) and the device of detection move in space on trajectories of complex shapes. It is shown that the accumulating probability of detection of object is calculated on the basis of function of the function of instrumental contact establishing."}, {"label": 0, "content": "This paper introduces the design and implementation of a Python-based software package for cyber-physical power system research called Andes. Andes is developed in an attempt to bridge the gap between the traditional power system analysis and the fast-growing needs for cybersecurity studies. First, the architecture design is proposed to accommodate for power grid prototyping, communication network set up, and the interactions between the two. Design considerations are discussed from the research and development perspective. Examples are shown using Andes for modeling, monitoring, and visualization of cyber-physical power system studies."}, {"label": 1, "content": "For an interior permanent magnet synchronous machine (IPMSM), parameter mismatching can have a significant impact on its performance and output efficiency. In this paper, we propose a variable parameter Maximum Torque Per Ampere (MTPA) control method based on online parameters identification.\n\nWe utilize an adaptive linear neural network algorithm to accurately identify the motor parameters, following which the MTPA control is modified by combining the identified parameters. Our approach demonstrates excellent adaptability to parameters mismatching, leading to improved output efficiency that is particularly advantageous for magnetic recording motor applications.\n\nOverall, the results of this study suggest that our proposed strategy presents a promising solution for increasing the performance and output efficiency of interior permanent magnet synchronous machines, even in the presence of parameters mismatching."}, {"label": 0, "content": "Lipreading is the task of looking at, perceiving, and interpreting spoken symbols. It has a wide range of applications such as in surveillance, Internet telephony, speech reconstruction for silent movies and as an aid to a person with speech as well as hearing impairments. However, most of the work in lipreading literature has been limited to the classification of speech videos into text classes formed of phrases, words and sentences. Even this has been based on a highly constrained lexicon of words which, then subsequently translates to restriction on total number of classes (i.e, phrases, words and sentences) that are considered for the classification task. Recently, research has ventured into generating speech (audio) from silent video sequences. In spite of non-frontal views showing the potential of enhancing performance of speech reading and reconstruction systems, there have been no developments in using multiple camera feeds for the same. To this end, this paper presents a multi-view speech reading and reconstruction system. The major contribution of this paper is to present a model, namely MyLipper, which is a vocabulary and language agnostic and a real-time model that deals with a variety of poses of a speaker. The model leverages silent video feeds from multiple cameras recording a subject to generate intelligent speech for that speaker, thus being a personalized speech reconstruction model. It uses deep learning based STCNN+BiGRU architecture to achieve this goal. The results obtained using MyLipper show an improvement of over 20% in reconstructed speech's intelligibility (as measured by PESQ) using multiple views as compared to a single view visual feed. This confirms the importance of exploiting multiple views in building an efficient speech reconstruction system. The paper further shows the optimal placement of cameras which would lead to the maximum intelligibility of speech. Further, we demonstrate the reconstructed audios overlaid on the corresponding videos obtained from MyLipper using a variety of videos from the dataset"}, {"label": 0, "content": "Estimation of the 6-Dof pose of 3d objects has been a hot research field for a long time. When robots and cameras are integrated into a system, the pose of the object can be estimated through the camera, and then the robot can be used to manipulate the object accurately. Traditional object pose estimation methods include the template-matching based method and invariant feature-based method. The method based on invariant features requires the extraction of invariant features from images with rich texture, so it is not suitable for texture-less parts, which are common in industrial applications. The template-matching method is based on edge and contour information, so it is more suitable for part detection and pose estimation of industrial applications. LINEMOD proposed by Hinterstoisser is a successful template matching method, which accelerates the template matching process through a specially designed storage structure. However, the template-based matching method generally adopts sliding window method and is very time-consuming in computation, which makes it impractical for robotic application. In this paper, we propose a new method, which combines Fully Convolutional Network (FCN) with LINEMOD algorithm. With this method, the detection and location of the object in the image can be archived quickly. Then the local image, instead of the whole image, is used for LINEMOD template matching. Experimental results show that, compared with the standard LINEMOD method, the pose estimation speed can be increased and consistent matching results can be obtained."}, {"label": 1, "content": "With the rapidly increasing number of Parkinson's Disease cases in the Philippines, there is an urgent need for Ambient Assisted Living systems to improve the quality of life and independent living of patients with the condition. Although there are existing systems such as the RAReFall Detection system that use various sensors to monitor human activities, they are complex and not easily accessible due to their high costs. To address this issue, this project aims to develop a cost-efficient and user-friendly smartphone-based Ambient Assisted Living system. It will incorporate the use of smartphone accelerometer and gyroscope sensors to detect and categorize daily activities and falls of Parkinson's Disease patients, providing immediate response and appropriate advice to prolong their active participation in their communities."}, {"label": 1, "content": "This project involves the use of IoT along with an intelligent event-driven system for the purpose of implementing an efficient attendance tracker system in quasi real-time. The primary objective of the proposed system is to maintain the whole system in standby mode, except for the low-power motion sensor. When an event is detected, the front-end embedded processor is alerted and activates the remaining system modules, such as the webcam and communication block. The utilization of the event-driven feature has resulted in improved system performance in terms of power consumption and resource utilization in comparison with the conventional approach.\n\nThe initial phase of the project has been successfully implemented and tested. The system is based on a Raspberry Pi 3 board which is integrated with two Passive Infrared (PIR) sensors and two webcams. On detection of an event, the webcam captures an image which is recorded via the Raspberry Pi webcam server and shared with other system modules via the Porta Space application, which functions as a hub between the Raspberry Pi and the cloud. Simultaneously, the attendance status is updated via the IFTTT on the cloud-based log, and the relevant authorities are notified via an email. This process is repeated each time a person enters or leaves the concerned location. The attendance log is available globally via the cloud and can be accessed at any time.\n\nThe system design flow has been described, and the devised system functionality has been tested with an experimental setup. The results have confirmed proper system operation."}, {"label": 1, "content": "This paper presents an innovative Viterbi method for Gaussian minimum shift keying (GMSK) used in the satellite based automatic identification system (AIS). The proposed method is based on one-bit differential detection and is easy to implement in hardware. The conventional hard decision based demodulation methods are greatly affected by Gaussian white noise, particularly when the BT value is small due to the inherent intersymbol interference (ISI) of the GMSK. The traditional correlation-based Viterbi method for GMSK requires a large amount of computational resources making it unsuitable for hardware implementation. Therefore, the paper proposes a phase rotation-based differential Viterbi method which consumes fewer calculation resources and exhibits superior bit error rate (BER) performance compared to the hard decision method. The corresponding BER results are presented in the paper."}, {"label": 0, "content": "An adaptive block-based compressive sensing (BCS) video reconstruction algorithm based on temporal-spatial domain characteristics is proposed. Firstly, assigned weight function is introduced, and the adaptive sampling scheme of joint wavelet coefficients and variance are developed. On the basic, the global measurement matrix is constructed by assigned weight matrix to realize global reconstruction. Secondly, combined with the block-based multi-hypothesis (MH) model and the minimum total variation (TV) model, the predictive-residual reconstruction model of joint temporal-spatial domain characteristics is constructed, and the prediction frame of the current frame is obtained by iteration. Thirdly, the residuals are calculated by global reconstruction and combined with the predictive frames of the current frame to reconstruct a new frame, To validate the effectiveness of proposed video reconstruction algorithm, the results are compared with other BCS algorithms which proposed from the aspects of video reconstruction in recent years. The experiment results show that the proposed algorithm can effectively improve the quality of video reconstruction and further reduce the computational complexity compared with other algorithms."}, {"label": 0, "content": "An adaptive sub-synchronous resonance (SSR) damping controller using Kalman filters are proposed for AC/DC hybrid system with high penetration wind in this paper. First, the supplementary damping coordinated controller of DFIG and HVDC are designed using multiple input multiple output LMIs. Then the adaptive method using Kalman online estimations for tracking the operating points caused by the variable wind power output. The AC/DC hybrid system integration with wind power generation is used as test system. The characteristic of variable SSR modes with wind power outputs is investigated, and the simulation results demonstrated that the adaptive SSR damping control method could not only track the operating condition, but also could give robust and effective control in the case of large fluctuation of wind power output."}, {"label": 1, "content": "As we approach the exascale era of high performance computing facilities, it becomes increasingly important to gain a detailed understanding of hardware failures. The extreme memory capacity of modern supercomputers can lead to data corruption errors that were previously statistically negligible. To mitigate the adverse effects of hardware faults on exascale workloads, we must learn from the behavior of current hardware.\n\nIn this study, we investigate the predictability of DRAM errors using field data from two recently decommissioned supercomputers. By applying statistical machine learning techniques to predict the probability of DRAM errors at previously un-accessed locations, we found that physically-informed models outperformed purely statistical methods. Our findings support the expected physical behavior of DRAM hardware and provide a mechanism for real-time error prediction.\n\nOur methods demonstrate the importance of spatial locality over temporal locality in DRAM errors and show that relatively simple statistical models are effective at predicting future errors based on historical data, allowing for proactive error mitigation. Furthermore, our approach is demonstrated to be real-world feasible by training an error model on one supercomputer and effectively predicting errors on another.\n\nIn summary, our study highlights the importance of understanding hardware failures in high performance computing facilities and provides a framework for predictive error mitigation in the exascale era."}, {"label": 1, "content": "A Flexible Machine Vision (FMV) Inspection System has been created with the goal of requiring minimal retuning in hardware and software for a smooth transition between various applications. To test the system's flexibility, it was utilized to inspect three different kinds of small parts: plastic gears, plastic connectors, and metallic coins. The system was required to distinguish between four different known styles of each part, and also one unknown style, resulting in a total of five classes. In previous work, a hybrid Support Vector Machine (SVM) classifier was utilized for the connector application. However, the hybrid SVM could not efficiently achieve the target performance of 95% accuracy when applied to the coin application. To overcome this problem, a new hybrid method was developed by combining SVM and an Artificial Neural Network (ANN) or ANN-SVM classifier. The results of this novel approach are presented in this paper, and the utilized image library is available at http://my.me.queensu.ca/People/Surgenor/Laboratory/Database.html."}, {"label": 0, "content": "With distributed energy resources widely integreted to an distribution network (DN) in multiple microgrid (MG) manner, the distributed dynamic optimal power flow (D-DOPF) becomes a major concern because DN and MGs may belong to different owners. This paper proposes a fully D-DOPF algorithm based on the cutting plane consensus (CPC) method and Ward equivalent. First, DN and MGs are decoupled by means of Ward equivalent, i.e., at optimizing each MG, other MGs and DN are replaced by their Ward equivalent circuits respectively, while at optimizing DN, MGs are replaced by their Ward equivalent circuits respectively. Then, high-precision linearization of nonlinear power flow equations is used to express node voltages approximately by a linear function of node injection powers respectively for each MG and DN. Hence, the D-DOPF model is built as a distributed quadratic programming (D-QP) problem, taking DN and MGs as independent agents. Finally, CPC is applied to solve D-DOPF. For each agent, a master problem is constructed to approximate the original problem, and only the cutting plane constraints are transferred between agents. This method does not need an upper level coordinator and also guarantees that each agent has good confidentiality."}, {"label": 0, "content": "This paper presents Navion, an energy-efficient accelerator for visual-inertial odometry (VIO) that enables autonomous navigation of miniaturized robots (e.g., nano drones), and virtual reality (VR)/augmented reality (AR) on portable devices. The chip uses inertial measurements and mono/stereo images to estimate the drone's trajectory and a 3-D map of the environment. This estimate is obtained by running a state-of-theart VIO algorithm based on non-linear factor graph optimization, which requires large irregularly structured memories and heterogeneous computation flow. To reduce the energy consumption and footprint, the entire VIO system is fully integrated on-chip to eliminate costly off-chip processing and storage. This paper uses compression and exploits both structured and unstructured sparsity to reduce on-chip memory size by 4.1\u00d7. Parallelism is used under tight area constraints to increase throughput by 43%. The chip is fabricated in 65-nm CMOS and can process 752 \u00d7 480 stereo images from EuRoC data set in real time at 20 frames per second (fps) consuming only an average power of 2 mW. At its peak performance, Navion can process stereo images at up to 171 fps and inertial measurements at up to 52 kHz, while consuming an average of 24 mW. The chip is configurable to maximize accuracy, throughput, and energy-efficiency tradeoffs and to adapt to different environments. To the best of our knowledge, this is the first fully integrated VIO system in an application-specified integrated circuit (ASIC)."}, {"label": 1, "content": "This article delves into the effectiveness of a scheduling algorithm aimed at increasing the efficiency of corporate information and computer networks. The algorithm is based on priority information processing models and aims to improve network performance by analyzing and considering the characteristics of the network during different modes of operation.\n\nBy introducing priorities to determine the order of servicing information flows of different types, the algorithm helps to reduce the average waiting time for system operation, thereby minimizing losses due to waiting. The task of analyzing the efficiency of priority systems in complex information and computing systems is a promising application-oriented task as the use of an effective dispatch algorithm can significantly enhance network performance.\n\nIn summary, this article highlights the potential benefits of using a scheduling algorithm that incorporates priority information processing models to optimize corporate information and computer networks. By considering the unique properties and characteristics of the network in various modes of operation, the algorithm presents a promising solution for improving network efficiency and reducing waiting times."}, {"label": 0, "content": "This paper presents a method to identify T-equivalent parameters of transformer using its port data. In order to use the port information of the transformer to determine the internal fault of the transformer, according to the model reference adaptive principle, the recognition model and the adjustment model of the transformer are constructed. The fitness function is composed of the response output of the two models, and PSO algorithm is used to identify its parameters of equivalent circuit. Firstly, the simulation platform is built in MATLAB/Simulink to iterate two times. The excitation parameters identified by the first iteration are given as the known quantity, and the second iteration can identify the leakage impedance parameters of the high and low voltage side of the equivalent circuit. Then, the feasibility of the algorithm is verified on the experimental platform."}, {"label": 1, "content": "State of charge (SOC) estimation is a critical technology for battery management systems (BMS) and is vital for the safe, reliable, and economical operation of electric vehicles (EVs). In this paper, we present a new approach to improve the accuracy of SOC estimation based on the Spherical Simplex-Radial Cubature Kalman Filter (SSRCKF) algorithm. Our results show that this approach outperforms the Unscented Kalman Filter (UKF) and Cubature Kalman Filter (CKF) in estimating SOC.\n\nTo validate our approach, we conducted experiments using the Worldwide harmonized Light Vehicles Test Procedure, which was introduced in the European Union last year. To test the anti-interference ability of the SSRCKF algorithm, we added noise to the measured data of current and voltage. The results showed that our approach estimated SOC more accurately than the UKF and CKF.\n\nIn conclusion, our new approach based on the SSRCKF algorithm is a promising method for improving the accuracy of SOC estimation in BMS for EVs. We believe that this will contribute to the broader adoption of EVs by enhancing their safety, reliability, and economic efficiency."}, {"label": 0, "content": "Hardware realization of scalable neurons and synapses are essential for the implementation of large scale Spiking neural network (SNN). In this paper, first, we propose, a novel transient Joule heating based the leaky-integrate and fire neuron (LIF) in scalable PrMnO3 (PMO) RRAM device experimentally. The Joule-heating based thermal runaway is utilized to achieve rectified linear unit (ReLU) voltage dependence of spiking frequency similar to a typical LIF neuron. Second, the Jouleheating hypothesis in PMO is validated by TCAD DC and transient simulations. PMO is extremely thermally resistive semiconductor (300x cf. Si) and hence enables low energy thermal dynamics. The excellent energy, area performance with a synapse in the same material system and thermal engineering makes PMO neuron attractive. Finally, PMO neuron shows software equivalent learning accuracy in SNN on the Fischer's Iris dataset."}, {"label": 0, "content": "Today, wireless sensor networks are widely used in the field of intelligent transportation system. For areas with different vehicle densities, the amount of data that needs to be wirelessly transmitted to the central server is different. In this paper, we propose a new data transmission scheme, i.e., the Sensor On/off scheme based on Polling Algorithm (SOP). In specific, we are going to realize one no packet loss scheduling transmission mechanism in different service density areas, and have effectively integrated switching technology into SOP. That is, when the sensor is idle, we can turn it off to achieve the goal of reducing energy consumption. Finally, we compare the SOP with the random scheme and sequential scheme. It is proved by the analysis results that SOP can effectively reduce the energy consumption of data transmission in wireless sensor networks."}, {"label": 1, "content": "In modern information warfare, there is a growing need for communication signal recognition technology to possess highly reliable and real-time performance capabilities. Research has been conducted on the reliability of communication signal recognition, but little has focused on its speed. This study aims to investigate improved feature extraction methods based on the extreme learning machine (ELM) to enhance the speed of communication signal recognition. Simulations conducted in this study have shown that this approach improves the speed of recognition while maintaining high reliability, achieving ideal recognition accuracy even at low SNRs."}, {"label": 0, "content": "In real-time applications, a fast and robust visual tracker should generally have the following important properties: 1) feature representation of an object that is not only efficient but also has a good discriminative capability and 2) appearance modeling which can quickly adapt to the variations of foreground and backgrounds. However, most of the existing tracking algorithms cannot achieve satisfactory performance in both of the two aspects. To address this issue, in this paper, we advocate a novel and efficient visual tracker by exploiting the excellent feature learning and classification capabilities of an emerging learning technique, that is, extreme learning machine (ELM). The contributions of the proposed work are as follows: 1) motivated by the simplicity and learning ability of the ELM autoencoder (ELM-AE), an ELM-AE-based feature extraction model is presented, and this model can provide a compact and discriminative representation of the inputs efficiently and 2) due to the fast learning speed of an ELM classifier, an ELM-based appearance model is developed for feature classification, and is able to rapidly distinguish the object of interest from its surroundings. In addition, in order to cope with the visual changes of the target and its backgrounds, the online sequential ELM is used to incrementally update the appearance model. Plenty of experiments on challenging image sequences demonstrate the effectiveness and robustness of the proposed tracker."}, {"label": 1, "content": "Our research focused on investigating the current understanding of the Internet of Things (IoT) based on a review of scientific literature. Additionally, we sought to identify potential legal and security risks associated with IoT use that could impact both end-users and the state. The ultimate goal was to categorize and better understand the challenges that arise when using IoT technology. \n\nAfter analyzing the available data, we identified data protection as the central issue. The two primary concerns were traceability and confidentiality. To elaborate, traceability refers to the ability to track and monitor the use of data, while confidentiality is related to the proper safeguarding of sensitive information. \n\nOur research findings underscore the need for greater emphasis on data protection and security measures within the IoT space. Without appropriate safeguards in place, the potential risks to end-users and associated stakeholders could be significant. As the IoT continues to evolve and become more integrated into our daily lives, addressing these challenges should remain a top priority for researchers, policymakers and industry stakeholders alike."}, {"label": 1, "content": "The Internet of Things (IoT) has become a ubiquitous part of our daily lives. However, it faces unique security challenges that must be addressed to ensure the safety of users and their data. One critical aspect of securing a wireless IoT network is intrusion detection.\n\nDesigning an effective intrusion detection system for a wireless IoT network is a challenging task. In this article, we explore the human-in-theloop active learning approach for improving the accuracy of wireless intrusion detection.\n\nActive learning is a method that leverages both machine and human intelligence to improve the performance of machine learning algorithms. By actively selecting the most informative data to learn from, active learning can reduce the amount of labeled data required to achieve high accuracy levels.\n\nWe propose employing active learning in the diverse applications of wireless intrusion detection. In our experimental example, we demonstrate the significant performance improvement of the active learning method compared to the traditional supervised learning approach.\n\nWhile machine learning techniques have been commonly used for intrusion detection, the integration of human intelligence through active learning is still in its infancy. We hope this article helps readers understand the key concepts of active learning and inspires further research in this area.\n\nIn conclusion, the human-in-theloop active learning approach has the potential to enhance the accuracy and efficiency of wireless intrusion detection for IoT networks. As IoT becomes more ubiquitous in our lives, it is crucial to prioritize security, and active learning is one promising approach to achieve this goal."}, {"label": 0, "content": "According to the existing problems of substation hard platen state recognition, this paper studies the intelligent recognition system based on machine learning without modifying the secondary device. A manual patrol inspection car is designed to facilitate image acquisition. Using the hard platen state recognition algorithm based on machine learning, a hard platen intelligent patrolling application is developed. The current state of the hard platens can be quickly obtained by simply input the collected hard platen image. In addition, the patrol database is set up, and the hard platen table is kept, and the patrol and maintenance tasks can be managed. The experiment of this system was carried out at a substation in Guizhou. The results show that the system can recognize the state of the hard platens quickly and accurately, and truly realize the intelligence of the hard platen inspection work."}, {"label": 1, "content": "With distributed energy resources now common in a distribution network (DN) environment using multiple microgrid (MG) configurations, power flow optimization becomes a major concern due to the separate ownership of the DN and MGs. This paper presents a fully distributed dynamic optimal power flow (D-DOPF) algorithm which utilizes the cutting plane consensus (CPC) method and Ward equivalent. In the initial phase, Ward equivalent is employed to decouple the DN and MGs by replacing them with their equivalent circuits during optimization. Next, high-precision linearization of nonlinear power flow equations is used to express node voltages approximately by a linear function. Consequently, the D-DOPF model is formulated as a distributed quadratic programming problem with the DN and MGs treated as independent agents. Finally, CPC is used to tackle the D-DOPF problem. For each agent, a master problem is formulated, with only the cutting plane constraints transmitted between agents, eliminating the need for an upper level coordinator while ensuring good confidentiality between agents."}, {"label": 0, "content": "Edge computing is the emerging architectural paradigm extending cloud technologies to the logical extremes of the network for on-demand and delay-sensitive services. However, once service placement on edge-enabling resources has been dealt with, a new challenge arises: how to process enormous volumes of streaming data to provide query-driven analytics while still satisfying the delay-critical servicing requirements. To overcome this challenge we introduce StreamSight, a framework for edge-enabled IoT services which provides a rich and declarative query model abstraction for expressing complex analytics on monitoring data streams and then dynamically compiling these queries into stream processing jobs for continuous execution on distributed processing engines. To overcome the resource restrictive barriers in edge computing deployments, StreamSight outputs the query execution plan so that intermediate results are reused and not continuously recomputed. In turn, StreamSight enables users to express various optimization strategies (e.g., approximate answers, query prioritization) and constraints (e.g., sample size, error-bounds) so that delay-sensitive requirements relevant to their deployment are not violated. We evaluate our framework on Apache Spark with real-world workloads and show that leveraging StreamSight can significantly increase performance by 4x while still satisfying all accuracy guarantees."}, {"label": 1, "content": "In the field of Cyber Security, there has been a notable shift from Cyber Criminality towards Cyber War in recent years. In response to these new challenges, the expert community has adopted two main approaches: incorporating the philosophy and methods of Military Intelligence and utilizing Artificial Intelligence methods to counter Cyber Attacks. This research paper focuses on the results achieved by the Technical University of Sofia in the implementation of a project that utilizes intelligent methods to improve computer network security. The analysis of various Artificial Intelligence methods has revealed that there is no single method that is equally effective across all stages of Cyber Intelligence. For Tactical Cyber Threats Intelligence, the research team experimented with a Multi-Agent System, while Recurrent Neural Networks were suggested for Operational Cyber Threats Intelligence."}, {"label": 0, "content": "A new multi-frame image super resolution (SR) algorithm via Bayesian modeling with natural image prior modeled by fields of experts (FoE) is proposed. Multi-frame SR can be used to obtain a high resolution (HR) image from a set of degraded low resolution (LR) images without changing any hardware device. However, SR is well known to be an ill-posed problem. So state-of-the-art solutions usually formulate the problem with Bayesian modeling techniques, which infer the HR image based on not only the LR input images but also on prior information about the HR image. Current Bayesian SR approaches typically use simple prior models such as L1 norm, TV prior and Laplacian prior, which cannot exploit the statistics of natural scenes well. In this paper, a Bayesian multi-frame image SR approach using a FOE model as the prior for natural images is presented. The Maximum a Posteriori (MAP) framework is used for estimating the HR image. The proposed method cannot only capture the statistics of natural images well, but also require less computational power than the other Bayesian modelling methods such as Sampling methods and Approximate inference. The proposed method shows superior or comparable results to the state-of-art multi-frame SR methods."}, {"label": 0, "content": "Accurately analyzing the sources of performance anomalies in cloud-based applications is a hard problem due both to the multi tenant nature of cloud deployment and changing application workloads. To that end many different resource instrumentation and application performance modeling frameworks have been developed in recent years to help in the effective deployment and resource management decisions. Yet, the significant differences among these frameworks in terms of their APIs, their ability to instrument resources at different levels of granularity, and making sense of the collected information make it extremely hard to effectively use these frameworks. Not addressing these complexities can result in operators providing incompatible and incorrect configurations leading to inaccurate diagnosis of performance issues and hence incorrect resource management. To address these challenges, we present UPSARA, a model-driven generative framework that provides an extensible, lightweight and scalable performance monitoring, analysis and testing framework for cloud-hosted applications. UPSARA helps alleviate the accidental complexities in configuring the right resource monitoring and performance testing strategies for the underlying instrumentation frameworks used. We evaluate the effectiveness of UPSARA in the context of representative use cases highlighting its features and benefits."}, {"label": 1, "content": "The prevalence of 802.11 WiFi and the continued miniaturization of devices due to Moore's law has opened the doors for the Internet of Things (IoT) to flourish. Nowadays, a multitude of home appliances, from lightbulbs to toasters, are not only equipped with connectivity to the Internet but are also interconnected via WiFi. This allows for the vision of smart homes that run themselves, leaving human operators in full control - or so we think. Despite the progress made since the early days of this technology, the same cannot be said for the vulnerabilities these smart devices pose. We conducted a thorough analysis of a selection of the most common smart home appliances, including lightbulbs, power switches, motion sensors, security cameras, and home assistants, in order to assess their susceptibilities to breaches by 21st century home intruders."}, {"label": 0, "content": "We categorized VPN's impact in to two different aspects; The affecting aspect, and the affected aspect. The affecting aspect of the impact encompasses factors such as security, algorithms, hardware, and software. Whereas the affected aspect is of the likes of network performance. While VPNs have managed to integrate security, one of the affecting aspect of the impact on one hand, on the other hand, VPNs should be regarded as a potential threat to network performance. In this study, for affordability purpose, we choose to use NS-2 simulated test-bed to shed light on the VPN's performance impact in a network. Considering the most common network performance metrics, throughput and delay; we assessed these performance metrics by means of average and percentage changes theories. The findings emphasize quantitative impact on the TCP/IP throughput than on its counterpart UDP/IP. We finally developed an analytical equation to model this VPN's performance impact."}, {"label": 0, "content": "Machine learning (ML) on edge computing devices is becoming popular in the industry as a means to make control systems more intelligent and autonomous. The new trend is to utilize embedded edge devices, as they boast higher computational power and larger memories than before, to perform ML tasks that had previously been limited to cloud-hosted deployments. In this work, we assess the real-time predictability and consider data privacy concerns by comparing traditional cloud services with edge-based ones for certain data analytics tasks. We identify the subset of ML problems appropriate for edge devices by investigating if they result in real-time predictable services for a set of widely used ML libraries. We specifically enhance the Caffe library to make it more suitable for real-time predictability. We then deploy ML models with high accuracy scores on an embedded system, exposing it to industry sensor data from the field, to demonstrates its efficacy and suitability for real-time processing."}, {"label": 0, "content": "This paper presents a novel equivalent modeling method for distributed photovoltaic (PV) power station clusters. Deep Learning (DL) is proposed to PV power station clusters modeling and the training algorithm is Deep Belief Network (DBN) made up of multiple layers of restricted Boltzmann machines (RBM). A DBN algorithm is applied to dynamic equivalent modeling of PV clusters after first-step clustering using the improved K-means algorithm. The input variables of the neural network are irradiance variation, voltage fluctuation, reactive power reference of the dual-loop controller, the output active and reactive power of PV clusters. The output variables are the output active and reactive power of PV clusters. The datasets are obtained through 6560 experiments. Then a layer-by-layer unsupervised learning method is used to pre-train the network followed by fine-tuning the parameters using a supervised back-propagation (BP) method. Finally, the equivalent model of PV clusters based on DBN is built and applied to a typical distribution network in Anhui Province. The validity and accuracy of the proposed model are verified in three different disturbance cases. At the same time, the computational complexity and the simulation time are reduced using the proposed model significantly."}, {"label": 1, "content": "The accurate modeling of spectrum occupancy is crucial for improving channel utilization in cognitive radio (CR) systems. Traditional models heavily rely on PU activity, which varies spatially and temporally, making it challenging to develop an accurate model. In this study, a generalized Gaussian Mixture model (GMM) is explored to characterize the spectrum occupancy of the PU in three spectrally distinct CR scenarios: VHF/UHF band, GSM band, and ISM band. The GMM is compared to a Beta distribution-based model and demonstrates superior goodness-of-fit performance. Additionally, the robustness of the GMM is validated through learning-based prediction using Recurrent Neural Networks (RNN). These findings suggest an approach that combines statistical and predictive modeling of spectrum occupancy to enhance dynamic spectrum access."}, {"label": 0, "content": "We introduce Generative Adversarial Network (GAN) into the radio machine learning domain for the task of modulation recognition by proposing a general, scalable, end-to-end framework named Radio Classify Generative Adversarial Networks (RCGANs). This method naively learns its features through self-optimization during an extensive data-driven GPU-based training process. Several experiments are taken on a synthetic radio frequency dataset, simulation results show that, compared with some renowned deep learning methods and classic machine learning methods, the proposed method achieves higher or equivalent classification accuracy, superior data utilization, and presents robustness against noises."}, {"label": 1, "content": "The assessment of power system static stability is crucial for ensuring the security and control of power systems. However, conventional methods for static stability assessment rely on physical models and intensive simulations, which are computationally expensive and not well-suited for online engineering applications. To address this, a novel static stability assessment method based on scale-invariant feature transformation (SIFT) is proposed in this paper.\n\nThe SIFT method directly extracts the associations among holographic data reflecting various static operating conditions of power systems. By improving the generalized elasticity index based on operating features, accurate assessments of static stability situations can be achieved. Simulation results using the New England 10-machine 39-bus system reveal that the improved grid generalized elasticity index has a higher slope and better engineering application value.\n\nOverall, this SIFT-based method provides a more efficient and effective way to assess static stability, ensuring the safe operation of power systems."}, {"label": 0, "content": "Energy-efficiency in Wireless Sensor Networks (WSNs) has been regarded as the core issue for designing any communication protocol. Sensor networks consist of limited battery-powered nodes and recharging or replacing is not practical being deployed in harsh environments like underground mines. So designing of WSNs should be concentrated on energy efficiency. Clustering technique is used very effectively to achieve scaling up and power saving in WSNs. It allows hierarchical structures to be built on the nodes and enables the more efficient use of scares resources. In this work, we have proposed a hybrid clustering scheme which able to meet energy constraints of WSNs. It allows data transmission from sensor nodes to the sink with reasonable consumption of energy."}, {"label": 1, "content": "Modern tactical wireless network (TWN) communication technologies not only transmit voice but also data, making their security crucial for network centric warfare operational (NCW) theory. Intrusion detection systems (IDS) are used to recognize and respond to information warfare attacks directed to the network. However, false detection of nodes in hostile environments remains a significant problem. Machine learning algorithms have shown applicability in addressing IDS in TWN. This paper analyzed seven machine learning classifiers: Multi-Layer Perceptron, Bayesian Network, Support Vector Machine (SMO), Adaboost, Random Forest, Bootstrap Aggregation, and Decision Tree (J48) using the WEKA tool. Ensemble-based learning methods outperformed single learning methods in terms of detection accuracy metrics such as AUC, TPR, and FPR. However, ensemble classifiers tend to have slower build and model test times."}, {"label": 0, "content": "The Internet of Things (IoT) provides a new paradigm for the development of heterogeneous and distributed systems, and it has increasingly become a ubiquitous computing service platform. However, due to the lack of sufficient computing and storage resources dedicated to the processing and storage of huge volumes of the IoT data, it tends to adopt a cloud-based architecture to address the issues of resource constraints. Hence, a series of challenging security and trust concerns have arisen in the cloud-based IoT context. To this end, a novel trust assessment framework for the security and reputation of cloud services is proposed. This framework enables the trust evaluation of cloud services in order to ensure the security of the cloud-based IoT context via integrating security- and reputation-based trust assessment methods. The security-based trust assessment method employs the cloud-specific security metrics to evaluate the security of a cloud service. Furthermore, the feedback ratings on the quality of cloud service are exploited in the reputation-based trust assessment method in order to evaluate the reputation of a cloud service. The experiments conducted using a synthesized dataset of security metrics and a real-world web service dataset show that our proposed trust assessment framework can efficiently and effectively assess the trustworthiness of a cloud service while outperforming other trust assessment methods."}, {"label": 0, "content": "In order to solve the problem of underwater object images classification under the condition of insufficient training data, a novel underwater object images classification method based on Convolutional Neural Network(CNN) is proposed. Firstly, an advanced method of Markov random field-Grabcut algorithm is adopted to segment images into two regions: shadow and sea-bottom. Then, considering the character of the dataset, a CNN is constructed referring to Alexnet structure, consisting of two parts with different functions: convolutional part and classification part. At last, the CNN is trained to classify three different shapes of underwater objects(cylinder, truncated cone and sphere) utilizing the transfer learning approach. The method is applied to synthetic aperture sonar(SAS) datasets for validation. Comparing with Support Vector Machine(SVM) and CNN which only use trial dataset, the proposed method can achieve a better accuracy."}, {"label": 1, "content": "To enhance the quality and the visual appearance of images that are blurred, we suggest using an image deblurring algorithm that is based on dictionary learning. Our proposed approach involves dividing the blurred image into structured groups of image blocks. Next, we utilize the K-SVD dictionary and the PCA dictionary to restore the image blocks. Finally, morphological operations are applied to the difference image to obtain the original image. Our experiments demonstrate that this algorithm outperforms others in terms of peak structural similarity and visual effects."}, {"label": 0, "content": "The reliability and lifetime determine the levelized cost of energy (LOCE) of photovoltaic (PV) modules and effectiveness of PV system. Although this theme has attracted researchers attention in recent years, there still lack an effective method to model PV modules power degradation. Therefore, this paper put forward to adopt gamma process to establish the relationship between PV modules power degradation, and temperature, relative humidity (RH). And then PV modules service lifetime is predicted under accelerated damp-heat conditions. Firstly, accelerated damp-heat tests are carried out on three different temperature and RH levels. Based on Peck model, a data transformed method is proposed to obtain more power degradation data under other seven damp-heat conditions. Secondly, gamma process with an exponential transformation is applied to model PV modules power degradation under accelerated damp-heat conditions. The relationship between power degradation and temperature, RH is established by theoretical derivation and validated by experimental data. Then Expectation Maximum (EM) algorithm is proposed to estimate model's parameters. Finally, PV modules lifetime under several different damp-heat conditions is predicted. It is found that PV modules lifetime is approximate 20 to 25 years under (50\u00b0/45% RH) condition. But we also conclude that PV modules lifetime sharply decreases as the increment of temperature and RH. Hence, more factors or other test types are recommended to be considered in later accelerated tests."}, {"label": 1, "content": "Dynamic voltage restorers (DVRs) have become a solution to compensate for voltage sags in distribution networks to prevent production losses, especially in the Premium Power Park(PPP). However, in order to make DVRs more popular and effectively improve the intelligent level of the power grid, the compensation strategy must be optimized to extend the compensation time as much as possible. In this paper, we propose a time-maximized compensation strategy based on the energy-minimized compensation strategy, which achieves a maximum compensation time.\n\nUsing the formula of dc-link discharge, we deduce the calculating formula of optimum phase angle jump of load voltage. We then analyze the principle and characteristic of the compensation strategy respectively for single-phase and three-phase systems. The simulation and experimental results affirm the effectiveness of the proposed compensation strategy.\n\nOverall, the time-maximized compensation strategy has demonstrated promising potential to make DVRs more practical and efficient for distribution networks. This can lead to better performance and cost-effectiveness, further advancing power system technology."}, {"label": 1, "content": "The revolution of deep neural networks (DNNs) has brought significant progress in the development of autonomous driving. However, achieving both timing predictability and energy efficiency in any DNN-based autonomous driving system is challenging due to their conflicting goals. In this paper, we introduce PredJoule, a novel framework for optimizing DNN workloads in a GPU-enabled automotive system that achieves both latency guarantees and energy efficiency.\n\nPredJoule relies on a layer-aware design that explores the specific performance and energy characteristics of different layers within the neural network. We evaluated the system on the automotive-specific NVIDIA Jetson TX2 platform for five state-of-the-art DNN models with both high and low variance latency requirements. The results show that PredJoule rarely violates job deadlines and can improve energy efficiency by 65% on average compared to five existing approaches and 68% compared to an energy-oriented approach.\n\nIn summary, PredJoule is a promising solution for achieving both timing predictability and energy efficiency in DNN-based autonomous driving systems. By taking into account the specific performance and energy characteristics of different layers, PredJoule demonstrates improved energy efficiency while meeting job latency requirements."}, {"label": 1, "content": "A new method for modelling the load area and simplifying the electrical network has been proposed. The method is called the multi-port area load equivalent modelling method (ALEEM) and is based on the extended generalized ZIP load model (EGZIP). Unlike the traditional ZIP load model, the EGZIP model takes into account the voltage magnitudes and voltage phase angles of all boundary buses, resulting in a more accurate model for area loads with multiple boundary buses. The load flow calculation considering EGZIP has been derived and analyzed.\n\nTo reduce the number of parameters to be identified in the equivalent model, a multi-port equivalence strategy has been proposed based on the hierarchical and partitioning characteristics of the power grid. The least square estimation (LSE) problem is constructed using the currents measured at varying operation conditions on the boundary bus for parameter identification. The interior point method is used to identify the model parameters.\n\nSimulation tests conducted on the 87 bus system demonstrate that the equivalent model derived from the proposed ALEEM based on EGZIP has higher accuracy. Additionally, the multi-port equivalence strategy reduces both the number of parameters to be identified and the time-consuming equivalent process."}, {"label": 0, "content": "This paper presents a fast depth selection algorithm for CTU (frame coding units) based on machine learning. In view of the fast depth selection algorithm for CTU based on machine learning, due to the lack of the depth discrimination in the initial division of coding units and the inefficiencies of the coding efficiency caused by the input feature selection of the classifier, The paper firstly design the initial division depth prediction strategy based on the texture complexity and quantization parameters to skip some nonessential sizes of coding unit by analyzing the relationship between the texture complexity of the coding unit, the quantization parameters of encoder and the depth selection of the coding unit, and by combining the texture complexity and the quantization parameters to predict the initial dividing depth of the current coding unit. Secondly, by exploring the relationship between the bit-rate, distortion and the depth selection of the coding unit, the input characteristics of the classifier are determined and the selection strategy of the coding unit termination depth based on the bit rate and distortion is designed. Finally, the partition problem of the coding unit is modeled as the problem of the two-element classification and the nearest neighbor classifier is used. By skipping the calculation process of the time-consuming rate distortion cost, the ending dividing depth of the current coding unit can be judged in advance and accelerate the process of the inter-frame coding. Experimental results show that the proposed algorithm can decrease the 34.56% of the frame encoding time, while maintaining the accuracy of the coding unit compared with HM-15.0."}, {"label": 1, "content": "In this paper, we investigate the outage performance of a general dual-hop multiple-input multiple-output (MIMO) amplify-and-forward (AF) relay network, where the source, relay, and destination are equipped with multiple antennas. Our analysis considers maximal-ratio-transmission (MRT) and maximal-ratio combining (MRC) for the transmitter and receiver, respectively. Firstly, we obtain the output signal-to-interference-plus-noise ratio (SINR) of the dual-hop AF relay system with multiple co-channel interferences (CCIs) and noise at the relay. Secondly, we derive closed-form expressions of the outage probability (OP) for both fixed-gain and variable-gain multi-antenna relaying systems. Finally, computer simulations are carried out to validate the performance analysis. Our new analytical expressions provide a fast and efficient method to evaluate the outage performance of the system and enable us to gain valuable insights into the effects of key parameters on the performance of the system. This analysis highlights the benefits of implementing multiple antennas at each node in the relaying network."}, {"label": 1, "content": "Although there are a variety of automated management systems available, their effectiveness in providing informational and decision support for production management at the operational level is limited. Numerous studies conducted by both Russian and foreign authors indicate that the informational aspect of management is essential for its efficiency. Therefore, the development and implementation of software that can collect, modify, interpret, and forecast information is significant for enhancing enterprise efficiency during the transition to the sixth techno-economic paradigm.\n\nIn assembling management, defect management is a critical process. Typically, defect management processes are supported by paper or electronic registers, catalogs, manuals, and the data of various corporate information systems. However, with the increasing intensification of production processes and the consequent increase in workers' informational workload, processing information required for decision-making related to defect processing can be challenging. To address this issue and enhance the quality and speed of processing information, an expert system that facilitates data collection and provides straightforward decision support resources should be utilized.\n\nTo improve the efficiency of engineering technologist work related to the processing of requests for registering defects revealed during aircraft assembling, a proposed solution would be the integration of an expert system. This system allows for the accumulation of data about typical solutions and offers simple resources for decision-making. Based on the first results of the expert system implementation, which was developed by the authors, it is evident that further research into expert systems as a tool for informational and decision support at the operational level of production management is necessary."}, {"label": 1, "content": "Coronary artery disease (CAD) is a rapidly spreading disease globally, which is a major factor for mortality and morbidity. Therefore, there is an increasing interest in developing non-invasive and simple automated methods for CAD diagnosis. Researchers have discovered that a single-channel phonocardiogram (PCG) signal can detect weak CAD murmurs caused by stenosed coronary arteries due to turbulent blood flow. In this study, a new multi-channel data acquisition system was proposed to classify CAD and normal subjects. This method does not require a reference signal such as an electrocardiogram (ECG) signal for PCG signal segmentation. Additionally, spectral moments, spectral entropy, moments of the PSD function, autoregressive (AR) parameters, and instantaneous frequency were used as five different features to capture specific details about the disease. An artificial neural network (ANN) was used for the classification task. The AR features performed particularly well, achieving an accuracy of 74.24%. With multi-channel recorded data, the proposed method performed even better with an accuracy of 69.69% compared to the best performance of the single-channel signal."}, {"label": 1, "content": "The introduction of distributed generators (DGs) and other emerging technologies, such as electric vehicle charging (EV), to distribution networks have significantly impacted the operating philosophy of these networks. As these technologies continue to emerge, the nature of distribution networks is moving from being passive to active. As a result, controlling and operating distribution systems must be reconfigured. A distribution system state estimation based on a real-time model is crucial in ensuring secure control and protection in distribution systems.\n\nThe objective of this paper is to compare branch-current-based distribution system state estimation in both polar and rectangular coordinates. Additionally, the paper discusses the inclusion of synchronized measurements obtained from a Micro-PMU. The methods used were conducted on the IEEE-13 bus distribution test feeder, and the results obtained are discussed in detail.\n\nOverall, the comparison between polar and rectangular coordinates in branch-current-based distribution system state estimation showed that rectangular coordinates outperformed polar coordinates in terms of estimation accuracy. Additionally, the synchronized measurements obtained from the Micro-PMU improved the accuracy of the distribution system state estimation.\n\nIn conclusion, this paper highlights the importance of distribution system state estimation in the control and protection of distribution systems. It also provides a comparison between two commonly used coordinate systems in branch-current-based distribution system state estimation and shows that the use of synchronized measurements obtained from Micro-PMU can enhance the accuracy of this estimation."}, {"label": 1, "content": "This paper discusses the general aspects of energy consumption and environmental effects of cryptocurrency mining technology. The main technical specifications defining energy efficiency of data mining equipment are analysed, and the aggregation of separate units within data mining pools or installation of respective farms are shown to have the most prominent energy saving effect. The paper also outlines design and operation issues related to power supply of data mining pools, highlighting the lack of sufficient standards prescribing power supply design for this type of energy consumer. In addition, the power supply of a data mining pool located in Moscow is studied, including a power quality survey, and the impact of data mining equipment on power factor and grid voltage variations is demonstrated."}, {"label": 0, "content": "Many recent variational optical flow methods are not robust for illumination variance, and they only consider local image relation in terms of illumination. In this paper, we propose a new efficient illumination-invariance total variation optical flow method called the weighted regularization transform, which uses and optimizes the Weber's Law. Our method exploits unequal probability as the weight that has non-local information to estimate stable optical flow despite illumination changes. The proposed method uses a coarse-to-fine pyramid model to reduce the influence on the data term from illumination. Then, an energy optimization procedure is introduced to constrain the minimization of the data term with the non-local regularization. Experimentation with the proposed method has been performed on three optical flow datasets and a face liveness detection database, which have challenging illumination variations, and the results demonstrate that the proposed method is quite robust with respect to variations in illumination."}, {"label": 0, "content": "A special feature of tracking control system is to ensure the accuracy of the control set in dynamic modes. The presence of non-linearities in the control object, in the system drives, and especially the non-stationarity of the parameters of the system as a whole makes it difficult to achieve the desired quality indicators using standard classical control algorithms. The paper deals with a new approach to the implementation of fuzzy servo control systems. At the same time, a special emphasis is placed on building the tracking control system. This principle of building an intelligent system not only expands the functionality in the implementation of corrective control action, but also makes it possible to provide self-adjustment of the controller under the selected optimization criteria. This approach is considered on an example of the tracking system model on the basis of the DC electric drive at working off difficult in form of changing signal."}, {"label": 1, "content": "The advancement in hardware and software technology, combined with the improvement in TV screen quality and affordability, has made it possible to simulate the visual terrain behind a train simulator window for locomotive driver training. However, it is important to note that despite the availability of modeling, a thorough system analysis is required to select the key components of the 3D modeling system. The driver interacts with simulated models instead of real ones, and the limitations of creating a complete model must be considered.\n\nIn order to perform a comparative analysis, it is essential to develop criteria for evaluating the visual situation mimicking system. The primary criterion for a successful 3D image modeling system in a locomotive simulator is the ability to train professional skills in judging the distance to visible 3D models. Previous studies have addressed similar tasks in aviation simulators and found that it can be solved using either one-channel or two-channel 3D imaging systems. One-channel systems affect accommodation and convergence of vision, while two-channel systems affect visual disparity.\n\nThe selection of a particular 3D simulation class is based on the simulator's objectives. Once the class is chosen, the main nodes of the visual environment simulator must be determined to meet the objectives of the simulator. The article presents one possible method for evaluating the primary nodes of the visual environment simulator, which permits the formation of the optical-software-hardware simulation system of the requisite 3D image for the train simulator of the locomotive driver. The quality of the information space must be of sufficient quality to impart the skills required for effective train management."}, {"label": 1, "content": "Ultra-wideband (UWB) technology is a promising tool for accurate indoor localization. Two commonly used algorithms for UWB to localize the mobile station (MS) are Time of Arrival (TOA) and Time Difference of Arrival (TDOA). However, these methods are susceptible to degradation under Non-Line-of-Sight (NLOS) conditions. In contrast, the Inertial Measurement Unit (IMU) is not prone to NLOS, but its accuracy is only reliable for a brief period due to drift errors. Combining these two systems can improve the accuracy of MS localization. To achieve this, the Extended Kalman Filter (EKF) algorithm is used to integrate IMU and UWB with TOA or TDOA approaches. The EKF can detect inaccurate UWB range measurements due to NLOS and incorporate the IMU measurements to improve accuracy. During real field tests, channel impulse response (CIR) from UWB is used with the support vector machine (SVM) algorithm to enhance the detection accuracy of NLOS. The EKF uses only the accurate measurements for further calculations. The proposed method shows promising results in both simulation and real field tests."}, {"label": 1, "content": "The paper presents a solution for the problem of migrating content from an old website to a new platform with enhanced interactive features. The decision to use the Drupal-based content management system was reached after an analysis of comparative characteristics of other CMSs. The implementation of desired functionality is outlined in detail. Additionally, the paper provides a concise overview of the functional requirements classes for information security. A particular class is used as an example to showcase the adaptation of the ranking method to evaluate the likelihood of unauthorized access, incorporating various factors with respective valid values. The analysis of password length, based on the size of the original character alphabet, is also presented. Calculations of probabilistic hacking probabilities are included, and suggestions for optimal combinations for password security are offered."}, {"label": 1, "content": "Cloud computing offers convenient access to IT resources via the internet. These resources are controlled by access control policies that allow or deny permissions. In this paper, we introduce a formalization of the policy language used in Amazon Web Services (AWS) and a tool called ZELKOVA that helps users verify policy properties. ZELKOVA is capable of encoding policy semantics into SMT, comparing behaviors, and checking that policies are correctly configured. By solving a PSPACE-complete problem, ZELKOVA is able to detect misconfigurations of policies and is used millions of times each day. With ZELKOVA, AWS users can rest easy knowing their access control policies are properly enforced."}, {"label": 0, "content": "To model the load area and simplify the electrical network, a multi-port area load equivalent modelling method (ALEEM) based on the extended generalized ZIP load model (EGZIP) is proposed. Different from the traditional ZIP load, the EGZIP load model incorporates the voltage magnitudes and voltage phase angles of all boundary buses, which can equivalently model the area load with multiple boundary buses more accurately. The load flow calculation considering the EGZIP is derived and analyzed. Also, based on the hierarchical and partitioning characteristics of the power grid, a multi-port equivalent strategy is proposed to reduce the number of parameters to be identified in the equivalent model. For parameter identification, the currents measured at varying operation conditions on the boundary bus are used to construct the least square estimation (LSE) problem. The interior point method is used to identify the model parameters. The simulation test conducted on the 87 bus system proves the equivalent model derived from the proposed ALEEM based on EGZIP has higher accuracy and the multi-port equivalence strategy can reduce both the number of parameters to be identified and the time consuming on equivalent process."}, {"label": 0, "content": "The problem of nontechnical losses (NTL) detection using pattern recognition methods has been studied by many research groups. However, a comparison between the methods proposed by those authors is hardly ever possible because a database for comparison of NTL detection methods has not been made available to the research community. In this paper, we propose four variations of a database based on the IEEE 123 Bus-Test Feeder for testing NTL detection methods. The models and hypotheses used to synthesize the system are explained. An application of the Optimum-Path Forest classifier for NTL detection using the aforementioned database is also presented. The database is in Matlab platform and is available at http://www.power.ufl.edu."}, {"label": 1, "content": "The presence of periodic impulses in vibration signals often indicates the occurrence of faults in roller bearings. However, detecting these faults in complex working conditions with heavy noises can be difficult. To solve this problem, a hybrid method of ensemble empirical mode decomposition (EEMD) and L-Kurtosis clustering-based segmentation is proposed. EEMD can express the intrinsic essence using a simple and understandable algorithm to solve the mode mixing phenomenon. L-Kurtosis is an improved version of kurtosis that recognizes the impulses without the influence of outliers. The L-Kurtosis value is employed as an indicator in the clustering-based segmentation method to extract fault features from background noises. To demonstrate the feasibility of this method, benchmark data simulations and experimental investigations were conducted to detect faults in bearings. Results show that this method efficiently recognizes faults in bearings."}, {"label": 1, "content": "In this paper, we focus on the scheduling of mixed-criticality (MC) systems with graceful degradation, which ensure that LO-criticality tasks are provided with some service in HI mode through minimum cumulative completion rates. To achieve this, we present an admission-control procedure that is easy to implement and determines which LO-criticality jobs should be completed in HI mode.\n\nTo ensure the schedulability of MC systems under EDF-VD scheduling, we propose a demand-bound-function-based MC schedulability test. This test takes into consideration two virtual deadline setting heuristics and runs in pseudo-polynomial time, making it efficient for various MC systems. \n\nFurthermore, we discuss a mechanism for the system to recover from HI to LO mode, and quantify the maximum time duration required for this process. Finally, we showcase the effectiveness of our proposed method by experimental evaluation, comparing it to state-of-the-art MC schedulers.\n\nOverall, our proposed method offers an efficient solution for the scheduling of MC systems with graceful degradation, ensuring that LO-criticality tasks are given some service in HI mode while maintaining the schedulability of the system."}, {"label": 1, "content": "The combination of data from multiple sources can provide valuable insights into various phenomena. However, integrating disparate data sources can be challenging due to differences in data representation, format, and collection patterns, as well as the integration of foreign data attributes. To address these challenges, this study proposes a scalable and distributed data integration framework called Confluence. This framework not only facilitates query joins, but also generates accurate interpolations for targeted spatiotemporal scopes while considering uncertainty in case of data misalignment. Confluence efficiently orchestrates computations and allows for distributed query evaluations with a dynamic relaxation of query constraints. The system is also locality-aware and leverages model-based dynamic parameter selection to provide accurate estimations. Empirical benchmarks demonstrate the improved performance of Confluence over GeoMesa, a Spark-based geospatial analytics framework, in terms of accuracy, latency, and throughput at scale in a distributed cloud computing environment."}, {"label": 0, "content": "This paper introduces a method of abnormal human activity recognition in surveillance video. The method uses Bayes Classifier and Convolutional Neural Network to detect four activities, including walking, running, punching and tripping. KTH dataset is used as the input of Bayes Classifier and Convolutional Neural Network. Moving human target in each frame is detected by Kalman Filter and three features of the target image are extracted. The features include length-width ratio, entropy, and Hu invariant moment. Meanwhile, convolutional neural network of abnormal human activity recognition is built and trained. Experiments show that recognition accuracy of Bayes Classifier reaches 88%, 92%, 92% and 100% for each activity, and Convolutional Neural Network reaches 92%, 96%, 100% and 100% for each activity."}, {"label": 1, "content": "Many South African universities are facing the challenge of below average throughput among Information Technology students specialized in software development. One of the factors contributing to this phenomenon is the difficulty students face in conceptualizing the associated information and manner of thinking required to be successful in their studies, particularly among first year students. This is even more pronounced when considering the object-oriented programming concepts and paradigms that students are required to master as part of their studies. \n\nLiterary evidence suggests that a high level of working memory, which is associated with abstract thinking ability, is required when learning and applying object-oriented programming concepts. The problem is compounded by the fact that the Information and Communication Technology sector of a country depends heavily on the graduating student population to grow sustainably. \n\nTo address this issue, a specialized software instrument was developed and tested on a sample of first year students at a University of Technology. The study focused on the effect of the instrument on improving the abstract thinking ability of these students, particularly in relation to object-oriented programming, as measured by instruments such as the General Scholastic Ability Test (GSAT), rather than on the instrument itself. The results of the study are expected to shed light on how universities can better equip their students with the necessary skills to succeed in their studies and contribute to the growth of the Information and Communication Technology sector."}, {"label": 1, "content": "Energy efficiency is a critical issue in designing communication protocols for Wireless Sensor Networks (WSNs). WSNs are composed of nodes with limited battery power, and it is not feasible to recharge or replace them, especially in harsh environments like underground mines. Therefore, WSN design should focus on energy efficiency. Clustering techniques have been used effectively to achieve scalability and power savings in WSNs. It enables hierarchical structures to be built on the nodes, facilitating more efficient use of scarce resources. In this study, we have proposed a hybrid clustering scheme that meets the energy constraints of WSNs. The scheme allows data transmission from sensor nodes to the sink while consuming a reasonable amount of energy."}, {"label": 0, "content": "This article is depicting the Strengths and weaknesses of Artificial Intelligence related to the improvement of customer online and offline experience, and the possible methods in order to measure them. These methods include both researches non-based and based on interviews. The presence of AI in the retail industry is becoming a key component of the customer experience. Through a deep analysis of existing tools to extract information, we try to explain ways to interpret them, in order for companies to create a real usage out of them, either on online or offline retail experience. Hence, with this research, we also want to provide an insight on how this experience could be improved in the future, and how it will most likely be inherent to our daily customer experience."}, {"label": 1, "content": "The widespread adoption of microgrids is a promising trend for the future of power systems. These systems integrate distributed power sources and cutting-edge technologies, but also pose significant challenges to traditional protection schemes. To address this issue, we propose a comprehensive protection scheme based on dynamic state estimation (DSE). The scheme uses settingless relays to monitor individual protection zones of the microgrid, continuously receiving measurements and performing DSE to detect abnormalities in the time domain.\n\nHowever, erroneous measurements due to hidden failures or malicious actors can also occur. To increase security and dependability, we add a centralized layer that uses DSE in the quasi-dynamic domain to determine whether an abnormality is due to a fault or other causes. This layer receives measurements from all settingless relays and is designed to block erroneous tripping actions.\n\nBy using a layered approach that combines both dynamic and quasi-dynamic state estimation, our protection scheme provides a more reliable and secure system for microgrid protection compared to traditional protection schemes. Further research on microgrid protection is needed to facilitate their expansion and integration into power systems."}, {"label": 1, "content": "Mauritius faces chronic water shortages, posing a significant threat to the economy and well-being of its population. The availability of surface and groundwater is tied to rainfall, which is influenced by large-scale circulation patterns such as the El Ni\u00f1o Southern Oscillation (ENSO) and the Indian Ocean Dipole (IOD). In this study, we examine the impact of these teleconnection patterns and present the results of a basic neural network used for precipitation forecasting based on ENSO and IOD states. We used data from the Vacaos station spanning 1961 to 2012. Our analysis revealed statistically significant correlation between average winter rainfall and ENSO and IOD indices, while the correlation for summer rainfall was negligible. However, the accuracy of predicting summer precipitation was lower than that of predicting winter precipitation. The findings of this research suggest potential for more efficient water resource planning and management on the island."}, {"label": 0, "content": "Neuromorphic is a relatively new interdisciplinary research topic, which employs various fields of science and technology, such as electronic, computer, and biology. Neuromorphic systems consist of software/hardware systems, which are utilized to implement the neural networks based on human brain functionalities. The goal of neuromorphic systems is to mimic the biologically inspired concepts of the nervous systems, envisioned to provide advantages, such as lower power consumption, fault tolerance, and massive parallelism for the next generation of computers. This brief presents a neural computing hardware unit and a neuromorphic system architecture based on a modified leaky integrate and fire neuron model in a spiking neural network for a pattern recognition task in register-transfer level. The neuron model and the spiking network are explored, considering digital implementation, targeting low-cost high-speed large-scale systems. Results of the hardware synthesis and implementation on field-programmable gate array are presented as a proof of concept. Accordingly, the maximum frequency of the implemented neuron model and spiking network are 412.371 MHz and 189.071 MHz, respectively."}, {"label": 0, "content": "We present NavREn-RL, an approach to NAVigate an unmanned aerial vehicle in an indoor Real ENvironment via end-to-end reinforcement learning (RL). A suitable reward function is designed keeping in mind the cost and weight constraints for micro drone with minimum number of sensing modalities. Collection of small number of expert data and knowledge based data aggregation is integrated into the RL process to aid convergence. Experimentation is carried out on a Parrot AR drone in different indoor arenas and the results are compared with other baseline technologies. We demonstrate how the drone successfully avoids obstacles and navigates across different arenas. Video of the drone navigating using the proposed approach can be seen at https://youtu.be/yOTkTHUPNVY."}, {"label": 1, "content": "Unmanned aerial vehicle-based base stations can provide wireless cellular service to ground users in various scenarios, and deploying these UAV-BSs while optimizing coverage area is a key challenge. This study focuses on the 3D placement of UAV-BSs to maximize the number of users covered while using minimal power and accommodating their Quality-of-Service (QoS) requirements. We model the placement problem as a multiple concentric circles placement problem to achieve our objective of maximizing the number of covered users. The UAV-BS deployment problem is decoupled in the horizontal and vertical dimensions without any loss of optimality. After some mathematical manipulations, we formulated a Mixed Integer Second Order Cone Problem (MISOCP) and proposed an improved Multi-Population Genetic Algorithm (MPGA) for horizontal dimensions placement problem. Our numerical simulations show that improved MPGA outperforms the Standard Genetic Algorithm (SGA) in solving this problem."}, {"label": 0, "content": "Radar Cross Section (RCS) is a measurement of scattering performance of an object. RCS plays an important role in the design of stealth weapon system. However, the method of calculating RCS of every angle of azimuth and elevation is complex, and the efficiency is low. Evidently, it is of great significance for predicting the unknown RCS given a set of RCS data of an object. Towards this aim, we propose a novel approach using multilayers Long Short Term Memory (LSTM) networks based on unsupervised learning - the mechanism is similar to that of autoencoder. The encoder LSTM maps the input RCS into a fixed length representation. The decoder LSTM decodes the representation to predict RCS. For the purpose of this work, we create an 3D object and collect its RCS for a dataset with the help of electromagnetic simulation software, FEKO. The proposed networks have been applied on the test dataset and yield satisfactory results."}, {"label": 0, "content": "This work is a continuation of a larger research work which advocates that Distance Education (DE) through audio-only learning mode can be developed into a full fledge audio-MOOC. Audio MOOC framework is an innovative framework which enables learning through mere phone calls. It has been conceived to digitally include low literate population in the education process by opening up access to learning materials to the unreached and the have-nots usually hindered by barriers such as language, literacy, culture, connectivity and distance which existing MOOCs have failed to address. This work demonstrates how our proposed framework is used to connect to a remote island lost in the middle of the Indian Ocean with limited maritime and air access but which since some few years back can be connected via basic phones through voice calls. Agalega is an ideal test case scenario for our research since it characterizes remoteness, limited connectivity, semi-literate population with limited access to education which our research aims at addressing. A group of 50 Fishermen was identified from both the Agalega islands. The course was of 9 days duration from 15 to 23 September 2017. The system was conducted live over the telephony network making use of our GSM gateway. The specificity of the system was that our GSM gateway resided in Mauritius connected to a cloud server, while the course was delivered to people of Agalega 1100 Km far from Mauritius over the sea. Nevertheless, our system performed as expected and proved to be a success."}, {"label": 1, "content": "Multiple Signal Classification (MUSIC), Steered Response Power-Phase Transform (SRP-PHAT), and Generalized Cross Correlation (GCC) are popular techniques used for Direction of Arrival (DoA) estimation in scenarios involving microphone arrays. However, in real-time applications, these methods encounter various limitations such as computational complexity and thresholding difficulties. To overcome these challenges, a novel and robust algorithm is proposed in this paper, which estimates DoA using subarray decomposition. This new method provides improved performance with effective thresholding and minimal computational complexity."}, {"label": 1, "content": "This paper investigates the deformation details of a soft actuator in a soft surface manipulator and establishes the relationship between inflation pressure and object displacement. Finite Element Analysis is used to examine the working principle of a single soft actuator and simulate its effect on object movement. The results demonstrate the importance of soft actuators in the manipulation of objects on a surface. By applying different pressures to the Finite Element model, the relationship between inflation pressure and object displacement is established. The soft surface manipulator can be viewed as a servo mechanism that acts on the object. To facilitate the development of a trajectory tracking algorithm in future work, the kinematic model of the object is established."}, {"label": 1, "content": "Recently, camera model identification has become an important topic in forensic science. With advances in artificial intelligence, researchers are revisiting these systems to improve accuracy and overcome persistent challenges. One such challenge is the impact of image manipulation on identification accuracy. Commonly used methods such as compression, scaling, and contrast enhancement can significantly degrade performance. To address this issue, we propose a new method that utilizes a state-of-the-art Convolutional Neural Network architecture to estimate manipulation parameters and dedicated feature extraction models to estimate the source camera. Multiplexers are used to shift the input image between the dedicated models through the output of the CNNs, resulting in significantly improved accuracy over existing methods, particularly in cases of heavy compression and down sampling. Testing was conducted using images from 10 different cameras, including different models from the same manufacturer, and validated across different devices to ensure robustness. This generic approach has the potential to revolutionize the design of camera model identification systems."}, {"label": 0, "content": "This paper deals with the processing of vehicle data in a stream using machine learning. In modern vehicles, a significant amount of sensor data are generated, which are only used temporarily before being discarded. The toolchain presented here aims to historize the data and evaluate it in near real time. Stream machine learning is used to process the data. Requirements for the toolchain are the platform-independent use of these and the free provision of the tools used. The result is a complete and innovative toolchain that maps everything required. From the reading of data on the vehicle to the use of stream machine learning and the evaluation of the data. An illustrative use case is presented and an outlook on extensions of the toolchain is given."}, {"label": 0, "content": "As the high-bandwidth and data-intensive applications evolving rapidly, traditional electrical networks are no longer able to meet the increasing traffic requirements. In order to improve the communication performance of data center network(DCN), the hybrid electrical/optical architecture has become a new research topic. Moreover, traditional architectures are often too complicate to manage. As a new technology, Software Defined network (SDN) addresses this issue effectively. In this paper, we propose a hybrid architecture based on SDN. The control manager is used to monitor and allocate traffic, then configure the network by SDN. There is a hybrid network platform under the implementation of virtual machine migration. The experiment shows that the proposed scheme reduces the total time of the virtual machine migration effectively compared to that running on the electrical architecture. In addition, our scheme can configure the network topology flexibly and achieve load balancing."}, {"label": 0, "content": "For an interior permanent magnet synchronous machine (IPMSM), parameters mismatching would affect the performance and reduce the output efficiency.In this paper, a variable parameter Maximum Torque Per Ampere(MTP A)control method based on online parameters identification was proposed. Firstly, an adaptive linear neural network algorithm is used to identify the motor parameters, and then the MTPA control is modified by combining the identified parameters. The results show that the proposed strategy has a nice adaptation to parameters mismatching, along with an improved output efficiency which is beneficial to magnetic recording motor."}, {"label": 1, "content": "This paper presents a technique for hand gesture recognition utilizing a convolutional neural network (CNN). The system operates in a three-dimensional radar environment, using continuous electromagnetic waves at 24GHz to detect and convert the scattered signals to intermediate frequency (IF) signals. The processed frequency spectrum is then used as input to the CNN. The feature detection layer of the CNN learns from data training, eliminating the need for supervised feature extraction. The learned features are enhanced through convolution, pooling and a softmax function. The system has demonstrated high accuracy, achieving a recognition rate of over 96%."}, {"label": 0, "content": "Cooperative navigation is a promising set of approaches for increasing the accuracy of navigating of vehicles as well as road safety in difficult environment such as urban canyons. DSRC (Dedicated Short-Range communication) is the radio-communication standard for vehicles. Usually cooperative navigation is based on sharing GNSS and other primary sensors measurements between nearby vehicles via DSRC (or other telecom systems such as 3G/4G). In addition to communication, the on-board IEEE 802.11p DSRC receiver allows to measure the angle between vehicle's building axes and direction of received signal (from nearby vehicle). DSRC signals could share GNSS measurements and mutual headings of other surrounding vehicles. On-board fusing of surrounding vehicles' coordinate and corresponding heading angles measurements leads to increasing of navigating accuracy. Here the possible effect of proposed approach is estimated."}, {"label": 1, "content": "In this article, we provide an analytical framework for analyzing the uplink performance of device-to-device (D2D)-enabled millimeter-wave (mm-wave) cellular networks, which incorporate clustered D2D user equipments (UEs). We model the locations of cellular UEs as a Poisson point process and the locations of potential D2D UEs as a Poisson cluster process. By employing tools from stochastic geometry, signal-to-interference-plus-noise ratio outage probabilities for both cellular and D2D links are derived. Among the distinguishing features of mm-wave communications are directional beamforming and having different path loss laws for line-of-sight and non-line-of-sight links. Therefore, we incorporate a flexible mode selection scheme and Nakagami fading into the outage analysis. We also investigate the effect of beamforming alignment errors on the outage probability to gain insight into performance in practical scenarios. Additionally, we determine the area spectral efficiency of the cellular and D2D networks for both underlay and overlay types of sharing. By considering the optimal weighted proportional fair spectrum partition, we determine the optimal spectrum partition factor for overlay sharing."}, {"label": 0, "content": "The article analyzes the application of artificial intelligence methods for control of mobile robots. The proposed method of selecting of the optimal artificial intelligence technology by the set of available technologies for control of mobile robots allows to improve the functional characteristics of the robot control system as a whole and its individual subsystems by the optimal coverage of a set of tasks and procedures solved by the robot control system. The method is based on the description of the initial information in the form of preference relations on a set of alternatives. As a solution to the problem of selecting an optimal information technology is adopted the most nondominant alternative. The proposed method was applied to the selection of the optimal artificial intelligence technology for modelling traffic control and technical operation field irrigation in developing control system of mobile robotic sprinkler system \u201cFregat\u201d, This method can be implemented in the design of software not only for mobile robots, but robots for other purposes."}, {"label": 0, "content": "In the engineering phase of modern manufacturing systems, simulation-based methods and tools have been established to face the increasing demands on time-efficiency and profitability. In the application of these simulation solutions, model-based digital twins are created, as multi-domain simulation models to describe the behavior of the manufacturing system. During the production process, a data-driven digital twin arises in the context of industry 4.0 based on an increasing networking and new cloud technologies. Recent developments in machine learning offer new possibilities in conjunction with the digital twin. These range from data-based learning of models to learning control logic of complex systems. This paper proposes a combined model-based and data-driven concept of a digital twin. It shows how to use machine learning in connection with these models, in order to archive faster development times of manufacturing systems."}, {"label": 0, "content": "The objective of the present work is to improve the epoch extraction performance from emotive speech by proposing a post processing approach to the conventional zero frequency filtering (ZFF) method using variational mode decomposition (VMD) based spectral smoothing. Due to the fast uncontrolled variations of the pitch in emotive speech signals, the reliable estimation of epochs is always challenging. In the proposed method, the spectra of the short frames of zero frequency filtered signal (ZFFS) is subjected variational mode decomposition to get component spectra in five modes. A smoothed short time spectra is then obtained by excluding the spectra from the two higher VMD modes which essentially have the high spectral variations. The modified ZFFS is then reconstructed using the sinusoidal parameters corresponding to single dominant frequency present in the smoothed spectra using VMD by parameter interpolation based sinusoidal synthesis. The resulting re-synthesized ZFFS has reduced spurious zero crossings as compared to that obtained from the conventional ZFF method for emotive speech signals. The effectiveness of the proposed VMD based spectral post processing is confirmed from the improved epoch identification rate and epoch identification accuracy across all the emotive utterances (with 7 emotions) present in German emotion speech database having simultaneous speech and electroglottographic (EGG) signal recordings. The performance of the proposed method is found to be better or comparable with the other existing ZFF based post processing methods proposed for emotive speech signals in terms of the epoch identification accuracy with respect to the corresponding reference epochs estimated from EGG signals."}, {"label": 0, "content": "Disasters whether natural or man-made have great impact on countries and civilians. Proper information across the main disaster phases need to be delivered on time and to the right people to minimize the impact and provide needed resources. Social media and Twitter in particular, is an important mean of information sharing in real-time as part of a complete cyber-physical emergency management system during a disaster. Twitter can be used in any place in the world through smartphones or other mediums with an internet access connection. The vast and varied number of tweets produced during a disaster will benefit from the cloud scalable storage and processing resources. As a centralized processing system is more vulnerable when a disaster strikes, there is a need for a more resilient distributed system architecture that allows for the distribution of both processing and storage resources. The goal of our study is to develop and evaluate a prototype of a microservice architecture for twitter data analytics during a disaster that meets the requirements of disaster management. In this paper, we design a cloud-based microservices twitter analytics framework for disaster management and implement a basic prototype system. Our prototype system demonstrates that the microservices approach allows for a distributed, dynamic, reliable and scalable system architecture on cloud platform that goes in hand with disaster domain requirements."}, {"label": 1, "content": "The NPK nutrient elements are macro nutrients that play a vital role in the growth and development of plants. It is essential to measure the NPK nutrient content before planting to assess the soil fertility condition accurately. However, conventional laboratory testing methods take a relatively long time. To address this issue, the researchers developed a prototype NPK nutrient measurement system using a mobile application and soil images.\n\nThe system utilizes local binary patterns and back-propagation neural networks to process the textural characteristics extracted from the soil images, which speeds up the measurement process significantly. The researchers collected sample data from rice fields in Yogyakarta Special Region, varying the distance from 30 cm to 110 cm and the angle of image capture from -30\u00b0 to 30\u00b0 with interval 10\u00b0.\n\nPre-processed results of the datasets were used to obtain texture features using local binary pattern uniform. The texture features were fed into the neural network model, which was trained using the back-propagation algorithm with varying parameters. The model was tested to determine the effect of distance and angle of image capture, system processing speed, and artificial neural network parameters.\n\nThe best model was subsequently implemented in a smartphone application that yielded an average computation time of 0.65s. The optimal results were obtained at a distance capture of 50 cm and an angle capture of 0\u00b0. The measurement accuracy was nitrogen at 91.80%, phosphorus at 83.49%, and potassium at 82.54%, with an average of 84.16%.\n\nIn conclusion, the NPK nutrient measurement system based on a mobile application using soil images is a promising method to accurately assess soil fertility conditions rapidly. It could revolutionize the agricultural industry by allowing farmers to make data-driven decisions quickly, improving crop yields and increasing profits."}, {"label": 1, "content": "Recently, there has been a growing interest in the issue of code-switching, which has become an active area of research. In bilingual communities, individuals often incorporate words and phrases from a non-native language into their day-to-day communication in their native language. Although code-switching is a global phenomenon among multilingual communities, there is still a lack of adequate acoustic and linguistic resources available. To develop effective speech-based applications, it is crucial for existing language technologies to be able to handle code-switched data. Code-switching is broadly classified into two categories: intra-sentential and inter-sentential. This study focuses on the intra-sentential problem in the context of code-switching language modeling. The contributions of this research include the creation of a Hindi-English code-switching text corpus by crawling selected blogging sites that educate about Internet usage, and the exploration of the use of parts-of-speech features to improve the modeling of Hindi-English code-switched data using monolingual language models trained on native (Hindi) language data."}, {"label": 0, "content": "Current anomaly detection systems (ADSs) apply statistical and machine learning algorithms to discover zero-day attacks, but such algorithms are vulnerable to advanced persistent threat actors. In this paper, we propose an adversarial statistical learning mechanism for anomaly detection, outlier Dirichlet mixture-based ADS (ODM-ADS), which has three new capabilities. First, it can self-adapt against data poisoning attacks that inject malicious instances in the training phase for disrupting the learning process. Second, it establishes a statistical legitimate profile and considers variations from the baseline of the profile as anomalies using a proposed outlier function. Third, to deal with dynamic and large-scale networks such as Internet of Things and cloud and fog computing, we suggest a framework for deploying the mechanism as Software as a Service in the fog nodes. The fog enables the proposed mechanism to concurrently process streaming data at the edge of the network. The ODM-ADS mechanism is evaluated using both NSL-KDD and UNSW-NB15 datasets, whose findings indicate that ODM-ADS outperforms seven other peer algorithms in terms of accuracy, detection rates, false positive rates, and computational time."}, {"label": 1, "content": "The advent of Bring Your Own Device (BYOD) has created new challenges for traditional intranets, which previously relied on perimeter-based security defenses to ensure internal security. The introduction of personal devices poses a serious threat to the security of internal networks. To address this issue, this paper proposes a Software-defined Intranet Dynamic Defense System (SIDD) that employs an isolation and dynamic approach to disrupt cyber kill chain activity.\n\nThe SIDD system is designed to address the problem of static network attributes, which can be easily exploited by attackers. To overcome this, virtual IP addresses are assigned to intranet terminals, with dynamic mapping between real IP addresses and virtual IP addresses to conceal the user's identity. The system also includes a software-defined dynamic defense architecture that utilizes three core modules: DNS, virtual and real address assignment, and virtual address maneuvering.\n\nFinally, the SIDD system is implemented using the OpenDaylight controller, with experiments demonstrating its ability to provide definable IP addresses that are maneuverable in terms of frequency and space. The system effectively reduces network reconnaissance availability and increases the difficulty of real-time attacks, without negatively impacting network applications.\n\nIn summary, the SIDD system presents an effective solution to the threat posed by BYOD devices to traditional intranet security. Its dynamic and isolation-based approach minimizes the risk of cyber attacks without negatively impacting network performance."}, {"label": 1, "content": "In this paper, we present a novel approach to enhance the singing voice of amateurs in a way that makes it similar to that of professional opera singers. Our proposed system emphasizes the singing voice of amateurs by using a frequency band that represents the noteworthy characteristics of the professional singer. Additionally, through the use of highway networks, our system is able to convert any song, even those that a professional opera singer has not sung before.\n\nOur experiments demonstrate that our singing-voice enhancement system successfully emphasizes the middle-high frequency range of the amateur singer's voice, which contains frequency components that greatly impact the overall glossiness of their voice. Furthermore, our system maintains the unique speaker characteristics of the amateur singer, resulting in a natural sound enhancement.\n\nOverall, our proposed singing-voice enhancement system provides a new and effective approach to enhance the singing voice of amateurs, allowing them to achieve a professional-level sound without sacrificing their own unique vocal traits."}, {"label": 1, "content": "Flowers play an exceptionally significant role in our lives, offering high research and application value. Traditional flower classification methods have generally been based on features such as shape, color, and texture. However, this approach is associated with low classification accuracy since it requires the manual selection of features. This paper aims to propose an effective flower classification approach using convolutional neural networks and transfer learning. The study utilized various models to compare network initialization and transfer learning models, including VGG-16, VGG-19, Inception-v3, and ResNet50. The experimental results highlight that transfer learning can effectively reduce issues such as deep convolutional neural networks being prone to local optimal and over-fitting problems. Compared to traditional methods, the proposed approach significantly improves flower recognition accuracy using the Oxford flowers dataset, as well as exhibit impeccable robustness and generalizability."}, {"label": 1, "content": "Workflow scheduling and resource provisioning algorithms heavily rely on accurate performance estimation of tasks to produce an effective scheduling plan. For this reason, a profiler capable of modeling task execution and predicting runtime accurately is an essential asset to any Workflow Management System (WMS). However, with the rise of multi-tenant Workflow as a Service (WaaS) platforms that deploy scientific workflows on clouds, task runtime prediction poses more significant challenges due to the processing of a significant amount of data in a near real-time scenario while dealing with the performance variability of cloud resources. Therefore, traditional methods like basic statistical descriptions or offline regression techniques are no longer suitable for such environments. In this paper, we introduce an online incremental learning approach to predict task runtime in scientific workflows on clouds. Our approach utilizes fine-grained resource monitoring data in the form of CPU utilization, memory usage, and I/O activities' time-series records to improve the performance of the predictions. We compare our approach to a state-of-the-art technique that leverages resource monitoring data based on regression machine learning. Our results show that the proposed strategy outperforms the state-of-the-art solutions by up to 29.89% in terms of error."}, {"label": 1, "content": "Disasters, whether natural or man-made, have a profound impact on countries and their citizens. To minimize this impact and provide the necessary resources, it is essential to deliver proper information across the main disaster phases on time and to the right people. Social media, and Twitter in particular, has emerged as an important means of sharing information in real-time during a disaster as part of a complete cyber-physical emergency management system.\n\nTwitter can be accessed from anywhere in the world through smartphones or other internet-enabled devices. The vast number of tweets generated during a disaster can benefit from the cloud scalable storage and processing resources. However, a centralized processing system is vulnerable when a disaster strikes. To address this, there is a need for a more resilient distributed system architecture that allows for the distribution of both processing and storage resources.\n\nOur study focuses on developing and evaluating a prototype of a microservice architecture for twitter data analytics during a disaster that meets the requirements of disaster management. We designed a cloud-based microservices twitter analytics framework and implemented a basic prototype system. Our prototype system demonstrates that the microservices approach allows for a distributed, dynamic, reliable and scalable system architecture on a cloud platform that aligns with the disaster domain requirements.\n\nIn conclusion, our proposed microservices architecture can help emergency managers to effectively respond to disasters using Twitter as a valuable source of real-time information. The development and deployment of such systems can significantly improve disaster management efforts and better prepare communities for future emergencies."}, {"label": 1, "content": "TomoSAR inversion of urban areas is a challenging task due to its inherently sparse nature, which can be addressed by using compressive sensing (CS) algorithms. The high cost associated with the acquisition of a large number of data sets required by TomoSAR can be mitigated by trading off the number of acquisitions with signal-to-noise ratio (SNR). We propose incorporating nonlocal (NL) estimation into the inversion to increase SNR and demonstrate that reasonable reconstruction of buildings can be achieved using only a few interferograms.\n\nThe second challenge addressed in this paper is the computational complexity of CS-based inversion techniques, which limits their suitability for large-scale applications. To address this, we introduce a novel and efficient algorithm for solving the NL L1-L2-minimization problem, which is central to CS-based reconstructions. Our proposed algorithm is demonstrated using simulated data and TerraSAR-X images over an area in Munich, Germany.\n\nOverall, our proposed solutions for the challenges in TomoSAR inversion of urban areas hold promise for more widespread and cost-effective use of this technique, with potential applications in urban planning, disaster management, and environmental monitoring."}, {"label": 1, "content": "This article presents research into optimizing the placement of phasor measurement units (PMUs) and flow measurement devices for ensuring topological observability of power systems during N-2 transmission contingencies. Previous studies have focused on achieving topological observability at the least possible cost. However, the proposed model not only minimizes the total investment cost but also optimizes the location-based joint placement of PMUs and flow measurement devices.\n\nThe proposed model also aims to avoid PMUs on radial nodes and encourages flow measurements for nodes adjacent to PMU installed buses. The model can be represented as a mixed-integer linear programming (MILP) problem and solved using a decomposition-based algorithm. The effectiveness of the proposed model is shown with the use of a 14-bus IEEE power system.\n\nNumerical results from experiments with the proposed approach show that optimal placements lead to significant improvements in state estimation accuracy when taking time skew errors into account. The results of this study indicate that the proposed approach for joint placement of PMUs and flow measurement devices is an effective tool for ensuring topological observability of power systems during N-2 transmission contingencies."}, {"label": 1, "content": "With the advancement of smart substations, there is a wealth of information available for state estimation in power systems. This has led to the development of a new method for secondary system state estimation based on information redundancy. The structure and content of this method have been outlined and a linear static model has been constructed using the redundant relationship of analog quantities. To verify the effectiveness of this method, a numerical example has been simulated and the results demonstrate a significant reduction in absolute error of analog quantities, increasing the reliability of uploading these quantities. This research method has significant potential for applications and further research in the field of secondary system state estimation."}, {"label": 0, "content": "Radio frequency(RF) communication technology has been applied widely in data acquisition system especially in low-voltage electric area. As a property of RF communication, co-channel interference could not be ignored in improving communication performance. This paper proposes an RF channel selection method based on adaptive sensing to increase acquisition efficiency. The key steps as channel evaluation, adaptive sensing, channel negotiation and power control are given. Both simulation and experiments in advanced metering infrastructure(AMI) are conducted to validate the proposed method."}, {"label": 1, "content": "Multispectral image change detection using deep learning requires a significant amount of labeled training data, which is challenging and expensive to obtain. To tackle this problem, we propose a generative discriminatory classified network (GDCN) that leverages unlabeled data and new fake data generated by generative adversarial networks. The GDCN comprises a discriminatory classified network (DCN) and a generator. The DCN segregates the input data into changed, unchanged, and fake classes, while the generator recovers the real data from input noises to deliver additional training samples for enhancing the performance of the DCN. Ultimately, the bitemporal multispectral images are processed by the DCN to generate the final change map. Our experiments on real multispectral imagery datasets prove that the proposed GDCN trained using a small amount of labeled data and unlabeled data can achieve competitive performance compared to existing methods."}, {"label": 1, "content": "Working with product lifetime data is of utmost importance when it comes to evaluating safety and reliability as well as predicting remaining useful life (RUL) and formulating maintenance strategies or replacement policies. In practical scenarios, datasets often consist of failure data and randomly censored data, known as general censored data. The inverse Gaussian (IG) distribution has widely been used to explain lifetime data because it provides flexible format expressions and explains the mechanism of first hitting time from a soft failure viewpoint. This paper presents a novel method for best linear unbiased estimations (BLUEs) for general censored data using a three-parameter IG distribution. A novel approach is established to optimize the distribution's skewness parameter enabling a calculation of BLUEs of mean and standard deviation. The proposed method can obtain closed-form parameter estimations in linear functions of order statistics, and its computation process has been further simplified for flexible applications. A frequently used maximum likelihood estimation method is also introduced in the reference for a better understanding. Results from comprehensive simulation study and empirical application showcase that the proposed method enhances estimation accuracy because more life information can be extracted from the censored datasets, and it maintains stable performance."}, {"label": 1, "content": "The goal of this project is to create a wireless sensor network using event-driven nodes to monitor plant health and larva population in a remote field. Rather than the traditional approach of clock-driven nodes, our event-driven sensing nodes are developed with a STM32F407VG board that only acquires and transmits data when a significant change is detected. This approach reduces power consumption and increases efficiency. Data collected from the sensors is transmitted via wireless ZigBee interface to a base station where it is collected and analyzed by a MATLAB application. Findings are displayed on a CPU to allow for timely interaction and care of the crop field. The system parameters have been adjusted to achieve effective module integration and performance. Validation with an experimental setup has shown promising results for the system's realization."}, {"label": 1, "content": "The Space-Ground Integrated Network (SGIN) will play a vital role in the future development of our country. As such, it is crucial that we protect it from cyber-attacks. In this study, we propose an association analysis algorithm that utilizes a knowledge graph of cyber security attack events to portray potential attack scenarios for the SGIN. By constructing a knowledge graph and conducting association analysis, we are able to showcase the potential attack scenes in a graphical format. An important aspect in the construction of this knowledge graph is the creation of an event ontology, which represents various relationships within the network attack process. This will ultimately result in a space-ground integration network security analysis system that utilizes the knowledge graph of cyber security attack events and the association analysis algorithm to analyze potential attack scenarios."}, {"label": 1, "content": "High-speed permanent magnetic synchronous machine (PMSM) drive systems offer numerous advantages compared to low and medium speed drive systems, including higher power density and the ability to directly drive high speed loads. In high-speed drive systems, sensorless drive is preferred due to reliability and rotor dynamic balance issues. \n\nThis paper proposes a model reference adaptive system based sensorless control method for a high-speed permanent magnetic machine with a 1-F startup strategy. This method ensures reliable startup ability and achieves a smooth transition between the open-loop and closed-loop control scheme. Additionally, the compensation of voltage in high frequency leads to more accurate rotor position estimation."}, {"label": 0, "content": "Due to the complexity of modeling deformable materials and infinite degrees of freedom, the rich background of rigid robot control has not been transferred to soft robots. Thus, most model-based control techniques developed for soft robots and soft haptic interfaces are specific to the particular device. In this letter, we develop a general method for stiffness control of soft robots suitable for arbitrary robot geometry and many types of actuation. Extending previous work that uses finite element modeling for position control, we determine the relationship between end-effector and actuator compliance, including the inherent device compliance, and use this to determine the appropriate controlled actuator stiffness for a desired stiffness of the end-effector. Such stiffness control, as the first component of impedance control, can be used to compensate for the natural stiffness of the deformable device and to control the robot's interaction with the environment or a user. We validate the stiffness projection on a deformable robot and include this stiffness projection in a haptic control loop to render a virtual fixture."}, {"label": 1, "content": "A robust MAC protocol is essential to facilitate communication between vehicles and infrastructure in Vehicular Ad Hoc Networks (VANETs). VANETs' data traffic is categorized as high-priority safety messages, control messages, and non-safety infotainment-related messages. Despite the proposed regional standards, their effectiveness is in question, specifically in the timely delivery of safety messages. As a result, researchers have focused on developing optimal MAC protocols. This paper surveys significant TDMA-based MAC protocols for VANETs, outlining their basic concepts, functioning, and performance. Additionally, it suggests several requirements that future researchers must consider in developing optimal TDMA-based MAC protocols for VANETs."}, {"label": 0, "content": "In recent years, the research on the rapid estimation of the parameters of maneuvering targets has received extensive attention. However, many existing parameter estimation algorithms have the problem of conflicting accuracy and computational complexity. In addition, when multiple maneuvering target parameters are estimated at the same time, the traditional time-frequency class algorithm will have cross-term interference. According to the above problem, we noticed that the auto item is constant and independent of the adjacent time delay while the cross term is a function of the adjacent time delay in the Higher-order Adjacent Cross Correlation Function (HACCF) expansion of radar echo signal. Based on that, a fast estimation algorithm for estimating multi - maneuvering target parameters is proposed. The algorithm firstly takes the mean extraction of the signal's HACFF to extract the auto items, and inhibits the cross term. Then we can estimate the frequency of auto items further and get accurate estimation of maneuvering target acceleration. Numerical simulations show that the calculation of the algorithm proposed is small and it can quickly estimate the maneuvering target parameters. This algorithm can estimate the parameters of multiple maneuvering targets simultaneously with high accuracy."}, {"label": 0, "content": "Opportunistic Networks, with users as a vital component, appears as a natural evolution of mobile Ad Hoc Networks field, and to some extent, an example of how the Internet of Things work. So, Opportunistic Networks due to their characteristics, need users- centric considerations when it comes to design Opportunistic Networks' routing, privacy, and authentications schemes. However, most mutual authentication schemes proposed for Opportunistic Networks do not consider, on the one hand, the gregarious aspect of users, and on the other hand, the parameter that pre-established contacts could be used in a mutual authentication process. Considering the factors pre-established contacts, Seed OppNet Identity, and traditional cryptography's principles, this paper proposes a realistic multiple levels authentication scheme for short-term and limited-time wireless network environment. The proposed scheme is realistic and achieves anonymity and privacy."}, {"label": 1, "content": "High computational requirements are commonly associated with hydraulic simulations of large-scale water distribution networks. The iterative procedures involved in achieving convergence have long been a topic of debate due to the network's non-linear properties. This obstacle hinders the development of online applications for water distribution network analysis and pressure control. Consequently, model-free techniques are replacing hydraulic simulations due to their computational efficiency. Artificial Neural Networks (ANNs) have proved to be one of the most effective model-free approaches for WDN management and analysis. This paper examines the advantages of using such techniques for large-scale non-linear networks and presents a literature synopsis of existing applications."}, {"label": 0, "content": "Vehicular Ad-hoc Network (VANET) is a definite form of Mobile ad-hoc Network (MANET), which delivers data communication in Vehicular environment using wireless transmission. Its key goal is to increase the service quality of intelligent transportation systems (ITS), such as road safety, logistics, and environmental kindliness as well as information exchange. Nowadays transportation systems are facing serious issues in terms of performance and efficiency of VANET applications, nevertheless these depend typically on the method in which messages are conveyed between the nodes. Finding a better routing protocol for dynamic VANET systems is one of the main challenges. This paper presents an Ant Colony Hybrid Routing Protocol (ACOHRP) to improve the service quality of ITS, by increasing the efficiency and reliability of vehicle traffic information message transmission. ACOHRP delivers high efficiency through better beginning of packet delivery ratio to end-to-end delay. A comparative study on the proposed ACOHRP and existing Dynamic Source Routing (DSR) protocol is conducted in a realistic scenario with VANET architecture, to demonstrate the performance of the proposed method. Simulation based testing is performed using Matlab with ACOHRP performing better than DSR in a dynamic environment of VANETs."}, {"label": 1, "content": "We consider networks where each link has two delay parameters: a guaranteed upper bound on the worst-case delay and an estimate of the typically encountered delay. The goal is to find routes between two nodes that have small delay, while ensuring that the end-to-end delay upper bound is respected. We formalize this problem as a shortest-paths problem on graphs and provide optimal algorithms for solving it."}, {"label": 0, "content": "In this paper, the problem of inter cell interference suppression which under the multi cell heterogeneous network system model is studied. Game theory is used to build the model of frequency and timeslot selection. The analysis shows that the proposed resource reuse model is a potential game, which can improve the throughput of the network."}, {"label": 0, "content": "In order to improve the quality and visual effects of blurred images, we proposes an image deblurring algorithm based on dictionary learning. Firstly, we divides the blurred image into image block structure groups, then we recovers the image block through the K-SVD dictionary and the PCA dictionary, and finally the morphological operations is applied to the difference image to obtain restored image. Experimental results show that the proposed algorithm is better than others in peak structural similarity and visual effect."}, {"label": 0, "content": "High bandwidth signal transmission between ASIC and memory is more and more important especially for high performance Cell Phone Application Processor (AP) application. Several advanced packaging technologies are developed and High bandwidth Package on Package (HBPoP) is one of them to take the place of FCMAPPOP for the high/middle-end mobile phone products. The structure of HBPoP is similar as sandwich, the substrate interposer (SI) substrate is stacked on silicon top side with fine pitch solder joints to routing and re-layout the I/O and provide high bandwidth requirement. However, the warpage control is the key challenge for PoP technology due to the top side memory package and bottom side HBPoP package would be mount on mother board at the same time; especially, the thinner and thinner package thickness requirement. In this work, pretreatment temperature had apparently effect on package warpage performance, a three-dimensional computational HBPoP simulation model was developed for analyzing warpage behavior. Besides, advanced metrology analyzer (aMA) based on three-dimensional digital image correlation technology, a non-contact optical displacement measurement scheme, was used to measure bare substrate in plan strain and shrinkage behavior with different temperatures. Also, thermal mechanical analyzer (TMA) was utilized to capture the substrate shrinkage properties by multi-cycles measurement. The warpage from the simulation was verified against the experimental data. The results predicted by finite-element model (FEM) indicated that the sensitivity of parameters, including thickness and materials of each laminates and the deviation comes from different suppliers would be evaluate in this study. Moreover, several design combinations are proposed to enhance the warpage control. This work provides design guidance for HBPoP technology development."}, {"label": 1, "content": "The processing and analysis of big data sets in real-time requires the high computational power of extreme-scale high-performance computing (HPC) systems. To schedule massive task/thread sets on these systems, various scheduling algorithms and virtualization techniques have been developed. However, these strategies do not guarantee the meeting of timing constraints such as deadlines, which are crucial in real-time science workflows. \n\nTo address this issue, a new project has been developed that focuses on real-time scheduling for extreme-scale HPC Message Passing Interface (MPI) tasks and ensemble workloads found in fine-grain many-task computing (MTC) applications. This new approach abstracts information about the tasks or threads and continuously dispatches the workload to ensure the meeting of deadlines and other timing constraints. The framework also aims to reduce execution time and energy consumption.\n\nThe new framework introduces a deadline-based scheduling approach in the tasking programming model. This approach guarantees that tasks are completed within their specified deadlines, allowing for efficient processing and analysis of real-time data sets. The new project departs from the established trend of best-effort scheduling in large-scale HPC systems, providing a solution that meets the evolving demands of real-time science workflows."}, {"label": 1, "content": "Predicting future states is a key component of intelligence that has numerous real-world applications, such as autonomous vehicles and robotics. To improve upon previous methods for future frame prediction, which were limited by environmental restrictions and produced poor accuracy and blurry predictions, this paper presents an unsupervised video prediction framework. This framework iteratively anticipates raw RGB pixel values in future video frames, and has been extensively tested on KTH and KITTI datasets. The results of these experiments demonstrate that our method achieves strong performance in accurately predicting future video frames."}, {"label": 1, "content": "Collisions between vehicles and pedestrians often result in fatalities for the vulnerable road users (VRUs). Therefore, it is crucial to develop new technologies aimed at protecting VRUs. In this paper, we propose using cloud computing technologies to handle the large amounts of safety-critical messages, given the high density of pedestrians and limited computing capabilities of base stations. Additionally, we adopt wireless multi-hop backhaul technology to overcome transmission bottlenecks, such as limited transmission capability and queueing delays between base stations and clouds. Using a multi-hop wireless transmission scheme, we derive the signal transmission success probability and delay between pedestrians and clouds for performance analysis. We conduct numerical simulations to demonstrate the relationship between transmission success probability and received signal to interference plus noise ratio (SINR) threshold."}, {"label": 1, "content": "Recognition of transportation and locomotion modes via multimodal smartphone sensors can offer on-the-spot context-aware assistance, however, the field currently lacks standardized datasets, evaluation criteria, and recognition tasks. Presently, recognition is performed on ad hoc datasets utilizing different sensors for one-off recognition problems, preventing systematic comparative assessments across research groups. Our objective is to solve these issues by introducing a publicly available dataset and suggesting 12 reference recognition scenarios, also specifying relevant sensor combinations based on energy considerations. We also define precise evaluation criteria, such as user-independent and sensor-placement independent evaluations, to inform future research. Using statistical and frequency features based on information theoretic criteria, our machine-learning recognition pipeline achieved systematic reference performance on all scenarios. Our work enables future researchers to evaluate their own methods in a comparable fashion and contributes to further advances in the field. The dataset and code are accessible through http://www.shl-dataset.org/."}, {"label": 0, "content": "In this paper, we report the influence of vegetation in paddy fields with antenna height as a parameter for 2.4GHz (3mW) and 920MHz (20mW) propagation. From the viewpoint of practicality, a commercially available wireless communication module equipped with a small pattern antenna was used for the experimental apparatus. In order to consider the worst case of communication loss, detailed measurements were made assuming the height of rice grown 105 cm, and the antenna height was 55cm, 105cm, 155cm. Based on the measurement results, fitting was performed on the Exponential Decay (EXD) model, and the propagation characteristics in paddy fields were verified. As a result, at 2.4GHz, even under line-of-sight conditions, there was attenuation due to the influence of scattering of reflected waves due to plant arrangement, and it was found that the same degree of remarkable attenuation due to vegetation occurred regardless of the height under rice top. On the other hand, at 920MHz, which is said to have strong diffraction, it was revealed that the loss characteristic of vegetation is proportional to the height."}, {"label": 1, "content": "This paper proposes a prediction study of transformer oil density using multi-frequency ultrasound, genetic algorithm GA, and backpropagation neural network BPNN. The study uses 110 sets of transformer oil samples from the China southern power grid to establish a prediction model based on the BPNN algorithm. The model takes the 242-dimensional multi-frequency ultrasonic data of the oil sample as input and the density as output. The number of hidden layer neurons was adjusted, and the GA algorithm was introduced to optimize the network parameters. The results show that the GA-BPNN model outperforms the traditional standard BPNN model in predicting the density of transformer oil, with much closer output values and smaller errors. This study establishes a solid foundation for testing other parameters of transformer oil using multi-frequency ultrasound technology."}, {"label": 0, "content": "To meet the challenges and needs of the evolving power grid with Phasor Measurement Units (PMUs) deployment, this paper applies the cloud computing technology as the new information infrastructure for utilities and Independent System Operators (ISOs). An architecture design of cloud-based synchrophasor analytical applications as a service for power system operation is proposed. The implementation proves the feasibility and effectiveness of using Software as a Service (SaaS) service model for power system operations at grid control centers."}, {"label": 0, "content": "In this paper, we present a new method for automatic mass function estimation and focal elements selection as a fundamental step to apply the Dempster-Shafer Theory (DST). The idea is to use the centroids and membership distributions obtained by applying the Fuzzy-C-Means algorithm (FCM) to define the mass function. The proposed method allows finding composite focal elements that represent the highest uncertainty and ambiguity. Experiments were conduced on multi-spectral and multi-temporal images for the purpose of change detection by integrating the proposed method of mass function estimation in a process of post-classification by DST. The proposed system of change detection is characterised by a multi-level of imperfection handling where the ambiguity is modelled firstly by FCM, then the uncertainty and the imprecision are handled in the step of mass function estimation. The effectiveness of the proposed methodology is demonstrated by applying it to find transformed region within two landsat images where we obtained high rates of classifications."}, {"label": 1, "content": "After reviewing the current methods for addressing risk assessment, it became clear that they suffer from limitations related to computational power and access to essential data. To overcome these barriers, fuzzy mathematical techniques are recommended. This study focuses on evaluating different approaches for using fuzzy methods to perform risk assessment with uncertain information."}, {"label": 0, "content": "As the scale of high performance computing facilities approaches the exascale era, gaining a detailed understanding of hardware failures becomes important. In particular, the extreme memory capacity of modern supercomputers means that data corruption errors which were statistically negligible at smaller scales will become more prevalent. In order to understand hardware faults and mitigate their adverse effects on exascale workloads, we must learn from the behavior of current hardware. In this work, we investigate the predictability of DRAM errors using field data from two recently decommissioned supercomputers: Cielo, at Los Alamos National Laboratory, and Hopper, at Lawrence Berkeley National Laboratory. Due to the volume and complexity of the field data, we apply statistical machine learning to predict the probability of DRAM errors at previously un-accessed locations. We compare the predictive performance of six machine learning algorithms, and find that a model incorporating physical knowledge of DRAM spatial structure outperforms purely statistical methods. Our findings both support expected physical behavior of DRAM hardware as well as providing a mechanism for real-time error prediction. We demonstrate real-world feasibility by training an error model on one supercomputer and effectively predicting errors on another. Our methods demonstrate the importance of spatial locality over temporal locality in DRAM errors, and show that relatively simple statistical models are effective at predicting future errors based on historical data, allowing proactive error mitigation."}, {"label": 0, "content": "This paper presents a method for realizing energy neutral operation on energy harvesting wireless sensor nodes (WSN) and its implementation, regarding that the available environmental energy is unpredictable and changes over time. The method utilizes adaptive duty cycling which provides energy-neutral operation according to the energy available in the environment and the instantaneous energy state of the node through an energy management circuit. The proposed method is implemented using a MicaZ mote as the WSN and two different vibration-based harvesters: piezoelectric and electromagnetic. The node incorporating a piezoelectric harvester, operates for only 130.5 s with a fixed duty-cycle of 0.21%, and requires an inactive time of 93.5 s for charging. On the other hand, with the proposed strategy, the node achieves energy-neutral operation by self-adjusting to 0.17% duty-cycle. Energy-neutral operation is also demonstrated by incorporating an electromagnetic energy harvester attached to the wrist of a runner: When no energy is available for harvesting, the proposed strategy shows about 64% increment in lifetime before going to sleep mode. These demonstrate that the proposed energy management policy proves to achieve energy-neutral operation in an efficient way."}, {"label": 1, "content": "The papers included in this special section of the journal are focused on various emerging technologies in the healthcare industry such as cloud computing, fog computing, the Internet of Things, and Big Data analytics, collectively known as Healthcare 4.0. This new era of the healthcare industry allows for greater flexibility in production, accelerating both manufacturing and market processes, enhancing both product quality and productivity, and changing traditional business models, altering the interaction with value chains, competitors, and clients.\n\nAs the healthcare industry moves into the fourth industrial revolution, significant investments and a change in mindset are required for cross-industry collaboration, agreements on data ownership, security, legal issue solving, product registration standards, new machine-to-machine communication protocols, and employment/skills development. The result of this transformation is a revolutionized market for health service provision to both patients and clinical operators.\n\nIn conclusion, Healthcare Industry 4.0 is a necessary evolution that allows for greater efficiency, flexibility, and productivity across all areas of the healthcare industry. The adoption of new technologies and changes in mindset will be crucial for ensuring the successful implementation and integration of these progressive changes."}, {"label": 1, "content": "To address the limited speed and scalability of modular multilevel converter (MMC) simulation in electromagnetic transient simulation programs (EMTP), this study presents a fast MMC electromagnetic transient model based on rotational transformation. The primary advantage of rotational transformation is the reduction of fundamental frequency of signals via coordinate system rotation. The implementation of the proposed method in EMTP enables an increase in simulation step size, resulting in fast electromagnetic transient simulation. \n\nThis paper derives the equivalent model of MMC switch function expressed by a state equation. Using this, the novel MMC model based on rotational transformation is further derived, combining the concept of rotational transformation. The state equation of the proposed MMC model is calculated in MATLAB, and simulation results and time-consuming are compared to those of a 51-level topological MMC electromagnetic transient model established in PSCAD/EMTDC. \n\nComparative analysis results show that the simulation duration of the MMC model based on rotational transformation is significantly less than that of the traditional MMC electromagnetic transient model without compromising accuracy."}, {"label": 1, "content": "Crowd simulation can be a vital tool in designing Smart Environments as it provides valuable insights on how pedestrians move through different areas and interact with the environment. However, most crowd simulations are developed using commercial game engines, limiting the experimentation and customization of the models. Additionally, crowd simulations often require significant resources that may not be available to scientists.\n\nTo solve these issues, this paper proposes a framework that allows for crowd simulation as a service. This approach would enable scientists to experiment with the latest advances without the burden of installation or expensive resources, while also allowing developers to evolve the tool in a scalable way. As a proof of concept, the paper demonstrates how crowd simulations can be used to generate datasets for studying the deployment of sensors in large facilities when run with a 3D representation.\n\nOverall, the framework presented in this paper has the potential to greatly enhance crowd simulation techniques and contribute to the development of Smart Environments."}, {"label": 0, "content": "Increasing longevity remains one of the open challenges for Lithium-ion (Li-ion) battery technology. We envision a health-conscious advanced battery management system, which implements monitoring and control algorithms that increase battery lifetime while maintaining performance. For such algorithms, real-time battery capacity estimates are crucial. In this paper, we present an online capacity estimation scheme for Li-ion batteries. The key novelty lies in: 1) leveraging thermal dynamics to estimate battery capacity and 2) developing a hierarchical estimation algorithm with provable convergence properties. The algorithm consists of two stages working in cascade. The first stage estimates battery core temperature and heat generation based on a two-state thermal model, and the second stage receives the core temperature and heat generation estimation to estimate state-of-charge and capacity. Results from numerical simulations and experimental data illustrate the performance of the proposed capacity estimation scheme."}, {"label": 1, "content": "Image retrieval is becoming increasingly important in the field of medical image processing, particularly in the realm of fundus images. The objective of this study is to introduce a robust algorithm for feature extraction in fundus images, and subsequently extract information via content-based image retrieval. The automatic extraction of significant features, such as exudates, assists medical professionals in efficaciously treating patients with various illnesses. Despite the availability of numerous methods for feature extraction, they often fall short in terms of information retrieval or the precision of feature extraction."}, {"label": 0, "content": "mHealth4Afrika is a collaborative research and innovation project, focused on supporting Horizon 2020 Societal challenges and Sustainable Development Goal 3. It is researching and evaluating the potential impact of co-designing and developing an open source, multilingual enabled mHealth platform to support quality community-based primary maternal healthcare delivery at semi-urban, rural and deep rural clinics, based on end-user requirements in Southern Africa (Malawi, South Africa), East Africa (Kenya) & Horn of Africa (Ethiopia). This paper aims to share the co-design process applied to develop and validate Beta platform v1, and the implications this had on the design of subsequent iterations of the beta platform. The Beta platform v1 validation was undertaken with 36 participants from 11 healthcare clinics across Northern Ethiopia, Western Kenya, Southern Malawi and Eastern Cape, South Africa during November - December 2017, using a mix of ethnographic observation and semi-structured interviews. These findings have informed the co-design of the subsequent iterations of the mHealth4Afrika beta platform, which is being used in the participating clinics on a phased basis during Q3 - Q4 2018. This platform integrates Electronic Medical Records, Electronic Health Records, medical sensors, visualisation and automatically generate monthly health indicators, thus supporting better clinical care and decision making, and freeing up time for clinical care and continuous professional medical education, thus reducing mortality rates. The expected outcome is a multi-region proof of concept that can make a significant contribution in accelerating exploitation of mHealth across Africa to improve health outcomes and stregthen health systems."}, {"label": 1, "content": "This study aims to address two problems with current intrusion detection technology. Firstly, the detection rate based on the Extreme Learning Machine (ELM) algorithm is not high, and secondly, intrusion detection based on Support Vector Machine (SVM) algorithm is slow. To overcome these limitations, the proposed solution is an intrusion detection method that employs Kernel Principal Component Analysis (KPCA) and the ELM algorithm. \n\nThe KPCA algorithm is used to reduce the dimensionality of the extracted feature matrix. The ELM algorithm then performs multi-classification detection on four common types of attacks. The simulation results demonstrate that this method is significantly more efficient and faster than intrusion detection based on either the ELM algorithm or SVM algorithm. As a result, the accuracy, false alarm rate, detection rate, and detection time in intrusion detection technology are all improved by using this approach."}, {"label": 0, "content": "In the recent past, we have witnessed steep growth in mobile data consumption. To address the capacity requirements resulting from the huge growth in mobile data traffic, the mobile network operators (MNOs) are adding more base stations and allocating more spectrum layers including outdoor and indoor small cells. Since the capacity requirement of the network varies over time, the scaling up of the network may increase the energy consumption of the Radio Access Network (RAN). Hence, we need to optimize the network to reduce the overall power consumption through Cloud based models, and deployment of power-efficient radio nodes. In this paper, we analyze the network evolution towards Cloud based Radio Access Network (CRAN) for a heterogeneous set of base stations such as those with Macro RRUs, Micro RRUs and Pico radio units. We derive the computational complexity using a flexible and \u2018future-proof\u2019 power model and apply it for the network. We also compare the computation complexity for various cases of User Equipment (UE) channel conditions, different sub-components within the given base station type and provide the results. We further use the Bin-Packing algorithm to analyze the number of base station cloud servers needed for this network and the power consumption of the base station cloud. We further evaluate whether the newer cloud servers with higher CPU cores are power efficient for a given load. We observe from the simulations, that the currently available base station cloud servers have more capacity and still are more power efficient than the baseline Compute Node servers used with the earlier power model."}, {"label": 0, "content": "This paper introduces a new imitation of neurons cells, based on the latest discoveries in neuroscience. After reobserving the latest revelations in the field of biological neurons, the conventional artificial models has proven strong potentials in image processing and pattern classification, but remains far from presenting a modern imitation of natural intelligent organisms. A Biomimetic cell design is thus proposed with a combination of registers to hold the inputs, outputs, and weights as information codes. These cells use binary equivalence gates to compare the inputs to the weights and deliver the required outputs. The abstraction model provided renders the training process highly simplistic, which speeds up the design phase and opens the way towards a new dimension in artificial intelligence."}, {"label": 1, "content": "The traditional mixed-criticality (MC) model only allows critical tasks to execute during an error or exception. However, the imprecise MC (IMC) model has been introduced to provide some (degraded) service to less critical tasks even during exceptional events, such as when a task overruns its execution demand. We are currently working on extending the IMC model to precise scheduling of tasks and integrating it with a dynamic voltage and frequency scaling (DVFS) scheme for energy minimization.\n\nScheduling precise MC systems is a challenging task since all tasks must be guaranteed to be timed correctly under both pessimistic and less pessimistic assumptions. To achieve this, we propose a utilization-based schedulability test and sufficient schedulability conditions for the earliest deadline first with virtual deadline (EDF-VD) scheduling policy. Using this unified model, we perform a quantitative study in the forms of a speedup bound and approximation ratio.\n\nFinally, we conduct both theoretical and experimental analysis to demonstrate the correctness and effectiveness of our algorithm. This work presents a significant advancement in the MC model and will enable energy-efficient, precise scheduling of tasks in mixed-criticality systems."}, {"label": 0, "content": "The majority of existing wireless communication systems relies on training-based signaling, i.e., schemes that rely on accurate channel estimates for detecting the transmitted symbols. Training requires its own hardware and algorithms and consumes a significant portion of the channel coherence interval, which is typically short in vehicular communications. The drawbacks of training can be alleviated by using noncoherent signaling schemes, which dispense with the training phase altogether. We will focus on a particular class of such schemes, i.e., Grassmannian signaling. This scheme is optimal for high signal-to-noise ratio (SNR) communication over noncoherent block-fading channels, which are likely to arise in many future networks."}, {"label": 1, "content": "Grinding in ball mills is a vital industrial process that involves reducing the size of particles with varying physical, chemical, and mechanical characteristics. Effective control performance of ball mills' grinding process is crucial to ensure maximum profitability, energy consumption, product quality and time efficiency. In this study, a nonlinear model predictive control system is developed for the ball mill grinding process. To evaluate the performance of the proposed control system, the economic performance, time delays, and energy consumption in the grinding process are analyzed using the Discrete Element Method (DEM) software. The experimental results indicate that the proposed control system is highly effective in optimizing the grinding process."}, {"label": 0, "content": "In order to realize the online evaluation and control of transient stability after power system fault, a decision tree method and emergency control scheme based on Fisher linear discriminant were proposed. This method firstly processed the offline simulation data based on Fisher liner discrimination (FLD), reduced the data dimension and obtained the distance from the data point to the classification hyperplane, and obtained a more adaptive classification decision tree by training the distance data. Fit the distance to obtain a stability index that can guide emergency control. On this basis, the optimal load shedding and generation shedding scheme were calculated according to the 0-1 linear programming, and the loss was reduced to the minimum under the premise of ensuring the stability of the power system. Finally, the simulation analysis was carried out in the WECC system and New England system. The results showed that the decision tree based on FLD is classified accurately and interpretatively. The online emergency control strategy based on stability margin can effectively deal with the transient instability after fault, and restore the system to stable operation."}, {"label": 0, "content": "Methods of solution of a problem of an estimation of uncompensated errors of the measurements caused by presence of noises and interferences, that not submitting to statistical regularities are investigated. The model of the signal presented in the form of a composition multiparameter quasidetermined model function and fixed on interval of measurement a background function is considered. The background function is described by an arbitrary ensemble of functions whose range of variation is limited by the E-layer. The proposed model provides the calculation of interval estimates of signal parameters for confidence values close to unity. The technique of calculation of interval errors of parameters on the basis of model of E-areas and nonlinear model function is described. This technique allows calculating interval estimations of parameters for a particular of a signal realization, provided that the range of values of the background function is limited. It is shown that defined in the space of parameters E-regions characterize the state of the control object. Based on the developed algorithm for diagnosing the state of an object from the particular signal realisation using E-regions in the space of parameters of a model function, an approach to identifying abnormal situations is proposed. Based on the developed algorithm for diagnosing the state of an object from the implementation of a signal using E-regions in the space of parameters of a model function, an approach to identifying abnormal situations is proposed. The results of practical application of E-regions for monitoring the process of power consumption of electric energy and other energy resources are presented."}, {"label": 0, "content": "Aiming at the issue how users in Windows domain cross-realm access cloud computing resources, a cross-realm authentication scheme based on federated identity was proposed. Based on the idea of the declaration, the scheme uses the federated identity provider to replace the gateway in the traditional gateway-based cross-realm authentication model, so as to realize the users in Windows domain access the cloud resources without re-authentication. The scheme uses SAML protocol to exchange user identity information between different domains, which ensures versatility and security of the system and realizes seamlessly secure communication between different security domains. Finally, based on claim provider, federated identity provider and application service provider, we give the design of the key components of the three modules, then the feasibility of the scheme is verified with the popular cloud platform OpenStack."}, {"label": 0, "content": "Regarding the stability analysis and protection control of line-commutated converter-based high voltage direct current (LCC-HVDC) sending-end power grid, the assessment and extraction of dynamic response features is of great importance. This paper proposes a two-stage approach incorporating local and global analyses to extract the spatiotemporal distribution features of dynamic frequency and voltage responses of large sending-end power grid under LCC-HVDC blocking. From the perspective of time, the local quantitative indices are constructed in the first stage to measure the response time delay, deviation magnitude, rate of change and cumulative effects of dynamic trajectories. Then in the second stage, the temporal features of different observation sites are compared globally, in which the observation space and dissimilarity matrices are created to depict the spatial disparities of the whole observation region. Based on the extracted spatiotemporal features, a typical dynamic response pattern of sending-end power grid is captured to detect the LCC-HVDC blocking. Sichuan power grid in China is selected as a case study to verify the proposed approach and demonstrate the obtained features and pattern."}, {"label": 1, "content": "The experimental research on the security and stability control system (SSCS) of UHVDC transmission project involves functional verification of various aspects such as information interaction between SSCS and UHVDC control and protection system, fault criterion of DC converters, calculation of DC power loss, and coordinated control strategies in case of failures in DC system or N-2 faults in AC system among other things. This paper proposes a modular modeling method for testing SSCS and analyzes the existing test methods. \n\nThe proposed modeling method converts the external system of the AC/DC hybrid network into REI nodes and performs coherency-based dynamic equivalence on the sending-end generator groups. This modular design reduces the granularity of simulation calculations, improves the speedup ratio of parallel computing, and improves the efficiency of processors usage to meet demands for large-scale closed-loop testing on UHVDC system-level protection technologies including DC co-control and precisely machines tripping. \n\nUsing this modeling design, the function of DC control and protection system is simulated, the interface between RTDS and SSCS is realized, and the simulation and test platform for UHVDC SSCS is built on RTDS. The stability control strategies and the system function verification of Zhalute-Qingzhou \u00b1800 kV UHVDC SSCS were carried out on the platform. The simulation and test results verified the effectiveness of the typical test scheme."}, {"label": 0, "content": "In this paper, we propose LayerOS, a proactive framework that can be used to schedule apps in order to improve the scalability of wearable devices. LayerOS generates scheduling strategies which wille be deployed to devices ahead of time based on the analysis of user behavior, device capability, etc. According to these policies, LayerOS can offload apps to the cloud or load apps from the cloud dynamically. Hence, users can execute any app provided by the cloud directly, and there is no need to modify installed apps and interrupt users' normal operations. In addition, LayerOS uses app state migration to ensure consistency of view and state of the application on different mobile devices. Experiments based on a prototype system have shown that, at the same level of limited space, LayerOS enables users to run virtually unlimited applications directly and increases the running fluency by 25.6% from the perspective of FPS. At the same time, LayerOS can effectively reduce the waiting time when launching one application. More specifically, the average waiting time can be reduced by up to 13.0% in the experiments."}, {"label": 1, "content": "Multi-hop broadcast communication is a critical aspect of vehicular ad hoc networks (VANETs). In this study, we introduce a receiver-oriented broadcast approach that leverages a symmetric volunteer's dilemma game to model the probability of forwarding messages to receiving vehicles. This game treats receiving vehicles as players, and it requires at least one of them to pay a cost and volunteer to rebroadcast the message so that all players can benefit from it.\n\nTo enhance the efficiency of the proposed scheme, we incorporate fuzzy logic techniques that take into account the local density and probability of transmission, and adjust the contention window size at the MAC layer. We evaluated the performance of the volunteer's dilemma inspired broadcast (VDIB) scheme using ns-3 simulations, analyzing reachability, rebroadcasts per covered vehicle, bytes sent per covered vehicle, and per-hop delay. We compared the VDIB with other protocols, including distance-to-mean broadcast, contention-based forwarding, distribution-adaptive distance with channel quality, statistical location assisted broadcast, fuzzy logic-based broadcast, bandwidth efficient fuzzy logic assisted broadcast, and intelligent hybrid adaptive broadcast schemes, in both highway and urban environments.\n\nOur results suggest that the VDIB approach outperforms other protocols in reachability, requiring fewer rebroadcasts per covered vehicle, and transmitting fewer bytes per covered vehicle. Furthermore, our simulations indicate that the VDIB scheme incurs less per-hop delay compared to alternative protocols. Therefore, the proposed protocol has significant implications on enabling efficient and reliable communication in VANETs."}, {"label": 1, "content": "The estimation of machinery health and identification of defects in dynamic mechanical and technological equipment requires the measurement of parameters of oscillatory processes. Vibroacoustic oscillatory processes are often used as a source of information for both vibroacoustic-based diagnostics and acoustic-emission control. Measurement of process peak parameters is the basis of many state evaluation criteria. However, given the random nature of the processes, estimating measurement error and determining peak values of signals becomes a crucial task.\n\nThis work presents the methodical bases of estimate of measurement error and determination of peak values with consideration of the laws of distribution of their instantaneous values. The paper offers a solution to determine the dependence of the asymptotic error estimate of the sample quantile on the value of discrete values in the sample, which infers the real-time realization of the signal. This allows for justification and verification of the metrology of measuring instruments while estimating the reliability of measurements of peak values of the vibroacoustic signal."}, {"label": 0, "content": "Rapid technological advancements are disrupting traditional job markets necessitating job seekers to develop new skillsets suitable for the digital economy. This phenomenon has a major impact on the economy, particularly in developing countries. Consequently, it is crucial for institutional offerings to be aligned to industry requirements in every discipline of higher education, in order to ultimately sustain and improve the economy. This paper presents a framework designed to determine the alignment of the digital skills that students acquire from higher education to the digital skills requirements of industry. This alignment will aid higher education institutions in improving the digital skills preparedness of their graduates, and ultimately sustaining the digital economy. Given that the digital economy requires its employees to possess a specific level of digital skills, which may vary in each sector, the proposed framework is therefore not discipline specific. Consequently, this framework may employed to establish an alignment between any discipline in higher education and the respective industry that its graduates feed into. The authors have systematically reviewed related articles to determine the factors influencing the digital skills preparedness of graduates for industry. Relevant studies were analyzed, thereby resulting in the development of a digital skills preparedness model."}, {"label": 0, "content": "An effective technique for the loss probability calculation in queueing systems with heavy tails is developed. The technique is used for analysis of queueing systems with power-law distributions which are widespread as models of network devices operating under fractal traffic. Dependence form of loss probability on a buffer capacity in the systems is analyzed. The effect of the channel number in queuing systems on the dependence is examined. In practice, the developed rapid technique and obtained results of its applying might be used for solving of engineering problems of analysis and design of modern computer networks."}, {"label": 0, "content": "With the development of the Internet, social bots are increasingly spreading on social platforms. Therefore, an effective detection algorithm is demanded to detect these social bot accounts that endanger social networks. In this paper, a social bots detection model based on deep learning algorithm (DeBD) is proposed. The model mainly includes three layers. The first layer is the joint content feature extraction layer, which focuses on the feature extraction of the tweets content and the relationship between them. The second layer is the tweet metadata temporal feature extraction layer, which regards the tweet metadata as temporal information and uses this temporal information as the input of the LSTM to extract the user social activity temporal feature. The third layer is the feature fusing layer, which fuses the extracted joint content features with the temporal features to detect social bots. To evaluate the effectiveness of the DeBD model, we conducted experiments on three different types of new social bot data sets from the real world and the experiment results also demonstrate the effectiveness of our proposed model."}, {"label": 0, "content": "Effective hyperspectral unmixing (HU) is essential to the estimation of the underlying materials' signatures (endmember signatures) and their spatial distributions (abundance maps) from a given image (data) of a hyperspectral scene. Recently, investigating HU under the non-negligible endmember variability (EV) and outlier effects (OE) has drawn extensive attention. Some state-of-the-art works either consider EV or consider OE, but none of them considers both EV and OE simultaneously. In this paper, we propose a novel HU algorithm, referred to as the variability/outlier-insensitive multi-convex unmixing (VOIMU) algorithm, which is robust against both EV and OE. Considering two suitable regularizers, a nonconvex minimization problem is formulated for which the perturbed linear mixing model proposed by Thouvenin et al., is used for modeling EV, while OE is implicitly handled by applying a p quasi-norm to the data fitting with 0 <; p <; 1 . Then, we reformulate it into a multi-convex problem which is then solved by the block coordinate descent method, with convergence guarantee by casting it into the block successive upper bound minimization framework. The proposed VOIMU algorithm can yield a stationary-point solution with convergence guarantee, together with some intriguing information of potential outlier pixels though outliers are neither physically modeled in the above problem nor detected in the algorithm operation. Finally, we provide some simulation results and experimental results using real data to demonstrate the efficacy and practical applicability of the proposed VOIMU algorithm."}, {"label": 1, "content": "mHealth4Afrika is a collaborative project focused on supporting Horizon 2020 Societal challenges and Sustainable Development Goal 3. It aims to research and evaluate the impact of developing an open-source, multi-lingual mHealth platform to support primary maternal healthcare delivery in Southern Africa, East Africa and the Horn of Africa. The co-design process was applied to the development and validation of the Beta platform v1, which was tested with 36 participants from 11 healthcare clinics in Ethiopia, Kenya, Malawi and South Africa using observation and semi-structured interviews. These findings informed the subsequent iterations of the mHealth4Afrika beta platform, which integrates Electronic Medical Records, Electronic Health Records, sensors and visualization to generate monthly health indicators. The expected outcome is proof of concept that can accelerate the exploitation of mHealth across Africa, improving health outcomes and strengthening health systems."}, {"label": 1, "content": "High bandwidth signal transmission between an ASIC and memory is becoming increasingly important, particularly for high-performance Cell Phone Application Processor (AP) applications. To meet this need, several advanced packaging technologies have been developed, including the High Bandwidth Package on Package (HBPoP), which is replacing FCMAPPOP for a range of high- and middle-end mobile phone products. The HBPoP structure is similar to a sandwich, with a substrate interposer (SI) substrate stacked on top of the silicon with fine pitch solder joints. This provides routing and re-layout of the I/O for high bandwidth requirements. However, warpage control is a key challenge in the PoP technology, especially with the thinner and thinner package thickness requirements.\n\nThis work focuses on analyzing warpage behavior in HBPoP through the development of a three-dimensional computational simulation model. The model considers the effects of pretreatment temperature on package warpage performance. To validate the simulation results, advanced metrology analyzer (aMA) based on three-dimensional digital image correlation technology was used to measure bare substrate in plan strain and shrinkage behavior with different temperatures. Thermal mechanical analyzer (TMA) was also utilized to capture the substrate shrinkage properties by multi-cycles measurement. The warpage predicted by the finite-element model (FEM) was found to be in good agreement with the experimental data. \n\nThe results of the study revealed that the thickness and materials of each laminate, as well as supplier deviations, were crucial factors in warpage control. Several design combinations were proposed to enhance warpage control. Overall, this work provides valuable design guidance for HBPoP technology development, especially in terms of warpage control."}, {"label": 0, "content": "Tile-based video systems have recently emerged as a viable solution to overcome the challenges of 360-degree video. For instance, the HEVC based viewport-dependent profile of MPEG OMAF allows serving clients independently coded tiles of the 360-degree video at varying resolution to enhance fidelity within the actual user viewport. During streaming, the client constantly adapts its tile selection and feeds a single merged bitstream to the video decoder. This paper addresses the open issue of rate assignment in a distributed encoding system in such a multi-resolution tiled streaming scenario. A model for tile rate assignment based on the spatio-temporal activity of the video is presented to reduce variance of the quality distribution and experimental results are reported."}, {"label": 1, "content": "Heterogeneous IoT-enabled networks accommodate both jitter-tolerant and intolerant traffic. To handle the resultant volume of such traffic, Optical Burst Switched (OBS) backbone networks transmit it in huge chunks called bursts. However, due to the lack of buffering capabilities in the core network, contentions and congestion occur frequently, affecting the overall quality of service (QoS). Burst losses occur frequently, particularly when traffic levels surge. Congestion is resolved by deflecting contending bursts to other less congested paths. This, however, may lead to differential delays incurred by bursts as they traverse the network, contributing to undesirable jitter that can compromise overall QoS.\n\nJitter is mostly caused by deflection routing, a result of poor wavelength and routing assigning. In this paper, we propose a Controllable Deflection Routing (CDR) scheme that allows the deflection of bursts to alternate paths only after controller buffer preset thresholds are surpassed. This ensures that bursts intended for a common destination are most likely to be routed on the same or least-cost path end-to-end. We describe the scheme and compare its performance to other existing approaches. Analytical and simulation results show that the proposed scheme lowers both congestion and jitter, improving throughput, and avoiding congestion on deflection paths."}, {"label": 1, "content": "The management and configuration of optical networks, as well as implementing new policies to keep up with the ever-changing network, can be tedious tasks. However, software-defined networking (SDN) has revolutionized the way network solutions are developed for electrical networks. The application of SDN for optical networks offers new opportunities to simplify and expedite these tasks. To achieve this objective, an optical network description language (ONDL) has been developed as a first step. ONDL is utilized to describe various network components, their configuration, and run-time states including modulation schemes, wavelength, and spectral width of a transponder, switching matrix of an optical switch, etc. The language is based on resource description framework (RDF). Moreover, a controller that understands and sends instructions in ONDL has been developed to different network devices to provide or change their states. The applicability of ONDL is demonstrated through the simulation of a network, where its nodes are controlled and managed using ONDL and the developed controller."}, {"label": 0, "content": "Ultra-wideband (UWB) is a very promising technology for accurate indoor localization. Time of arrival (TOA) and time difference of arrival (TDOA) are two of the most widely used algorithms for UWB to localize the mobile station (MS). However, the accuracy performance of the position estimation based on these algorithms dramatically degrades under Non-Line-of-Sight (NLOS) condition. Inertial measurement unit (IMU) is not effected by NLOS. However, the localization results based on IMU are only accurate for a short period of time due to the drift errors. The integration of these two systems will allow to profit from their advantages. In this paper, the Extended Kalman Filter (EKF) is used to combine IMU and UWB with TOA or TDOA approach. Based on the IMU measurements, the inaccurate UWB range measurements or range difference measurements due to NLOS could be detected in the EKF. Furthermore, in order to further improve the detection accuracy, the channel impulse response (CIR) from UWB are also used for NLOS detection during the real field test. The support vector machine (SVM) is used to train the data. Only the identified accurate measurements will be used for further calculation in the EKF. The performance of the proposed method will be evaluated in the simulation and real field tests. The proposed method shows both in simulation and real field tests very promising results."}, {"label": 1, "content": "As a result of the numerous advantages that the cloud can offer to robots, the concept of cloud robotics has emerged as a new paradigm. This approach allows robots to offload computing and storage tasks to the cloud, resulting in smaller on-board computers. Additionally, cloud robotics enables robots to share knowledge with others in the community through a designated cloud space. In recent years, several cloud robotics platforms have been developed to promote the creation of robots that benefit from this paradigm. This paper critically reviews and compares these platforms to provide recommendations for future use and identify gaps that need to be addressed. The investigation involved an analysis of 8 cloud robotics platforms. The review found that there are varying underlying architectures and models employed by these platforms, as well as a range of features offered to end-users."}, {"label": 0, "content": "This paper presents a shape descriptor-based approach to human activity classification in devices such as iPod Touch, smartphones, and other similar devices. In this work, signals acquired from the built-in accelerometer and gyroscope sensors of iPod Touch are analyzed to recognize different activities performed by a user. In order to extract the discriminative information, shape descriptor-based features are computed from the captured signals. These features are then normalized and concatenated to form a consolidated feature vector. To recognize an activity performed by the user, k-nearest neighbor classifier is employed. The proposed approach is evaluated on the publicly available dataset namely, physical activity sensor data. Our experimental results demonstrate the effectiveness of the proposed shape descriptors for activity classification. Additionally, the experimental results on the aforementioned dataset show significant improvement in classification accuracy as compared to the existing work."}, {"label": 0, "content": "Sounds are ubiquitous in our daily lives, for instance, sounds of vehicles or sounds of conversations between people. Therefore, it is easy to collect all these soundtracks and categorize them into different groups. By doing so, we can use these assets to recognize the scene. Acoustic scene classification allows us to do so by training our machine which can further be installed on devices such as smartphones. This provides people with convenience which improves our lives. Our goal is to maximize our validation rate of our machine learning results and also optimize our usage of hardware. We utilize the dataset from IEEE Detection and Classification of Acoustic Scenes and Events (DCASE) to train our machine. The data of DCASE 2017 contains 15 different kinds of outdoor audio recordings, including beach, bus, restaurant etc. In this work, we use two different types of signal processing techniques which are Log Mel and HPSS (Harmonic-Percussive Sound Separation). Next we modify and reduce the MobileNet structure to train our dataset. We also make use of fine-tuning and late fusion to make our results more accurate and to improve our performances. With the structure aforementioned, we succeed in reaching the validation rate of 75.99% which is approximately the seventh highest performing group of the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge 2017, with less computational complexity comparing with others having higher accuracy. We deem it a worthy trade-off."}, {"label": 1, "content": "In this paper, we present a novel approach for addressing the issue of low signal to noise ratio (SNR) in 2-dimensional (2-D) Direction of Arrival (DOA) estimation. Our proposed method involves extending the antenna steering vectors prior to implementing the MUltiple SIgnal Classification (MUSIC) algorithm for 2-D DOA estimation. Results from simulations demonstrate a significant improvement in location accuracy."}, {"label": 1, "content": "This article introduces a data assimilation technique for social agent-based simulations that utilizes a reinforcement learning method to automatically fit real world data. The approach involves using a hidden Markov model to estimate system states during the reinforcement learning process. By incorporating new real data, this method can gradually improve simulation models without requiring total optimization. To demonstrate the feasibility of the proposed technique, it was applied to a housing market problem using real Korean housing market data."}, {"label": 1, "content": "The manufacturing industry faces new challenges due to the increasing complexity of parts and growing quality requirements. Multi-Stage Production Systems, which consist of multiple process steps, need to adapt to remain cost-effective and flexible while still meeting high quality standards. Zero-Defect Manufacturing aims to reduce waste and rework by analyzing and optimizing production systems through data-driven and learning-based approaches. To achieve this goal, a Knowledge Capturing Platform is introduced, which utilizes inter-stage correlation methods, part variation approaches, and intelligent monitoring systems to extract a deeper understanding from collected data."}, {"label": 1, "content": "The importance of automatic disease detection using visible symptoms on leaves is increasing rapidly. In this article, we present a novel algorithm that utilizes machine learning to detect diseases in a diverse range of plants and diseases. Even in the presence of noisy images, distinct backgrounds, and varying disease coverage, our algorithm has achieved a high accuracy rate of over 93%.\n\nFurthermore, this algorithm has the added benefit of being capable of self-training, which reinforces its ability to deliver even greater accuracy with continued use. It can operate on numerous platforms, including smartphones, providing non-specialist farmers with the means to manage diseases more successfully.\n\nWith its numerous benefits, the automatic disease detection algorithm is poised to revolutionize the way diseases are detected and managed in agriculture."}, {"label": 0, "content": "Most of the available graph-based semisupervised hyperspectral image classification methods adopt the cluster assumption to construct a Laplacian regularizer. However, they sometimes fail due to the existence of mixed pixels whose recorded spectra are a combination of several materials. In this paper, we propose a geometric low-rank Laplacian regularized semisupervised classifier, by exploring both the global spectral geometric structure and local spatial geometric structure of hyperspectral data. A new geometric regularized Laplacian low-rank representation (GLapLRR)-based graph is developed to evaluate spectral-spatial affinity of mixed pixels. By revealing the global low-rank and local spatial structure of images via GLapLRR, the constructed graph has the characteristics of spatial-spectral geometry description, robustness, and low sparsity, from which a more accurate classification of mixed pixels can be achieved. The proposed method is experimentally evaluated on three real hyperspectral datasets, and the results show that the proposed method outperforms its counterparts, when only a small number of labeled instances are available."}, {"label": 1, "content": "A new algorithm, called \"Snake,\" is proposed in this paper to efficiently solve regularized optimization problems over large and unstructured graphs. The regularization term is tied to the graph geometry, and typical examples include the total variation and Laplacian regularizations. While off-the-shelf algorithms can easily solve problems over simple paths without loops, they cannot be used directly over large and unstructured graphs. Thus, \"Snake\" selects random simple paths in the graph and performs the proximal gradient algorithm over them. This algorithm is part of a new general stochastic proximal gradient algorithm, which has proven convergence. The paper also provides applications to trend filtering and graph inpainting, as well as numerical experiments over large graphs."}, {"label": 0, "content": "Cloud computing provides on-demand access to IT resources via the Internet. Permissions for these resources are defined by expressive access control policies. This paper presents a formalization of the Amazon Web Services (AWS) policy language and a corresponding analysis tool, called ZELKOVA, for verifying policy properties. ZELKOVA encodes the semantics of policies into SMT, compares behaviors, and verifies properties. It provides users a sound mechanism to detect misconfigurations of their policies. ZELKOVA solves a PSPACE-complete problem and is invoked many millions of times daily."}, {"label": 0, "content": "In this paper, we propose a method for resolving simple signals based on the method of dividing the spectra, which makes it possible, by calculating the phase of the received signal, to specify the moment of recording the signal at the receiver. It is shown that the implementation of this method in primary data processing systems of interferometric sonar leads to an increase in the accuracy of determining the spatial position of the object at the bottom. An estimation of the resolution of the chirp signals is made, on the basis of which it was concluded that it is possible to determine the signal fixing time by linear correlation method no worse than one sampling, at a sampling frequency lower than the value of the central carrier frequency. The method of compression and recovery of simple signals used in side-scan sonar is considered. The article also contains the results of modeling the dependence of the error in determining the spatial position of the object at the bottom of the angle of rotation of the base of the interferometric sonar of the side survey, the duration of the probing premise, and the frequency of the discretization of the recorded response."}, {"label": 1, "content": "Mechanical equipment often generates nonlinear and non-stationary signals, which makes it challenging to diagnose faults using traditional methods such as Fourier transform and wavelet transform. To address this issue, this paper proposes a novel machine fault diagnosis method based on Industrial Internet of Things (IIoT), industrial wireless sensor networks (IWSNs), Hilbert-Huang transform (HHT), and support vector machine (SVM). The approach combines HHT and SVM for fault feature extraction and diagnosis respectively. The proposed method is tested on an IWSN sensor node, and the fault feature extraction is verified using MATLAB simulation. The accuracy of the approach is evaluated using bearing vibration data, with a fault diagnosis accuracy of 100% achieved for four machine working conditions and 92% for five working conditions, demonstrating the effectiveness of this approach."}, {"label": 1, "content": "We are focused on addressing two issues related to the detection of rumors. The first is stance classification, which involves identifying a user's stance towards the underlying rumor in a Twitter conversational thread. The second problem is rumor veracity prediction, which aims to verify the authenticity of a tweet that contains a rumor. \n\nTo address these issues, we have developed two different models, each designed to handle a specific task. Our veracity prediction model is based on MLPs and features, while our stance detection model is based on a hierarchical LSTM approach. \n\nOur system has been evaluated against various state-of-the-art models, and we have found that our proposed system outperformed these other models. Our models show promise in addressing the challenges associated with detecting rumors and are likely to be useful tools for social media analysis."}, {"label": 0, "content": "The prevalence of sleep apnea hypopnea syndrome (SAHS) is increasing year by year and up to 14% in 2016. The disease poses a threat to human sleep safe. To detect the disease effectively and pre-alert, a method for the diagnosis of SAHS using respiratory signals was proposed. According to the characteristics of respiratory signal, nonlinear characteristics fractal dimension and sample entropy were introduced based on time-domain characteristics variance and clinical zero passing numbers as well as frequency-domain characteristics wavelet coefficients and wavelet energy. Then a 6-dimension feature vector was structured and input into the support vector machine (SVM), back propagation neural network (BPNN) and improved back propagation neural network (IBPNN) based on morphology. The method was verified by the 8 sets data including 3840 samples of the Physionet Apnea Database. Experimental results showed that classification accuracy of SVM and BPNN was 71.2% and 84.5% respectively. The IBPNN based on morphology improved the 85% of classification results and increased accuracy by 7.3%. It is very effective to detect the SAHS using the feature vector and IBPNN based on morphology proposed in this paper and provide an efficient and convenient method for the diagnosis of diseases."}, {"label": 0, "content": "In this work, we consider the problem of inferring links in a communication network, using limited, passive observations of network traffic. Our approach leverages transfer entropy (TE) as a metric for quantifying the strength of the automatic repeat request (ARQ) mechanisms present in next-hop routing links. In contrast with existing approaches, TE provides an information-theoretic, model-free approach that operates on externally available packet arrival times. We show, using discrete event simulation of a wireless sensor network, that the TE based topology inference approach described here is robust to varying degrees of connection quality in the underlying network. Compared to an existing approach which uses the linear regression based formulation of Granger Causality for network topology inference, our approach has better asymptotic time complexity, and shows significant improvement in network topology reconstruction performance. Our approach, though sub-optimal, also has better time complexity, while still retaining reasonable performance, compared to a causation entropy based optimal algorithm proposed in the literature."}, {"label": 1, "content": "The large-scale virtualized data centers in the Cloud environment consume a significant amount of energy and emit greenhouse gases, resulting in high operational costs. A possible solution to this issue is to dynamically consolidate virtual machines (VMs) into a minimum number of physical machines using live migration.\n\nHowever, the dynamic workload of VMs makes the consolidation problem more challenging. To address this problem, we have proposed a prediction-based migration technique that makes use of predicted CPU utilization, reducing energy consumption, the number of VM migrations, and Service Level Agreement (SLA) violations within a data center. Our simulations show that our proposed technique can accomplish these improvements effectively.\n\nExcessive migration of VMs leads to performance overheads that can extend the time needed by the VMs to complete their jobs. To overcome this issue, we have proposed a deadline-aware VM migration technique which reduces the time taken by the VMs to execute their jobs significantly, thereby improving the Quality of Service (QoS). While there is a slight increase in energy consumption, our simulation results show that an appropriate setting of deadlines for VMs can establish a trade-off between energy consumption and QoS, resulting in noticeable improvements in overall data center performance."}, {"label": 0, "content": "Due to the excessive number of databases, unbalanced development and behindhand sensing infrastructures, distributed network data suffers from inconsistency, data missing, large measurement error and other data quality problems, which hinder the development of smart distribution network. In order to discover more complex deep-seated rules and provide more effective decision support for power system decision-making, it is necessary to study data mining and analysis methods that are suitable for massive data under current situation. This paper studies on the method of identifying bad data for multi-temporal and multi-spatial data in distribution networks and propose a method to identify bad data using likelihood-ratio test for 3D spatio-temporal data. In order to speed up the data processing rate, a 3D-LRT method based on multi-threading and Hadoop parallelization methods is proposed."}, {"label": 1, "content": "Image description has been a popular topic in the areas of multimedia computing and computer vision. Recent research has revealed that learning the local semantic concepts, along with image features, as contextual information may assist in better understanding the image scene. However, current image description techniques treat local features as bag-of-visual-words and fail to capture the interaction and structure of objects embedded in the image. In this study, we propose a novel captioning framework that integrates geometry structure and local concepts as side information. We introduce an Object Structure Graph to encode object positions and distribution in the image. To embed the graph into an efficient representation, we use a semantic matching schema to match the embedded graph with the corresponding sentences. Our experiments, based on the MS-COCO and Flickr30k public image captioning datasets, show that our improved solution outperforms current state-of-the-art techniques that leverage local semantic concepts. Moreover, our best model on the same splitting has results comparable to other recent approaches."}, {"label": 1, "content": "As the population ages, chronic diseases such as Parkinson's disease (PD) have become a growing concern. To address this issue, researchers are exploring the potential of ubiquitous healthcare (u-Health) systems that incorporate telemedicine, context awareness, and decision support capabilities. In this study, we propose a u-healthcare system that uses speech signals to pre-diagnose PD during voice calls. We sample and process the speech stream using machine learning (ML) algorithms to make a diagnosis. \n\nOur experiment involved a PD voice dataset comprising 40 individuals and five different ML algorithms. Using a linear Support Vector Machine (SVM) model, we achieved a false negative rate of only 10% when classifying the pronunciation of the number \"three\". This promising result demonstrates the feasibility of using speech signals and ML algorithms for pre-diagnosing PD.\n\nOverall, our study showcases the potential of u-healthcare systems in improving pre-diagnosis capabilities for chronic diseases such as PD. By leveraging the power of ML and speech analysis, we can develop more effective and accessible healthcare solutions that benefit the population as a whole."}, {"label": 0, "content": "Multi energy flow coupled network computing provides the constraints of network balance and system energy security for optimal operation of regional integrated energy system, and it is also an important basis for the system security and stability analysis. Considering the dynamic response characteristics of multi energy coupling system with multiple time scales, the network model for combined calculation of electric, gas, thermal / cold multi energy flow is constructed from two aspects of steady state and dynamic in this article. Furthermore, in the process of dynamic model solving, for improving the computational efficiency without affecting the simulation accuracy, the hybrid step time domain simulation method with electromechanical transient simulation for electric power system and medium-long term transient simulation for non-electrical system is proposed, and this method can effectively support the network computation of the multi-energy coupling system. Finally, the model and algorithm are verified by building an example of an electro-thermal coupling network and carrying the simulating."}, {"label": 1, "content": "A custom-designed communication system for distribution networks can maintain fast and reliable transmission of information and optimize the operation and management of distribution automation systems. This paper analyzes the distribution automation transformation of a regional power grid and proposes a simulation model for three types of units: the intelligent distributed protection unit, the three-remote unit, and the master station unit. The simulation model is constructed using the Optimized Network Engineering Tool (OPNET) Modeler platform, which is the main innovation of this study. Finally, an intelligent distributed protection system is simulated using the proposed models. The simulation results demonstrate that the models can effectively analyze network performance and provide a quantitative basis for designing distribution network communication systems."}, {"label": 1, "content": "Ground loop is a serious problem in both laboratory and industrial settings. It interferes with low-level signal instruments, and at times can pose a danger to humans. However, identifying ground loops within large instruments is a challenging and time-consuming task. To solve this issue, a novel technique for detecting ground loops in complex situations is now being implemented through the Internet of Things (IoT).\n\nThis innovative approach employs an exciter module with an IoT device that generates a 100 kHz ground loop current, as well as a detector that receives the test current and determines the affected cable. Multiple detectors are utilized to identify the virtual cable ID in a complex area to locate the faulty cable. Once the ground loop is detected, the affected cable ID is sent to the server to take action to prevent it from occurring again.\n\nBy utilizing the IoT and this new technique for detecting ground loops, we are able to identify and address these issues more efficiently, saving time and resources in the process. Furthermore, it provides a safer environment for both instruments and individuals involved in laboratory and industrial settings."}, {"label": 0, "content": "In this paper, we deal with a double control task for a group of interacting agents that have second-order dynamics. Adopting the leader-follower paradigm, the given multiagent system is required to maintain a desired formation and to collectively track a velocity reference provided by an external source only to a single agent at time, called the \u201cleader.\u201d We prove that it is possible to optimize the group performance by persistently selecting online the leader among the agents. To do this, we first define a suitable error metric that is able to capture the tracking performance of the multiagent group while maintaining a desired formation through a (even time-varying) communication-graph topology. Then, we show that this depends on the algebraic connectivity and on the maximum eigenvalue of the Laplacian matrix of a special directed graph depending on the selected leader. By exploiting these theoretical results, we finally design a fully distributed adaptive procedure that is able to periodically select online the optimum leader among the neighbors of the current one. The effectiveness of the proposed solution against other possible strategies is confirmed by numerical simulations."}, {"label": 1, "content": "The Unified Power Flow Controller (UPFC) is a highly advanced and sophisticated FACTS device that has the ability to control a range of line transmission parameters, including voltage, line impedance, and phase angle. This paper focuses on studying the steady-state model of the UPFC, as well as the power flow algorithm of a power system with a UPFC.\n\nThe parallel and series sides of the UPFC were treated as separate power injections, and an alternating iteration algorithm for power flow in a system with a UPFC was developed. To enhance the convergence of the algorithm, this paper analyzed the impact of various control strategies of the UPFC on the convergence of power flow calculations. A control strategy for converting constant power control to constant variable control during the iterative process was proposed.\n\nThe model and algorithm were verified through the actual project of Nanjing Western UPFC, which demonstrated the accuracy and effectiveness of the approach. The results confirm that the UPFC is an important device that can facilitate independent and complete control of power transmission parameters. Furthermore, the findings of this study have the potential to be used in the design and implementation of future power system models."}, {"label": 1, "content": "This paper presents a novel approach to multilabel classification of High-Voltage (HV) discharges captured using the Electromagnetic Interference (EMI) method for HV machines. The method involves feature extraction from EMI time signals emitted during the discharge events, utilizing 1D-Local Binary Pattern (LBP) and 1D-Histogram of Oriented Gradients (HOG) techniques, and combining them to provide a feature vector. This feature vector is then implemented in a naive Bayes classifier that is designed to identify the labels of two or more discharge sources present within a single signal. The proposed approach is evaluated using various metrics, including average precision, accuracy, specificity, hamming loss, etc.\n\nThe results of this study demonstrate successful performance that is comparable to similar applications in other fields such as biology and image processing. This first attempt at multilabel classification of EMI discharge sources opens up new possibilities for research in the field of HV condition monitoring."}, {"label": 0, "content": "For intermittent fault diagnosis, it is important to extract fault features. An intermittent fault feature extraction method based on wavelet transform is proposed. Firstly, the stress-intensity model is used to describe the fault process of intermittent faults, analysing the main part of intermittent fault signal characteristics. Considering the randomness and suddenness of intermittent faults, wavelet transform is used to extract intermittent fault features based on the advantages of wavelet transform in signal mutation and singularity detection. By extracting signal energy through the wavelet coefficients, intermittent fault features are obtained, which can be used to identify and isolate the intermittent faults. Finally, by a simulating circuit, transient intermittent fault response characteristics are obtained by simulation in the circuit, and the features are extracted by wavelet transform for the intermittent fault location. Results show that the intermittent fault features can be extracted by the wavelet transform, and the features can be used for intermittent fault diagnosis."}, {"label": 1, "content": "A unique aspect of tracking control systems is their ability to maintain precise control in dynamic modes. However, the presence of non-linearities in the control object, system drives, and the non-stationarity of system parameters in their entirety can make achieving desired quality indicators challenging using classical control algorithms. This paper proposes a novel approach for the implementation of fuzzy servo control systems, with a specific focus on the construction of tracking control systems. By developing an intelligent system that can expand corrective control action functionality, this approach enables the controller to self-adjust based on selected optimization criteria. The example used in this paper is the tracking system model based on a DC electric drive, using a changing signal to work off difficult control scenarios."}, {"label": 1, "content": "Biometric features, such as Speaker Recognition, are increasingly being used in law enforcement to identify suspects. The aim of this study is to classify audio samples on evidence with respect to the suspect's voice. A prototype application has been developed in this final project to classify the voice of the suspect using Speaker Recognition. The sound features on the evidence are compared by extracting them using the Mel-frequency Cepstral Coefficients (MFCC) method, and the classification is carried out using the Learning Vector Quantization Neural Network (JST-LVQ) method.\n\nThe LVQ method used in this study has shown a high level of accuracy in recognizing the speaker's voice. The use of LVQ method resulted in an accuracy rate of 73.33% to recognize the speaker's voice with the same sentence, and 46.67% for different sentences, which is in line with the predetermined expectations."}, {"label": 1, "content": "The urgency for effective intrusion detection systems (IDS) for the trillion connected internet of things (IoT) devices expected by 2020 has sparked increased interest in anomaly detection. However, there are currently no mechanisms to evaluate the performance of such systems in real-life deployments. This paper presents the results of applying asymptotic analysis to assess the performance of an anomaly detection algorithm designed using fuzzy logic methodologies for use in intrusion detection software for ZigBee Wireless Sensor Networks (WSNs). The software addresses the vulnerability of the ZigBee protocol to flood attacks during node discovery and association to the network. The IDS is hosted externally to the WSNs, preserving sensor node resources by employing a lightweight solution."}, {"label": 0, "content": "The packet classification problem aims to determine the behavior of incoming packets at network devices. The linear search classification algorithm assigns each packet according to its prior actions, which are determined by comparing the packet header with classification rules until a match is found. As the processing latency of packet classification is proportional to the number of rules, a large number of rules can result in serious communication delay. This problem is generalized to Optimal Rule Ordering (ORO), which aims to identify the rule ordering that minimizes the delay caused by packet classification. If two different rules match a single packet, conventional ORO does not allow the posterior rule to be placed in a higher position than the prior rule. However, interchanging the rules does not violate the policy if the actions of those rules are the same. Thus, in this paper, we specifically consider the Relaxed Optimal Rule Ordering (RORO) problem, in which rules can be interchanged if their actions are the same. In RORO, the weight of rules may vary as they are interchanged. Hence, we propose a method of calculating the weights using a zero-suppressed binary decision diagram. We prove the difficulty of estimating the weights and propose an algorithm for RORO. This algorithm computes a rule list that ensures lower latency than in several conventional algorithms and accurately computes the latency. We demonstrate the effectiveness of our method by comparing it with previous models and reordering methods."}, {"label": 1, "content": "Cloud computing and its utility computing paradigm provide on-demand resources, enabling users to adapt applications to meet current demands seamlessly. However, managing deployed applications in an elastic environment becomes increasingly complex, thus necessitating automation. To solve this problem, autonomous systems are required to continuously monitor and analyze deployed workload and the underlying infrastructure, which serve as a knowledge base for deriving corrective actions such as scaling.\n\nExisting monitoring solutions, though, are not designed to cope with frequently changing topology. Therefore, we propose a monitoring and event processing framework that follows a model-driven approach. This framework enables users to express their monitoring needs directly through referencing deployment context entities, aggregating monitoring data through mathematical expressions, processing events based on monitoring data, and attaching scalability rules to those events. \n\nOur model-driven language is accompanied by a monitoring orchestration and distributed complex event processing framework, which is capable of enacting the model in a continuously evolving multi-cloud infrastructure. We also consider cloud-specific aspects like communication costs in our solution."}, {"label": 1, "content": "The use of wireless channels to transmit sensitive information in IoT systems has become increasingly common. However, traditional cybersecurity methods are not enough to handle all the security threats that these systems face. Covert wireless communication is one approach that can provide stronger protection for IoT devices by preventing adversaries from detecting a user's transmission.\n\nIn this article, we examine covert communication in the context of a noisy wireless network. We find that the uncertainty of the aggregated interference experienced by the adversary can benefit potential transmitters. From a network perspective, wireless channels can be hidden within the interference of the noisy environment, creating a \"shadow\" network that is difficult for adversaries to detect.\n\nMoreover, we investigate the impact of an active eavesdropper on covert communication. In this scenario, the square root law becomes ineffective, and even jammer-assisted methods have limited effectiveness in preserving covertness. We conclude by highlighting some potential areas for future research in this field."}, {"label": 1, "content": "The proposed framework utilizes discriminative analysis to estimate gaze using kernel discriminative multiple canonical correlation analysis (K-DMCCA), which takes into account variations in head pose, illumination, and occlusion. The feature extraction component incorporates spatial indexing, statistical and geometrical elements, and gaze estimation is achieved through feature aggregation and transforming features into a higher dimensional space via the RBF kernel and spread factor. The output of fused features through K-DMCCA is robust against illumination and occlusion and does not require calibration. The framework is tested on MPII, CAVE, ACS, and EYEDIAP datasets, achieving an accurate gaze estimation of 4.8\u00b0 using Cave, 4.6\u00b0 using MPII, 5.1\u00b0 using ACS, and 5.9\u00b0 using EYEDIAP datasets. The framework's two main contributions are enhancing the performance of DMCCA with the kernel and introducing quadtree as an iris region descriptor, which includes statistics and geometrical indexing and is calibration free. Overall, the proposed framework provides insight into multi-feature fusion methodology for gaze estimation."}, {"label": 0, "content": "The proliferation of wireless devices and appliances is facilitating the rapid development of the Internet of Things (IoT). Numerous state-of-the-art applications are being used in, for example, smart cities, autonomous vehicles, and biocomputing. With the popularization of IoT, new challenges are emerging with respect to privacy issues. In this article, we first summarize privacy constraints and primary attacks based on new features of IoT. Then we present three case studies to demonstrate principal vulnerabilities and classify existing protection schemes. Built on this analysis, we identify three key challenges: a lack of theoretical foundation, the trade-off optimization between privacy and data utility, and system isomerism over-complexity and high scalability. Finally, we illustrate possible promising future directions and potential solutions to the emerging challenges facing wireless IoT scenarios. We aim to assist interested readers in investigating the unexplored parts of this promising domain."}, {"label": 0, "content": "The spread of microgrids is one of the most promising recent trends in the field of power systems, as they can help integrate distributed power sources and other cutting-edge technological advancements into power systems. In order to facilitate their expansion, more research on microgrid protection is needed, as they pose serious challenges to or even totally invalidate traditional protection schemes. In this paper, a comprehensive protection scheme for microgrids is proposed. Our scheme is based on an already extensively researched framework based on dynamic state estimation (DSE) and aims to utilize it for microgrid protection. The individual protection zones of a microgrid are monitored by settingless relays which continuously receive measurements and perform DSE in the time domain to detect abnormalities. However, power faults are not the only root cause of abnormal measurements. Relays can also receive erroneous measurements due to hidden failures in the system or due to malicious actors that try to inject false measurements. For this reason, we add a centralized layer to our scheme. This layer receives measurements from all the settingless relays of the microgrid and uses DSE in the quasi-dynamic domain to determine whether a settingless relay detects an abnormality due to a fault or due to a reason that does not warrant tripping action, which allows us to block erroneous tripping actions. Therefore, our layered approach increases the security and dependability of microgrid protection compared to traditional protection schemes."}, {"label": 1, "content": "In recent times, there has been a surge in the number of generator interconnections to the power grid. However, some of these interconnections are located in areas where there is insufficient transmission capability, leading to curtailment or penalization during real-time operations. To address this issue, this paper proposes a procedure for calculating the maximum possible MW injections, using Power Transfer Distribution Factors at each bus in the power grid. This calculation helps to identify areas with abundant transmission capability and prevent transmission limit violations. Although the computation involved in this procedure is intensive, GPUs were employed to accelerate it, resulting in a significant speed-up gain of up to a factor of 186 for a 9241 bus system."}, {"label": 1, "content": "In smart substations, MMS packets are commonly utilized to encapsulate protection settings information while TCP protocols are employed to transmit the MMS packets. However, TCP protocols are known to be vulnerable to HoL blocking attacks, which may result in the failure of the transmission of the protection settings that falls short of high-reliability requirements for vital power information. \n\nTo address this issue, this paper proposes a new approach to SCTP protection setting and transmission based on the IEC61850 intelligent substation information structure. The optimization of SCTP's multi-host function is carried out to ensure seamless handover of dual MMS network during failure time while multi-stream mechanisms are employed to avoid HoL blocking attacks. \n\nThis paper delves deeper to examine the implementation process and realization method of SCTP transmission featuring protection setting. The proposed method is poised to enhance the reliability of the transmission of protection settings for important power information in smart substations."}, {"label": 0, "content": "The holomorphic embedding method (HEM) applied to the power flow problem has more robust performance than the Newton-Raphson method. But as the saddle-node bifurcation point (SNBP) is approached, more terms must be included in the Maclaurin series representation of the voltage to achieve a converged solution, leading to matrices that are ill-conditioned. This ill-conditioning may prevent HEM from converging or may interfere with the ability to predict the SNBP using the so-called roots method. In this paper, we look at the effect of the robust Pad\u00e9 approximation (RPA) method on both of these issues."}, {"label": 1, "content": "With the increasing number of disturbance sources like high-speed railways and renewable energy generation, the complexity of power quality problems has also increased, which impacts the reliable operation of the power grid. Therefore, identifying the types of disturbance sources using power quality monitoring data has become crucial for targeted control of disturbance sources and determining the contribution between customers and operators.\n\nThis paper proposes a method for identifying disturbance sources using random forests. Firstly, it selects analysis indices and extracts temporal and statistical characteristics of the selected indicators from historical data. After balancing datasets, the data is used as eigenvectors of training random forests. Secondly, combining with OOB, the method adjusts random forest parameters in a closed-loop to construct a cost-optimized random forest classifier. Finally, the type of disturbances can be identified online using the classifier.\n\nThe method was verified using data from a power quality monitoring system in a regional grid of China. The study demonstrates that the proposed method has high accuracy for identifying disturbance sources, including electric railways, converter stations, wind power, photovoltaics, and smelters. This approach has substantial real-world implications for power quality management, which can further improve the reliability of power grids."}, {"label": 0, "content": "This paper introduced a hand gesture recognition method based on convolutional neural networks (CNNs). The recognition scenario consisted in a three dimensional radar array to transmit and receive 24GHz continuous electromagnetic (EM) wave, and convert the scattered EM wave to the intermediate frequency (IF) signals. This paper used the the processed frequency spectrum as the input to the CNN. Then the CNN feature detection layer learned through data training, avoiding supervised feature extraction while learning implicitly from training data. It highlighted these features through convolution operating, pooling and a softmax function. Results showed that this system could achieve a high recognition accuracy rate higher than 96%."}, {"label": 1, "content": "Local matching approaches remain prevalent tools in real-time applications. Mismatches are a frequent occurrence in stereo vision, particularly in local approaches. In this study, we introduce the truncated majority voting method (TMVM) to differentiate and decrease mismatches in local matching approaches for stereo vision. Our experiments using the Middlebury benchmark demonstrate that the proposed method can effectively identify and decrease mismatches while maintaining real-time capabilities."}, {"label": 0, "content": "This paper proposes the Eigensystem Realization (ER) to estimate dynamic phasors. Synchrophasor estimates such as amplitude, phase, frequency and rate of change of amplitude are provided in one-cycle. Thus, a new ER-based phasor estimator has been proposed and assessed under steady-state and dynamic conditions in theoretical and actual signals coming from a commercial PMU. This paper compares the performance of ER-based phasor estimates method with that of the well-known Discrete Fourier Transform. Finally, the results exhibit that the proposed phasor estimator attains reliable estimates even though under polluted conditions by high harmonic content, being able to tracking the changes in amplitude, phase and frequency."}, {"label": 1, "content": "Phasor measurement units (PMUs) are a critical component for accurate state estimation as they provide synchronized voltage phasor and current phasor measurements. Optimal PMU placement (OPP) techniques aim to minimize the number of PMUs required for complete system observability. This research paper proposes a DC state estimation model that employs the mixed integer semidefinite programming (MISDP) approach for the OPP problem.\n\nTo compare the effectiveness of the MISDP approach, the study conducts a comparison between MISDP and mixed integer linear programming (MILP). The research conducted investigates power flow measurements, injection measurements, limited communication facility, and single PMU failure for both of these approaches. The study also proposes a formulation for MISDP-based PMU placement that considers a single PMU failure.\n\nOverall, the paper discusses the advantages and disadvantages of each formulation. The results indicate that MISDP outperforms MILP in terms of accuracy and computation time. Moreover, considering a single PMU failure, the MISDP-based PMU placement approach outperforms MILP-based approaches. \n\nThe study's findings provide valuable insights into the potential of MISDP for OPP problems with applications in power systems. Further research should focus on extending this model to consider dynamic systems and include more complex constraint formulations."}, {"label": 1, "content": "Access to safe and affordable surgery remains a major challenge for five billion people globally. Studies have consistently shown that hospitals in low-and-middle income countries lack access to basic surgical equipment, including theatre lights, anesthesia machines, and electro-surgical units. Moreover, most equipment is designed with a focus on high income countries, which fails to consider financial resources and access to maintenance, spare parts, and consumables in low-and-middle income countries. \n\nTo address this issue, a roadmap for designing surgical equipment for worldwide use has been proposed. The roadmap consists of four phases, starting with identifying the need for certain surgical equipment and thoroughly researching the context in which the equipment will be used. Design requirements should be determined in Phase 2, with prototyping and close interaction with local end-users starting in Phase 3. Throughout this process, designers should prioritize safety and quality, while also ensuring that the equipment is optimized for the local context and easily applicable in comparable settings globally. \n\nBy following this roadmap, we can work towards ensuring access to safe and affordable surgery for all, regardless of income level or geographic location."}, {"label": 0, "content": "Recently, the user-side data of power grids gradually exhibit massive data and high complexity features. The traditional power anomaly detection model has been difficult to meet the existing requirements. In recent years, the neural networks and machine learning methods that are widely used in anomaly detection, but all of these methods have a high demand for training samples and cannot be well applied when missing sample tags on the power data set. This paper designs an unsupervised power data anomaly detection model mainly based on the isolated forest algorithm. The model includes modules for feature extraction, feature reduction, and isolated forest computing. The research results show that using this model to detect abnormal power usage data can process large amounts of data in a short time, but also can accommodate the lack of training samples and can better meet the practical needs of the power sector."}, {"label": 1, "content": "Based on an analysis of the fault characteristics of inverter-interfaced distributed generation under PQ control, and the impact of distributed generation on traditional protection technology, a new active distribution network protection scheme has been proposed. This scheme considers the capability of regional measurement, and is divided into two parts.\n\nFor regions with sufficient measurement capability, a protection scheme based on the direction of zone current has been proposed. The distribution network is divided into zones and the fault zone is identified by collecting the fault component of current at breakers. This approach solves the issue of distributed generation access on traditional protection.\n\nFor regions with poor measurement capability, a reclosing scheme has been proposed, which is coordinated with DG re-grid timing. This scheme successfully addresses the impact of distributed generation access on traditional protection and reclosing of the distribution network.\n\nSimulation results of examples in PSCAD/EMTDC show the effectiveness of the proposed scheme. Overall, this new active distribution network protection scheme is a significant improvement on traditional protection technology, and successfully addresses key issues related to distributed generation access."}, {"label": 1, "content": "The trend of posting food photos on the internet is on the rise, with users eager to showcase their culinary creations. Whilst the majority of these photos appear attractive, they do not always accurately represent the taste of the dish. In a previous study, a method was developed to assess the appeal of food photos by analysing their visual features. However, this method did not take into account the specifics of the dish. Our work focuses on a preference experiment, in which subjects were asked to evaluate pairs of food photos whilst measuring their gaze. Our proposed method analyses the image features of local regions selected based on the gaze information, and uses regression parameters to estimate the level of attractiveness. The results of our experiments suggest that the image features extracted from outside of the gaze regions were more effective in evaluating the appeal of a dish than those extracted from inside."}, {"label": 1, "content": "Recent studies have exposed the vulnerability of deep neural networks (DNNs) to small alterations in the input vector. This paper presents an analysis of a restricted attack scenario where only one pixel can be modified. To this end, a new method is proposed that generates one-pixel adversarial perturbations based on differential evolution (DE). This black-box attack requires minimal adversarial information and is capable of fooling a variety of networks due to DE's inherent features. Using this method, the study found that around 67.97% of natural images from Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images could be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. The same vulnerability was observed in the original CIFAR-10 dataset. Therefore, this study presents a different perspective on adversarial machine learning by exploring the susceptibility of current DNNs to low dimension attacks. In addition, the use of DE in this context highlights its effectiveness in generating low-cost adversarial attacks against neural networks to assess their robustness."}, {"label": 1, "content": "This paper explores the use of multithreaded processing of images on graphic processing units (GPUs) for feature detection and matching. Specifically, the paper focuses on the problem of feature detection and feature correspondence as they relate to image stitching and panorama creation. \n\nTo address this problem, the paper presents a parallel GPU implementation based on nVidia CUDA. This implementation is then experimentally evaluated and compared to parallel multithread CPU processing for a shared memory parallel computational model. \n\nOverall, the results show that the GPU implementation provides significantly faster processing times than the CPU implementation. This suggests that the use of GPUs for feature detection and matching in image processing tasks can lead to major improvements in efficiency and overall performance. \n\nIn conclusion, this paper highlights the potential benefits of multithreaded processing on GPUs for feature detection and matching in image processing applications. By leveraging the power of parallel processing, researchers and practitioners working with image stitching and panorama creation can achieve faster and more accurate results."}, {"label": 0, "content": "The safe operation of transmission overhead lines is often threatened by the fast growing and high growth trees under their line corridor. When the safe distance between the line and the tree is insufficient due to the limited height of transmission network, it is easy to occur tree related fault and tripping. Therefore, in order to effectively prevent the damage to overhead lines caused by the growth of extra high trees, it is necessary to know the growth rule of extra high trees and predict their growth height. In this paper, the deep learning algorithm is used to study the growth rule of extra high trees under overhead transmission lines. Different deep learning and artificial neural network algorithms such as Deep Belief Network, Auto-Encoder and Long-Short-Term-Memory Algorithm are used to predict the tree height, and the validity of these algorithms is verified. Furthermore, these algorithms are combined to verify that the combined algorithm has higher prediction accuracy than the single multilayer perceptron and mathematical statistical model."}, {"label": 1, "content": "Introduction:\n\nThis paper presents a graph computation-based power flow algorithm designed to solve large-scale AC/DC hybrid systems with multiple LCC (Line Commuter Converter) based DC grids. The proposed approach focuses on improving the computational efficiency of constructing related matrices and obtaining power flow results without altering the existing sequential iteration method.\n\nMethodology:\n\nThe hybrid system is modeled as a graph of vertices and edges that contains relevant topology and parameter information. This enables local computation to be performed independently for formulating mismatch vectors of \u0394P, \u0394Q, \u0394\u03b8, \u0394V, as well as matrices of B', and B\", only with parameters associated with its linked edges and adjacent vertices.\n\nBy leveraging hierarchical parallel computing, the fast decoupled power flow (FDPF) of the AC system, calculation of DC grids, and mismatch comparisons for each iteration of the sequential method can be done in parallel. The proposed method is implemented on a graph database platform and tested on IEEE 300-Bus, modified South Carolina 500-Bus system, and a Chinese system to verify its accuracy and time-saving performance.\n\nResults:\n\nThe graph computation-based power flow algorithm exhibits superior time-saving performance as compared to traditional power flow algorithms. This method can solve large-scale AC/DC hybrid systems with multiple LCC based DC grids with high accuracy and efficiency.\n\nConclusion:\n\nThe proposed graph computation-based power flow algorithm is a viable solution to the computational challenges posed by the large-scale AC/DC hybrid systems with multiple LCC-based DC grids. Its advantages lie in constructing related matrices and getting power flow results without modifying the conventional sequential iteration method."}, {"label": 1, "content": "Based on the principles of training selection decomposition and collective estimation, a synthesis technique for multilevel nonparametric systems of pattern recognition for multialternate classification problems has been proposed. This technique provides significant computing performance for information processing of large datasets. Two approaches have been considered. The first approach utilizes poorly dependent feature sets of the classified objects. The second approach is based on a dichotomy method, where at each stage, private decision functions corresponding to various feature sets are formed and integrated using methods of nonparametric statistics to create a non-linear decisive rule. The generalized decision on the class membership of a particular situation is made in the space of values of private decision functions. This technique also allows for the use of parallel computing technology."}, {"label": 1, "content": "This paper introduces a novel approach to representing microprocessor instruction set truth tables utilizing High-Level Decision Diagrams (HLDD). The microprocessor control components' behavior level fault model is established based on instruction level truth tables. Two methods have been suggested for generating HLDD from TTs while minimizing the edges on graphs: greedy algorithm and branch and bound algorithm (B&B). Additionally, the B&B algorithm utilizes a simple and fast computable lower bound to prune the search space. The experiment results reveal that the proposed high-level fault model is more efficient than the gate-level Stuck-at-Fault (SAF) model for several microprocessors, demonstrating the superiority of this approach."}, {"label": 1, "content": "Human activity measurement and classification has been a popular research topic for multiple years. While most solutions revolve around mobile phones, some wearable devices have specific functionalities for tracking activity. The goal of this paper is to propose a solution for recognizing human activity and detecting falls, which will offer more safety for individuals working in demanding environments. The system works in conjunction with a monitoring solution that provides real-time information about all workers and automatically raises an alarm in the event of an accident or unusual conditions."}, {"label": 0, "content": "With the emergence of big data and the development of mobile devices, mobile data mining has received more and more attention. It shows its unique advantages, but it also exposes its inability to handle large datasets efficiently. Based on the traditional mobile data mining project, we combined cloud computing and proposed and implemented the MobileWeka2 model based on cloud computing. In order to prove the feasibility of the model, we conducted different data mining experiments on multiple data sets. Experimental results show that this model can efficiently process large data sets and solve the problems of traditional mobile data mining."}, {"label": 1, "content": "The Internet of Things (IoT) is rapidly penetrating different aspects of our lives, including smart homes, wearable devices, and healthcare. However, traditional cryptographic schemes may not be suitable for resource-constrained IoT devices. The selection of cryptographic algorithms for IoT systems primarily depends on the trade-off between security and performance, particularly power consumption. Security architects need to benchmark various cryptographic algorithms to design secure protocols and schemes for IoT systems. Therefore, this paper provides a benchmark of the most popular cryptographic algorithms on the Raspberry Pi platform, along with a comparison with the Arduino benchmark results mentioned in the existing literature."}, {"label": 0, "content": "Energy imbalance market (EIM) provides an opportunity that allows larger shares of variable renewable energy sources in the grid. Under highly volatile weather conditions, an accurate forecasting of photovoltaic (PV) power is necessary for grid stability and market operation. Most of existing forecasting methods strongly rely on the accuracy of measurements, and the adaptability of these methods to complex weather conditions is rarely discussed. In this paper, a weather classification multivariate adaptive regression spline (MARS) forecasting model is introduced for complex weather conditions in all seasons. It can be updated incrementally and its high computational efficiency satisfies EIM operations. A data set that consists of the historical power and meteorological parameters produced by a small-scale PV platform is classified and used to train MARS models with forecast horizons ranging from 15 min to 24 h in different seasons. The tests and analyses results indicate higher accuracy, adaptability, and efficiency of the novel model."}, {"label": 0, "content": "The revolution of deep neural networks (DNNs) is enabling dramatically better autonomy in autonomous driving. However, it is not straightforward to simultaneously achieve both timing predictability (i.e., meeting job latency requirements) and energy efficiency that are essential for any DNN-based autonomous driving system, as they represent two (often) conflicting goals. In this paper, we propose PredJoule, a timing-predictable energy optimization framework for running DNN workloads in a GPU-enabled automotive system. PredJoule achieves both latency guarantees and energy efficiency through a layer-aware design that explores specific performance and energy characteristics of different layers within the same neural network. We implement and evaluate PredJoule on the automotive-specific NVIDIA Jetson TX2 platform for five state-of-the-art DNN models with both high and low variance latency requirements. Experiments show that PredJoule rarely violates job deadlines, and can improve energy by 65% on average compared to five existing approaches and 68% compared to an energy-oriented approach."}, {"label": 0, "content": "In view of the traditional BP neural network, high-dimensional complex data is prone to slow detection rate and low accuracy in network intrusion detection. To reduce data dimension and improve BP neural network performance, an intrusion detection method of KPCA-BP neural network is proposed. Firstly, the KPCA's good dimensionality reduction capability is used to reduce the dimension of network data. Then, by changing the initialization initial value method and loss function of traditional BP neural network, the learning performance of BP neural network is improved, and the learning effect of improved BP neural network is better. Experiments show that the KPCA-BP based intrusion detection method proposed in this paper has a better improvement effect on detection rate and accuracy."}, {"label": 1, "content": "The present study outlines the challenge of obtaining accurate data for evaluating insolation and electric power generation in solar photovoltaic systems. To this end, the paper examines three sources of information: climate reference books, satellite-based meteorological data and weather station records. A more precise assessment of these factors can be achieved by combining various sources of information. The paper, therefore, puts forth a novel method for estimating electricity generation by utilizing reference data along with readily accessible weather station records. The method accounts for various factors that can impact electric power generation such as beam, sky-diffuse and ground-reflected components of solar radiation. Furthermore, it considers the effects of total and low-level clouds on solar radiation as well as temperature on solar photovoltaic system efficiency. The assessment of solar radiation and electric power generation by solar photovoltaic systems is conducted in the village of Narin-Kunta, located in the Irkutsk region on the banks of Lake Baikal."}, {"label": 0, "content": "With the ageing and growth of the population, some chronic diseases, such as Parkinson's disease (PD), urge the society to a health-conscious looking for better health system designs. Some recent research endeavour has been supported by solutions grounded in ubiquitous healthcare (u-Health) coupling telemedicine, context awareness and decision support capabilities. In this work, we propose a u-healthcare system to pre-diagnose PD based on the speech signal of people under voice call. The speech stream is sampled as well as processed to support the pre-diagnose using machine learning (ML). Experiments were conducted over a PD voice dataset composed of 40 individuals by using five different ML algorithms. Based on a linear Support Vector Machine (SVM) model, a false negative rate of 10% was obtained when classifying the locution of number \"three\"."}, {"label": 0, "content": "Social media today is something that cannot be separated from each person, lik Instagram, twitter, facebook, path, line and many more. Everyone has at least 2 to 5 social media accounts on his smartphone. From this phenomenon its makes social media as a source of data that can be used to seek public opinion instantly.In this paper, sentiment analysis about public satisfaction in using data service of telecommunication operator in Indonesia, either at official account of each cellular operator or using the related keywords with cellular operator. The method used by the author is Support Vector Machine with TF-IDF weighting and utilization of POS Tagging and Negative Handling as improvement of accuracy before classification.In this paper, a system of sentiment analysis classification on the level of user satisfaction of operator data service. That is classification using support vector machine method. SVM with RBF kernel (Radial Basis Function). After preprocessing, POS Tagging is then TF-IDF. The results in this study showed an average f1-score rate of 95,43%, precision 92,45%, recall 93,90% and accuracy 99,01%."}, {"label": 1, "content": "This paper presents a novel hardware support method for detecting periodic components in measurement signals using zero-crossing numbers. The proposed algorithm has low computational costs and can be used to track the progress of resonant phenomena and determine the presence of carrier signals. Additionally, the basic modules required for constructing hardware support devices have been proposed, which are relatively easy to implement and can be manufactured using FPGA technology. A block diagram of a detection device of periodicity with a known frequency has been provided, along with its operation description and main time diagrams. Overall, this paper provides a promising approach for efficient and accurate detection of periodic components in measurement signals, with potential applications in a variety of fields."}, {"label": 0, "content": "Image description has become a popular topic in multimedia computing and computer vision areas. Recent works have demonstrated that learning the local semantic concepts, in addition to the image features, as the contextual information can help to understand the image scene better. However, current image description methods treating the local features as the bag-of-visual-words that do not capture the interaction and structure of the objects embedded in the image. In this paper, we propose a novel captioning framework that learns to integrate local concepts with their geometry structure as the side information. We design an Object Structure Graph to encode the positions and the distribution of the objects in the image. In order to embed the graph into an efficient representation, we introduce a semantic matching schema that matches our embedded graph with their corresponding sentence. Our experiments based on the public image captioning data sets, the MS-COCO and the Flickr30k, show that our improved solution is significantly better than current state-of-the-art techniques that leverage local semantic concepts; and our best model on the same splitting has competitive results compared to other recent approaches."}, {"label": 1, "content": "In this paper, we propose a method to determine the accurate allowed penetration level of small hydropower (SHP) by considering the random characteristic of the loads and SHP output. We establish stochastic models that take into account the variability of weather conditions in different regions and the regional synchronization feature of SHP generation. To model the randomness of hydropower output, we use a non-parametric kernel density estimation method.\n\nWe present expressions for calculating the capacity of SHP based on the node voltage constraint. We also consider the differences between the two stochastic models and introduce a correction coefficient in the expression. \n\nTo validate the effectiveness of our proposed method, we conduct a case study based on the load data of a real distribution network in Southern China Power Grid. Our results show that our method provides accurate and reliable estimation of the allowed penetration level of SHP. \n\nOverall, our proposed method demonstrates the importance of considering the randomness of loads and SHP output in determining the accurate allowed penetration level of SHP."}, {"label": 1, "content": "The aim of this study is to establish a precise and dependable means of evaluating propagation characteristics both inside and outside aircraft cabins, in order to facilitate Wireless Avionics Intra-Communication (WAIC) system design. Specifically, this paper assesses the EMF distributions originating from a 4.4 GHz wireless transmitter inside the cabin, and applies the resulting analysis to the propagation of the WAIC system from within the cabin to its exterior-mounted antenna on a passenger aircraft (Airbus 320-200 model). By deriving key characteristics from this analysis, we aim to further our understanding of WAIC system performance."}, {"label": 1, "content": "Lies can have disastrous consequences for individuals who are deceived, and unfortunately, they are quite common in the general population. Fortunately, there are some physical cues that can help us detect when someone is lying. One of these cues is related to our eyes, which can reveal changes in eye tracking and pupil diameter that occur unconsciously when someone is telling a lie.\n\nIn the context of this final task, researchers have developed a lie detector method that leverages eye tracking and pupil diameter changes through the use of the Wavelet Transform and Gabor Image Processing techniques. By employing a decision tree algorithm to classify the results, researchers were able to determine with high accuracy whether someone is lying or not.\n\nThis new lie detector could provide a valuable tool for people who need to detect lies, as the final testing results showed a precision value of 97%, 94%, and recall accuracy of 95%. With such high levels of accuracy, researchers are optimistic that this method could be an effective means of identifying deception in a variety of settings."}, {"label": 0, "content": "A novel spectral-domain singularity subtraction technique for accelerating the convergence of Sommerfeld integral tails is proposed for planar stratified media that include a perfect electrically conducting layer. Numerical results show that the extension avoids catastrophic cancellation in the spatial domain between the analytically computed and the numerically integrated terms, yields a rapidly decaying spectral tail, and enables accurate calculation of the Green's functions."}, {"label": 1, "content": "Images have a powerful impact on human emotions and decision-making by conveying a wealth of information in a single frame. With the development of \"Affective Computing\", machines can also be designed with emotional intelligence, enabling them to make decisions based on emotions. This emotional aspect of machine learning has been applied to various fields such as E-Health and E-learning. This study focuses on the use of emotional aspects in machines for Geo-tagging of images. The proposed solution employs a hybrid approach to Affective Image Classification by combining Elements-of-Art based emotional features (EAEF) and Principles-of-Art based emotional features (PAEF). The experiment involves testing each set of features individually and then combining them to form a Hybrid feature vector. Results show that the hybrid approach significantly outperforms either individual approach. Images for this project were obtained from the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset, which includes millions of images with corresponding coordinates and are free to use."}, {"label": 1, "content": "Object detection, a crucial task in computer vision, has witnessed significant advancements due to the immense progress made in big visual data analytics and deep learning. Reinforcement Learning (RL) has emerged as a promising framework to model the object detection problem because the detection procedure can be modeled as a Markov decision process (MDP). In this article, we introduce a RL system that utilizes parameterized action space for image object detection.\n\nThe proposed RL system involves an active agent exploring a scene to identify the location of a target object. The system then learns a policy that refines the geometry of the agent by taking simple actions in parameterized space that integrate the discrete actions and their corresponding continuous parameters. The generated region proposals are then optimized using the discriminative multiple canonical correlation analysis (DMCCA) in preparation for classification with Fast R-CNN.\n\nExperiments on PASCAL VOC 2007 and 2012 datasets demonstrate the efficacy of the proposed method. By leveraging the power of RL and integrating it with DMCCA, we achieve remarkable results in object detection. Therefore, the proposed method has great potential to lead to significant advancements in the field of computer vision."}, {"label": 1, "content": "In this paper, we present a novel uplink scheduling scheme for wireless powered communication networks (WPCNs) utilizing a downlink energy signal design. While harvest-then-transmit protocols and optimal resource allocation have been previously studied, prior works fail to address the explicit transmission of scheduling information. Our proposed solution tackles this issue by designing a downlink energy signal with power level modulation to communicate scheduling information to users. The hybrid-access point allocates different power levels to the subslots of the downlink energy signal and users deduce their uplink subslot lengths from their corresponding downlink subslot power levels. The scheduling is optimized based on user channel state with respect to the sum rate. The paper formulates the sum throughput maximization problem for this scheme, which is convex, and considers the proposed scheme in a noisy environment. The results demonstrate that our proposed WPCN scheme significantly outperforms conventional schemes in terms of throughput, even in imperfect synchronization scenarios. Specific numerical results are provided to support this claim."}, {"label": 1, "content": "Stochastic dynamics is a research field centered around railway vehicles that involves a wide range of randomness or uncertainty. However, stochastic dynamic systems are often high-cost and low-efficiency in terms of modeling and calculation. To address these issues, researchers have turned to neural networks as an effective machine learning tool driven by data. This paper aims to bridge the gap between neural networks and stochastic dynamics and to apply this technique in railway vehicles to achieve proper uses.\n\nThe mapping capability of neural networks for different stochastic suspension dynamics is validated through the proposed random repetition scheme. This powerful computational tool is applied to predict the dynamic performance of high-speed trains in service, instead of relying on dynamics calculations. To illustrate the advantages of dynamic performance evaluation considering the coupling of various factors, a typical case is analyzed. It is demonstrated that this approach can improve security and reliability by attaining prognostic and health management and condition-based maintenance.\n\nOverall, the combination of neural networks and stochastic dynamics exhibits great potential for enhancing the efficiency and efficacy of railway vehicle research. By leveraging the strengths of both fields, researchers may be able to achieve better modeling and calculation results, as well as more accurate predictions of vehicle performance under uncertain conditions."}, {"label": 1, "content": "An effective Physical Protection System (PPS) requires reliable methodologies and tools for automatic evaluation of its effectiveness, providing redesign suggestions and strategies, and assisting NPPs in training their staffs. To revolutionize the evaluation, design and training of PPS, a virtual reality platform, known as IPAD, was proposed previously by the authors. In order to further enhance the training experience, this paper proposes a training module utilizing the IPAD platform for virtual training of PPS. The aim of this training is to equip detection staff and response forces with the necessary operational procedures in the event of an intrusion or emergency."}, {"label": 0, "content": "An ultra low power acoustic wake-up detector based on high frequency signal analysis is presented in this paper. Focused on environmental or military Internet of Things (IoT) applications, it aims at detecting in real time the presence of specific animal species or drones for generating alerts and for triggering power consuming tasks such as high frequency signal recording only when needed. This wake-up detector continuously monitors the presence of specific frequencies in an analog acoustic signal, with a good frequency selectivity and a high frequency detection capability. It is based on an ultra-low analog frequency to voltage converter using a current-mirror, analog timers and comparators. Dedicated to long term stealth environmental or military surveys, a strong emphasis has been put on power consumption reduction in order to limit size and weight of the system. This power consumption has been reduced to 34\u03bcW, leading to a full year of autonomy including the microphone when powered by 3 coin cell CR2032 batteries."}, {"label": 1, "content": "This paper presents a novel approach for recognizing semantic action through the use of a pose lexicon. The pose lexicon is made up of a set of semantic poses, a set of visual poses, and a probabilistic mapping between them. Both the visual poses and the mapping are assumed to be hidden, and the proposed method offers a solution for simultaneously learning a visual pose model and a pose lexicon model. \n\nThe method comprises two-level hidden Markov models. The first level defines the alignment between the visual and semantic poses, while the second level represents a visual pose sequence with each pose modeled as a Gaussian mixture. An expectation-maximization algorithm is used for the training of the pose lexicon. \n\nAction classification is then formulated as the problem of finding the maximum posterior probability of a given sequence of video frames, using the learned lexicon. The results of the proposed method were evaluated on several action datasets using cross-subject, cross-dataset, zero-shot, and seen/unseen protocols. \n\nIn conclusion, the proposed method provides an efficient and effective approach to semantic action recognition through the use of a pose lexicon. The results of the experiments conducted demonstrate the robustness and versatility of the method in different experimental settings."}, {"label": 0, "content": "Object detection is a fundamental task in computer vision. With the remarkable progress made in big visual data analytics and deep learning, Reinforcement Learning (RL) is becoming a promising framework to model the object detection problem since the detection procedure can be cast as a Markov decision process (MDP). We propose a Reinforcement Learning system with parameterized action space for image object detection. The proposed system uses an active agent exploring in a scene to identify the location of a target object, and learns a policy to refine the geometry of the agent by taking simple actions in parameterized space, which integrates the discrete actions and its corresponding continuous parameters. We then optimize the representation of the generated region proposals with the discriminative multiple canonical correlation analysis (DMCCA) [11] in preparation for classification with Fast R-CNN. Experiments on PASCAL VOC 2007 and 2012 datasets show the effectiveness of the proposed method."}, {"label": 0, "content": "Fundamental matrix estimation based on RANSAC will encounter the problems of computational inefficiency and low accuracy when outlier ratio is high. In this paper, an optimized method via modification of the RANSAC algorithm is proposed to solve these problems. First, an isolation forest-based algorithm is performed to detect outliers from putative SIFT correspondences according to distribution consistency of features in location, scale and orientation. Then, a number of obvious outliers are eliminated from putative correspondences, which will enhance the inlier ratio efficiently. Finally, fundamental matrix is estimated with the optimized set. Repeated experiments indicate that the proposed method has testified result in speed and accuracy."}, {"label": 1, "content": "In this paper, we examine the parameters of the derivative of blood pressure signals to classify the waveform of the wrist pulse. Our approach analyzes the relationship between pulse waveforms and the amplitudes of characteristic points in the first derivative of blood pressure signals. By adopting this algorithm, it is possible to computerize pulse diagnostics, thereby enhancing the efficacy of this medical procedure."}, {"label": 0, "content": "Installation of flexible alternating current transmission system (FACTS) devices is a widespread approach for reducing ohmic losses in power networks. However, the location and control parameters of these devices should be determined through an optimization procedure. An interphase power controller (IPC) is one of the FACTS devices that can change the active power flow in network branches. However, the placement of this device has not been discussed yet. Therefore, an efficient optimization method will be introduced in this paper based on a genetic algorithm (GA) combined with optimal power flow (OPF) for IPC placement. Firstly, a simple novel method for entering the steady state model of IPC into the power flow equations is proposed. Secondly, the optimal values of decision variables such as IPC location and its control parameters will be determined by the proposed optimization method. The simulation results on IEEE 30-bus and 118-bus test systems show that the GA-based optimization process is able to obtain optimal solutions for the mixed integer placement problem, which would result in a more energy-efficient transmission system."}, {"label": 1, "content": "The acoustic emission phenomenon is the result of local changes in the dynamic structure of a solid body, primarily caused by micro and macro cracks, friction, and shifts. Active and passive acoustic emission methods are commonly used to study material strength, landscape stability, and seismic activity. Geoacoustic emission, which includes sound waves in the micro and macro displacement interaction, is of particular interest in earthquake prediction research. Geoacoustic signals consist of short pulses with a specific shape, and their anomalies may serve as early warning signs of an impending earthquake. To further refine the study of geoacoustic signals, a model based on the sum of modulated Berlage and Gauss functions has been proposed. However, the matching pursuit method, which determines model coefficients, has a cubic computational complexity depending on the number of functions used for signal decomposition. This article explores different numerical schemes, which increase the adaptive property of the matching pursuit method for geoacoustic emission signals and compares their effectiveness in improving the accuracy of the proposed model."}, {"label": 1, "content": "In this paper, a multi-cloud marketplace model for the Infrastructure-as-a-Service (IaaS) layer is proposed, which involves multiple cloud providers, intermediate brokers, and end users. The proposed model allows the brokers to aggregate virtual machine resources from cloud providers to serve their end users while maximizing their profits. The cloud providers also allocate their supply of virtual machines to brokers to increase their profits. \n\nTo evaluate the market structure, the notion of social welfare is defined, and two trading schemes are studied. The first scheme involves centralized control aimed at maximizing social welfare. However, it may contain unstable producer-consumer pairs, which have an incentive to deviate from the current allocation. The second scheme eliminates these unstable pairs using a generalization of the stable matching algorithm, which may lead to sub-optimal social welfare. \n\nThe proposed stable matching algorithm is a particular way of generalizing the original Gale-Shapley algorithm. The paper examines the effectiveness of the proposed multi-cloud marketplace model and compares its efficiency under different trading schemes."}, {"label": 1, "content": "This paper presents an innovative object-based approach for SAR image change detection using a newly developed statistical distance. The proposed method involves multi-temporal segmentation to segment two SAR images simultaneously, producing homogeneous objects in spectral, spatial and temporal dimensions. The use of different segmentation parameters enables segmentation of multi-temporal images at different scales, which helps to reduce spurious changes and optimize the scale of change detection. Additionally, the method employs a multiplicative noise model called the Nakagami-Rayleigh distribution to describe SAR data, which is then applied to Bayesian formulation to derive a new statistical distance that is insensitive to speckles. This distance measure is used to compute pairwise distances between objects in the segmented images. Finally, a cluster ensemble algorithm is implemented to improve the accuracy and obtain the final change detection map. The effectiveness of the proposed method is validated using multi-temporal Radarsat-2 images and shows improved performance compared to four other methods."}, {"label": 1, "content": "With the increasing adoption of distributed photovoltaic generation and energy storage systems in the power system, there is a need for more comprehensive demand-side model structures to accurately depict the dynamic performance of the system. This paper proposes a composite demand-side model structure that includes load, distributed photovoltaic generation, and energy storage systems. The traditional load model identification is improved with the proposed model structure and parameter identification method. The demand-side model structure is first proposed and then simplified for identification at a high voltage level bus. An identifiability analysis is conducted using the sensitivity method. A model parameter identification method is proposed using the differential evolution optimization method with the ambient signal data and disturbance data. The case study results on the WSCC 9 bus system demonstrate the effectiveness of the proposed model structure, while the results on a simplified 500-kV network of the Guangdong Power Grid verify the effectiveness of the parameter identification method."}, {"label": 1, "content": "This paper examines the application of different models of fuzzy implication in a 3-axis CNC milling machine that features an autonomous cooling system for the cutting tool. The authors compare and analyze four models, namely the Mamdani (minimum) product, the product (Prod), the Einstein product, and the bounded difference. Through simulation, the authors discover that the Mamdani (minimum) operator results in the best performance. \n\nIn addition, the authors conduct a numerical simulation of the fuzzy control system using the methods of the center of gravity and the difference in areas. The system employs a fuzzy-logical approach to control the machine's parameters and the cooling system, enabling more intelligent and efficient processes. \n\nThe paper also includes a description of the connection between the 3-axis CNC milling machine and the autonomous cooling system for the cutting tool. The authors demonstrate the practical application of the fuzzy-logical system in enhancing the overall performance of the machine. \n\nOverall, this paper provides valuable insights into the utilization of fuzzy implication models and fuzzy-logical systems in the context of CNC milling machines, particularly in improving the performance and efficiency of cutting processes."}, {"label": 1, "content": "The Gas turbine distributed energy supply system (DESS) is an essential component of future power systems, particularly as a crucial black-start unit. To improve the efficiency and speed of black-start planning, this paper proposes a method that uses the Support Vector Machine (SVM) model for fast amplitude determination of transmission line switching overvoltage in black-start plans based on Gas turbine distributed energy supply systems.\n\nThe paper discusses the significance of black-start plans as the last line of defense in ensuring power system reliability. It emphasizes that black-start plays a vital role in the process of system recovery and maintains system security. However, the process of making black-start plans requires repeated modeling and simulation of various scenarios, giving rise to significant costs and taking up considerable time and effort.\n\nRecent advancements in distributed integrated energy supply systems, particularly gas turbine integrated energy supply systems, have been widely supported by governments due to their high efficiency and low pollution levels. These systems possess remarkable self-start and flexible adjustment abilities, making them ideal black-start units.\n\nThe paper classifies black-start scenarios according to the function and type of black-start units. It then models transmission line switching overvoltage using PSCAD/EMTDC simulation software and conducts extensive simulations to analyze the results. Next, the paper establishes an SVM model for fast amplitude determination of overvoltage in a black-start scenario. The SVM method selects characteristic inputs based on the features of Gas turbine distributed energy supply systems and important technical problems using orthogonal decomposition methods.\n\nThe paper conducts a study comparing the effectiveness of SVM versus artificial neural network (ANN), using 200 samples in the training set and more than 1,400 samples in the testing set. The error analysis confirmed that SVM is more effective than ANN in cases where the training sample size is small.\n\nFinally, the paper provides an actual example analysis on the Guangzhou Higher Education Mega Center distributed energy station as a black-start unit, demonstrating that the fast amplitude determination of switching overvoltage model can significantly reduce the manpower and time required for black-start planning."}, {"label": 0, "content": "Server consolidation and resource elasticity are among two of the most important resource management features in cloud and edge computing. One of two forms of elasticity is often adopted. While horizontal elasticity is concerned with the acquisition and release of computational nodes in accordance with demand, vertical elasticity focuses on the distribution of a node's resources among its hosted virtual machines (VMs) or containers, by adjusting the capacity of the resource types allocated to each individual VM in accordance with its respective application's needs. In the case of vertical elasticity, when insufficient resources are available to allocate to a given VM, its application's performance may suffer degradation. For online applications, the only alternative is to live-migrate the VM to another server. On the other hand, when running batch jobs, the resource-constrained VM could also be suspended or saved to disk and revived elsewhere or on the same host, when resources become available. Given that memory availability has a significant influence on performance and system throughput, this paper investigates the viability of integrating VM migration, pausing and suspension schemes as part of a VM scheduling strategy to support the execution of both online and batch applications in a virtualized infrastructure employing memory elasticity. Results show that combining such schemes can provide utilization benefits for cloud service providers when memory is scarce."}, {"label": 0, "content": "In satellite-borne Terahertz wave ground detection situation, the quantitative estimation of Terahertz wave atmospheric absorption attenuation loss, especially with designated satellite position and down-looking angle information, has always been fundamental and a key technology application for various Terahertz communication modes, such as wideband & high-speed network, interstellar communication, satellite-ground station link, stratosphere aero-craft communication, long distance vast data transfer, short range wireless security communication etc. This paper designed an atmospheric absorption loss estimation software on satellite global THz wave ground detection. The realized functions of this software including scene establishment, basic functions and calculation methods were explained in detail. Finally, the monthly change calculation results of satellite ground detection with 0.34THz working band in 10\u00b0\u201390\u00b0 down-looking angle are given."}, {"label": 0, "content": "Attitude motion periods of the unstable satellites are important parameters for space target surveillance. Traditional period estimation methods can only be used for single period. Aiming at rotation and precession of unstable satellite, a double-period estimation method based on variational mode decomposition (VMD) and mutual information is proposed. First, the radar cross section (RCS) of unstable satellite is processed by VMD to obtain intrinsic mode functions (IMFs) and corresponding center-frequencies. Then, through calculating and comparing the mutual information of IMFs and RCS sequence, the rotation period and precession period of satellite are obtained. The experimental results indicate that both rotation period and precession period can be estimated correctly. Compared with spectrum analysis, autocorrelation function and empirical mode decomposition (EMD), the phenomenon of frequency multiplication and mode mixing can be restrained effectively, and the accuracy of period estimation is improved."}, {"label": 0, "content": "The radio frequency spectrum crunch has triggered the harnessing of other sources of bandwidth, for which visible light is a promising candidate. Even though visible light communication (VLC) ensures high capacity, coverage is limited. This necessitates the integration of VLC and device-to-device (D2D) technologies into heterogeneous networks. In particular, mobile users which are accessible by the VLC transmitters can relay data to mobile users which are not, by means of D2D communication. However, due to the distributed behaviors of mobile users, determining optimal data transmission routes from VLC transmitters to end mobile devices is a major challenge. In this paper, we propose a reinforcement learning (RL)-based approach to determine multi-hop data transmission routes in an indoor VLC-D2D heterogeneous network. We obtain the rewards for the RL-based method dynamically, by formulating the interactions between the mobile users relaying the data as an equilibrium problem with equilibrium constraints and using alternating direction method of multipliers to solve it. The proposed technique can achieve optimal data transmission routes in a distributed manner. The simulation results demonstrate the effectiveness of the proposed approach, showing that transmission routes with low delays and high capacities can be achieved through the learning algorithm."}, {"label": 0, "content": "On the basis of the principle of multi-frequency ultrasound, genetic algorithm GA and back propagation neural network BPNN, this paper proposed a prediction study of density of transformer oil. Taking 110 sets of transformer oil belonged to China southern power grid as an example, a prediction model of density of transformer oil was established based on BPNN, with the 242 dimensional multi-frequency ultrasonic data of oil sample as the input and density as the output. By adjusting the number of hidden layer neurons, the network was trained. Moreover, the genetic algorithm GA was introduced to optimize the network parameters. All results show that compared with the traditional standard BPNN model, the output value of density of transformer oil with the GA-BPNN model is much close to the real value with small errors, which lays a solid foundation to test transformer oil other parameters with tell multi-frequency ultrasonic technology."}, {"label": 1, "content": "This paper addresses the issue of target localization in multi-station redundancy systems, which have widespread applications in various fields, such as sonar, radar, wireless sensor networks, and location-based services. Although previous solutions have focused on minimum systems, such as the TOA method with three sensors or matrix inversion, they cannot be applied to complex systems. In this paper, we introduce a simple closed-form solution for multi-station redundancy localization systems that use the estimation variance as a weighting coefficient to calculate the average localization result for each group. This proposed method requires only simple algebraic calculations and does not involve matrix inversion, making it suitable for low-cost hardware devices. We derive the method using the TOA solution, and it can be extended to other locating technologies. In addition, we provide numerical examples to demonstrate the performance of the proposed method in root-mean-square error. The positioning accuracy achieved by the proposed method is almost as good as the Cram\u00e9r-Rao low bound."}, {"label": 0, "content": "We revisit the two main SAT-based algorithms for checking liveness properties of finite-state transition systems: the k-LIVENESS algorithm of [1] and the FAIR algorithm of [2]. These approaches are fundamentally different. k-LIVENESS works by translating the liveness property together with fairness constraints to the form F Gq, and then bounding the number of times the variable q can evaluate to false. FAIR works by finding an over-approximation R of reachable states, so that no state in R is contained on a fair cycle. Each technique has unique strengths on different problems. In this paper, we present a new algorithm k-FAIR that builds upon both techniques, synergistically leveraging their strengths. Experiments demonstrate that this combined approach is stronger than running both in parallel."}, {"label": 0, "content": "Virtual Machine (VM) live migration is one strategic approach that can be employed to reduce energy consumption and increase the utilisation of a single computer in large computing infrastructure. However, virtualisation in High Throughput Computing (HTC) has received limited attention in the literature. In this paper, we present an extension of an existing trace-driven simulation to incorporate virtualisation. Furthermore, we implement the pre-copy live migration algorithm to provide a test environment for job live migration in HTC system. Our simulation provides the total number of migrations and their overall time of migrations as well as calculates the energy consumption of migrations during its runtime. In this paper, we propose two methods to perform the live migration in the HTC system. We demonstrate that our responsive migration could save up to 75% of the system wasted energy."}, {"label": 0, "content": "Beamforming with conventional array processing utilizes linear, additive processing techniques to combine the signals from the different array elements. In previous work, a multiplicative processing technique was proposed for combining the signals from sensor arrays for super-resolution beamforming. The multiplicative processing technique is derived from standard linear array processing concepts and was shown to emulate the performance of larger array apertures. In this paper, experimental measurements from an acoustic sensor array are used to validate the proposed method for direction-of-arrival (DoA) estimation by comparing these results with those from linear processing of measurements from much larger aperture arrays. The processing results show that the multiplicative processing of experimental measurements from smaller apertures perform as well as linear processing of measurements from larger apertures."}, {"label": 0, "content": "In this letter, a novel 2-D square-root-based memory polynomial behavioral model is proposed. A new set of square-root-based basic functions is adopted to describe the characteristic of the predistorter. Since the proposed model has only two nested summations, the number of coefficients of the proposed model is greatly reduced compared with models which have three nested summations. The experimental results show that the proposed model can reduce the coefficients by more than 66.7% compared with the 2-D digital predistortion (2D-DPD) model. Moreover, the proposed model can achieve better adjacent channel power ratio (ACPR) performance. Compared with the 2D-DPD model and the simple online coefficient update model, the proposed model can improve the normalized mean-square error by 11 dB. Compared with the 2-D modified memory polynomial model, the proposed model can obtain similar ACPR performance with a shorter running time."}, {"label": 1, "content": "A Ground Control Station (GCS) is a crucial component in overseeing and managing autonomous vehicles executing intricate missions in real-time. In the new era of the Internet of Things, where systems are widely connected, these missions require colossal amounts of computational power to coordinate all the vehicles involved efficiently. Consequently, the set of Unmanned Vehicles (UVs) involved in the mission must achieve more demanding tasks daily. Therefore, the development of a strong, reusable, and adaptable GCS framework has become crucial to enable a single operator to supervise and control a team of heterogeneous agents. This raises several research and engineering challenges.\n\nIn this paper, we present an adaptive event-driven framework explicitly designed for GCSs involved in multi-agent missions that are heterogeneous. Our framework takes advantage of two critical features. Firstly, it enables the GCS to add or remove drivers, whether simulated or actual, at any moment, consequently changing the number or types of monitored agents. Secondly, from a software design perspective, the graphical user interface dynamically changes its outlook to reduce operator fatigue and mental workload, ultimately enhancing the chances of mission success in such intricately designed environments.\n\nFurthermore, we showcase one of the tests performed using the adaptive framework, where we observe how a real UV deployed on a water surface successfully performs a set of pre-planned trajectories. Then, we demonstrate the addition of a simulated UV joining the mission to assist in fulfilling a leader-follower maneuver."}, {"label": 1, "content": "The holomorphic embedding method (HEM) is a powerful technique used for solving power flow problems, having demonstrated more robust performance than the commonly used Newton-Raphson method. However, as the saddle-node bifurcation point (SNBP) is approached, the method encounters a hurdle - more terms must be included in the Maclaurin series representation of the voltage to achieve a converged solution. This, in turn, leads to ill-conditioned matrices, which can cause issues with HEM's convergence and its ability to predict the SNBP using the roots method.\n\nTo address this problem, a new technique is proposed in this paper, called the robust Pad\u00e9 approximation (RPA) method. This method can alleviate the ill-conditioning issue by introducing a rational function interpolation of the voltage Maclaurin series using Pad\u00e9 approximants. As a result, RPA can lead to better convergence rates, higher accuracy and more robust performance compared to the conventional Pad\u00e9 method.\n\nTo investigate the effectiveness of the RPA method, we conduct simulation studies on various power systems. The results demonstrate that, for systems near the SNBP, the RPA method outperforms the conventional Pad\u00e9 method and provides superior prediction capabilities. Furthermore, RPA can provide accurate solutions for very weak system conditions, a region in which the Newton-Raphson method often fails.\n\nOverall, the proposed RPA technique represents a significant step forward in improving the performance of the HEM in power flow studies, particularly in systems with SNBPs or weak system conditions. Its potential applications extend beyond power flow studies and may be relevant to other domains requiring numerical approximation of ill-conditioned matrices."}, {"label": 1, "content": "This article introduces the definition of the function of instrumental contact establishing for short-range radio detection devices, and defines an analytical expression for the function of distance change between the object and the detection device. It also discusses the analytical dependences of the function of instrumental contact establishing in both two-dimensional and three-dimensional coordinate systems, taking into account complex object and device trajectories. The article also shows that the accumulating probability of object detection is based on the function of instrumental contact establishing."}, {"label": 0, "content": "The recent past has seen an influx of new generator interconnection to the power grid. Some of these new generator interconnections occur at places where there isn't enough transmission capability and hence get curtailed or penalized during the real time operations. In this paper a procedure is proposed to calculate the maximum possible MW injections with the help of Power Transfer Distribution Factors at each of the buses in the power grid without violating transmission limits. This in turn identifies areas on the grid with abundant transmission capability. The above calculation is computationally intensive, and GPUs were used to accelerate the computation and a speed up gain of up is a factor 186 for 9241 bus system."}, {"label": 1, "content": "Fog computing provides a ground-breaking model for executing Internet of Things (IoT) services. However, achieving coordinated cooperation among computational, storage, and networking resources in the fog can be problematic due to the volatility of resources. To address this issue, the FogFrame architecture is designed and implemented as a representative framework, which defines the necessary communication mechanisms for instantiating and maintaining service execution in the fog. \n\nTo assess the effectiveness of the FogFrame architecture, a series of experiments are conducted. These experiments demonstrate how service placement, deployment, and execution are performed by the framework, and how the framework operates at runtime. The framework adapts to changes in available resources, balances the workload and recovers from resource failures and overloads. \n\nIn summary, the FogFrame framework is a robust solution for executing IoT services in the fog. Its cutting-edge architecture provides the necessary communication mechanisms for instantiating and maintaining service execution in the fog. Furthermore, it is capable of adapting to changes in available resources, maximizing efficiency and ensuring reliable service execution."}, {"label": 1, "content": "Automatic age estimation from facial images has become an area of growing interest due to its potential use in various real-life computer vision applications. However, the challenging nature of this task cannot be ignored due to factors such as unpredictable environments, inadequate training data, and noticeable person-specific and large within-age variations. Hierarchical age estimation, which uses a multi-level approach, has been found to produce comparable results to single level approaches. Most hierarchical age estimation methods employ support vector machines to classify age groups and support vector regression for within-age group age estimation. \n\nThis paper introduces a novel hierarchical Gaussian process framework for automatic age estimation. The framework comprises of a multi-class Gaussian process classifier that categorizes the input images into distinct age groups, followed by a warped Gaussian process regression model to determine the age progression within each group. The framework also includes separate tuning of hyper-parameters for each age group. Compared to single level Gaussian process approaches, the proposed method is computationally efficient at both levels of the hierarchy. Dividing the data into different age groups and learning group-specific hyper-parameters is much more efficient than using complete training data. Misclassifications at group boundaries are corrected at the regression stage through overlapping neighboring age ranges. \n\nExtensive experiments on two popular aging datasets, the FG-NET and the Morph-II, demonstrate the efficacy of the method in improving age estimation performance. Overall, the proposed hierarchical Gaussian process framework for automatic age estimation provides a promising alternative to single level approaches, offering improved computational efficiency and accuracy."}, {"label": 0, "content": "The paper analyses using various models of fuzzy implication in 3-axis CNC milling machine with an autonomous cooling system of a cutting tool. The authors discuss the following models: Mamdani (minimum) product, product (Prod), Einstein product and the bounded difference. Simulation has showed that the best result is obtained using the operator of the fuzzy implication Mamdani (minimum). The authors have also carried out a numerical simulation of the fuzzy control system by the methods of the center of gravity and the difference in areas. The paper describes how the 3-axis CNC milling machine with autonomous cooling system of the cutting tool is connected. A fuzzy-logical system of controlling the parameters of the machine and the cooling system are used for intellectualizing the processes."}, {"label": 1, "content": "This article discusses the utilization of machine learning for processing vehicle data in a stream. With the emergence of modern vehicles, a substantial amount of sensor data is generated which is usually temporary and thrown away. The aim of this toolchain is to enable the data to be historicized and evaluated in near real time. Stream machine learning is utilized for processing the data, and the requirements for the toolchain include platform independence and free provision of tools. The outcome is an innovative and comprehensive toolchain which caters to all requirements, from reading data from the vehicle to using stream machine learning and evaluating the data. A use case of the toolchain is illustrated, and possibilities for extending the toolchain are highlighted."}, {"label": 0, "content": "The acoustic emission phenomenon arises due to local changes of dynamic structure of solid body. Main sources of emission are micro and macro cracks, friction and shifts. Active and passive acoustic emission methods are widely used to study the strength of materials, the landscape stability and various stages of seismic process. Geoacoustic emission is an acoustic emission of the sound range and it describes the interaction of micro and macro dislocations. Anomalies of geoacoustic signals may be earthquake precursors and they are of great interest to researchers. A typical geoacoustic signal consists of sequence of specific shape short pulses. Authors propose an additive model of geoacoustic signal. According to the model the signal decomposes into a sum of components described by the modulated Berlage and Gauss functions. The use of the matching pursuit method was offered to determine model coefficients. Unfortunately, this method has cubic computational complexity depending on number of functions on which the signal is decomposed. This article is devoted to ways to improve accuracy of the model offered by authors. Various numerical schemes allowed to increase the adaptive property of matching pursuit method with respect to geoacoustic emission signals are considered and compared."}, {"label": 0, "content": "We created a mathematical model of a sodium high-pressure lamp. This model is used in production before sending lamps to the consumer. To develop the model, we used a mathematical model. An analytical method was used to describe the operation of a sodium lamp based on differential equations. We also used the singular value decomposition algorithm to find the coefficients of the ARMA model. Also, the transfer function of the ARMA model was obtained. Then we tested the models to control the quality of sodium lamps in production. The obtained results of the simulation coincide with the experimental results. A graphical dependence is obtained in the case when the standard deviation is 1. Using a series of tests based on the singular value decomposition method, we confirmed the adequacy of elaborated model by Kolmogorov-Smirnov criterion."}, {"label": 0, "content": "Power system static stability situation assessment is the core of power system security prevention and control. Most of traditional static stability assessment methods are focused on physical models and high-intensity simulations. These methods have a large amount of calculation and high dimensionality, and its online engineering applicability cannot be guaranteed. In order to better ensure the safe operation of power system, this paper proposes a static stability assessment method based on scale-invariant feature transformation(SIFT). This method directly extracts the association of holographic data under various static operating conditions of power system. Based on the operating feature it is improved for the generalized elasticity index and it is achieved for an accurate assessment of static stability situation. The simulation result of the New England 10-machine 39-bus system indicates that the improved grid generalized elasticity index has a higher slope and better engineering application value."}, {"label": 0, "content": "Recently, accurate target tracking is widely used in the field of Unmanned Aerial Vehicles (UAV). In this paper, we focus on the application of detecting and following a walking pedestrian in real time from the moving platform with many interferences. We present a scheme that uses CNN model (YOLO-V2) to detect pedestrian and matches the walking pedestrian with a postprocessing and feature queue and Locality constrained Linear Coding algorithm. After that the ground station receives and analyses the video stream from the parrot and sends back commands to control the motion of UAV. At the beginning of the tracking process, the UAV is hovering when one pedestrian will be selected as the special target. Visual information is acquired only through a front camera without assistant sensors. A parrot Bebop 2 is adopted in the experiment, which is the basis for doing experiments outdoors and experimental result verify the effectiveness of our solution."}, {"label": 0, "content": "Aiming at the shortcomings of traditional speaker segmentation and clustering methods, this paper proposes a multilevel speaker re-segmentation and re-clustering algorithm based on GMM-UBM. The algorithm is based on the method of statistical modeling in the field of speaker recognition, and makes full use of the speaker information after segmenting and clustering in traditional methods to re-segment and re-cluster speech files, which improves the performance of the system effectively."}, {"label": 1, "content": "Non-contact assessment of breathing is a valuable tool for health screening, especially in situations where it is not practical or advisable to use devices that attach to the patient, such as with infants, neonates, or patients with infectious diseases. In order to address this need, a mobile phone application has been developed that uses a thermal camera to measure respiration rate and respiration rate variability. This low-cost application has particular potential for global health applications where alternative instrumentation is not affordable or available, as well as for use in homes by consumers in wealthier areas.\n\nThe mobile application uses machine vision algorithms to detect a human face and then locates the nostril points below the nose for temperature measurement. Temperature fluctuations within the nostrils are used to estimate air flow rates and calculate timing parameters. Eleven human subjects were tested in a clinic and validated against a gold-standard impedance pneumography belt using three intensities of breathing (normal, shallow, and deep) and two different camera positions (0 and -20 degrees). The results from the mobile phone tool closely matched those of the clinical device, with a mean standard error ranging from 0.8 to 1.8 bpm for the different breathing intensities, indicating high accuracy.\n\nOverall, the non-contact assessment of breathing using a mobile phone application with a thermal camera is a promising method for health screening, especially in situations where alternative instrumentation is not available or affordable. The low cost and ease of use make it a valuable tool for global health applications and at-home use by consumers in wealthier areas."}, {"label": 1, "content": "In this paper, a new model for calculating total transfer capability (TTC) has been proposed, which is based on stacked denoising autoencoder (SDAE) and takes into account static security, static voltage stability, and transient stability constraints. The model consists of three main components: feature pre-screening, SDAE, and the regression layer.\n\nTo improve the training efficiency of SDAE, the fast correlation-based filter (FCBF) is used to eliminate irrelevant and redundant features. SDAE then extracts high-order features from the original features using the deep structure, which are relevant to TTC. Finally, the regression layer creates a mapping between the high-order features and the TTC value.\n\nExperimental results on a real power system show that the proposed TTC calculation model has higher computational accuracy than shallow machine learning models. Furthermore, the feature pre-screening technique has significantly decreased the training time of the TTC calculation model.\n\nOverall, the proposed model shows promising results for accurately calculating TTC, which is crucial for ensuring the reliable and secure operation of power systems."}, {"label": 1, "content": "The classification of gear safety reliability has long been a complex issue in the transmission industry due to the difficulty in calculating coupled parameters with insufficient data, resulting in great classification errors.\n\nTo address this problem, this paper proposes a model that uses a generative adversarial network (GAN) as a pre-treatment to improve the accuracy of reliability classification. The bounded-GAN model generates gear data within required boundaries without the need for massive computations. This model includes three bounded layers designed to bound generated data based on different data characteristics, smooth targets to enhance the generator's ability to create high-quality instances, and utilizes the Adam optimizer to train both the generator and discriminator while avoiding nonconvergence.\n\nTo overcome the unlabelling defect of bounded-GAN, a mean-covariance labeling scheme is introduced to label the data according to the nearest classes of gear reliability within specific ranges. The model combines the original and qualified data to train classifiers.\n\nThe simulation of gear data from industry shows that the proposed model outperforms other methods on operational metrics. The paper concludes by stating that the bounded-GAN model holds great potential for improving the reliability classification of gear safety in the transmission industry."}, {"label": 1, "content": "In this paper, a refined phase estimation based parallel carrier recovery algorithm for high speed wireless communication systems is presented. The proposed algorithm utilizes a parallelization of a serial DPLL carrier recovery feedback architecture in combination with a novel refined phase estimation module. The goal of this approach is to achieve high speed communication while maintaining high accuracy.\n\nTo validate the proposed algorithm, a 32 parallel baseband simulation model of 16QAM modulation was performed on the MATLAB platform. Simulation results show that the proposed algorithm introduces a performance loss of less than 0.3% in EVM (Error Vector Magnitude), which is only half of the traditional coarse compensation algorithm. This demonstrates the effectiveness of the refined phase estimation based parallel carrier recovery algorithm in achieving high speed communication with high accuracy in wireless communication systems.\n\nOverall, the proposed algorithm represents a significant advancement in the field of carrier recovery for high speed wireless communication systems. Its ability to achieve high accuracy while maintaining high speed communication makes it a promising candidate for future applications."}, {"label": 0, "content": "With the development of renewable energy technology, it is possible that base stations (BSs) are powered by renewable energy. In this paper, we study the problem of minimizing the on-grid energy consumption in heterogeneous cellular networks with hybrid energy supplies, and propose an energy-aware user association algorithm. The key idea of our proposed algorithm is to adaptively set the biasing factor for each small BS (SBS) according to the renewable energy storage and make more utilization of the renewable energy for minimizing the on-grid energy consumption. The proposed algorithm is compared with the max-RSRP algorithm and the traditional cell range expansion (TCRE) algorithm. The simulation results show that the proposed algorithm can effectively reduce the consumption of on-grid energy and improve the utilization of renewable energy in the network."}, {"label": 0, "content": "Being used for synchronizing and triggering environmental and military Internet of Things (IoT) wireless networks, an ultra low power wake-up system based on frequency analysis is presented in this paper. With an average power consumption of 34\u03bcW, this wake-up detector is able to detect signals in different frequency bands, generating a separate output interrupt for each of them. Adding an additional frequency band detector only costs an additional 500nA. Applications to data retrieval in a military smart dust using a drone on a battlefield, and for activation and synchronization of a environmental wireless sensor network are presented. This later uses a multi-frequency light pulses burst propagation algorithm for triggering a whole wireless sensor network in harsh conditions such as a dense rain forest in 50ms, and for synchronizing it with a timing precision of less than 20\u03bcs in a large network."}, {"label": 1, "content": "The importance of technology in driving sustainable development has been recognized for quite some time now. Technology helps to increase productivity and contributes to the overall progress of society. Unfortunately, many parts of Africa still lack the necessary financial and human resources to adopt modern technologies. In 2015, the United Nations General Assembly adopted the 2030 Development Agenda, which consisted of 17 sustainable development goals (SDGs). Creating a knowledge-based economy is crucial for achieving these goals, as it helps to reduce inequality and improve people's quality of life in developing countries.\n\nEducation is a crucial factor in promoting effective and sustainable development, and one way to achieve this is by strengthening academic institutions in developing countries. This requires a flexible framework that can respond to local needs, and we've embarked on an international effort to implement such an approach. Our goal is to achieve the SDGs related to good health and well-being, quality education, and decent work and economic growth. We're currently working on a project called INTERREG MACBioIDi, which aims to create a learning community in the fields of medicine and engineering.\n\nThis project involves participants from different countries, speaking different languages and coming from various cultures with social and economic differences. African participants come from Cape Verde, Mauritania, Senegal, and Mozambique, while technology partners come from Canary Islands, Azores, and Madeira. The project's main technological platform is 3D Slicer, an extensible and free software for the visualization and processing of biomedical images. We're providing training and mentoring to African physicians, biomedical engineers, and researchers in the use of 3D Slicer.\n\nThe training program faces several challenges, such as internet connectivity issues, maintenance of medical equipment, and linguistic and social differences. Our training sessions are based on a blended learning program with face-to-face lessons, coaching, collaboration, multimedia, web-based learning, and support resources. We've established a training center at the University of Las Palmas de Gran Canaria to support these activities.\n\nOur first results from the program have been promising, but we're still facing challenges that require us to readjust our action plan in the coming months. Overall, we believe that by leveraging technology and education, we can achieve sustainable development and improve the quality of life in developing countries."}, {"label": 1, "content": "The reliability and lifetime of photovoltaic (PV) modules are crucial in determining the levelized cost of energy (LOCE) and effectiveness of PV systems. Despite recent attention given to this theme by researchers, there remains a lack of an effective method to model the power degradation of PV modules. To address this, this paper proposes to use a gamma process to establish the relationship between the power degradation of PV modules and temperature and relative humidity (RH). It then predicts the service lifetime of PV modules under accelerated damp-heat conditions.\n\nTo begin, accelerated damp-heat tests are carried out at three different temperature and RH levels. A data transformed method based on the Peck model is proposed to obtain more power degradation data under seven other damp-heat conditions. Next, a gamma process with an exponential transformation is applied to model the power degradation of PV modules under accelerated damp-heat conditions. The relationship between power degradation and temperature and RH is established through theoretical derivation and validated through experimental data. An Expectation Maximum (EM) algorithm is developed to estimate the model's parameters. Finally, the lifetime of PV modules under different damp-heat conditions is predicted.\n\nThe results indicate that PV modules have a lifetime of approximately 20 to 25 years under conditions of 50\u00b0/45% RH. However, it is also found that the lifetime of PV modules sharply decreases with the increase of temperature and RH. Therefore, more factors or other test types should be considered in later accelerated tests."}, {"label": 0, "content": "Working on product lifetime data is of significant importance for evaluating safety and reliability, predicting remaining useful life and formulating maintenance strategy or replacement policy. In practical applications, observed datasets often consist of failure data and randomly censored data, which are referred as general censored data. Meanwhile, inverse Gaussian (IG) distribution has been widely adopted to depict lifetime data because it not only can possess flexible expression formats but also can explain the mechanism of first hitting time from a soft failure viewpoint. Motivated by these two regards, this paper develops a novel method on best linear unbiased estimations (BLUEs) for general censored data. A three-parameter IG distribution type is adopted. A novel method is established to optimize the skewness parameter. Then, BLUEs of mean and standard deviation can be obtained. The proposed method can construct closed-form parameter estimations in linear functions of order statistics. The computation process has been further simplified for flexible applications. The frequently utilized maximum likelihood estimation method is also introduced as a reference for a better understanding. Comparative results of both comprehensive simulation study and empirical application illustrate that the proposed method can significantly enhance the estimation accuracy and keep a stable performance, because more life information can be extracted and adopted from the censored datasets."}, {"label": 0, "content": "The increasing complexity of parts and the growing quality requirements pose new challenges for today's manufacturing industry. Multi-Stage Production Systems, which are known for complex links and sequences of many different process steps, must be adapted to these requirements. This means, being cost-effective and flexible while still meeting high quality standards. The idea of Zero-Defect Manufacturing aims to reduce scrap, rework and special operations by analyzing and optimizing multi-stage production systems through data-driven and learning-based approaches. A Knowledge Capturing Platform concept is introduced to extract a deeper understanding from collected data with inter-stage correlation methods, part variation approaches along the line and intelligent monitoring systems."}, {"label": 0, "content": "The agriculture sector is important to the economic growth of agriculture-inclined countries like South Africa. Issues regarding agriculture may not only directly affect consumers, but cause the increase in food price. This may further affect the price control policy on food. This paper discusses soil manuring as one of the problems faced in optimum food productivity. Since leaves droppings could be decomposed to fertilize the soil, the need to understand the factors that may affect the process or outcome is important, hence our project. We designed a compact monitoring circuit using the Redboard and GSM/GPRS module. Three TCS3200 color sensors were used to detect leaves droppings, giving a large coverage area. The colors detected were categorized to numbers, for possible data analysis. Other sensors were used to collect parameters for possible factors that may affect decomposition. We measured soil moisture, soil temperature, ambient temperature, relative humidity, and dew point. Our Heroku-deployed developed cloud platform was synced to the monitoring circuit for remote monitoring. Our whole project embraced a continuous real-time monitoring from the site to the cloud platform."}, {"label": 0, "content": "The aspects of developing three-dimensional displacement control system with application of Case-based reasoning technology are offered in the paper. Fuzzy set theory is implemented for increasing accuracy of the system while determining three-dimensional location of the studied object. Fuzzy logic mechanism is used for processing signals, a linguistic variable is entered for fuzzificating input data, a term-set of seven values is formed, a mechanism of searching membership function is described and a rule base for defuzzificating output data is formed as well."}, {"label": 0, "content": "High computational requirements are commonly associated with the hydraulic simulation of large-scale water distribution. The convergence of the cumbersome iterative procedures involved has been a well-debated issue for the past decades. The large-scale and non-linear properties pose a great hindrance towards the development of online applications for water distribution network (WDN) analysis and pressure control thereof. Consequently, there has been a great interest in the deployment of model-free techniques to mimic the rather computationally expensive non-linear hydraulic simulations. As the hydraulic simulation based research is still being conducted, the advantages of model-free techniques make them more suitable alternatives. Artificial neural networks (ANN) is one of the most successful model-free methods for WDN analysis and management. In this paper, a literature synopsis of existing applications of model-free approaches in water distribution is presented. The technical advantages of applying such technique in a large-scale non-linear network are brought up in this paper."}, {"label": 1, "content": "In today's digital world, nearly every aspect of our lives is influenced by the proliferation of technology. From our smartphones to our cars, every device we use relies on computer chips and software to function. These technologies are interconnected via the Internet of Things (IoT) and make up what are known as cyber-physical systems. However, with this increased connectivity also comes increased risks, particularly via the IPv6 over Low Power Wireless Personal Area Networks (6LowPAN) infrastructure. Data breaches and manipulation continue to be a significant concern. This paper aims to address the development of Intrusion Detection Schemes for the IoT, examining recent hacker methodologies and tools to identify the most effective ways to protect IoT networks. After discussing the problems posed by these risks, the paper will explore existing intrusion detection schemes and their limitations before proposing a more effective solution."}, {"label": 1, "content": "Analyzing the sources of performance anomalies in cloud-based applications accurately is a challenging task due to both the multi-tenant nature of cloud deployment and changing application workloads. To aid in effective deployment and resource management decision-making, several different resource instrumentation and application performance modeling frameworks have been developed in recent years. However, the significant differences among these frameworks in terms of their APIs, ability to instrument resources, and interpretation of the collected information make it difficult to use these frameworks effectively. If not addressed, these complexities can result in incompatible and incorrect configurations that lead to inaccurate diagnosis of performance issues and incorrect resource management. \n\nIn order to tackle these challenges, UPSARA is introduced as a model-driven generative framework that provides a lightweight, scalable, and extensible performance monitoring, analysis, and testing framework for cloud-hosted applications. UPSARA helps reduce the accidental complexities in configuring the right resource monitoring and performance testing strategies for the underlying instrumentation frameworks used. \n\nThe effectiveness of UPSARA is evaluated in the context of representative use cases, highlighting its features and benefits."}, {"label": 1, "content": "Ad-hoc Mobile Cloud (AMC) was developed as a solution to the connectivity challenges faced in Mobile Cloud computing (MCC). In AMC, mobile devices form groups to share their resources, including Web Services, Storage, and Computing resources. However, potential participants in AMC are concerned about losing control over their personal and confidential data during resource sharing. To address this issue, a trust management system is needed to safeguard the interests of the participating mobile devices. \n\nThis study proposes a Multi-Criteria Trust Management system (MCTM) architecture for AMC that identifies trustworthy mobile devices and alerts participants of any malicious acts. An evaluation procedure was conducted, which showed that the proposed architecture achieved a high level of precision compared to other existing research. \n\nThe MCTM system provides mobile users with the confidence to participate in the AMC system, knowing that their personal and confidential data is protected. By implementing this trust management system, AMC can increase its appeal to users who wish to share resources through mobile devices."}, {"label": 1, "content": "The main objective of this paper is to develop an embedded platform that uses the TMS320VC5509A processor for snore recognition and controlling pillow height to alleviate associated symptoms. This entire embedded hardware platform collects sound through the microphone module while also storing, processing and interacting with data through other peripherals. After several pre-processing operations, a dual threshold detection algorithm utilizing short-time energy and short-time zero crossing rate is employed for endpoint recognition. Additionally, MFCC is employed for feature extraction while KNN algorithm is selected for recognition. Experimental results demonstrate that the system operates effectively and the design is robust. Furthermore, the platform achieves high accuracy in snore analysis and recognition."}, {"label": 1, "content": "Lip reading is a demanding task that requires recognizing text information through the movement of a speaker's mouth. The current end-to-end model, which maps a sequence of video frames to text, faces challenges in real-life situations where lips unintentionally move without speaking. This article aims to enhance the performance of lip reading in practical settings. The proposed model comprises two networks - a visual to audio feature network and an audio feature to text network. The model's accuracy was tested on a dataset that included unintentional lips movement, and the results showed a 92.76% accuracy in lip reading tasks."}, {"label": 1, "content": "With the increasing prevalence of cyber-attacks, the risks of information leakage, falsification and forgery, and bypass control are heightened, and attackers can now attack primary stations with ease, increasing the range of security threats. To address this, quantum cryptography, which offers a higher level of security than traditional cryptography, is proposed in this paper as a potential solution. Specifically, a quantum cryptosystem that is suitable for power communication access networks is introduced, along with various practical quantum communication devices. Also presented are a key reading mode between the service terminal and the quantum key mobile storage device, an interface protocol between the service terminal device and the quantum key mobile storage device, and a key management method that is applicable to the terminal for the service of electric system. The application of quantum communication technology in power systems is essential for ensuring the safe, stable and efficient operation of the power grid."}, {"label": 1, "content": "The demand for food has been escalating with the burgeoning world population. However, cultivable land is diminishing rapidly due to the pace of urbanization, necessitating a need to increase farm-yields to better satisfy food security requirements. The rapid advancements of Internet of Things (IoT) have made it possible to implement comprehensive monitoring of various aspects of farming to enable a more efficient use of resources while achieving enhanced performance. To achieve a more detailed view of farming, we have established an IoT-based precision farming system that incorporates a cluster of devices to measure over 14 ambient parameters below the soil, at the crop level, and the ambient environment. This system has been integrated with our digital farming platform that enables us to collect sensor data from various sources and provide contextual advice to the farm supervisor in order to carry out specific actions on the field. This setup has been utilized to monitor two horticultural crops, cabbage and capsicum, in the Rabi (Winter) season of 2017. Our experience with the deployment has shown a 20% reduction in agri-input cost and an increase in yields of more than 10%."}, {"label": 1, "content": "With the increasing application of Internet of Things (IoT) technology in the medical field, the collection of medical data has grown rapidly. However, labeling these data requires significant costs and specialized domain knowledge. Thus, the demand for building an efficient and high-quality clinical decision support model with small amounts of labeled medical data has become a pressing research topic. To address this issue, we propose a novel semi-supervised learning approach combined with generative adversarial networks (GANs) to support clinical decision making in the IoT-based health service system. Our approach utilizes GANs to increase the number of labeled data and to compensate for imbalanced labeled classes through the generation of additional artificial data, thereby improving the semi-supervised learning performance. Our extensive evaluations on a variety of benchmarks and real-world medical datasets demonstrate that our proposed technique outperforms existing methods, offering a potential solution for practical applications."}, {"label": 1, "content": "Security is a highly significant concern when it comes to Vehicular Ad-Hoc Networks (VANETs), especially with regards to preventing misbehaving vehicles from posing a threat to the safety of others. In this paper, we provide an overview of revoking misbehaving vehicles based on the conventional Certificate Revocation List (CRL) in the IEEE Standard. However, these algorithms have a major disadvantage \u2013 the Certification Authority (CA) is overwhelmed by the responsibility of distributing the entire CRL to all the requesting vehicles. To address this drawback in the European Telecommunication Standards Institute (ETSI) standard, we propose a solution that aims to mitigate the burden on the CA. This involves breaking down the CRL into different chunks and having them distributed separately by the various RSUs within the same zone."}, {"label": 1, "content": "In a LTE-based cellular network, the mobility management entity (MME) plays a crucial role in facilitating non-data signaling between the user equipment of multiple base stations within a specific geographic area and the core network. Therefore, improving the performance of an LTE-based cellular network largely depends on optimizing the MME residence time (MRT). Although several mobility and network scenarios have been studied to determine the impact of various factors on cell residence time, the MRT has not been modeled effectively. \n\nTo address this gap, this paper aims to model the MRT for diverse mobility and network scenarios using various probability distributions. The statistical performance of these distributions in modeling MRT has been analyzed and evaluated. Through extensive simulations, it has been found that the Lognormal and Generalized Pareto distributions are the most suitable for modeling the MRT in specific network and mobility scenarios."}, {"label": 1, "content": "Machinery condition monitoring has ushered in the era of big data, with some research conducted based on this technology. However, abnormal segments such as missing segments, drift segments, and other segments acquired from harsh industrial environments are inevitable due to temporary sensor failures, network transmission delays, or accidental data loss. These independent abnormal segments not only reduce the quality of the data for condition monitoring and big data analysis, but also add a heavy computation load. Unfortunately, there are few reports that address abnormal segment detection, which is necessary for further data cleaning in machinery condition monitoring. To address this challenge, this paper proposes an abnormal segment detection method to improve data quality. \n\nThe proposed method employs a sliding window to separate the data into different segments, followed by extracting 14 kinds of time-domain features from each segment. Principle component analysis (PCA) is applied to extract the principle components from these features. Additionally, local outlier factor (LOF) is calculated based on the principle components to evaluate the degree of the segment being an outlier. Finally, the proposed method is validated through the use of real wind turbine data, including a drift segment. \n\nIn conclusion, the proposed abnormal segment detection method shows great promise as a way to improve data quality in machinery condition monitoring. By addressing independent abnormal segments that reduce the quality of data and increase computation load, this method could play a vital role in further data cleaning in the field of machinery condition monitoring, leading to more accurate results and better decision-making."}, {"label": 0, "content": "We present novel low-level audio features that are based on correlations between sub-band audio signals decomposed by undecimated wavelet transform. Under the assumption that SVM is used for classifier learning, the experimental results on GTZAN dataset showed that the proposed method demonstrated the best accuracy of 81.5%, outperforming the conventional methods."}, {"label": 0, "content": "We present our research on understanding innovation for education in two Rwandan secondary schools. Our innovation for education project focused on developing spatial thinking skills via Geographic Information and Communication Technologies (GeoICT)-based training. Specific GeoICT used focused on 2D, vector-based maps used on Android tablets and commercial desktop Geographic Information Systems (GIS) software. Trainings were conducted in the context of a research program that sought to develop new approaches for Rwandan education innovation. We discuss qualitative results from teacher and student reflections gathered from a Web-based survey about what it was like to be part of the innovation for education process, broader opportunities spatial thinking provides, and innovation for education process feedback. We also conducted extensive group interviews with teachers at the two schools based on data collected from Web surveys. The interviews and surveys allowed us to assess four ways our innovation for education approach impacted teachers and students. First, teachers and students identified broader societal benefits and individual opportunities the innovation for education process is creating. Second, Rwandan teachers identified education and societal benefits for problem solving and reasoning stemming from increased thinking ability, GeoICT training, and space-time thinking ability. Third, teachers found new roles and identities for themselves through incorporation of spatial thinking-oriented curriculum and GeoICT training. Fourth, the importance of certificates and recognition artifacts as tools for students and teachers to establish their new competencies. Our focus on innovation for education, spatial thinking and GeoICT inform the literature onbroader technology-enhanced quality education delivery research on the value of spatial thinking and GeoICTs."}, {"label": 0, "content": "This paper presents a Mixed Integer Linear Programming (MILP) model for optimal allocation of Automatic Switching Devices (ASDs) and Distributed Generation (DG) in distribution networks. The model's formulation considers the application of ASDs for protection and post-fault restoration purposes, as well as the role of DG units in the restoration process. From the reliability perspective, System Average Interruption Frequency Index (SAIFI) is used as a metric index for obtaining an optimal placement of ASDs and DG units. Decoupling the optimal allocation problems of ASDs and DG units does not provide an optimal solution, since both are inherently interdependent with system reliability. Hence, the simultaneous allocation of ASDs and DG units is considered in this work. A Goal Programming approach is used to establish the optimal trade-off between the placement of ASDs and DG units, and the improvement on SAIFI. In other words, the developed model aims to i) minimize the number of interrupted consumers due to protection operation; ii) maximize the number of consumers restored by automatic network reconfiguration, and iii) minimize costs associated with the allocation of ASDs and DG units. Restoration is ensured by a set of linear power flow equations. The model's solutions are obtained from a Branch and Bound-based technique, and results are presented using the IEEE 123-bus system."}, {"label": 0, "content": "An FFT-based algorithm is presented for rapidly post-processing the integral-equation based solution of scattering problems to evaluate the fields at an arbitrary number of nearby points. The proposed algorithm uses a similar approach to the adaptive integral method (AIM) but contends with the fact that the fields are not Galerkin tested with basis functions but instead point tested. It reduces the computational costs compared to the brute-force method, especially when the number of observer points is large."}, {"label": 0, "content": "This article describes the approach to implement the fuzzy interconnected control system for sheet metal forming electric drives. The displacements of forming rods of sheet metal forming electric drives system depend on various factors and some of which can only be described by qualitative characteristics (indexes). The possible way to achieve desired output parameters of all local drives systems is using compensation method. However, it is difficult to achieve desired outputs by using conventional compensation methods because of having interrelation error (effect) between local forming electric drives. In this article, suggested compensating interrelation effect between local forming electric drives by using fuzzy compensation method. The fuzzy compensation method based on theory of differential inclusion. Mathematical equations of this fuzzy compensation approach are described. Algorithm for implementation of fuzzy controller to achieve the specified control accuracy is presented."}, {"label": 1, "content": "A model of neurons for biometric authentication that efficiently processes highly dependent features based on agreement criteria (Gini, Cramer-von-Mises, Kolmogorov-Smirnov, maximum intersection areas of probability densities) is presented in this study. To compare its efficiency with neurons based on different and hyperbolic Bayesian functionals that are capable of processing highly dependent biometric data, an experiment was conducted. In addition, various variants of hybrid neural networks capable of being trained on a small number of examples of biometric patterns (about 20) were proposed.\n\nTo collect dynamic biometric patterns, ninety individuals entered handwritten and voice patterns over the course of a month in the experiment. The intermediate results of recognition of subjects based on hybrid neural networks were positive. The number of errors in verifying a signature (handwritten password) was below 2%, and verification of a speaker by a fixed passphrase was under 6%. Testing was performed on biometric samples obtained at a future time after the formation of the training sample."}, {"label": 1, "content": "The purpose of this research is to create an innovative, low-cost platform for smart home control and energy monitoring that utilizes augmented reality. The goal is to educate people about energy usage, particularly as fuel costs continue to rise, and provide new methods of interaction for those with disabilities. To achieve this, we have developed an interactive system that employs augmented reality to display real-time energy usage of electrical components. This system enables users to view their energy consumption in real-time and interact with devices via augmented reality. The energy data is collected and stored in a database for energy monitoring purposes. Through this combination of complex smart home applications and transparent user interface, we believe that energy consumption awareness will increase."}, {"label": 1, "content": "This paper presents a review of registering a binary element in a discrete channel with erasure using a nonlinear scale, which is constructed based on the concept of fuzzy sets theory and a fuzzy membership function. Traditional methods of recording a binary element lead to information loss, which is discussed in detail. However, this paper also proposes a mechanism for compensating for these losses using the nonlinear scale. Overall, this approach offers a promising solution for dealing with information losses in binary element registration, and has the potential to significantly improve the accuracy and reliability of such systems."}, {"label": 0, "content": "The traditional mixed-criticality (MC) model does not allow less critical tasks to execute during an event of the error and exception. Recently, the imprecise MC (IMC) model has been proposed where, even for exceptional events, less critical tasks also receive some amount of (degraded) service, e.g., a task overruns its execution demand. In this work, we present our ongoing effort to extend the IMC model to the precise scheduling of tasks and integrate with the dynamic voltage and frequency scaling (DVFS) scheme to enable energy minimization. Precise scheduling of MC systems is highly challenging because of its requirement to simultaneously guarantee the timing correctness of all tasks under both pessimistic and less pessimistic assumptions. We propose an utilization-based schedulability test and sufficient schedulability conditions for such systems under earliest deadline first with virtual deadline (EDF-VD) scheduling policy. For this unified model, we present a quantitative study in the forms of speedup bound and approximation ratio. Finally, both theoretical and experimental analysis will be conducted to prove the correctness of our algorithm and to demonstrate its effectiveness."}, {"label": 0, "content": "Learning to predict human body motion has emerged as a meaningful research in computer vision and artificial intelligence. This paper presents the study on predicting human body motion from video sequences. We propose a human body motion prediction network integrating the recent advanced 2D feature extraction and video sequences prediction. Based on the temporal characteristics extracted from video sequences, our network realizes the prediction of the human motion. We train the network using the video based human pose datasets and demonstrate good performance of our network on 2D human body motion prediction through quantitative and qualitative results. Experimental results prove the feasibility of our method."}, {"label": 0, "content": "Component sizing for renewable generation and battery storage is a key economic driver for small off-grid electrical systems (minigrids). Stochastic, time-series simulation is the go-to tool for sizing studies. For computational efficiency, these simulations typically avoid electrical system models and utilize power-balance methods that capture net energy flows without modeling current and voltage - parameters often utilized for common supervisory control actions. As a result, power-balance models often overestimate performance or fail to simulate critical supervisory control states necessary for control system development. In this work we demonstrate a modeling method that retains computational efficiency while calculating required currents and voltages. Since microgrid components frequently utilize battery voltage as a key control input, the method emphasizes an efficient representation of battery terminal voltage as a function of state-of-charge and battery current. We demonstrate that critical battery parameters can be extracted from simple pulsed discharge tests which could be conducted in field conditions. Application to typical microgrid illustrates that the method will identify control instabilities which would be missed by power-balance methods, while executing at acceptable speeds."}, {"label": 0, "content": "Fine-grained visual categorization aims to distinguish objects in subordinate classes instead of basic class, and is a challenge visual task due to the high correlation between subordinated classes and large intra-class variation (e.g. different object poses). Although, deep convolutional neural network (DCNN) has brought dramatic success on generic object classification, detection and segmentation with the availability of the large-scale training samples, direct application of DCNN on fine-grained visual categorization, where only decades or at most hundreds of training samples for each subordinate class are available in most public finegrained image datasets, cannot lead to satisfactory classification results due to small number of training samples. This study explores the transfer learning strategy for finegrained dog breed categorization based on the learned CNN models with the large-scale image dataset: ImageNet, and prove promising performance with two DCNN models: AlexNet and VGG-16. Furthermore, we argue that different DCNN architecture may extract the representation of different image aspects due to the previously defined CNN kernel sizes, number and various operations in the model learning procedure, and thus result in different performance for visual categorization. This study proposes to fusion multiple CNN architectures for combining different aspect representations to give more accurate performance. We compressively study the fusion of different layers such as Fc6 and Fc7 in AlexNet and VGG-16, and manifest 2.88% improvement of the fusion architecture over the best performance of the only one DCNN model: VGG-16 from 81.2% to 84.08%."}, {"label": 0, "content": "Insulators are important components of power transmission and transformation equipment in power systems. Insulator identification is a basis work for evaluation of insulation status of insulators in computer vision. With the recent development of big data and cloud computing technologies, terminal-to-terminal picture recognition was accomplished based on deep learning algorithms, making it possible to apply insulator image recognition in power systems. This paper firstly introduced the research background of deep learning: the recent YOLO (You Only Look Once) convolutional neural network algorithm, established insulator image databases for train and test, and preprocessed the images of training insulator images with the TensorFlow platform. With the YOLO algorithm applied, the training of the image database for 5 days was completed, and a good recognition result was achieved. Then the results were compared between the Fast R-CNN algorithm and the YOLO algorithm in identify speed and accuracy. Based on relevant paper, the accuracy of insulator image recognition is defined, and the factors that affect the accuracy of insulator image recognition are discussed: It is concluded that the accuracy of identification increases with the increase in the number of training insulators, which is the next step in the identification of insulators."}, {"label": 1, "content": "In this paper, we revisit two SAT-based algorithms for checking liveness properties of finite-state transition systems: the k-LIVENESS algorithm introduced in [1], and the FAIR algorithm presented in [2]. Both of these methods have unique strengths; the k-LIVENESS algorithm works by translating the liveness property along with fairness constraints into the F Gq form, then limiting the number of times the variable q can evaluate to false. On the other hand, the FAIR algorithm finds an over-approximation R of reachable states so that no state in R is contained on a fair cycle.\n\nTo leverage the strengths of both algorithms, we introduce a new approach called k-FAIR. The k-FAIR algorithm builds upon the k-LIVENESS and FAIR algorithms synergistically, and provides a stronger approach than running both algorithms in parallel. Our experiments demonstrate the effectiveness of the k-FAIR algorithm.\n\nOverall, this paper presents a new approach to checking liveness properties in finite-state transition systems, which combines the strengths of two previously developed algorithms. The resulting approach, k-FAIR, has been shown to be more effective than running k-LIVENESS and FAIR in parallel."}, {"label": 0, "content": "Motivated by the remarkable performance achieved using deep learning strategies in solving action recognition tasks, an effective, yet simple method is proposed for encoding the spatiotemporal information of skeleton sequences into color texture images, referred to as Skeletal Optical Flows (SOFs). SOFs collectively represent the kinetic energy, predefined angles and pair-wise displacements between joints over consecutive frames of skeleton data, as color variations to capture meaningful temporal information and make them highly interpretable. A novel Convolutional Neural Network with Correctness-Vigilant Regularizer (CVR-CNN) is then employed to exploit the discriminative features of SOFs for human action recognition. Empirical results show that the efficiency of the proposed method is superior in terms of the generalizability of the generated model, the training convergence speed, and the resulting classification accuracy on commonly used action recognition datasets, such as MHAD, HDM05 and NTU RGB+D."}, {"label": 1, "content": "Traffic congestion and road obstructions are major issues in modern cities, leading to an increase in traffic accidents. As a result, traffic flow management is crucial to avoid these issues, reduce time wastage, and prevent tragic accidents. Optimizing the timing of traffic signals is one solution to this problem. This paper proposes a low-cost camera-based algorithm to manage traffic flow on roads. The algorithm involves three main steps: vehicle detection, counting, and tracking. Background subtraction is used to isolate vehicles from their surroundings, while Kalman filtering tracks the motion of the vehicles. To associate labels with the tracked vehicles, the Hungarian algorithm is employed. This algorithm is applied to both day and night videos captured by CCTV and IR cameras. The experimental results demonstrate the effectiveness of the proposed algorithm."}, {"label": 0, "content": "Virtualization is one of the key enabler technologies of cloud computing in providing on-demand sharing of computing resources. Virtualization requires mechanisms and algorithms for virtual resource allocation, virtual machine deployment, migration, and servers consolidation. Most of the existing studies have only focused on how to solve the problem of virtual resource allocation among servers. However, as cloud servers with multi-core architectures become popular, the virtual machine resource allocation in a single server becomes a critical challenge. In this paper, we propose a multi-objective virtual machine placement algorithm by jointly considering energy efficiency and load balancing criteria in a multi-core server with the Network-on-Chip architecture. Our proposed algorithm is based on Markov approximation optimization theory. We perform extensive experiments to evaluate our proposed algorithm. The results show that our proposed algorithm achieves higher energy efficiency, load balancing, and calculation speed compared with the state-of-the-art algorithms."}, {"label": 1, "content": "With the increasing utilization of distributed energy sources, it is essential to establish a new load model that is more accurate. This paper focuses on the modeling and simulating of photovoltaic power generation systems and battery energy storage modules using MATLAB/Simulink. A detailed model of the photovoltaic system is first built, incorporating the Boost circuit and MPPT control on the DC side. For grid inverter control, a double closed loop control method involving current internal loop and external voltage loop is deployed, with consideration for the LVRT capability. The model is then equaled using the precise initial models. The battery energy storage module is given a general mathematical model based on the charging and discharging principle, with charge and discharge being controlled through the DC/DC converter, while connection to the power grid after inverting and filtering is achieved. Common PQ control is selected in the inverter control mode, and the energy storage model equaled along the same principle. Finally, with the addition of the above load model to the traditional one, a new generalized integrated load model is established. This is further replaced by the actual model to establish an equivalent model, with a comparison made to ascertain the equivalence of the description ability."}, {"label": 0, "content": "Texture analysis is used in a very broad range of fields and applications, from texture classification (e.g., for remote sensing) to segmentation (e.g., in biomedical imaging), passing through image synthesis or pattern recognition (e.g., for image inpainting). For each of these image processing procedures, first, it is necessary to extract\u2014from raw images\u2014meaningful features that describe the texture properties. Various feature extraction methods have been proposed in the last decades. Each of them has its advantages and limitations: performances of some of them are not modified by translation, rotation, affine, and perspective transform; others have a low computational complexity; others, again, are easy to implement; and so on. This paper provides a comprehensive survey of the texture feature extraction methods. The latter are categorized into seven classes: statistical approaches, structural approaches, transform-based approaches, model-based approaches, graph-based approaches, learning-based approaches, and entropy-based approaches. For each method in these seven classes, we present the concept, the advantages, and the drawbacks and give examples of application. This survey allows us to identify two classes of methods that, particularly, deserve attention in the future, as their performances seem interesting, but their thorough study is not performed yet."}, {"label": 0, "content": "Kinship verification from faces is a challenging task that is attracting an increasing attention in the recent years. The proposed methods so far are not robust enough to predict the kin between persons via facial appearance only. The initial studies using deep convolutional neural networks (CNN) have not shown their full potential as well, mainly due to limited training data. To mitigate this problem, we propose a new approach to kinship verification based on color features and extreme learning machines (ELM). While ELM aims to deal with small size training sets, color features are proven to provide significant enhancement over gray-scale counterparts. We evaluate our proposed method on three benchmark and publicly available kinship databases, namely KinFaceW-I, KinFaceW-II and TSKinFace. The obtained results compares favorably against some state-of-the-art methods including those based on deep learning."}, {"label": 1, "content": "The use of Cloudlet technology for Cloud services offers various advantages in terms of Quality of Experience (QoE) for consumers, including free of cost, low-latency, and one-hop WiFi network consumption. These benefits are largely facilitated by a well-defined business model that emphasizes value creation. Value creation is a key aspect of any business model, as it helps identify customer segments, value propositions, and mechanisms for delivering value. However, poorly defined value logic can lead to business failure if customer needs are not effectively addressed. While previous research has focused on evaluating Cloud business models and their value creation potential, there has been little attention to the creation or deployment of a Cloudlet business model. To address this gap, this study proposes a Cloudlet Business Model (CBM) that can be easily implemented by small and medium-sized enterprises (SMEs), such as coffee shops and shopping malls, to meet their customers' needs and maintain their competitiveness. To articulate the value perspective of a CBM, the study employs a Four Box model, which identifies small data as the key value proposition and explores how it can be generated, delivered, and compensated for in a cost-effective manner. Through this approach, the study aims to enable SMEs to leverage Cloudlet technology to enhance their QoE and sustain their businesses."}, {"label": 0, "content": "Recommendation systems for mobile phones are of great importance for mobile operators to achieve their desired profit targets. In a client inferred market, the number of contract users and contract phones is especially significant for mobile service operators. The tremendous growth in the number of available mobile cellular telephone contracts necessitates the need for a recommender system to assist users discover suitable contracts based on their usage patterns. This study used a hybrid of both collaborative and content-based filtering. A prototype of a mobile recommender system was developed and evaluated using precision and recall. The developed recommender system was able to successfully recommend packages to subscribers. A precision-recall curve was produced, and it showed good performance of the system. This study successfully showed that a hybrid system was able to recommend products to the mobile subscribers."}, {"label": 1, "content": "In this paper, the authors discuss the enhancement of existing work done on the Internet of Things (IoT) with regards to Indian farmers. Specifically, the authors focus on soil testing and the constituents required for crop growth. The main achievement of this paper is the process that should be followed after soil testing. Soil testing provides data and parameters about soil nutrients such as phosphorous, potassium, and nitrogen. After collecting soil data, the data is entered into a developed software application that determines which crop is best suited for that soil. The software application is now equipped with preloaded data about soil, temperature, and weather conditions, which it continuously monitors through sensors. Based on these parameters, the application recommends the most suitable crop for that specific soil. The authors go on to discuss the Application Program Interface (API) systems software that has been developed for this application. This software is supported through a user-friendly mobile app that is freely accessible to researchers in soil testing and IoT for agricultural applications. The authors aim to use this solution to help Indian farmers industrialize their individual farming practices by connecting them directly with the market through a centralized mobile app."}, {"label": 1, "content": "Malware detection has become more challenging with the rise of android malicious programs and current issues with android malicious detection. This research proposes a novel method for detecting android mobile malware using a deep neural network. The system is based on an optimized deep Convolutional Neural Network that learns from opcode sequences extracted from decompiled android files. The optimized network is trained multiple times to effectively learn feature information and enable more accurate detection of malicious programs. The proposed method also incorporates a k-max pooling method that improves detection accuracy. Experimental results demonstrate that the system achieved 99% accuracy, which is significantly higher than the accuracy of machine learning detection algorithms using the same dataset. Additionally, indicators such as Fl-score, Recall, and Precision are maintained above 97%."}, {"label": 0, "content": "Head detection plays an important role in localizing and identifying persons from visual data. Most existing methods treat head detection as a specific form of object detection. Head detection is nontrivial due to the considerable difficulty in building the local and global information under conditions of unconstrained pose and orientation. To address these issues, this paper presents an effective adaptive relational network to capture context information, which is greatly helpful to suppress missed detection. We show that the fundamental contextual properties, such as the global shape priors from different heads and the local adjacent relationship between the head and shoulders, can be systematically quantified by visual operators. Specifically, we propose a two-step search algorithm to quantify the global intergroup conflict with adaptive scale, pose and viewpoint. Meanwhile, a structured feature module is introduced to capture the local relation of intraindividual stability. Finally, the global priors and local relation are integrated seamlessly into a single-stage head detector that is end-to-end trainable. An extensive ablation analysis demonstrates the effectiveness of our approach. We achieve state-of-the-art results on two challenging datasets, i.e., HollywoodHeads and Brainwash."}, {"label": 1, "content": "Urban gun violence is a critical problem that affects public safety agencies worldwide. To address this issue, we have developed the EDNA drone, an aerial robotics solution that equips first responders in high-risk settings with cutting-edge tools for situational awareness and non-lethal conflict resolution.\n\nThe EDNA drone is an unmanned aerial vehicle that incorporates our patent-pending \"Predictive Probable Cause\" technology. Its design is intended to provide automated, real-time analysis to support teams entering dangerous situations where gun violence is a possibility. The EDNA drone utilizes machine learning, biometric sensors, and advanced materials to provide autonomous threat detection and bullet-stopping capabilities. It was designed to be used by groups such as police and sheriff's departments, fire departments, and EMT and emergency rescue crews.\n\nThe EDNA drone's sensors gather data that are fed to machine learning algorithms running on the drone in real-time. The EDNA drone autonomously detects the presence and location of firearms and explosives, even through walls or other obstacles, using a neural network trained on past data. In addition, the armored drone can stop bullets using advanced metal foams and composite materials.\n\nOverall, the EDNA drone is an innovative solution that promotes public safety, peacekeeping, and humanitarian efforts."}, {"label": 0, "content": "Unit commitment (UC) problems are the most fundamental problems that system operators solve every day in both day-ahead and real-time markets to guarantee secure and economic operation. UC problems are usually formulated as a mixed-integer linear programming (MILP) or a mixed-integer quadratic programming (MIQP) model with binary variables representing ON/OFF statuses of generators, and is solved via Branch and bound (B&B) type algorithms. With the prosperity of applying semidefinite programing (SDP) in the power system field, researchers attempt to solve UC under the SDP framework by relaxing integrality requirements, seeking an optimal solution or a better lower bound than the traditional linear programming (LP) relaxation. This paper uses 2-order moment relaxation technique to reformulate UC problems as an SDP model, which can be solved by an interior point method. Considering significant computational burden by 2-order moment relaxation, a variable reduction strategy and two refined moment relaxation based UC models are further applied. The relationship among the proposed models in terms of their tightness is studied. In addition, a sufficient condition under which a solution is exact is stated. Numerical studies illustrate effect of the proposed models and potential applicability of the proposed models is also discussed."}, {"label": 1, "content": "Texture analysis is a technique that finds applications in diverse fields such as remote sensing, biomedical imaging, pattern recognition, and image synthesis. The first step in all of these image processing techniques is to extract meaningful features that describe texture properties from raw images. Over the years, several feature extraction methods have been proposed, each with its advantages and limitations.\n\nThis paper presents a comprehensive survey of texture feature extraction methods and categorizes them into seven classes: statistical approaches, structural approaches, transform-based approaches, model-based approaches, graph-based approaches, learning-based approaches, and entropy-based approaches. For each category, the concept, advantages, and drawbacks are discussed along with examples of their applications.\n\nThe survey helps identify two categories of methods that require further study due to their promising performance. By highlighting the strengths and weaknesses of each method, this survey serves as a valuable resource for researchers in the field of texture analysis."}, {"label": 0, "content": "When we estimate the DOA of LFM signal based on the traditional algorithm, it occurs many problems such as large sampled data and low estimation accuracy under low SNR. This paper proposes a new DOA estimation method based on compressed sensing theory, we use simulation experiments to verify the validity of its estimation of direction of arrival of LFM signals."}, {"label": 0, "content": "Effective management and provisioning of communication resources is as important in meeting the real-time requirements of smart city cyber physical systems (CPS) as managing computation resources is. The communication infrastructure in Smart cities often involves wireless mesh networks (WMNs). However, enforcing distributed and consistent control in WMNs is challenging since individual routers of a WMN maintain only local knowledge about each of its neighbors, which reflects only a partial visibility of the overall network and hence results in suboptimal resource management decisions. When WMNs must utilize emerging technologies, such as time-sensitive networking (TSN) for the most critical communication needs, e.g., controlling traffic and pedestrian lights, these challenges are further complicated. An attractive solution is to adopt Software Defined Networking (SDN), which offers a centralized, up-to-date view of the entire network by refactoring the wireless protocols into control and forwarding decisions. This paper presents ongoing work to overcome the key challenges and support the end-to-end real-time requirements of smart city CPS applications."}, {"label": 0, "content": "In this paper, we introduce our recent studies on human perception in audio event classification. In particular, the pre-trained model VGGish is used as feature extractor to process audio data, and DenseNet is trained by and used as feature extractor for our electroencephalography (EEG) data. The correlation between audio stimuli and EEG is learned in a shared space. In the experiments, we record brain activities (EEG signals) of several subjects while they are listening to music events of 8 audio categories selected from Google AudioSet. Our experimental results demonstrate that i) audio event classification can be improved by exploiting the power of human perception, and ii) the correlation between audio stimuli and EEG can be learned to complement audio event understanding."}, {"label": 1, "content": "We have developed a mathematical model for sodium high-pressure lamps and implemented it in our production process. The model was created using an analytical method based on differential equations and the singular value decomposition algorithm to determine the ARMA coefficients and transfer function. We conducted tests to ensure the model's accuracy, and the simulation results matched the experimental data. A graphical representation was obtained for a standard deviation of 1, and the model's adequacy was confirmed using a Kolmogorov-Smirnov criterion with test results based on the singular value decomposition method. Overall, our mathematical model provides a reliable method for controlling the quality of sodium lamps in production."}, {"label": 0, "content": "Precision viticulture (PV) aims to improve the grapevine production efficiency, quality, and profitability, while reducing the environmental impact. The promises of PV are realized only if large areas are monitored with high spatial and temporal resolutions. This paper considers the integration of a wireless sensor network and a smart unmanned aerial vehicle platform. To this end, local variations of factors that influence grape yield and quality are measured and site-specific management practices are applied. This approach achieves real-time, uninterrupted monitoring of the vine growth environment, and on-demand imaging and high-resolution data collection from any specific location, thereby optimizing the production efficiencies and the application of inputs in a cost-effective way."}, {"label": 0, "content": "Functional encryption is a recent generalization of public-key cryptography which aims at enabling secret-key owners to decrypt only functions of the encrypted data. This model is very promising in terms of applications. Yet, although general constructions of theoretical interests do exist, practical functional encryption is presently limited to the evaluation of low-degree functions of the encrypted inputs. In this paper, we investigate how Inner-Product Functional Encryption (IPFE) may enable the design of tax calculation system with built-in privacy. The paper is also concluded by performances results demonstrating the practicality of the approach on the concrete issue of carbon tax calculations."}, {"label": 1, "content": "The power load of residential communities is characterized by wild fluctuations, complex influence factors, and difficult forecasting. To address this issue, a short-term load forecasting method based on gated recurrent unit (GRU) neural networks was proposed. The method utilizes least absolute shrinkage and selection operator (Lasso) and partial correlation analysis to analyze the impact of temperature, humidity, rainfall, and wind speed on load. Results showed that average temperature has the greatest impact on load change among various factors, leading to the addition of average temperature as an input variable in the load forecasting model based on GRU network. Simulation results demonstrated that the proposed method is faster than long short-term memory (LSTM) networks and traditional recurrent neural networks (RNN) while retaining similar accuracy, thus proving to be a highly effective residential community short-term load forecasting method."}, {"label": 1, "content": "Diagnostics and monitoring are crucial for cellular operators to provide dependable services, but these processes require significant resources. The cost of existing measuring systems is high, and there is a need for constant participation from highly skilled specialists, which limits their use. Consequently, finding an alternative solution for diagnosis and monitoring tasks is essential. \n\nOne proposed solution is a distributed automated information and measuring system based on a smartphone that uses a passive monitoring method. This alternative solution leverages the capabilities of modern smartphones and offers relative simplicity in development, implementation, and operation. Moreover, the system offers a high degree of flexibility and mobility.\n\nOverall, this alternative solution has the potential to significantly reduce the costs associated with diagnostics and monitoring for cellular operators, making it an attractive option for businesses looking to streamline their operations."}, {"label": 0, "content": "As information technology has advanced in recent years, services which include personal authentication systems such as ATM are increasing. Current main personal authentication systems include IC cards, passwords, and biometrics authentication such as fingerprint authentication. However, there are several problems in these systems. Therefore, better systems are needed. As such systems, we propose a method to write numerals in the air using the Leap motion and to carry out personal authentication from such aerial handwriting data. We try to authenticate numerals 0 to 9 which are written by three subjects. After applying some pre-processing to inputs, learning and identification are carried out using CNN which is a method of machine learning. As a result, average identification accuracy was 90.3%. From this result, it is suggested that input numerals in the air can be authenticated and there is a possibility to construct a new personal authentication system."}, {"label": 0, "content": "The existing sporadic task model is inadequate for real-time systems to take advantage of Simultaneous Multithreading (SMT), which has been shown to improve performance in many areas of computing, but has seen little application to real-time systems. A new family of task models, collectively referred to as SMART, is introduced. SMART models allow for combining SMT and real time by accounting for the variable task execution costs caused by SMT."}, {"label": 1, "content": "Precision viticulture (PV) is a farming technique that aims to enhance grapevine production efficiency, quality, and profitability while minimizing environmental impacts. Achieving the full potential of PV requires monitoring large areas with high temporal and spatial resolutions. In order to achieve this, we propose the integration of a wireless sensor network and a smart unmanned aerial vehicle platform. This will enable the measurement of local variations in factors that impact grape yield and quality, allowing for site-specific management practices to be applied in real-time. With uninterrupted monitoring of the vine growth environment and the ability to capture high-resolution data from any location, this approach will optimize production efficiencies while reducing input costs."}, {"label": 0, "content": "The automated system inspection with defect detection in large-scale photovoltaic (PV) farms is an urgent problem to be addressed. This paper presents a transfer learning based solution for visible module defects diagnosis in these conditions. The solution mainly includes three parts: a normative, time-updated, sundry dataset; a pre-trained deep learning model; a transfer learning strategy. Image augmentation technology is applied in the dataset. The proposed transfer learning based solution is able to extract the defects' features from local representation to global one and low level to high one with more robustness in comparison with the enhanced CNN based method. The proposed solution is evaluated through extensive experiments and the numerical results clearly demonstrate its effectiveness."}, {"label": 1, "content": "For mechanical systems, accurate fault diagnosis of a gearbox is crucially important. However, due to the many mechanical parts that constitute a gearbox, there are a variety of failure modes, making accurate diagnosis challenging. Additionally, while raw vibration signals can be easily obtained from real gearbox applications, labeling them \u2013 particularly in cases of multi-fault modes \u2013 can be costly. These issues pose challenges to traditional supervised learning methods for fault diagnosis.\n\nTo address these challenges, we propose a new diagnostic method for a gearbox based on an active learning strategy that utilizes uncertainty and complexity. Specifically, our method employs empirical mode decomposition-singular value decomposition (EMD-SVD) to generate feature vectors from raw signals, and then uses an active learning scheme to select the most valuable unlabeled samples for labeling and addition to the training data set. Finally, a random forest (RF) algorithm trained on the updated data set is employed to recognize gearbox fault modes.\n\nIn experimental tests on two cases of gearbox fault diagnosis data, the proposed method outperformed both a supervised learning method and other active learning methods. This validates the method's effectiveness and superiority."}, {"label": 1, "content": "Machine learning (ML) is becoming increasingly popular in the field of network security due to the rising number of network-connected devices, the increasing stealthiness of malicious activities, and the emergence of new technologies such as Software Defined Networking (SDN). ML-based SDN security models have the ability to control the routing and switching of an entire SDN from the application layer, making them a prime target for hackers. Previous research on adversarial machine learning and general SDN vulnerabilities has been insufficient in protecting these ML models. Therefore, it is crucial to develop more secure ML-based SDN security applications to counter these vulnerabilities. This paper examines recent advancements in ML-based SDN security applications and highlights specific vulnerabilities unique to this field, including a successful attack on StratosphereIPS. Overall, this paper emphasizes the need for enhanced security measures to counter the evolving threats in network security that accompany the application of ML and SDN technologies."}, {"label": 0, "content": "Real life systems are mostly nonlinear. Controlling nonlinear systems is accomplished in several ways, mostly linearizing the system which may cause problems regarding the robustness of the controller. The controllers, on the other hand may be linear or nonlinear. This paper investigates the effects of nonlinearities on the robustness of various controllers (linear controllers, such as PID with first order derivative filter, state-feedback with integral control and, CRONE, and nonlinear controllers, such as smoothed first-order integral SMC) with an application on a DC motor. The motor's model includes four types on nonlinearities (voltage saturation, current saturation, dead zone, and backlash deadband). The performance of the different controllers is presented and compared using the nonlinear model, the linearized model, uncertainty in the model, and harmonic load disturbance."}, {"label": 1, "content": "This paper proposes a fault line detection method based on variational mode decomposition (VMD) and phase space reconstruction. The method enhances the traditional line detection approach using transient zero sequence current (TZSC) waveform comparison. The first step involves extracting the first modal component by VMD of the TZSC of each line. Next, phase space reconstruction of the extracted modal component is performed using the coordinate delay method. The C-C algorithm is used to estimate the optimal phase space reconstruction dimension and time delay, and similar phase spaces are created for the second feature extraction. Finally, the phase space similarity matrix is established by comparing the similarity degree of every two zero sequence current phase spaces. The comprehensive correlation coefficient is calculated, and the line with the minimum comprehensive correlation coefficient is selected as the fault line. The simulation results show that the approach is highly sensitive and robust to noise. This method can improve the effectiveness of traditional line detection techniques based on TZSC waveform comparison."}, {"label": 1, "content": "Most software defect prediction models assume the availability of sufficient historical training instances with labels and that the training data and predicted instances share the same features to ensure prediction accuracy. However, datasets with different granularities and dimensions of data are prevalent in practice, making it challenging to use such data as training instances. To address this challenge, we propose a heterogeneous data orienting multiview transfer learning method for software defect prediction, denoted as MTDP, which leverages small scale and different dimensions of data to improve the prediction model's performance. The MTDP method achieves this by automatically learning labels through neural network models, generating quasi-real instances and expanding the training set through co-training. The experimental results demonstrate that the quasi-real instances are similar to real instances in terms of their effects, and the software defect prediction performance is improved by introducing the quasi-real instances into the training dataset."}, {"label": 0, "content": "Critical infrastructure is vulnerable to a broad range of hazards. Timely and effective recovery of critical infrastructure after extreme events is crucial. However, critical infrastructure disaster recovery planning is complicated and involves both domain-and user-centered characteristics and complexities. Recovery planning currently uses few quantitative computer-based tools and instead largely relies on expert judgment. Simulation modeling can simplify domain-centered complexities but not the human factors. Conversely, human-centered design places end-users at the center of design. We discuss the benefits of combining simulation modeling with human-centered design and refer it as human-centered simulation modeling. Human-centered simulation modeling has the capability to make recovery planning simpler and more understandable for critical infrastructure and emergency management experts and other recovery planning decision-makers. We qualitatively analyzed several resilience planning initiatives, post-disaster recovery assessments, and relevant journal articles to understand experts and decision-makers' perspectives. We propose a conceptual design framework for creating human-centered simulation models for critical infrastructure disaster recovery planning. This framework consists of three constructs: 1) user interaction with design features that end-users interact with, including model parameters assignment, decision-making support, task queries, and usability; 2) system representation that refers to system components, system interactions, and system state variables; and 3) computation core that represents computational methods required to perform processes."}, {"label": 1, "content": "Electromagnetic transient (EMT) simulation programs are commonly utilized to determine the most effective circuit and control parameter values. This paper introduces an EMT-simulation-based optimization methodology that employs the Kriging method. The method is then employed in the design of automatic reactive power regulator (AQR) controllers for static synchronous compensators (STATCOMs). The results indicate that the proposed technique can efficiently obtain the optimal solution for the AQR controller."}, {"label": 1, "content": "The switchgear is a crucial piece of equipment in any power system. One of the primary causes of switchgear failure is overheating of the contacts. Unfortunately, these contacts are often sealed inside the switchgear, making it difficult to measure their temperature directly. To address this issue, we propose a new technique that uses infrared window temperature measurements and a neural network to predict the internal temperature of the contacts. \n\nThe approach we used involves using ANSYS Workbench finite element software to simulate the switchgear and collect the temperature data. We then trained a BP (backpropagation) neural network model using this data, and tested it to determine whether it could accurately diagnose contact faults. The results show that the model we developed is highly accurate, with a maximum error between expected and predicted data of only 1.5\u00b0C. \n\nWith this new model, maintenance personnel can easily measure the infrared window temperature to determine the contact status of the switchgear. This approach is a significant step forward in ensuring the safe operation of switchgear, as it allows for direct measurement of a key performance indicator that has previously been difficult to monitor."}, {"label": 1, "content": "This paper describes a learning-based approach for real-time recognition of bimanual (two hands) gestures in situations where this type of gesture is not well-studied in the literature. To address the challenge posed by hand-hand self-occlusion in bimanual gestures, this solution employs a system of multiple cameras from diverse angles. A custom multi-camera system is established to collect multi-angled bimanual gesture data, and each view's data is used to train a separate classifier. To aggregate the results of these classifiers, the paper proposes a weighted sum fusion scheme, with the weights optimized based on the performance of each view's recognition. The experimental results demonstrate that multi-view recognition outperforms single-view recognition."}, {"label": 0, "content": "The ubiquity of 802.11 WiFi and the miniaturization as a result of Moore's law has recently enabled the success of IoT. From smart lightbulbs to smart toasters, many home appliances are now becoming both Internet-enabled and interconnected through WiFi. Soon, these futuristic smart homes will be able to run themselves, allowing the human operators to be fully in control of their homes - or will they? Despite the physical advancements made since the '90s, the same cannot be said of the vulnerabilities of these smart devices. We analyze a set of common smart home appliances - a lightbulb, power switch, motion sensor, security camera, and home assistant - putting their vulnerabilities to the test to see what a 21st century home intruder could discover."}, {"label": 1, "content": "Systems based on the concept of the Internet of Things (IoT) exhibit a multi-tiered architecture, an abundance of utilized 'things,' the influence of new types of attacks, as well as incomplete and ambiguous parameters. This is why effective security management tasks in IoT networks, such as network traffic analysis, require the use of intelligent approaches and methods. This paper aims to develop and evaluate a new algorithm for analyzing network traffic in real-time or near-real-time situations. Furthermore, the paper explores various options for implementing intelligent agents designed for network traffic analysis in IoT networks, including high-performance computers, embedded devices, and systems-on-chip. The agents operate using a pseudo-gradient anomaly detection algorithm and fuzzy logical inference. Notably, the proposed algorithm operates in real-time, and experimental assessments indicate that it can achieve up to 50% improvement in accuracy and 90% in speed."}, {"label": 1, "content": "The estimation of pulse rate from photoplethysmogram (PPG) signals has been a significant advancement in smart wristband technology. However, this study goes further by introducing a novel method for estimating heart rate recovery (HRR) using a custom-made wrist-worn device and a consumer smart wristband. This new approach allows for the acquisition of instantaneous pulse rate, making it possible to estimate HRR parameters more accurately compared to previous methods.\n\nThe feasibility of using PPG-based devices to estimate HRR parameters was investigated by comparing them to the reference electrocardiogram obtained synchronously. The study analyzed three HRR parameters on pulse rate data collected during a standardized stair climbing test from 22 healthy participants. Results demonstrated that the HRR parameters estimated using the wrist-worn device were associated with a significantly lower absolute error compared to the consumer smart wristband. \n\nThis finding highlights the importance of instantaneous pulse rate in ensuring a more accurate estimation of HRR parameters. The new approach presented in this study is a significant advancement in smart wristband technology, providing an improved method for measuring HRR and, therefore, improved monitoring of cardiovascular health."}, {"label": 1, "content": "The world has long been concerned about environmental problems and the associated issues. However, the emergence of IoT and the development of smart cities, buildings, and grids have opened up new avenues for addressing these pressing concerns. The successful implementation of IoT solutions depends on our ability to address these problems effectively.\n\nThis paper proposes an IoT-based technology for the protection and monitoring of the environment within a poultry house. The proposed software-based hardware can monitor environmental parameters such as air temperature, humidity, O2, CO2 concentration levels, and NH3 concentration. The wireless sensor is responsible for collecting data on these parameters and coordinating and controlling them.\n\nThe hardware has been successfully implemented at various sites within the poultry shed, and the experimental setup has been found to be highly effective and accurate. By implementing this scheme, the poultry industry can achieve both a safe environment and increased profits.\n\nOverall, the IoT-based technology proposed in this paper represents a significant step towards creating a safer and more sustainable future for the poultry industry and beyond."}, {"label": 1, "content": "This paper presents a proof of Typical Worst-Case Analysis (TWCA), which is an analysis technique for real-time systems. TWCA was originally introduced for systems with fixed priority preemptive (FPP) schedulers and has since been extended to fixed-priority nonpreemptive (FPNP) and earliest-deadline-first (EDF) schedulers. \n\nThe authors have developed a generic analysis technique that is applicable to any system model. The analysis is based on an abstract model that characterizes the properties needed to make TWCA applicable. The authors have formalized their results and checked them using the Coq proof assistant along with the Prosa schedulability analysis library.\n\nThe authors emphasize that formalizing real-time systems analyses helps to increase confidence in the results obtained. It also helps to understand the exact assumptions required by a given analysis, its intermediate steps, and how the analysis can be generalized. \n\nOverall, this paper contributes to the ongoing effort to develop rigorous and reliable analysis techniques for real-time systems."}, {"label": 0, "content": "We describe a procedure of compact behavioral models design of the communication channel through a borehole pipe for measurement while drilling (MWD) microwave systems based on results of measurements of radio pulse signals during testing of the channel. Characterization of the microwave channel for the decline-directed borehole and a transmitter on the Gunn diode is described as an example of the offered process. The procedure is based on measurement of signal samples of the transmitter with the subsequent accounting of electrodynamical parameters, attenuation in the media, and on the pipe walls."}, {"label": 1, "content": "A direction finding algorithm is presented for noncircular sources under unknown mutual coupling. The proposed method is based on fourth-order cumulants and utilizes the symmetric Toeplitz structure of the mutual coupling matrix. By constructing a series of cumulant matrices, a closed form solution for the DOA estimation is obtained. Simulation results demonstrate the effectiveness of the algorithm and its ability to handle unknown mutual coupling. Additionally, the utilization of the noncircularity of the sources leads to improved estimation performance."}, {"label": 0, "content": "An effective Physical Protection System (PPS) requires methodologies and tools to automatically evaluate the PPS effectiveness, provide the suggestions or strategies for the further redesign or optimizations, and assist NPPs to conduct training for their staffs. A virtual reality platform for evaluation, design and training of PPS (IPAD) was proposed in one of the authors' previous works. This paper supplements a training module using IPAD platform for the virtual training of PPS. The virtual training will help detection staffs and response forces grasp the operational procedure of PPS when intrusion or emergency events occurred."}, {"label": 0, "content": "The reliable operation of the relay protection equipment and its secondary circuit is an important factor to ensure the safety and stability of power grid. It is of great significance to improve the operation and maintenance efficiency of the relay protection equipment and its secondary circuit. In this paper, the design of relay protection intelligent mobile operation and maintenance management system based on power wireless virtual private network is elaborated, including the architecture of system and the deployment mode of communication network. In addition, information synchronization methods between the system and relay protection statistical analysis and operation management module, condition-based maintenance assistant decision-making module and some other system data modules are illustrated in detail. Through collection, collation, interaction and sharing of intelligent information in substation, the system realizes the purpose that equipment information are mobilely, paperlessly and standardizedly managed and controlled, which can improve the convenience and efficiency of on-site operation and maintenance work and its management level, and as a result is quite significant to ensure the safety and stable operation of power grid. The system has been successfully applied in substation and achieved good results."}, {"label": 0, "content": "The number of food photos posted to the Web has been increasing. Most of the users prefer to post delicious-looking food photos. They, however, do not always look delicious. A previous work proposed a method for estimating the attractiveness of food photos, that is, the degree of how much a food photo looks delicious, as an assistive technology for taking a delicious-looking food photo. This method extracted image features from the entire food photo to evaluate the impression. In our work, we conduct a preference experiment where subjects are asked to compare a pair of food photos and measure their gaze. The proposed method extracts image features from local regions selected based on the gaze information and estimates the attractiveness of a food photo by learning regression parameters. Experimental results showed the effectiveness of extracting image features from outside the gaze regions rather than inside them."}, {"label": 0, "content": "The paper presents a new methodological approach to the automated construction of software for solving the design problems of heating systems. The methodological approach is based on the Model-Driven Engineering paradigm. The essence of this paradigm is that the software is generated on the basis of formal description represented by models. The knowledge about heating systems, applied problems, and the applied software is formalized in the form of ontologies. The automated construction of the software system is performed on the basis of a computer model of a heating system, the ontologies and modern metaprogramming technologies. The proposed approach allows us to successfully solve the problem of separation of methods for solving applied problems and models of heating system elements. To this end, the methods are implemented in the form of software components that are not related to properties and models of specific equipment. And the models of heating system elements are automatically compiled into software components. In the process of software system construction, software components that implement models and methods are integrated dynamically. As a result, the software system oriented to solving a specific applied problem is created in an automated mode. The developed approach has been used for the implementation of the SOSNA software. The software is applied to design urban heating systems."}, {"label": 0, "content": "This paper aims at presenting the approach for the process of attributes verification in the Attribute-Based Encryption schemes. We considered ABE methods in the Internet of Things environment. The main idea is to allow users or device owners to verify attributes using standardized and well-known authorization protocols like Oauth2. More accurately, Attribute Authority will use Authentication as a Service approach for users and device controllers authentication. We described software service for Attribute-Based Encryption methods in the context of FIWARE platform. Components of this platform use Oauth2 for authentication and authorization mechanisms. More specifically, we developed a web application which allows devices to create orders for ABE secret keys with particular attributes. Further users should approve or deny these orders. We used FIWARE components for users and devices authentication. To the best of our knowledge, it is the first implementation of ABE algorithms in FIWARE ecosystem."}, {"label": 1, "content": "This paper aims to analyze the impact of external meteorological factors on galloping and proposes a BP neural network learning algorithm to predict galloping probability. Input vectors such as wind, inducted angle of wind direction, line, relative humidity, and ambient temperature are considered. The proposed method judges whether the prone-galloping weather conditions are satisfied and assesses its prediction performance with several test metrics. \nTo illustrate this method, a case study is conducted using historical galloping data of Henan power grid, and the result confirms the effectiveness and practicality of the proposed method. The method can help power system operation staff make reasonable decisions and ensure the power grid operation during peak-load periods in winter."}, {"label": 1, "content": "The concept of Cyber-Physical Systems (CPS) is undoubtedly a complicated process. As a system architect, one might not be well-versed with all the diverse components of the system. Moreover, when all these components are put together, they pose a new level of intricacy. In order to study and validate CPS, simulation is often used. However, the CPS simulation adds yet another layer of complexity as it may be distributed or very intricate to handle. \n\nThis is where our proposed methodology of distributing a simulation model comes into effect, where the full potential of multiple processing units can be utilized without causing any impact on the simulation of our modeled system. We ensure that the distribution of the simulation is done in a manner that does not affect the reliability of the system."}, {"label": 1, "content": "To address the limitations of conventional speaker segmentation and clustering techniques, this paper presents a novel multilevel speaker re-segmentation and re-clustering algorithm utilizing GMM-UBM. Drawing upon statistical modeling techniques in speaker recognition, the algorithm maximizes the use of speaker information post-segmentation and clustering in traditional methods, resulting in improved system performance through re-segmentation and re-clustering of speech files."}, {"label": 0, "content": "We considered the issues of synthesis of the algorithms for adaptive nonlinear signal processing using feed-forward blocks of nonlinear transformation under the influence of non-Gaussian noise with unknown density of distribution of instantaneous values or its envelope. It is shown that to plot the adaptive feed-forward blocks of nonlinear transformation, the algorithms for estimating the parameters of linear model of probability density function of noise can be used. This model is presented in the form of a generalized polynomial of decomposition in a series of linearly independent functions, and, also, in the form of nonlinear models, such as generalized Gaussian distribution and abnormally cluttered distribution."}, {"label": 0, "content": "We discuss a two-frequency subtraction technique to reduce the energy leakage in a Fourier spectrum. In our method, the two strongest frequency components are determined by finding the periodogram over an interval such that the two frequencies will not interfere with each other. Such a method allows the subtraction of the two main frequency components more accurately from the original signal. The energy leakage from the main components is minimized to allow identification and more accurate determination of weaker components. Statistical error from the subtraction technique can be several times smaller than the FFT method. We show that the subtraction method is relatively robust for signals with varying amplitude or frequency."}, {"label": 0, "content": "The goal of industry 4.0 is to use all the information that can be extracted from a supply chain to continuously optimize all aspects of its operation. The data acquisition is still a big challenge and the first step of the fourth industrial revolution. Getting data from software is much easier than getting data out of hardware like manufacturing machines. Especially if the Programmable Logic Controller (PLC) data is either poorly documented or not designed for these requirements. Therefore, we created a methodology to track the most common movement of a machine, which is the linear motion. The solution is an IoT-device: a small, wireless, and low cost sensor, designed to provide data about linear motions within a machine in real-time. We designed a static generic model and a method for machine optimization with data acquisition results comparable to results in other approaches. By implementing the methodology to a real industrial scenario, the results enable us to prove our hypothesis. The IoT-device data was as good as the PLC data and even closer to real-time. Our methodology also shows a higher potential to automate the data analysis."}, {"label": 1, "content": "The relevance of developing high-performance computing systems based on heterogeneous computer systems is becoming increasingly important due to the growing volume of processed information, calculations, and studies involving large data sets. Therefore, the purpose of this work is to create a model that predicts the performance of heterogeneous computing systems and to experimentally evaluate it by simulating access to memory and modeling fundamental parallel algorithms.\n\nBy using the developed model, it is possible to make an accurate estimate of the time it takes to execute parallelized tasks using graphics processor-based heterogeneous computer systems. This will ensure that the computations are carried out efficiently and effectively."}, {"label": 0, "content": "In this paper, we investigate the outage performance of a general dual-hop multiple-input multiple-output (MIMO) amplify-and-forward (AF) relay network, where the source, relay and destination are all equipped with multiple antennas. By considering maximal-ratio-transmission (MRT) and maximal-ratio combining (MRC) for the transmitter and receiver, respectively, we first obtain the output signal-to-interference-plus-noise ratio (SINR) of the dual-hop AF relay system with multiple co-channel interferences (CCIs) and noise at the relay. Then, we derive closed-form expressions of the outage probability (OP) for both the fixed-gain and variable-gain multi-antenna relaying systems. Finally, computer simulations are carried out to validate the performance analysis. Our new analytical expressions not only provide a fast and efficient method to evaluate the outage performance of the system, but also enable us to gain valuable insights into the effects of key parameters on the performance of the dual-hop AF relaying system benefit from implementing multiple antennas at each of the three nodes in the relaying network."}, {"label": 0, "content": "We consider a two user Gaussian multiple access channel with an additive Gaussian state process. The past values of both the state and the received symbols are strictly causally made available to the encoders at each instant. The capacity region for the noiseless case, without any feedback, was recently solved in literature. Here we study the model with noise as well as feedback. We propose a communication scheme which effectively utilizes the feedback symbols as well as the state information to enhance the achievable region. In particular, Wyner-Ziv binning on the state information and Ozarow feedback scheme for the MAC are effectively utilized, using a suitable interleaving technique. The obtained region is significantly better than the feedback capacity region with no state information."}, {"label": 1, "content": "Improved Audiovisual Speech Synchrony Detection using Deep Canonical Correlation Analysis\n\nAudiovisual speech synchrony detection is an essential component of talking-face verification systems. Previous research has mainly focused on visual features and joint-space models, while standard mel-frequency cepstral coefficients (MFCCs) have commonly been used to represent speech. In this study, we concentrate on audio by examining the impact of context window length for delta feature computation and comparing MFCCs with simpler energy-based features in lip-sync detection. State-of-the-art hand-crafted lip-sync visual features, space-time auto-correlation of gradients (STACOG), and canonical correlation analysis (CCA), are chosen for joint-space modeling. To enhance joint space modeling, we adopt deep CCA (DCCA), a nonlinear extension of CCA.\n\nOur experiments using the XM2VTS dataset show significantly improved audiovisual speech synchrony detection, with an equal error rate (EER) of 3.68%. Further analysis indicates that failed lip region localization and subjects with beards account for most of the errors. Thus, the description of lip motion is the limiting factor, while the use of novel audio features or joint-modeling techniques is unlikely to boost lip-sync detection accuracy further."}, {"label": 0, "content": "Real-time task scheduling for wireless networked control systems provides guarantees for the quality of service. This paper introduces a new model for joint network and computing resource scheduling (JNCRS) in real-time wireless networked control systems. This new end-to-end real-time task model considers a strict execution order of segments including the sensing, the computing and the actuating segment based on the control loop of WNCSs. The general JNCRS problem is proved to be a NP-hard problem. After dividing the JNCRS problem into four subproblems, we propose a polynomial-time optimal algorithm to solve the first subproblem where each segment has unit execution time, by checking the intervals with 100% network resource utilization and modify the deadlines of tasks. To solve the second subproblem where the computing segment is larger than one unit execution time, we define the new timing parameters of each network segment by taking into account the scheduling of the computing segments. We propose a polynomial-time optimal algorithm to check the intervals with the network resource utilization larger than or equal to 100% and modify the timing parameters of tasks based on these intervals."}, {"label": 0, "content": "Farming of soft shell crab has been practiced in south-east Asian countries such as Indonesia. In the crab farming, a poor water quality increases the mortality rate of the crab in the pond. Then, in this paper, we propose a design and implementation of a water quality monitoring system for crab farming using IoT technology to give awareness to a farmer for maintaining acceptable levels of water quality in the pond. Hence, it contributes to increase the survival rate of crab and achieve higher yield of soft shell crab. Our proposed system uses a LoRa-based wireless sensor network and a lightweight Message Queuing Telemetry Transport (MQTT) protocol for exchanging messages between small embedded devices, mobile devices, and sensors. The system mainly consists of sensor node as publishers, and Raspberry pi MQTT broker, and mobile client devices as subscribers. The sensor nodes are built with small embedded devices, LoRa wireless interface, and water quality sensors, i.e. water temperature sensor, pH sensor, and salinity sensor. We also setup a web-based monitoring application using node-red dashboard for accessing water quality levels remotely."}, {"label": 0, "content": "In this paper, we implemented the architecture of DV700 which is deep learning based image recognition accelerator in edge computing applications, and measured performance in FPGA environment. As a result, it operates at 12.9 fps in GoogleNet and 15.6 fps in SSD algorithms with 79MHz operating frequency environment."}, {"label": 1, "content": "We have proposed a new framework for modulation recognition called Radio Classify Generative Adversarial Networks (RCGANs) that utilizes Generative Adversarial Network (GAN) technology in the radio machine learning domain. Through an extensive, data-driven GPU-based training process, our method learns features through self-optimization. We conducted several experiments using a synthetic radio frequency dataset, which demonstrated that our proposed method achieves higher or equivalent classification accuracy when compared to renowned deep learning and classic machine learning methods. Our technique also shows superior data utilization and robustness against noise."}, {"label": 1, "content": "Traditional BP neural networks often encounter problems such as slow detection rates and reduced accuracy when dealing with high-dimensional and complex data for network intrusion detection. In light of this, the proposed intrusion detection method of KPCA-BP neural network aims to reduce data dimensions and improve neural network performance. \n\nTo achieve this, the method utilizes KPCA's effective dimensionality reduction capabilities to reduce the network data's dimensions. This, in turn, leads to improved learning performance by altering the initialization initial value method and loss function of traditional BP neural networks. As a result, the improved BP neural network exhibits better learning outcomes. \n\nExperimental results show that the KPCA-BP-based intrusion detection method has a better detection rate and accuracy improvement effect. Thus, it presents itself as an effective solution to overcome the limitations of the traditional BP neural network when it comes to network intrusion detection."}, {"label": 1, "content": "An adaptive block-based compressive sensing (BCS) video reconstruction algorithm is proposed in this study that utilizes temporal-spatial domain characteristics. To achieve this, an assigned weight function is introduced, and an adaptive joint wavelet coefficients and variance sampling scheme is developed. A global measurement matrix is constructed using the assigned weight matrix to enable global reconstruction. The predictive-residual reconstruction model of joint temporal-spatial domain characteristics is constructed using a block-based multi-hypothesis (MH) model and the minimum total variation (TV) model. The proposed algorithm calculates residuals by global reconstruction and iteratively obtains prediction frames of the current frame. These residuals are then combined with the predictive frames of the current frame to reconstruct a new frame. The effectiveness of the proposed video reconstruction algorithm is validated by comparing results with other BCS algorithms proposed for video reconstruction in recent years. The experiment results show that the proposed algorithm can effectively improve the quality of video reconstruction and reduce the computational complexity compared with other algorithms."}, {"label": 1, "content": "Recently, the rising popularity of Artificial Intelligence (AI) has garnered significant attention, particularly in the power systems field where AI techniques have been utilized for over two decades to effectively solve complex problems. With the emergence of new generation AI technologies, it is expected that energy transition will be promoted and future power systems will be supported. This paper aims to provide an overview of the research hotspots, frontiers, and mainstream trends in AI research for power systems. Using the widely used knowledge mapping tool CiteSpace, literature data collected from the Web of Science (WOS) database between 2010 and 2018 was analyzed. The collaboration networks among different countries, regions, and institutions were investigated, and a detailed discussion was provided based on general statistical data and visualized knowledge maps. Pivotal and mushrooming articles were identified and reviewed using betweenness centrality and citation bursts as indicators."}, {"label": 0, "content": "Orthogonal frequency division multiplexing (OFDM) has been widely used in modern wireless communication systems. In OFDM system, channel estimation is a key technology to improve the system performance. However, most conventional channel estimation methods cannot suppress the noise effectively, which affects the quality of the final received OFDM signals. To solve this problem, this paper utilizes the raise cosine (RC) filter and square root raise cosine (SRRC) filter to suppress the inter-symbol-interference (ISI) and the noise in OFDM-RC and OFDM-SRRC systems. The proposed method first filters out the noise by using RC filter or SRRC filter, while obtaining the noise standard deviation as the threshold. Then, the impact of noise is further reduced by setting the noise suppression threshold. Simulation results are shown to verify the effectiveness of the OFDM-RC and OFDM-SRRC systems over multipath propagation conditions."}, {"label": 0, "content": "As an alternative to voice, sign language and artificial larynx can be used. However, there are disadvantages where they require a long-term training and are expensive. Therefore, researches on detection of utterance by electromyography (EMG) analysis around the lips have been conducted. On the one hand, it is necessary to construct a personal authentication system to identify speakers. The electrode used in this paper is 2 electrodes sensor, which is small in size and a dry type. Three sensors are attached in the orbicularis muscle, the zygomatic major muscle, and the depressor angle oris muscle which can acquire myoelectric information necessary for identification in Japanese vowel utterance. EMG signals are measured using P-EMG plus. In order to eliminate noises, signal cutting is carried out before and after the central point of the acquired raw data. Furthermore, EMG data are divided to increase the number of data while overlapping. These are named \u201cDATA 1\u201d. A Hamming window is then applied for them, and the amplitude values of the power spectra are calculated by fast Fourier transform. Automatic verification and elimination of noise parts by quartile method were carried out. In order to reconstruct signals after noise elimination, the inverse Fourier transform is carried out and then a inverse Hamming window is applied. These are named \u201cDATA 2\u201d. Learning identification is carried out using a convolutional neural network. A large difference was found in accuracy depending on the data set created separately by measurement date. Therefore, it was found that intra-individual variation by each subject was large. In the future, it is necessary to further improve the data and to reduce individual variation within each subject."}, {"label": 1, "content": "A statistical model has been developed to predict the output power and energy of Solar Photovoltaics (PV), using a multiple input single output (MISO) system based on Jackknife regression. The model generates PV power in kilowatts, with inputs that include irradiance, precipitation, ozone, ambient temperature, and atmospheric aerosol components. Training and testing the model on data from National Renewable Energy Laboratory, residual statistical tests are applied to validate the estimation results. The results show an absolute error of less than 1 kW for 90.6% of the predicted values, corresponding to a percentage error of less than 8.33%, for the 12 kW system under study."}, {"label": 0, "content": "Time-Step method and Station-Pair method are prominent techniques for estimation of spatial gradients. Since GBAS is meant to serve a limited area of about 50km of an airport for aircraft Precision Approach and landing, these two methods were considered for gradient estimation within the GBAS service area. Much of the work on these techniques has been reported for GPS-based GBAS applications. In this paper, the suitability of these methods to IRNSS-based GBAS applications is investigated. It is observed that since IRNSS satellites are either GEO or GSO, the time interval (At), of Time-Step method should be significantly high (30min for GSO), to obtain gradient data for GBAS' service area. With the Station-Pair method, a dense network of stations, each separated by not more than 1-2 kms is required."}, {"label": 0, "content": "According to rules of interaction between the subject of the Electricity Market and JSC ATS, subjects of the Electricity Market are obliged to implement the daily hourly forecast in the mode \"for a days ahead\". To ensure of high-quality prediction of the electricity loads, subjects of electricity market need to prepare the regulatory base, to develop a technique of creation of the forecast of the electricity loads, and also to count the risks connected to the accuracy of the used models. On the one hand, the complexity of the problem being solved is characterized by the availability of data on the supply points, since not always the subject of the electricity market has the opportunity to collect data on the consumption of individual power facilities in the hourly mode. From the other hand, the introduction of commercial accounting systems can solve this problem with the investment of a large investment in the installation automatic system for commercial measurement of the electricity loads (ascme), but as a rule subject of electricity market goes for such long-term payback costs. The work can be useful both to specialists of power sales companies who are engaged in building forecast models, as well as to specialists of the electricity market entities, who carry out forecasts for the electricity market for the day-ahead. The main aim of the study is applying methodology forecasting using neural network for building predictive models for LLC \"Omsk Energy Retail Company\". The methods used in the study: Holt-Winters model, the ARIMA, neural networks, temperature and wind index. The results. It considered methods of construction of predictive models, the path of their evolution since the launch of Electricity market. Method of constructing the forecast of \"Omsk Energy Retail Company\" was developed using neural network, taking into account the temperature and wind index and allocation of common types of days by electricity load."}, {"label": 0, "content": "Demand response and active participation of end-users (prosumers) are expected to play a critical role in the future power grids. Market based transactive exchanges between prosumers are triggered by the increased deployments of renewable generations and microgrid architectures. Transactive Energy Systems (TES) employ economic and control mechanisms to dynamically balance the demand and supply across the electrical grid. Effective transactive mechanisms leverage on a large number of distributed edge-computing and a communication architecture. Given the prolific usage of digital devices, the assets within a transactive environment are vulnerable to various threats. This paper utilizes a machine learning technique to detect possible anomalies within a transactive energy framework. An ensemble based methodology is used to detect anomalies in the market and physical system measurements. The proposed technique is validated for satisfactory performance to detect anomalies and trigger further investigation for root cause analysis and mitigation."}, {"label": 0, "content": "The paper discusses multithreaded processing of images on graphic processing units for the purposes of feature detection and matching. The problem of feature detection and feature correspondence is applied for image stitching and panorama creation. Parallel GPU implementation based on nVidia CUDA is presented and experimentally evaluated and compared by parallel multithread CPU processing for shared memory parallel computational model."}, {"label": 1, "content": "Currently, residential electricity consumption is growing at a much faster rate than industrial and agricultural electricity consumption. For this reason, it is crucial to optimize residential electricity consumption behavior in order to reduce the power grid peak-valley gap and decrease energy loss. A key step towards achieving this goal is to study non-intrusive load identification technology, which involves analyzing residential electricity usage patterns and providing residents with electricity usage suggestions.\n\nIn this paper, we delve into the details of non-invasive load identification technology and propose a layered classification algorithm based on multidimensional load characteristic matching. We have also developed an intelligent power meter equipped with residential load identification function, which is built on the State Grid single-phase carrier intelligent electricity meter.\n\nTo verify the efficacy of our smart electricity meters, we have set up a test environment in the laboratory, where we were able to achieve 100% identification accuracy of electric appliance type. Our smart meter accuracy for power consumption identification is over 90%, despite extremely complicated working conditions.\n\nOverall, this study provides valuable insights and tools that can help optimize residential electricity consumption behavior and ultimately, reduce energy loss."}, {"label": 1, "content": "In recent years, there has been significant interest in the rapid estimation of parameters for maneuvering targets. However, many existing algorithms are plagued by accuracy and computational complexity issues. Furthermore, when multiple target parameters are estimated at the same time, traditional time-frequency methods suffer from cross-term interference. To address these problems, we propose a fast estimation algorithm for multi-maneuvering target parameters. \n\nThe algorithm leverages the Higher-order Adjacent Cross Correlation Function (HACCF) expansion of radar echo signals, in which the auto item is constant and the cross term is a function of the adjacent time delay. Specifically, the algorithm first extracts the auto items via mean extraction of the HACFF, and then inhibits the cross term. The algorithm then estimates the frequency of the auto items to obtain an accurate estimate of the maneuvering target acceleration. \n\nNumerical simulations show that the algorithm performs with high accuracy and requires minimal computations, enabling quick estimation of maneuvering target parameters. Additionally, the algorithm can estimate the parameters of multiple maneuvering targets simultaneously."}, {"label": 1, "content": "Railway detection tasks require a large number of images, but the lack of effective image classification methods makes it challenging to analyze detection images deeply. Utilizing convolutional neural networks (CNN) to implement railway image scene classification is an effective technical solution. This study presents a method for reducing database bias using Gradient-weighted Class Activation Mapping (Grad-CAM) to enhance scene classification accuracy, achieving an accuracy rate of 95.3% (top3) on Railway12 database. Our approach combines two key insights: (1) the limited amount of railway scene database makes it difficult for CNN to achieve high performance, so we transfer pre-trained ImageNet-CNN for fine-tuning on railway scene database; (2) we introduce the Grad-CAM visualization method to analyze the model's classification pattern and intuitively display possible database biases, providing a clear strategy for reducing dataset bias."}, {"label": 1, "content": "In this paper, we present a solution that aims to make reading and comprehending medical documents easier by automatically simplifying complex medical terms found in web pages. Our proposed approach combines natural language processing and heuristics to identify these terms and calculate the probability of them being medical and complex. We then use a customized dictionary to simplify these terms, and leverage the Microsoft Bing cognitive API to retrieve related images and videos, which are presented to the users for a better understanding of the document being read."}, {"label": 1, "content": "The infrared thermal image plays a pivotal role in the condition monitoring of electrical equipment. However, due to its coarse and fuzzy edges coupled with serious noise, it poses great challenges in infrared image processing. To overcome this limitation, this paper proposes an adaptive optimal threshold edge extraction algorithm based on improved Sobel operator. The algorithm uses eight-direction Sobel operators to extract the infrared image edge in high-temperature areas with noisy backgrounds. Furthermore, the wavelet coefficients in sub-band are described by a general Gaussian distribution and the variance is estimated from the local neighborhood information of sub-band wavelet coefficients, creating an adaptive optimal denoising threshold. Finally, the edge extraction infrared image is denoised by combining it with the improved Sobel operator and the optimal threshold. The Matlab simulations demonstrate that this algorithm effectively detects the edge of infrared images while tremendously enhancing its anti-noise ability."}, {"label": 0, "content": "With large-scale distributed generators (DGs) penetrated into the active distribution network (ADN), conventional load flow convergence failure is incurred by heavy power transmission. The Holomorphic Embedding Load Flow Method (HELM) has proven to be more robust than the Newton-Raphson method under heavy power transmission and is not sensitive to the initial points. At present, HELM is mainly designed for balanced transmission networks. In this study, we developed a three-phase HELM model to accommodate DGs, delta connection loads, and ZIP loads for ADN. The effectiveness and better performance of the proposed method under heavy load situations were validated using modified unbalanced IEEE 13, 34, 37, and 123 test feeders."}, {"label": 0, "content": "We propose a top-down approach for formation control of heterogeneous multiagent systems, based on the method of eigenstructure assignment. Given the problem of achieving scalable formations on the plane, our approach globally computes a state feedback control that assigns desired closed-loop eigenvalues/eigenvectors. We characterize the relation between the eigenvalues/eigenvectors and the resulting interagent communication topology, and design special (sparse) topologies such that the synthesized control may be implemented locally by the individual agents. Moreover, we present a hierarchical synthesis procedure that significantly improves computational efficiency. Finally, we extend the proposed approach to achieve fixed-size formation and circular motion, and illustrate these results by simulation examples."}, {"label": 0, "content": "Blockchain technologies, such as smart contracts, present a unique interface for machine-to-machine communication that provides a secure, append-only record that can be shared without trust and without a central administrator. We study the possibilities and limitations of using smart contracts for machine-to-machine communication by designing, implementing, and evaluating AGasP, an application for automated gasoline purchases. We find that using smart contracts allows us to directly address the challenges of transparency, longevity, and trust in IoT applications. However, real-world applications using smart contracts must address their important trade-offs, such as performance, privacy, and the challenge of ensuring they are written correctly."}, {"label": 0, "content": "The conception of Cyber-Physical Systems is a complex task: the multiple components making up those systems might not be fully known by the system architect, and putting those components together generates a new source of complexity. Study and validation of those systems is often done through simulation. Moreover, the CPS simulation is often studied through distributed simulation, as the CPS might itself be distributed or too complex. We present a methodology to distribute a simulation model in order to take full advantage of multiple processing units. We ensure that said distribution does not impact the simulation of our modeled system."}, {"label": 0, "content": "As fog computing brings processing and storage resources to the edge of the network, there is an increasing need of automated placement (i.e., host selection) to deploy distributed applications. Such a placement must conform to applications' resource requirements in a heterogeneous fog infrastructure, and deal with the complexity brought by Internet of Things (IoT) applications tied to sensors and actuators. This paper presents four heuristics to address the problem of placing distributed IoT applications in the fog. By combining proposed heuristics, our approach is able to deal with large scale problems, and to efficiently make placement decisions fitting the objective: minimizing placed applications' average response time. The proposed approach is validated through comparative simulation of different heuristic combinations with varying sizes of infrastructures and applications."}, {"label": 1, "content": "This paper presents a study on the differentially private problem of the average consensus for multi-agent network systems (MANSs) in discrete-time. A new distributed differentially private consensus algorithm (DPCA) is proposed based on the MANSs, which adopts an intermittent communication strategy dependent on an event-triggered function to avoid continuous communication between neighboring agents. The paper provides a detailed analysis of the algorithm's convergence, accuracy, privacy, and the trade-off between accuracy and privacy level. The algorithm is found to preserve the privacy of the initial states of all agents during the consensus computation process. The trade-off analysis identifies the best achievable accuracy of the algorithm under free parameters and fixed privacy level. Numerical experiments are conducted to validate the theoretical analysis, and the results demonstrate the validity of the proposed algorithm."}, {"label": 1, "content": "There are numerous color spaces with different properties in literature. To find the appropriate and relevant color space for the fabric defect classification problem, we propose investigating the performance and robustness of the Local Binary Pattern (LBP) descriptor in a supervised context, using an SVM classifier. Our experimental results demonstrate that the luminance-chrominance spaces are suitable for coding fabric defects, with a classification accuracy of 92.1%."}, {"label": 1, "content": "In this study, we focus on minimizing a block separable convex function that includes constraints and Laplacian regularization. This problem arises in various applications, such as model fitting, regularizing stratified models, and multi-period portfolio optimization. Our goal is to develop a distributed majorization-minimization method for this general problem and provide a general proof of convergence.\n\nOur approach is highly scalable, which makes it suitable for handling large datasets. To demonstrate the effectiveness of our method, we present two applications where we apply our approach to solve large optimization problems accurately.\n\nOverall, our study provides a comprehensive and straightforward method to minimize block separable convex functions with constraints and Laplacian regularization. Our method is highly effective and can handle large-scale problems efficiently, making it an essential tool for researchers and practitioners dealing with similar optimization problems."}, {"label": 1, "content": "Traditional intelligent diagnosis methods and current popular deep learning-based diagnosis methods typically utilize batch learning, which can result in wasted time and computing resources due to the need to discard the previously learned model and retrain a new one based on newly added data and prior data. Furthermore, manual feature extraction is often required for intelligent diagnosis, which heavily relies on prior knowledge. To address these issues, this paper proposes a fault diagnosis method that utilizes class incremental learning and eliminates the need for manual feature extraction. \n\nThe proposed method is based on denoising autoencoder and obtains the autoencoders using the raw data acquired for each health state. During the class incremental learning process, only the autoencoder of the new health state needs to be trained, while the previously trained autoencoders are retained. Test data is classified according to the minimal reconstruction error calculated through the autoencoders. \n\nThis paper validates the proposed method through the vibration data of rolling bearings for a rail vehicle. The experimental results demonstrate the effectiveness of the presented method."}, {"label": 1, "content": "With the deployment of significant HVDC lines, the likelihood of cascading failures has increased significantly. As a result, the propagation mechanism of cascading failures in hybrid AC/DC systems has been investigated, and a complete search model for cascading failure chains in AC/DC systems in large-scale power grids is established. A two-step approach to rapidly evaluate AC faults to DC has been suggested. First, the short circuit ratio of the system following the occurrence of the failure chain is calculated, and the worst-case failure chain is identified. Then, time domain simulation is used to verify if it will cause a DC block. For non-extreme accident chains, the fault's magnitude is assessed based on the area where the voltage after the fault exceeds the normal voltage value. A Levenberg-Marquardt neural network is used to compute the voltage dips degree swiftly, followed by a time domain simulation to examine the severe fault. To evaluate the line failure probability index and the line failure risk index, the model defines these two indices. After the occurrence of the accident chain, a pruning search is executed based on the line outage risk at each level. This method reduces the search space, ensuring accuracy and improving search efficiency. Finally, simulations for the Shandong power grid in China confirmed the effectiveness of the approach for discovering high-risk cascading failures chains in AC-DC Hybrid Power Systems."}, {"label": 0, "content": "Fog computing provides a paradigm for executing Internet of Things services. Enabling the coordinated cooperation among computational, storage, and networking resources in the fog can be challenging due to the volatility of resources. For this reason, we design an architecture and implement a representative framework called FogFrame that defines the necessary communication mechanisms for instantiating and maintaining service execution in the fog. To evaluate our approach, we conduct a series of experiments that show how service placement, deployment, and execution is performed by the framework, and how the framework operates at runtime, i.e., adapts to changes in the available resources, balances the workload and recovers from resource failures and overloads."}, {"label": 1, "content": "In this letter, we propose a sensor localization technique for partially calibrated arrays that are highly deformed and have multiple moving targets. Our method involves dividing the deformed array into several subarrays. The first subarray comprises the pre-calibrated sensors, while the sensors in the other subarrays are uncalibrated. We estimate the positions of the sensors by analyzing the phase differences between the pre-calibrated and uncalibrated sensors. To solve the phase ambiguities caused by the highly deformed sensor positions, we use subspace orthogonality and the movement of the multiple targets. We evaluate the performance of our proposed method through simulation results and compare it against the Cramer-Rao bounds."}, {"label": 1, "content": "PV power generation has a number of disadvantages, namely, periodicity, volatility and the like. When it comes to connecting with the power grid, it can have a negative impact on the power system. That's why accurately predicting the power of the PV power generation system is vital. In fact, the more similar the weather conditions, the more similar the photovoltaic power generation system. To address this issue, this research aims to introduce a novel PV Generation Power Prediction model that is based on GA-BP Neural Network with Artificial Classification of History Day. Initially, we classify historical weather data manually. Then, we develop different PV Generation Power Prediction models for each specific weather condition utilizing BP neural network and genetic algorithm (GA-BP neural network). Ultimately, we predict the power of the next day using GA-BP neural network tailored for the weather condition of the following day."}, {"label": 0, "content": "In this article, we study the problem of anti-jamming through channel selection in fading environment. Different from the most existing works which ignored the fading characteristic of the channels, we use a Markov channel model to capture the influence of variable channel transmission rate in fading environment. Then, we model the anti-jamming channel selection problem as a Markov decision process. On this basis, an online reinforcement learning algorithm (Q-learning) is proposed to select the optimal channel intelligently. Simulation results show that the proposed algorithm outperforms the sensing algorithm. The reason is that it can learn both the pattern of jamming signal and the condition of channel, so as to select the channel with better condition for data transmission. Based on the proposed algorithm, we develop a real-life dynamic spectrum anti-jamming testbed based on the USRP platform in the indoor environment to demonstrate the effectiveness of the algorithm."}, {"label": 0, "content": "Degradation modeling plays a key role in accelerated degradation test data analysis and condition-based maintenance. The degradation rate of a degradation process usually depends on both unit's age and its state. Several agestate-dependent degradation models have been proposed in the literature. A main drawback of these models is their intractability. In this paper, we propose an analytically tractable age-state-dependent degradation model. The proposed model is defined in terms of degradation rate, whose mean function is represented by a bivariate power-law model. The degradation increment is modeled by a non-homogeneous gamma process, whose mean function depends on both age and degradation level and has a closed-form expression. The model includes age-and state-dependent models as its special cases. An approach is proposed to make selection among the general case and special cases based on the Akaike information criterion. A real-world example is used to illustrate the flexibility and usefulness of the proposed model and approach."}, {"label": 1, "content": "Effective hyperspectral unmixing (HU) is crucial for estimating the underlying materials' signatures and their spatial distributions from a given hyperspectral scene image. However, non-negligible endmember variability (EV) and outlier effects (OE) have made it challenging to obtain accurate results. While some state-of-the-art works have considered either EV or OE, none has considered both EV and OE simultaneously. Therefore, this paper proposes a novel HU algorithm called the variability/outlier-insensitive multi-convex unmixing (VOIMU) algorithm that can handle both EV and OE simultaneously.\n\nTo achieve this, the VOIMU algorithm formulates a nonconvex minimization problem using two appropriate regularizers, with the perturbed linear mixing model proposed by Thouvenin et al. used for modeling EV. OE is implicitly handled by applying a p quasi-norm to the data fitting with 0 < p < 1. The problem is then reformulated into a multi-convex problem that is solved using the block coordinate descent method. This algorithm guarantees to yield a stationary-point solution with convergence guarantee, and interesting information on potential outlier pixels.\n\nThe results of the simulation and experimental studies using real data demonstrate the efficacy and practical applicability of the VOIMU algorithm. By handling both EV and OE simultaneously, the VOIMU algorithm offers a robust and accurate solution for HU."}, {"label": 0, "content": "In this paper we propose a novel method for detecting adversarial examples by training a binary classifier with both origin data and saliency data. In the case of image classification model, saliency simply explain how the model make decisions by identifying significant pixels for prediction. A model shows wrong classification output always learns wrong features and shows wrong saliency as well. Our approach shows good performance on detecting adversarial perturbations. We quantitatively evaluate generalization ability of the detector, showing that detectors trained with strong adversaries perform well on weak adversaries."}, {"label": 1, "content": "The downscaling of CMOS technology has led to an increase in susceptibility of integrated circuits (ICs) to soft errors, which has become a significant concern. The absence of error-protection mechanisms could result in system malfunctions, making radiation-induced transient faults in combinational logic one of the most challenging issues. This paper proposes an efficient and accurate layout-based soft error rate (SER) estimation analysis for ICs, taking into account both single and multiple transient faults. The proposed SER estimator is based on Monte-Carlo simulations, coupled with a detailed grid analysis of the circuit layout to identify vulnerable areas of a circuit. Temperature is also considered, as it is one of the factors that impact the pulse width generated by the circuit. The results show that elevated temperature increases the SER due to the widening of the fault pulses. The simulation results obtained from the proposed framework for some of the ISCAS'89 benchmark circuits were compared to those obtained from SPICE, with a fairly good correlation observed."}, {"label": 0, "content": "DC voltage control for modular multilevel converter (MMC) not only needs to control the total DC voltage, but also must control the capacitor voltage balancing of each sub-module (SM). The sorting-based traditional capacitor voltage balancing control algorithm is simple and easy to implement, but it needs to sort the capacitor voltages of all SMs of the valve arm and has low calculation efficiency. In this paper, a novel capacitor voltage balancing control algorithm based on the dynamic tiered sorting is proposed. By dynamically layering all sub-modules (SMs) on the valve arm, the number of SMs participating in the sorting is greatly reduced. Finally, the proposed algorithm is applied to the electromagnetic transient program of ADPSS (Advanced Digital Power System Simulator, ADPSS), and the correctness and the effectiveness of the proposed algorithm are verified by the back-to-back MMC-HVDC simulation case. The results show that the proposed algorithm not only can achieve the same control effect as the traditional algorithm, maintain the stability of all SMs capacitor voltage on the valve arm, but also can greatly improve computational efficiency."}, {"label": 0, "content": "The PT low frequency nonlinear oscillation is easily excited by external disturbances in the distribution network with ungrounded neutral point, which can cause the PT fuse fuse, burning loss and even the explosion accident frequently. The mechanism of PT low frequency oscillation is analyzed theoretically, and the phenomenon and key factors are simulated based on the electromagnetic transient simulation program PSCAD. A new low frequency oscillation suppression method for PT is proposed, that is, the PT neutral point is grounded by low frequency surge suppressor. The effectiveness of the suppression method is verified by simulation, and the low frequency inrush current can be controlled within the safe range. The theoretical research and the new suppression method proposed in this paper can provide guidance for PT fault analysis and the formulation of countermeasures in distribution network."}, {"label": 0, "content": "A distributed energy management system for an interconnected multi-microgrid system is developed and tested. The distributed energy management system is formulated using the alternating direction method of multipliers (ADMM) and is implemented using the CVX platform of the MATLAB environment. In this work, microgrids are interconnected and communicate with each other in order to minimize the operational cost of the system and to derive maximum profits via energy exchanges with the main grid. Simulation results demonstrate the effectiveness of the proposed method."}, {"label": 0, "content": "Cloud computing and its computing as a utility paradigm offers on-demand resources, enabling its users to seamlessly adapt applications to the current demand. With its (virtually) unlimited elasticity, managing deployed applications becomes more and more complex raising the need for automation. Such autonomous systems leverage the importance to constantly monitor and analyse the deployed workload and the underlying infrastructure serving as knowledge-base for deriving corrective actions like scaling. Existing monitoring solutions, however are not designed to cope with a frequently changing topology. We propose a monitoring and event processing framework following a model-driven approach, that allows users to express i) the monitoring demand by directly referencing entities of the deployment context, ii) aggregate the monitoring data using mathematical expressions, iii) trigger and process events based on the monitoring data and finally iv) attach scalability rules to those events. We accompany the modelling language with a monitoring orchestration and distributed complex event processing framework, capable of enacting the model in a frequently changing multi-cloud infrastructure, considering cloud-specific aspects like communication costs."}, {"label": 0, "content": "In this paper the authors would discuss about the enhancement in the existing work done on the Internet of Things (IoT) with reference to the Indian farmers. Here the major discussion would be related to the soil testing and its constituents required for the growth of crops. The major achievement of the paper would be the action/process to be followed post soil testing of the region. Soil testing would give the data and parameters about the soil which usually comprise of soil nutrients like phosphorous, potassium and nitrogen. The work done in this regard is that after the collection of soil data, these data would be entered in the developed software application which would tell the user which crop to grow in that soil. These data about the soil would already be preloaded in the software application and further in addition to current existing temperature and weather conditions (which would be continuously monitored through the sensors) it would recommend the suitable crop to be grown in that specific soil. The authors would discuss about the Application Program Interface (API) systems software which has been developed with reference to this application. The developed software is supported through mobile app for easy access. This app is in use and is a free source for the researchers who are working in soil testing areas or IoT for agriculture application. The authors in paper are also trying to use the above solution to help Indian farmers industrialize the individual farming using the Internet of Things (IoT) and connecting directly with market with the help of one centralized mobile app."}, {"label": 0, "content": "Many algorithms in workflow scheduling and resource provisioning rely on the performance estimation of tasks to produce a scheduling plan. A profiler that is capable of modeling the execution of tasks and predicting their runtime accurately, therefore, becomes an essential part of any Workflow Management System (WMS). With the emergence of multi-tenant Workflow as a Service (WaaS) platforms that use clouds for deploying scientific workflows, task runtime prediction becomes more challenging because it requires the processing of a significant amount of data in a near real-time scenario while dealing with the performance variability of cloud resources. Hence, relying on methods such as profiling tasks' execution data using basic statistical description (e.g., mean, standard deviation) or batch offline regression techniques to estimate the runtime may not be suitable for such environments. In this paper, we propose an online incremental learning approach to predict the runtime of tasks in scientific workflows in clouds. To improve the performance of the predictions, we harness fine-grained resources monitoring data in the form of time-series records of CPU utilization, memory usage, and I/O activities that are reflecting the unique characteristics of a task's execution. We compare our solution to a state-of-the-art approach that exploits the resources monitoring data based on regression machine learning technique. From our experiments, the proposed strategy improves the performance, in terms of the error, up to 29.89%, compared to the state-of-the-art solutions."}, {"label": 1, "content": "Iris segmentation is a crucial stage in iris recognition systems, as it separates the iris and non-iris components of the acquired image. This is the foundation of subsequent processing, and any errors in this stage can significantly impact the system's recognition rate. While most segmentation algorithms require significant user cooperation during image acquisition, the quality of the images cannot always be guaranteed. In this paper, we propose a more robust iris segmentation method using a fully convolutional network (FCN) with dilated convolutions.\n\nOur approach involves reducing the downsampling factor of the FCN model and using dilated convolutions to extract more global features, making our method better equipped to handle details. Additionally, our model supports end-to-end prediction, eliminating the need for pre-processing such as adjusting the image to a fixed size. We trained and tested our model on three datasets, including CASIA-iris-interval-v4, UBIRIS v2 and IITD Delhi.\n\nExperimental results demonstrate a significant reduction in the error rate of the current state-of-the-art methods on all three datasets, with our model achieving respective error rate reductions of 79%, 84% and 79%. Our method outperforms existing approaches and provides a more effective and robust solution for iris segmentation, even under non-ideal conditions."}, {"label": 0, "content": "In this paper, we propose a novel method to estimate the relative camera motions of three consecutive images. Given a set of point correspondences in three views, the proposed method determines the fundamental matrix representing the geometrical relationship between the first two views by using the eight-point algorithm. Then, by minimizing the proposed cost function with the fundamental matrix, the relative camera motions over three views are precisely estimated. The experimental results show that the proposed method outperforms the conventional two-view and three-view geometry-based method in terms of the accuracy."}, {"label": 1, "content": "The WTCM system plays a crucial role in enabling wind farm operators to utilize a condition-based O&M approach, which ultimately minimizes O&M costs while enhancing wind turbine reliability. The paper proposes an innovative WTCM method that exclusively relies on SCADA data and utilizes a data mining algorithm. The ARD algorithm is initially deployed to identify effective variables that impact the condition of wind turbines. The feature vector is then created using the effective variables to depict the operational state of the wind turbines. Finally, the outlier detection algorithm is implemented based on the feature vector to ascertain the condition of a wind turbine. The efficacy of the proposed method is established using real-world datasets. The analysis confirms that the proposed method has the capability of issuing advanced failure alarms days before the failure occurs. The implementation of the proposed WTCM method in a condition-based O&M strategy is shown to reduce O&M costs."}, {"label": 1, "content": "The use of distributed generators and non-linear loads can have a negative impact on power quality at the point where the microgrid connects with the low voltage distribution network. To ensure safe and stable operation of the microgrid, it is crucial to track and compensate for power quality issues caused by converters. However, the current research on power quality problems only focuses on pre-emptive and post-processing strategies, which are not enough to ensure continuous and stable operation.\n\nThis paper proposes an advanced converter with situation awareness and orientation functions, which can be implemented in three main aspects: awareness, decision-making, and execution. Firstly, situation awareness is achieved by establishing an information management system and optimizing the prediction algorithm to manage power quality-related elements. Secondly, the decision model of the grid-connected converter and optimal control are used to actively make decisions to solve power quality problems. Finally, the situation orientation of power quality problems is accomplished by establishing a smooth switching model of an adaptive control strategy.\n\nBy considering these three aspects, the proposed advanced converter provides a more effective approach to manage power quality problems in microgrid. With its comprehensive functionalities, it can ensure the continuous and stable operation of the microgrid, thereby contributing to a more sustainable and reliable power supply network."}, {"label": 0, "content": "Apache Mesos, a two-level resource scheduler, provides resource sharing across multiple users in a multi-tenant clustered environment. Computational resources (i.e., CPU, memory, disk, etc.) are distributed according to the Dominant Resource Fairness (DRF) policy. Mesos frameworks (users) receive resources based on their current usage and are responsible for scheduling their tasks within the allocation. We have observed that multiple frameworks can cause fairness imbalance in a multi-user environment. For example, a greedy framework consuming more than its fair share of resources can deny resource fairness to others. The user with the least Dominant Share is considered first by the DRF module to get its resource allocation. However, the default DRF implementation, in Apache Mesos' Master allocation module, does not consider the overall resource demands of the tasks in the queue for each user/framework. This lack of awareness can lead to poor performance as users without any pending task may receive more resource offers, and users with a queue of pending tasks can starve due to their high dominant shares. In a multi-tenant environment, the characteristics of frameworks and workloads must be understood by cluster managers to be able to define fairness based on not only resource share but also resource demand and queue wait time. We have developed a policy driven queue manager, Tromino, for an Apache Mesos cluster where tasks for individual frameworks can be scheduled based on each framework's overall resource demands and current resource consumption. Dominant Share and demand awareness of Tromino and scheduling based on these attributes can reduce (1) the impact of unfairness due to a framework specific configuration, and (2) unfair waiting time due to higher resource demand in a pending task queue. In the best case, Tromino can significantly reduce the average waiting time of a framework by using the proposed Demand-DRF aware policy."}, {"label": 0, "content": "This paper proposes an artificial neural network based approach to implement electric demand response (DR) programs. In order to maintain the continuous electricity supply, the total generated power must meet the load demand with spinning reserves for unforeseen contingencies. If a power system fails to satisfy all or part of its loads, then load curtailments cannot be avoided. However, load curtailments can be reduced or even avoided by implementing DR programs. The effectiveness of DR programs to reduce/avoid load curtailments depends on efficient and accurate identification of appropriate hourly loads for DR programs. In this work, a feed-forward artificial neural network approach is used to classify system hourly loads based on customers' potential participation in DR programs as well as to identify the effective periods of participation. In addition, a mathematical model based on the demand-price elasticity, incentives, and penalties is developed to calculate interruptible/curtailable loads for DR programs. The proposed method is demonstrated on the IEEE reliability test system. The results of the case study show that the proposed method is effective in identifying appropriate hourly loads to implement DR programs."}, {"label": 1, "content": "PD pattern recognition is a crucial aspect of PD-based condition monitoring and diagnosis of high voltage cables. Various techniques such as Support Vector Machine (SVM), Back-propagation Neural Network (BPNN), and Deep Learning have been developed and utilized for PD pattern recognition. However, one of the most significant challenges encountered in this regards is a lack of adequate training samples that restricts the accuracy of pattern recognition.\n\nTo overcome this challenge, a research paper proposes two methods based on data augmentation techniques. The Variable Noise Superposition (VNS) and Generative Adversarial Network (GAN) methods were evaluated using 1500 sets of experimental PD data along with Support Vector Machine (SVM), Logical Regression (LR), and Random Forest (RF) pattern recognition methods. The results demonstrate that the PD pattern recognition accuracy using the VNS data augmentation method is improved by 0.99% and 0.96%, evaluated with SVM and RF, respectively. Similarly, the PD pattern recognition accuracy using the GAN data augmentation method is improved by 0.52% and 1.72%, evaluated with SVM and LR, respectively.\n\nThe research concludes that both methods (VNS and GAN) are efficient for PD data augmentation and can be utilized for PD pattern recognition in HV cables and other HV equipment, particularly for pattern recognition techniques requiring significant training samples."}, {"label": 1, "content": "Wireless sensor networks are vulnerable to report fabrication attacks, in which an adversary can use compromised nodes to flood the network with false reports. En-route filtering is a solution to drop bogus/false reports while they are being forwarded towards the sink. Many of the existing en-route filtering schemes are probabilistic in nature, meaning that the originality of forwarded reports is checked with fixed probability by intermediate nodes. This system presents the problem of false reports travelling multiple hops before being dropped. \n\nAdditionally, deterministic based en-route filtering schemes which have been proposed require fixed paths for reports to be sent through. To address these issues, a novel deterministic en-route filtering scheme has been suggested that allocates secret keys to sensor nodes based on a combinatorial design. The design facilitates direct communication between any two nodes without creating additional key storage overhead. The proposed en-route filtering scheme has been extensively analyzed and is found to be superior to existing schemes in terms of expected filtering position of false reports. \n\nFurthermore, the scheme is more durable against selective forwarding and report disruption attacks. Additionally, it performs comparably with existing schemes in terms of protocol overheads."}, {"label": 1, "content": "With the increasing demands for power supply quality due to industrialization, ensuring normal electricity supply has become a pressing issue for power maintenance personnel. Therefore, this paper proposes a cable tunnel inspection system that locates cable connectors in images and maps them to infrared images using the convolution neural network method. Analyzing the temperature of cable connectors over time, this monitoring system triggers an alarm in case of abnormalities, thus maintaining power supply safety and extending the cable lifespan. To address the relatively small number of sample images collected, a transfer learning method is utilized, which reduces training intensity while guaranteeing improved positioning and recognition effectiveness."}, {"label": 0, "content": "There is a numerous color spaces with different properties in literature. In order to find the appropriate and relevant color space for the fabric defect classification problem, we propose to investigate the performance and robustness of the Local Binary Pattern (LBP) descriptor in supervised context by using SVM classifier. The experimental results show that the luminance-chrominance spaces are suitable for coding fabric defect with the classification accuracy obtained is 92.1%."}, {"label": 0, "content": "Overall Equipment Efficiency (OEE) is applied to measure the actual production capacity of equipment, and Theory of Constraints (TOC) is adopted to improve the system production efficiency. In order to obtain the OEE improvement method based on TOC, bottleneck identification model and buffer model are established. Thus, a multi-attribute bottleneck identification model is constructed based on Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) and Entropy Method. In addition, a time buffer model based on the Drum-Buffer-Rope (DBR) theory is proposed. This OEE improvement method can significantly increase OEE of bottleneck equipment. Moreover, due to the optimization of bottlenecks, the system production efficiency is improved. The effectiveness of this method is also verified in a semiconductor package process."}, {"label": 1, "content": "In satellite-borne Terahertz wave ground detection, accurately estimating the atmospheric absorption attenuation loss of Terahertz waves is crucial for various Terahertz communication modes, including wideband and high-speed networks, interstellar communication, satellite-ground station links, stratosphere aerocraft communication, long distance data transfer, and short-range wireless communication. To address this, this paper introduces an atmospheric absorption loss estimation software for satellite global THz wave ground detection. The software offers key features such as scene establishment, fundamental functions, and calculation methods, all of which are explained in detail. Lastly, the monthly change calculation results of satellite ground detection in the 0.34 THz working band and 10\u00b0\u201390\u00b0 down-looking angle are presented."}, {"label": 0, "content": "Deployment of distributed energy resources (DERs) introduces more dynamics to the distribution system, and brings system operators more challenges in monitoring and controlling the distribution system. These challenges can be addressed by developing methods for real time extraction of the operating state and model of the system, that is advanced state estimators for distribution systems with high penetration of DERs. This paper presents an object-oriented Distributed Quasi-Dynamic State Estimator (DQDSE) that employs three-phase detailed models and enables data from sensors to be streamed and used by the DQDSE. Based on three-phase detailed quasi-dynamic models that track slow dynamics (e.g., controls of power electronics, electromechanical transients of motors, etc.), DQDSE forms network measurement model by using network-wise measurements, performs quasi-dynamic state estimation, and provides the best estimate of the distribution system states. The proposed DQDSE has the following advantages: (1) detailed modeling approach ensures accurate results even when accommodating unbalanced and asymmetric systems; (2) the measurement set of DQDSE contains measurements from sensors as well as other measurement types to increase redundancy; (3) the distributed architecture of DQDSE enables fast data processing. The paper presents the method via an illustrative example that substantiates the effectiveness of DQDSE."}, {"label": 1, "content": "Skin cancer is caused by the abnormal growth of skin cells that cannot be controlled. This condition occurs when UV radiation from the sun damages the DNA in skin cells, leading to mutations that cause the cells to grow rapidly and form a tumor that cannot be controlled. Detecting skin cancer early before it spreads or metastasizes is key to overcoming this condition. Unfortunately, many people are indifferent and reluctant to check or consult with doctors, which can make their condition worse without them realizing it.\n\nTo address this issue, an application for Skin Cancer Detection with Image Processing and Expert System using Forward Chaining and Certainty Factor method was designed. This system provides an accurate assessment of the risk level of nevus conditions in patients, categorized as High Risk, Low Risk, or Medium Risk. The ultimate goal of this system is to help raise awareness and encourage early detection of skin cancer.\n\nThe results of the study show that the skin cancer detection system is highly accurate, with a 100% accuracy rate. The system produces the same results as an expert, making it a reliable tool in detecting skin cancer early. With this innovative app, people can take control of their health and take early steps to overcome skin cancer."}, {"label": 0, "content": "A fast and accurate analytical inverter-fed induction machine iron loss calculation model is proposed in this paper. The proposed model takes account of the influence of the output voltage harmonics from the inverter on the iron loss of the motor based on the piecewise variable coefficients method. Additionally, the proposed method incorporates slot harmonic component's influence on the iron loss. The validity of the proposed model is verified by comparing its calculated core loss values with measured ones of a 5.5- and a 55-kW inverter-fed induction motors under different speeds and load conditions. Compared with the classical iron loss model and piecewise variable coefficient iron loss model based on the finite-element method, the proposed model can reduce the computational burden significantly with desirable accuracy."}, {"label": 1, "content": "The use of online platforms for real estate sale and rental is becoming more popular, and there is a growing need to automatically retrieve similar floor plans to assist architects and buyers. Existing research has explored sketch-based image retrieval, but far less work has been done on hand-drawn floor plan retrieval. In response, we propose a new framework named REXplore (Real Estate eXplore), which uses a sketch-based query mode to retrieve corresponding similar floor plan images from a repository. Our approach utilizes Cyclic Generative Adversarial Networks (Cyclic GAN), which map between the sketch and image domains. The combination of Cyclic GANs and Convolution Neural Networks (CNNs) allows for hand-drawn floor plan image retrieval. Our framework offers several contributions, including a novel sketch-based floor plan retrieval system utilizing an intuitive and convenient sketch query mode, and a new approach to hand-drawn floor plan image retrieval, which has extensive experimental results and comparisons with baseline methods to validate our claims."}, {"label": 1, "content": "With cloud and fog computing becoming increasingly popular and wireless IoT terminals having limited computing capability, big data is often sent to clouds/fogs for processing and analysis. However, ensuring security and privacy while carrying out tensor analysis and processing in cloud/fog-based wireless IoT applications is a major challenge. Tensors are powerful tools for multi-dimensional data analysis and processing in wireless IoT applications. To address this challenge, we propose novel privacy-preserving tensor analysis and processing models in cloud/fog computing for wireless IoT applications.\n\nWe discuss a privacy-preserving cyber-physical-social big data processing model in cloud and present privacy-preserving tensor analysis models based on tensor train networks in cloud-fog computing. Furthermore, we introduce an optimization model for privacy-preserving tensor analysis and processing. We demonstrate the security and effectiveness of the proposed models by implementing a social recommendation system in smartphones.\n\nOverall, our proposed models enable effective tensor analysis and processing while preserving privacy and security in cloud/fog-based wireless IoT applications."}, {"label": 1, "content": "This paper presents Andes, a Python-based software package designed specifically for cyber-physical power system research. Andes aims to address the emerging need for cybersecurity studies in power system analysis by providing an integrated solution for power grid prototyping, communication network setup, and interactions between the two. The architecture design of Andes is discussed in detail, with a focus on research and development considerations. Various examples are presented to demonstrate the capabilities of Andes in modeling, monitoring, and visualizing cyber-physical power system studies. Overall, Andes represents a valuable tool for researchers working in the field of power system analysis, and its flexible design allows for future extension to new research areas."}, {"label": 1, "content": "In this article, we presented a highly effective method for recognizing facial expressions that is based on a combination of G-2DPCA feature extraction and Sparse Representation-based Classification (SRC). We began by applying Gabor filters with five different scales and eight different directions to extract features from facial images. To deal with the high dimensionality of these features, we selected the optimal filter with the best parameter pair of scales and directions to filter the images. Next, we utilized two-dimensional principal component analysis (2DPCA) to represent the images and reduce the dimensionality of the data. This method preserves the 2D geometric structure of the image, eliminating the need to transform the image matrix into a vector and significantly reducing computation time. Ultimately, the Gabor plus 2DPCA (G-2PCA) features were used as the atoms of the dictionary in SRC. The experimental results show that our proposed method outperforms existing facial expression recognition algorithms."}, {"label": 0, "content": "In this paper, a graph computation based power flow algorithm is introduced to solve large-scale AC/DC hybrid systems with multi LCC (Line Commuter Converter) based DC grids. The proposed approach is to improve the computational efficiency of constructing related matrices and getting power flow results without changing the conventional sequential iteration method. The hybrid system is modeled as a graph of vertices and edges with info of topology and parameters, thus local computation could be done independently for like formulating mismatch vectors of \u0394P, \u0394Q, \u0394\u03b8, \u0394V and matrices of B', B\", only with parameters on its linked edges and adjacent vertices. Then by taking advantages of hierarchical parallel computing, the FDPF (fast decoupled power flow) of AC system, calculation of DC grids and mismatch comparing for each iteration of the sequential method could be done parallelly. This method is implemented on a graph database platform, and tested on IEEE 300-Bus, modified South Carolina 500-Bus system and a Chinese system to verify the accuracy and time-saving performance."}, {"label": 1, "content": "The majority of wireless communication systems today rely on training-based signaling, where accurate channel estimates are necessary for detecting transmitted symbols. However, this method requires specific hardware and algorithms and takes up a significant portion of the channel coherence interval, which is often limited in vehicular communications. Fortunately, there is a solution - noncoherent signaling schemes - that can help alleviate the drawbacks of training. One such scheme is the Grassmannian signaling scheme, which is ideal for high signal-to-noise ratio (SNR) communication over noncoherent block-fading channels, and has the potential to become a critical component of future networks."}, {"label": 1, "content": "This paper proposes a robust control strategy for quadrotor helicopters using deep reinforcement learning. The strategy involves a neural network that directly maps system states to control commands for the quadrotor in an end-to-end manner. The learning algorithm is developed based on the deterministic policy gradient algorithm, and an integral compensator is incorporated into the actor-critic structure to enhance tracking accuracy and robustness. The proposed approach includes an offline and online learning phase, where an offline policy is first learned based on a simplified quadrotor model, and the policy is then optimized online during actual flight. The proposed approach is evaluated in a flight simulator, and the results demonstrate that the offline learned policy is highly robust to model errors and external disturbances. Additionally, the online learning phase significantly improves the control performance."}, {"label": 0, "content": "In this paper cubic splines are designed as tone correction functions applied to the achromatic component of a spherical color model. Compared with the commonly used color models HSV and HSL, an advantage of the spherical color model is that color changes more perceptually smoothly along its coordinates. However, the spherical color model contains more color points than what can be displayed with the RGB color model, so a tone correction function might cause a gamut issue. The paper demonstrates the gamut issue can be avoided when tone correction functions are well designed and the general tone correction techniques still work well in the spherical color model. A particular type of cubic splines is designed to serve the purpose, and these splines can be adopted by the general tone correction techniques including those for low-key, middle-key and high-key images with correspondingly selected parameters. Experimental results demonstrate that these cubic splines work well for tone correction under the spherical color model."}, {"label": 0, "content": "At present, as the residential electricity power consumption growth rate is far higher than that of industrial and agricultural electricity power consumption, optimize the residential electricity power consumption behavior is of great significance to reduce the power grid peak-valley gap and comprehensive energy loss. To study the non-intrusive load identification technology, analyze residential electricity usage details and provide electricity usage suggestion to residents is of paramount importance. In this paper, the non-invasive load identification technology is deeply studied and the layered classification algorithm based on multidimensional load characteristic matching is proposed. On the basis of State Grid single-phase carrier intelligent electricity meter, the intelligent power meter with residential load identification function is developed. In order to verify the load identification function of smart electricity meters, the test environment was set up in the laboratory. Test results displayed that the identification accuracy of the electric appliance type can be 100%. In addition to the extremely complicated working conditions, the power consumption identification accuracy of smart electricity meters is more than 90%."}, {"label": 0, "content": "There have been many varieties of driving assistance, and one aspect of them is the scope of emergency braking. Several researches have been analyzing emergency braking and proposed approaches to detect them. A focused but significant case is mistaken pedal pressing during emergency braking, which occurs when accelerator pedal is pressed instead of brake pedal. This paper aims to evolve a classifier to recognize mistaken pedal pressing based on behavior shown during pressing the pedals by using evolutionary computation. A driving simulator is used to collect the data, and genetic programming was used to perform the evolution."}, {"label": 0, "content": "This prototype system known as the Wearable Instantaneous Ball Speed Estimator (WIBASE) was designed to measure the bowling speed of a cricketer during training. When fast bowlers are training, coaches have to assess their ability to bowl consistently fast balls even when they are required to perform long bowling spells, hence the need for reliable, accessible and affordable equipment for measuring their bowling speed cannot be over emphasised. The WIBASE seeks to fill in this gap. It is made up of two hardware components; a computer and a wrist-worn electronic board that houses among other components, a 3-dimensional (3D) acceleration sensor. The system tracks the three-axis acceleration generated by the movement of the arm when delivering the ball and stores these values. The raw sensor data from three different sensors namely accelerometer, gyroscope and magnetometer is processed by a Digital Motion Processor (DMP) on the board in a process known as Sensor Fusion before it is sent via Bluetooth to the computer. The computer runs a Python script that receives the filtered acceleration which consists of both static acceleration and dynamic acceleration. The acceleration is numerically integrated over a minute period of time around the release point using the Trapezoidal method of integrating numerical data to derive the speed of the cricket bowler. The results obtained from the three sets of experiments that were conducted show that the WIBASE can track the 3D acceleration of the hand when bowling, derive the speed of the bowlers and display the speed on a computer while logging all the data into a file."}, {"label": 0, "content": "This work considers the problem of diagnostics and control of technical state (TS) of various industrial equipment. The relevance of the problem is caused by a significant increase in the level of requirements for the reliability of industrial facilities, the aging of existing equipment and technological complexes and thus the complexity of systematizing of various diagnostic information for decision-making. Because of this there is a need to develop a universal automated information system based on the method of diagnostics of the TS of a variety of industrial objects. Due to the multitude of factors affecting the TS of the equipment, the technical state index (TSI) is used as a complex indicator, the calculation method of which is based on the mathematical model of fuzzy sets. The diagnostics of TS is carried out based on a system of indicators, the configuration of which is related to the structure of the equipment. This work presents the results of developing a decision support system for managing complex technical and software objects. The work also shows a block diagram of the algorithm of how this system functions, describes the functionality and interface of the developed software."}, {"label": 1, "content": "Diseases such as positional obstructive sleep apnea and pressure sores are closely linked to the body postures a person assumes while lying in bed. Therefore, accurately estimating these postures is crucial, both for diagnosing such diseases and for identifying associated risk factors. To achieve this, we have proposed a new classification system that employs unconstrained measurements of ballistocardiogram (BCG) signals.\n\nFor this system, we embedded a flexible piezo-electric polymer film sensor into a mattress in order to collect BCG data. We then utilized amplitude features based on the morphology of the BCG waveform, applying a Bayesian classifier with piecewise smoothing correction to accurately classify four common body postures (i.e. supine, left lateral, prone, and right lateral) on the bed. We tested this system with 12 healthy individuals, and found that its final average prediction accuracies for these four postures all exceeded 97%. Additionally, a list of kappa coefficients calculated from classification results across subjects demonstrated almost perfect agreement.\n\nOverall, this innovative system represents a significant step forward in the unobtrusive and precise monitoring of body posture in our daily lives. Its potential impact on the prevention and treatment of diseases associated with incorrect sleep postures cannot be overstated."}, {"label": 0, "content": "Interface design flaws are often at the root cause of use errors in medical devices. Medical incidents are seldom reported, thus hindering the understanding of the incident contributing factors. Moreover, when dealing with a use error, both novices and expert users often blame themselves for insufficient knowledge rather than acknowledge deficiencies in the device. Simulation-Based Medical Education (SBME) platforms can provide appropriate training to professionals, especially if the right incentives to keep training are in place. In this paper, we present a new SBME, particularly targeted at training interaction with medical devices such as ventilators and infusion pumps. Our SBME functions as a game mode of the PVSio-web, a graphical environment for design, evaluation, and simulation of interactive (human-computer) systems. An analytical evaluation of our current implementation is provided, by comparing the features on our SBME with a set of requirements for game-based medical simulators retrieved from the literature. By being developed in a free, open source platform, our SBME is highly accessible and can be easily adapted to specific use cases, such a specific hospital with a defined set of medical devices."}, {"label": 1, "content": "In simulations of voltage source converter (VSC)-based DC grids, the use of fast protection schemes for overhead lines, such as traveling wave protection, requires a small time step when simulating high voltage direct current (HVDC) circuit breakers. This paper presents a new method to address this issue and accurately simulate protection processes and realize hardware-in-the-loop (HIL) simulation. The proposed method involves modeling the HVDC circuit breaker using the transmission line modeling method (TLM) to solve the arrester and constant impedance model of the switch. This results in the admittance matrix of the HVDC breaker remaining constant, which greatly reduces computing time. To demonstrate the effectiveness of this new method, a simplified HVDC breaker is used and a test circuit is implemented on a field programmable gate array (FPGA) board to obtain efficient and accurate simulation results."}, {"label": 0, "content": "This paper introduces an object-based method based on a new statistical distance for SAR image change detection. Firstly, multi-temporal segmentation is carried out to segment two temporal SAR images simultaneously. It considers the homogeneity in two temporal images, and could generate homogeneous objects in spectral, spatial and temporal. In addition, through setting different segmentation parameters, the multi-temporal images can be segmented in a set of scales. This process exploits the advantages of OBIA that could effectively reduce spurious changes, and considers the scale of change detection task. Secondly, a multiplicative noise model called Nakagami-Rayleigh distribution is employed to describe SAR data, and then applied to Bayesian formulation. Thus, a new statistical distance that is insensitive to speckles is derived to measure the distances between pairs of parcels. Then, cluster ensemble algorithm is utilized to improve accuracy of individual result in each scale to obtain the final change detection map. Finally, multi-temporal Radarsat-2 images are employed to verify the effectiveness of the proposed method compared with other four methods."}, {"label": 0, "content": "Multispectral image change detection based on deep learning generally needs a large amount of training data. However, it is difficult and expensive to mark a large amount of labeled data. To deal with this problem, we propose a generative discriminatory classified network (GDCN) for multispectral image change detection, in which labeled data, unlabeled data, and new fake data generated by generative adversarial networks are used. The GDCN consists of a discriminatory classified network (DCN) and a generator. The DCN divides the input data into changed class, unchanged class, and extra class, i.e., fake class. The generator recovers the real data from input noises to provide additional training samples so as to boost the performance of the DCN. Finally, the bitemporal multispectral images are input to the DCN to get the final change map. Experimental results on the real multispectral imagery datasets demonstrate that the proposed GDCN trained by unlabeled data and a small amount of labeled data can achieve competitive performance compared with existing methods."}, {"label": 0, "content": "In this paper, we present a scheduling scheme for household Electric vehicles based on deep neural network based demand forecast. A novel clustering based Short Term Load Forecasting (STLF) using deep neural network (DNN) is presented in this paper to forecast the household and EV demand. The forecasting is performed on electricity demand profiles for 200 households from the Midwest region of the United States. Tensor-flow based deep learning platform was used to develop deep learning structure. The households are clustered according to demand profiles and the grouped consumers are used as the forecasting parameters. The scheduling model uses the forecasted household and EV demand values to develop a linear programming based optimization model to minimize the electricity cost for consumers. Household and cluster constraints are considered in the optimization model to limit the sudden surge in power demand during low-price time periods."}, {"label": 1, "content": "This paper introduces a flex-rigid soft robot that is capable of flipping locomotion. The robot has been designed in the shape of a strip and is comprised of three rigid limbs that are connected through two active flexible hinges. Its unique movement is achieved by the active folding and unfolding of these hinges. \n\nTo test the robot's locomotion ability, an experiment was conducted on level ground. Results demonstrated that the flex-rigid robot successfully performed flipping locomotion at an average velocity of 60 mm/s. \n\nOverall, the proposed robot presents a promising solution for achieving specialized forms of locomotion in soft robotics. Its design and experimental results open up doors for future research and development in this field."}, {"label": 1, "content": "This article presents an approach for verifying attributes in Attribute-Based Encryption schemes, specifically in the context of the Internet of Things environment. The aim is to make attribute verification more accessible for users and device owners by utilizing well-known authorization protocols such as Oauth2. The proposed solution involves the use of Authentication as a Service for authentication, with the Attribute Authority serving as the central authority. To demonstrate the feasibility of this approach, a web application was developed using the FIWARE platform, which offers authentication and authorization mechanisms via Oauth2. The application allows devices to create orders for ABE secret keys with specific attributes, which users can approve or deny. This application represents the first implementation of ABE algorithms within the FIWARE ecosystem."}, {"label": 1, "content": "Pose estimation of 3D objects in six degrees of freedom has been a topic of interest in the research community for quite some time now. The integration of cameras and robots in a system enables the estimation of an object's pose through the camera, enabling it to be accurately manipulated by the robot. Traditional object pose estimation methods include template matching-based and invariant feature-based methods. While the method based on invariant features performs well on images with rich texture, it is not suitable for texture-less parts, which are commonplace in industrial applications. On the other hand, the template-matching method based on edge and contour information is better suited for industrial part detection and pose estimation. LINEMOD, proposed by Hinterstoisser, is a well-known and successful template-matching method that utilizes a specially designed storage structure to accelerate the template matching process. However, the template-based matching method is generally computationally intensive and therefore impractical for robotic applications. \n\nThis paper presents a new method that combines Fully Convolutional Network (FCN) with LINEMOD algorithm. With this method, the detection and location of the object in the image can be quickly achieved, and the local image is used for LINEMOD template matching, instead of the entire image. Experimental results indicate that this approach results in increased pose estimation speed and consistent matching results when compared to the standard LINEMOD method."}, {"label": 0, "content": "This project employs the IoT with an intelligent event-driven system in order to realize an efficient quasi real-time attendance tracker. The idea is to keep the whole system in the standby mode except for the low power motion sensor. On the detection of an event, when a person enters and originates a motion, the front-end embedded processor is alarmed. Afterwards, it activates the remaining system modules like webcam, communication block, etc. The event-driven feature improves the system performance in terms of resources utilization and power consumption compared to the counter classical ones. A first system implementation is realized and successfully tested. It is based on a raspberry pi 3 board, which is integrated with two Passive Infrared (PIR) sensors and two webcams. On the occurrence of an event the webcam is activated and it captures an image. The image is recorded via the Raspberry Pi webcam server and is shared with other system modules via the Porta Space application, which acts as a hub between the Raspberry Pi and the cloud. Simultaneously the attendance status is updated via the IFTTT on the cloud-based log. Moreover, the concerned authorities are notified via an email. The process is repeated every time when a person enters or leaves the concerned place. The attendance log remains globally available via the cloud and can be accessed anytime. The system design flow is described. The devised system functionality is tested with an experimental setup. Results have confirmed a proper system operation."}, {"label": 0, "content": "The fundamental theory of energy networks in different energy forms is established following an in-depth analysis of the nature of energy for comprehensive energy utilization. Based on the physical theories of analytical mechanics, thermodynamics, heat transfer, fluid mechanics (fluid network), electromagnetic field theory (electric network), the generalized equations of energy transfer and transformation are established. Then the generalized lumped element models are derived, which include the generalized resistance, capacitor and inductance. To establish the equations of energy networks, the Kirchhoff's Law in electric networks is extended to energy networks, which is called the Generalized Kirchhoff's Law. The equations are finally unified into a complete energy network equation system. A numerical example is given to testify the effectiveness of energy network theory."}, {"label": 1, "content": "The emergence of the Fifth Generation (5G) mobile networks has garnered significant attention from various stakeholders worldwide. In recent years, prototyping 5G infrastructures and deploying 5G services have gained traction in academia, and towards realizing market-oriented 5G trials. However, accessing and programming on real-world 5G infrastructure is a considerable challenge for most 5G researchers, especially those in academia. It is crucial to build cost-effective yet realistic 5G infrastructure emulators for 5G research labs to enable credible research activities. \n\nThis paper proposes a 5G infrastructure emulator that can be used in a lab setting to emulate a realistic 5G network based on a small number of commercial-off-the-shelf servers. The emulator utilizes virtualization and other technologies, providing a service provider to deploy 5G services automatically from empty machines through advanced automation. The paper provides a detailed description of the emulation platform along with the 5G infrastructure and service deployment procedure featured. Results are also presented to show the performance of the proposed emulator. \n\nIn conclusion, the presented 5G infrastructure emulator provides a viable solution for researchers looking to conduct credible 5G research activities. It eliminates the high costs of accessing and programming on real-world 5G infrastructure. As such, this emulator could be a significant boost for the development and advancement of 5G networks."}, {"label": 0, "content": "The switchgear is the key equipment in the power system. The overheating of the switchgear contacts is the main cause of the failure of the switchgear. Since the switchgear is relatively closed so that the internal contacts cannot be directly measured, which need to push out the contact status through a portion that is easy to measure. Therefore, this paper uses the collected temperature of the infrared window to push out the contacts temperature based on the BP neural network to determine whether the switchgear contacts are faulty. Firstly, the ANSYS Workbench finite element software is used to simulate the switchgear and obtain the contacts temperature and infrared window measurement temperature as the BP neural network test data. Then, the BP neural network model is trained to determine the model parameters. Finally, the trained BP model is used to test and determine the accuracy of the fault diagnosis of the switchgear contacts based on the BP model. The experimental results show that the maximum error between the expected data and the predicted data based on the BP neural network model does not exceed 1. 5 \u00b0C, which is within the normal range. Therefore, based on this model, the operation and maintenance staff can acquire the contacts state of the switchgear by measuring the infrared window temperature. It is of great significance to the safe operation of the switchgear."}, {"label": 0, "content": "The rise of Bring Your Own Device (BYOD) now poses new challenges to the traditional intranet, which used to deploy boundary-based defenses to guarantee internal security. The bringing of personal devices has threatened the internal security. Based on the idea of isolation and dynamic, this paper designs and implements a Software-defined Intranet Dynamic Defense System (SIDD) to harass cyber kill chain. Firstly, to solve the issue that network can be easily reconnoitered due to its static attributes, we allocate virtual IP address space for intranet terminals and implement the dynamic mapping between real IP addresses and virtual IP addresses to hide the real IP address. Secondly, we propose a software-defined dynamic defense architecture scheme, which manages to provide a general control of the intranet, including three core modules (e.g. DNS, virtual & real address assignment and virtual address maneuvering). Finally, we implement a dynamic defense system oriented to the production environment, based on the OpenDaylight controller. Our experiments indicate that this method can achieve a definable IP address, which frequency and space are maneuverable, thus it could significantly reduce the availability of network reconnaissance and increase the difficulties of attacker's realtime attack without affecting network applications."}, {"label": 0, "content": "A class of nonlinear equation solvers known as the holomorphically embedding method, when applied to the power-flow problem is theoretically guaranteed to find an operable solution to the power-flow problem, if one exists, provided rather mild conditions are satisfied. To date, all of the published approaches use a Gauss-Seidel-based fixed-point form as the starting point for the embedding. We show that a fixed-point form based on a Newton-Raphson scheme may also be use and has some advantageous properties when attempting to find the saddle-node bifurcation point (SNBP)."}, {"label": 0, "content": "In recent years, great success has been achieved in visual object detection, which is one of the fundamental tasks in the field of industrial intelligence. Most of existing methods have been proposed to deal with single well-captured still images, while in practical robotic applications, due to nuisances, such as tiny scale, partial view, or occlusion, one still image may not contain enough information for object detection. However, an intelligent robot has the capability to adjust its viewpoint to get better images for detection. Therefore, active object detection becomes a very important perception strategy for intelligent robots. In this paper, by formulating active object detection as a sequential action decision process, a deep reinforcement learning framework is established to resolve it. Furthermore, a novel deep Q-learning network (DQN) with a dueling architecture is proposed, the network has two separate output channels, one predicts action type and the other predicts action range. By combining the two output channels, the action space is explored more efficiently. Several methods are extensively validated and the results show that the proposed one obtains the best results and predicts action in real time."}, {"label": 0, "content": "With the increasing penetration of distributed photovoltaic generation and energy storage systems in the demand side of the power system, new demand side model structures are necessary in order to better describe the dynamic performance of the power system. In this paper, a composite demand side model structure with load, distributed photovoltaic generation, and energy storage system together with a model parameter identification method are proposed to improve the traditional load model identification. The structure of the demand side model is proposed first and is further simplified so as to be identified at a high voltage level bus. The model parameter identifiability analysis is conducted based on the sensitivity method. The ambient signal data and disturbance data based model parameter identification method is proposed for the new demand side model structure using the differential evolution optimization method. The case study results for the WSCC 9 bus system show the effectiveness of the proposed model structure. Then, the case study results in a simplified 500-kV network of the Guangdong Power Grid show the effectiveness of the parameter identification method."}, {"label": 0, "content": "This paper analyzes the external meteorological factors that influencing galloping, a BP neural network learning algorithm is established by taking wind, inducted angle of wind direction and line, relative humidity, and ambient temperature as input vectors. The galloping probability is predicted by judging whether the prone-galloping weather conditions are satisfied utilizing the proposed method, and its prediction performance is assessed through several test indexes with the purpose of improvement. A case study is presented by adopting historical galloping data of Henan power grid, and the result shows that the proposed method is effective and practical, which can provide support for power system operation staffs to make reasonable decisions as well as ensure the power grid securely tiding over the peak-load during winter."}, {"label": 1, "content": "Fog computing aims to provide cloud computing capabilities at the network's edge, offering benefits such as lower latencies, improved mobility support, and location awareness to end-users. The combination of IoT and Fog computing creates a complex scenario with a vast amount of data and a varying number of devices that must collaborate. Fog computing networks can be designed as autonomous networks, and effective management and orchestration mechanisms are required to ensure that applications and services perform optimally, while still leveraging cloud capabilities. Traditional mechanisms used for \"cloud-only\" implementations are not suitable for the Fog due to its specific characteristics. Thus, new management and orchestration mechanisms for the Fog must be designed and developed. In this paper, we propose using finite state machines to enhance the decision-making process in an autonomous Fog computing network. The proposed method is expected to improve the network's autonomy, enhance performance, and reduce costs."}, {"label": 1, "content": "The use of customized wireless devices is crucial in the exchange of information among various electrical facilities. Unfortunately, the development of these devices has faced numerous challenges such as the diversity of interfaces, high costs, and outdoor environments. However, we have proposed a solution to these issues in the form of a \"system on module\" approach.\n\nWith this approach, we have optimized and saved storage space on the baseband chip for the embedded control system. This has allowed us to replace independent chips and significantly reduce the size, cost, and power consumption of the devices. Additionally, our SOM wireless devices can support multiple interfaces, enabling them to realize \"plug and play\" functionality.\n\nFurthermore, we have developed wireless devices that allow for multimode wireless communications, including operator's wireless networks such as LTE FDD/TDD, UMTS, GSM, and self-built LTE-G 1800MHz network. Finally, we have embedded positioning chip and a client for the management system into the wireless devices to simplify operation and maintenance.\n\nIn summary, our proposed 'system on module' solution offers a practical solution to the challenges facing the development of customized wireless devices for electrical facilities. With the ability to support multiple interfaces, multimode wireless communication and simplified management, our SOM wireless devices are indeed the future of wireless devices."}, {"label": 1, "content": "In order to diagnose intermittent faults, it is crucial to extract fault features. This can be achieved through a proposed method of intermittent fault feature extraction based on wavelet transform. The stress-intensity model is initially employed to depict the fault process of intermittent faults, thus analysing the primary part of intermittent fault signal characteristics. Given the sporadic and abrupt nature of intermittent faults, wavelet transform is utilized to extract intermittent fault features, utilising the advantages of wavelet transform in singularity detection and signal mutation. By deriving signal energy through the wavelet coefficients, intermittent fault features are attained, which can be used to both identify and isolate intermittent faults. Finally, a simulated circuit is utilised to obtain transient intermittent fault response characteristics, and wavelet transform is used to extract features for intermittent fault location. Our results illustrate that the wavelet transform can successfully extract intermittent fault features, which are applicable for intermittent fault diagnosis."}, {"label": 0, "content": "Phoneme lattices have been shown to be a good choice to encode in a compact way alternative decoding hypotheses from a speech recognition system. However the optimal phoneme sequence is produced by tracing all the phoneme identities in the lattice. This not only makes the search space of the decoder huge but also the final phoneme sequence may be prone to have false substitutions or insertion errors. In this paper, we introduce the split lattice structures that is generated by splitting the speech frames based on the manner of articulation. Spectral flatness measure (SFM) is exploited to detect the two broad manner of articulation sonorants and non-sonorants. The manner of sonorants includes broadly the vowels, the semivowels and the nasals whereas the fricatives, stop consonants and closures belong to non-sonorants. The conventional way of speech decoder produces one lattice for one test utterance. In our work, we split the speech frames into sonorants and non-sonorants based on SFM knowledge and generate split lattices. The split lattice generated are modified according to the manner of articulation in each split so as to remove the irrelevant phoneme identities in the lattice. For instance, the sonorant lattice is forced to exclude the non-sonorant phoneme identities and hence minimizing false substitutions or insertion errors. The proposed split lattice structure based on sonority detection decreased the phone error rates by nearly 0.9 % when evaluated on core TIMIT test corpus as compared to the conventional decoding involved in the state-of-the-art Deep Neural Networks (DNN)."}, {"label": 1, "content": "Both road users and administrators are interested in knowing the traffic volume on a particular stretch of road. In China, closed large-regional freeway networks have established charging systems, which have resulted in vast amounts of toll collection data. This data can be utilized to forecast traffic volume at any designated cross-section on a freeway. A systematic process has been proposed to derive traffic volume, starting with obtaining the average traveling speed of each vehicle on the shortest path, estimating the traveling time in each road segment, and finally deriving historical traffic volume. To make the resulting traffic volume data more practical, a deep learning-based autoencoder has been used to forecast it, while evaluating its prediction accuracy. These proposed processes have been evaluated with a collection of toll data covering more than 5000 km of freeway under a centralized regional charging system for one month. One location, located 2 km from the upstream toll gate on a road segment of the Xi\u2019an ring, was chosen at random as the designated cross section. The experimental results indicate that predicting traffic volume at the designated cross-section through available toll collection data is effective and satisfactory compared to data captured by traffic video detection equipment. This method provides a practical way to derive traffic information without installing additional regularly maintained detectors and equipment on the freeway, enabling rapid and successful prediction."}, {"label": 1, "content": "The paper proposes the use of image classification techniques as a means of protecting internet users from inappropriate content. The authors describe a modular hierarchical classification system and suggest an approach to integrating additional image classification modules without the need for further retraining. They also outline and analyze the experiments carried out to implement this system into the current infrastructure of the web classification system."}, {"label": 0, "content": "The paper describes the solution of the problem of the porting content from the old website to the new platform with added interactive functionality. Made the rationale for the selection of the content management system Drupal-based on the analysis of the comparative characteristics of the other CMS. The paper provides detailed practical implementation of the desired functionality. A brief overview of the classes of functional requirements for information security is given. On the example of one of the classes the adaptation of the ranking method to assess the probability of unauthorized access taking into account the factors formulated. For each factor are valid values. The analysis of the password length depending on the size of the original alphabet of characters. Probabilistic estimates of hacking are calculated, recommendations on the optimal combination are given."}, {"label": 1, "content": "This work builds upon a larger research project that proposes Distance Education (DE) through audio-only learning mode as a viable model for creating a full-fledged audio-MOOC. Our innovative Audio MOOC framework offers a means of learning via phone calls and is designed to provide access to learning materials for low literate populations who are often hindered by language, literacy, culture, connectivity, and distance barriers that existing MOOCs have failed to address. Our research demonstrates the practical application of our framework by connecting with a remote island in the Indian Ocean, Agalega, which has limited maritime and air access but can be connected via basic phones through voice calls. Agalega offers an ideal test case scenario for our research which aims to address the needs of semi-literate populations with limited access to education. We conducted a course for a group of 50 fishermen over nine days from September 15 to 23, 2017. Our system was delivered live over the telephony network using our GSM gateway, which was located in Mauritius and connected to a cloud server. Despite the geographical barriers, our system functioned as expected and proved to be a success."}, {"label": 0, "content": "This paper proposes a method to identify the equipment anomalies based on convolution neural network, aiming at the weak-light situation inside the cable tunnel. This paper has proposed a method to identify the equipment anomalies with weak light situation inside the cable tunnel, based on the convolution neural network. On the basis of the gray image, this method adds the Sobel operator to enhance edge-preprocessing effect and start training through Convolution Neural Network (CNN). The convergence criteria is the Loss Function related with the Weight Parameter W and Bias Parameter b. The convergence method is the one named Backpropagation, which updates the parameters each time to reduce the loss. The fast operating speed of full connection layer can help getting the direct classification result of equipment status in the image. Based on the experimental analysis of the internal images of the Zhuhai tunnel, it can be seen that this method is suitable for the dark and chaotic environment of the tunnel. Additionally, it has a high recognition rate for the image segmentation of the lighting equipment and a high accuracy for the classification of the abnormal situation of the image."}, {"label": 1, "content": "Dual or multiple rear cameras on handheld smartphones are considered to be the future of mobile photography. Many new smartphones with dual-rear cameras, including one wide-angle and one telephoto, have recently been released, such as the Apple iPhone 7 and 8 Plus, iPhone X, Samsung Galaxy S9, LG V30, and Huawei Mate 10. These devices are capable of producing better quality pictures and acquiring 3D stereo photos with depth information collected, similar to our two-eye system. As a result, these phones are becoming more affordable and more powerful. \n\nThis paper proposes a system that uses commercial dual rear-camera phones, such as the iPhone X, to assist people with visual impairments. The proposed system involves placing the phone on the user's chest and having one or two Bluetooth headphones in their ears to listen to phone audio outputs. The system consists of three main modules: scene context recognition to audio, 3D stereo reconstruction to audio, and interactive audio/voice controls. \n\nThe wide-angle camera captures live photos to be investigated by a GPS guided Deep Learning process, which describes the scene in front of the user. The telephoto camera captures the more narrow-angle and reconstructs it with the wide angle's one to form a depth map. This map helps the user determine the distance to all visible objects and notifies the user of critical ones. Additionally, the phone vibrates when an object is located in close proximity to the user. The user can also ask the system various questions to receive automatic voice answering. The system also includes a manual rescue module in case of errors or technical issues.\n\nOverall, this paper proposes a system that provides assistance to visually impaired individuals using commercial dual rear-camera phones. Users can obtain a better understanding of their surroundings through the combination of scene context recognition, 3D stereo reconstruction, and interactive audio/voice controls. Further details of the system's design and implementation are described in this paper."}, {"label": 0, "content": "This paper presents a generic proof of Typical Worst-Case Analysis (TWCA), an analysis technique for weakly-hard real-time uniprocessor systems. TWCA was originally introduced for systems with fixed priority preemptive (FPP) schedulers and has since been extended to fixed-priority nonpreemptive (FPNP) and earliest-deadline-first (EDF) schedulers. Our generic analysis is based on an abstract model that characterizes the exact properties needed to make TWCA applicable to any system model. Our results are formalized and checked using the Coq proof assistant along with the Prosa schedulability analysis library. Our experience with formalizing real-time systems analyses shows that this is not only a way to increase confidence in our claimed results: The discipline required to obtain machine checked proofs helps understanding the exact assumptions required by a given analysis, its key intermediate steps and how this analysis can be generalized."}, {"label": 1, "content": "The appearance of a small, round or oval-shaped nodule in a Computed Tomography (CT) scan of the lung is cause for concern and suspicion of lung cancer. In order to avoid misdiagnosis of lung cancer at an early stage, Computer Aided Diagnosis (CAD) technology is used by oncologists to classify pulmonary nodules as either malignant (cancerous) or benign (noncancerous).\n\nThis paper presents a novel approach for pulmonary nodule classification that uses three accumulated views (top, front, and side) of CT slices and Canonical Correlation Analysis (CCA). First, the nodule is extracted from the 2D CT slice to obtain the Region of Interest (ROI) patch. Next, all patches from sequential slices are accumulated from three different views. The vector representation of each view is then correlated with two training sets - malignant and benign sets - using CCA in the spatial and Radon Transform (RT) domains. Based on the correlation coefficients, each view is classified, and the final classification decision is taken based on a priority decision.\n\nTo train and test the proposed method, 1010 patients were downloaded from the Lung Image Database Consortium (LIDC). The final results show that the proposed method achieved the best performance with an accuracy of 90.93% compared to existing methods.\n\nOverall, this study highlights the potential for CAD technology to aid in the early detection and classification of pulmonary nodules, ultimately improving patient outcomes and reducing the mortality rate associated with lung cancer."}, {"label": 1, "content": "The use of machine learning (ML) on edge computing devices is gaining popularity in industry as it enhances control systems' intelligence and autonomy. The latest trend involves utilizing embedded edge devices with higher computational power and larger memories over cloud-hosted deployments, making traditional ML tasks now possible for edge-based devices. This work aims to assess real-time predictability and data privacy concerns between traditional cloud services and edge-based ones for some data analytics tasks. By investigating a subset of ML problems appropriate for edge devices, we identify whether they can result in real-time predictable services through widely used ML libraries. We enhance Caffe library's capabilities, making it more suitable for real-time predictability. Then we deploy high-accuracy ML models on an embedded system, exposing it to industry sensor data from the field, demonstrating its efficacy and suitability for real-time processing."}, {"label": 0, "content": "In this paper, we introduce an alternative solution to the many existing IoT data acquisition and storage systems. We present a self-designed and developed prototype electronic circuit extension for Raspberry Pi development board used for collecting sensor data. There is also presented a Pi4Java API based Java application used for sensor data collection and storage. We set up an Apache Cassandra database cluster, to stores large amounts of sensor data on low-cost servers, providing high availability. In addition, a web application is also presented, that allows different data visualization operations to be performed on the stored data. The presented system is a full IoT data acquisition, storage and visualization solution."}, {"label": 0, "content": "Rise in use of mobile and wearable electronic devices increase the demand of solutions to power these devices conveniently and safely for humans and environment. Harvesting of human body energy is one of perspective solutions for this problem. This paper studies performance of harvesting of human body waste heat using thermoelectrical generators. Generated form of electricity is not suitable for direct use by electronic devices and special converters have to be used to transform and store electrical energy. Study analyzes three commercially available low voltage step-up converters in conjunction with series of 5 thermoelectric elements located on lower leg. An approach for defining efficiency of energy harvesting is presented for comparison of step-up converters and finding the best solution in required conditions."}, {"label": 0, "content": "Partial Discharge (PD) pattern recognition is one of the most important steps of PD based condition monitoring and diagnosis of High Voltage (HV) cables. Although different types of pattern recognition methods, e.g. Support Vector Machine (SVM), Back-propagation Neural Network (BPNN) and Deep Learning, have been developed and applied to PD pattern recognition, limited training samples is one of the most important factors which restricts the PD pattern recognition accuracy. To overcome the challenge two PD data augmentation methods, based on the Variable Noise Superposition (VNS) and Generative Adversarial Network (GAN), are presented in the paper, which are evaluated with 1500 sets of experimental PD data and three pattern recognition methods, Support Vector Machine (SVM), Logical Regression (LR) and Random Forest (RF). The results show that PD pattern recognition accuracy, based on the VNS data augmentation method, is improved by 0.99%, evaluated with SVM, and is improved by 0.96%, evaluated with RF; PD pattern recognition accuracy, based on the GAN data augmentation method, is improved by 0.52%, evaluated with SVM, and is improved by 1.72%, evaluated with LR. Both two methods, VNS and GAN, are effective for PD data augmentation, which are applicable for PD pattern recognition of HV cables and other HV apparatuses, especially for pattern recognition methods which requires large volume of training samples."}, {"label": 1, "content": "To effectively distinguish between real targets, clutter, and dense multi-false targets detected by radar, we propose an algorithm based on a factorized convolutional neural network (CNN). Our CNN model uses depthwise separable convolution to factorize the processing of inputs and reduce model parameters. We also introduce a simplified version of the model with reduced numbers of convolutional filters and fully connected layers, further reducing parameters. Our results with measured data demonstrate that the simplified factorized CNN significantly improves discrimination of real targets, clutter, and dense multi-false targets compared to existing models, and achieves a parameter reduction of more than 90% compared to a recently proposed model."}, {"label": 1, "content": "A soft, ring-shaped actuator inspired by the human stomach has been developed, which is capable of contracting inward, mimicking the movement of the stomach. This innovative actuator has potential applications in medical science and food engineering. To investigate the actuator's deformation, this study used finite-element simulations and motion tracking. The simulations were carried out to examine how different pressures influence the actuator's performance. The motion tracking system was used to track the mid-points on the deformed surface as the actuator was pressurized. The results of this investigation showed that the actuator can attain axisymmetric contraction when inflated. The mid-points moved predominantly on the horizontal plane, while changes in the axial direction were negligible. This study offers valuable insights into the deformed membrane's profile and can contribute to constructing mathematical models."}, {"label": 1, "content": "The objective of Industry 4.0 is to utilize all accessible information from a supply chain to continually improve its functionality. However, data acquisition remains a significant obstacle and serves as the first step towards revolutionizing industrial practices. Collecting data from software is relatively simple compared to retrieving data from hardware, especially in instances where Programmable Logic Controllers (PLCs) are either poorly documented or not designed for this purpose. In response to this challenge, we developed a technique to monitor the most common movement of a machine, linear motion. The solution we propose involves an IoT device: a compact, wireless, and cost-effective sensor that can provide real-time data about linear motion within machinery. We designed a static, general model and a strategy for optimizing machines using results obtained from data acquisition comparable to those obtained through other methods. We were able to validate our assumptions by analyzing the results obtained from a real industrial scenario. The data obtained from the IoT device was on par with PLC data and even closer to real-time. Furthermore, our methodology possesses significant potential for automating the data analysis process."}, {"label": 1, "content": "Load monitoring (LM) is a critically important step in successfully implementing energy management programs. There are two types of LM methods: Intrusive LM (ILM) and Non-Intrusive LM (NILM). Of these, non-intrusive approaches tend to have several benefits such as low cost, easy installation, and potential scalability for commercialization. \n\nThis article provides a comprehensive overview of effective NILM system frameworks and advanced load disaggregation algorithms. It reviews the different types of load signature models, existing datasets, and performance metrics. It also provides insight into commercial applications such as demand response programs. Furthermore, this article highlights the common challenges associated with NILM and identifies potential future research directions. \n\nOverall, implementing load monitoring is essential for any effective energy management strategy. By using NILM methods, it\u2019s possible to achieve the benefits that the approach offers, including scalability and ease of use. With continued research, NILM can provide broader benefits, and drive greater energy efficiency and cost savings."}, {"label": 0, "content": "This paper proposes a new generalized power load modeling method, which contains multiple RBF neural network model structures. Firstly, based on the RBF neural network, a sub-model is built to describe typical load characteristics. It breaks through the limitations of load components, and adapts to the diverse development of modern power system. Then, based on Bayesian estimation theory, multiple typical load models are merged into one integrated load model. It solves the problem of insufficient generalization ability of traditional load model, and adapts to the time variation of power load. Finally, the proposed method was applied in IEEE 14-bus test system. The results obtained proved its validity."}, {"label": 1, "content": "With the increasing number of geostationary satellites revolving around the earth's orbit, there has been a renewed interest in using the Global Positioning System (GPS) to understand various phenomena in the earth's atmosphere. GPS devices are popular among remote sensing research communities as they offer a scalable and wide range of applications. This paper focuses on how GPS signals can be used to estimate the amount of water vapor present in the atmosphere. Additionally, we demonstrate the importance of precipitable water vapor (PWV) in the atmosphere for the task of rainfall detection. We present a detailed analysis of meteorological data over a 3-year period, and our study shows that the use of PWV in rainfall detection can help to reduce the false alarm rate by almost 12%."}, {"label": 1, "content": "The message passing model, represented by MPI (Message Passing Interface), is a crucial tool for parallel programming in distributed computer systems. Many MPI-programs contain collective communications that involve all the processes in a parallel program, and the effectiveness of these collective communications has a significant impact on the total execution time of the program. In this study, our focus is on designing an adaptive algorithm for barrier synchronization, which is one of the most common types of collective communications. \n\nWe have developed an adaptive algorithm that suboptimally selects the barrier synchronization scheme in parallel MPI-programs among the Central Counter, Combining Tree, and Dissemination Barrier algorithms. Our adaptive algorithm chooses the barrier algorithm with the minimal evaluation of execution time in the model Lo g P, which considers performance of computational resources and interconnect for point-to-point communications. The proposed algorithm has been implemented for MPI. \n\nWe conducted experiments on cluster systems and analyzed the dependency of the algorithm selection on LogP parameter values. Our findings show that for the number of processes less than 20, the adaptive algorithm selects the Combining Tree algorithm, while for a larger number of processes, it selects the Dissemination Barrier algorithm. Moreover, our developed algorithm minimizes the average time of barrier synchronization by 4% compared to the most common determined barrier algorithms. \n\nOverall, our study contributes to the optimization of collective communications in parallel programming for distributed computer systems, particularly in the context of barrier synchronization. The proposed adaptive algorithm has shown promising results, and further research can explore its potential for other types of collective communications."}, {"label": 1, "content": "We are addressing a communication scheduling and remote estimation problem in the presence of a strategic adversary within a worst-case scenario. Specifically, we consider a remote sensing system consisting of a sensor, an encoder, and a decoder that is designed to observe, transmit, and recover a discrete-time stochastic process. The sensor makes an observation of the state variable of the stochastic process at each time step. However, it is restricted by the number of transmissions it can make over the time horizon. Therefore, after making each measurement, it needs to decide whether to transmit its observation or not. If the sensor decides to transmit, it sends the observation to the encoder, who encodes and transmits it to the decoder. If it doesn't, the sensor and the encoder remain silent. The decoder needs to provide a real-time estimate of the state variable. Our objective is to minimize the sum of the communication cost for the sensor, the encoding cost for the encoder, and the estimation error for the decoder. \n\nThere is the presence of a jammer who interferes with the communication between the encoder and decoder by adding an additive channel noise to the communication channel. The jammer aims to minimize its net cost, which is the charging for the jamming power and the reward for the estimation error generated by the decoder. To address this issue, we consider a feedback Stackelberg game, where the sensor, encoder, and decoder act as the composite leader, and the jammer acts as the follower. Under some technical assumptions, we derived a feedback Stackelberg solution, which is threshold based for the scheduler and piecewise affine for the encoder and decoder. We also generated numerical results to demonstrate the performance of the remote sensing system under the feedback Stackelberg solution."}, {"label": 1, "content": "Dynamic security assessment (DSA) is an essential component of dispatching operation systems in power grids, and its calculation speed is crucial to its performance. This paper proposes a Siamese neural network model to predict the transient stability indicators of power systems, specifically the critical clearing time (CCT), which is much faster and suitable for online analysis than simulation. The proposed method involves constructing a simulation sample database using historical online data and training the Siamese model, which uses static state quantities such as the active power of generators as inputs. \n\nWhen a new online power flow needs to be evaluated, the high-level features of the Siamese model are obtained, and a k-NN algorithm is used to find the most familiar samples in the database using the selected features. The final result is then determined comprehensively by the familiar samples. The proposed method's validity is verified through simulation using online data from the State Grid Corp of China (SGCC) and different key faults. Results demonstrate that the method meets the requirements for speed and accuracy of an online analysis system, especially for small sample sets."}, {"label": 1, "content": "As a multimedia security mechanism, CAPTCHAs serve as a completely automated public Turing test to differentiate between computers and humans. Despite several efforts to crack CAPTCHA, it still remains a challenging problem for real-life solutions. In this demonstration, we present a text-based CAPTCHA cracking mechanism, utilizing convolutional neural networks (CNNs). To tackle the small sample problem, we propose combining conditional deep convolutional generative adversarial networks (cDCGAN) and CNNs, which has resulted in a significant accuracy improvement. Additionally, multiple models with low Pearson correlation coefficients are selected for majority voting ensemble, which further enhances the system's accuracy. The experimental results have shown remarkable progress in the accuracy of the system, thus providing a new method for cracking CAPTCHAs."}, {"label": 0, "content": "This paper describes a geometric approach to parameter identifiability analysis in models of power systems dynamics. When a model of a power system is to be compared with measurements taken at discrete times, it can be interpreted as a mapping from parameter space into a data or prediction space. Generically, model mappings can be interpreted as manifolds with dimensionality equal to the number of structurally identifiable parameters. Empirically it is observed that model mappings often correspond to bounded manifolds. We propose a new definition of practical identifiability based the topological definition of a manifold with boundary. In many ways, our proposed definition extends the properties of structural identifiability. We construct numerical approximations to geodesics on the model manifold and use the results, combined with insights derived from the mathematical form of the equations, to identify combinations of practically identifiable and unidentifiable parameters. We give several examples of application to dynamic power systems models."}, {"label": 1, "content": "Software Defined Networking (SDN) is a technology that enables network programmability from a logically centralized control point, breaking the vertical integration of existing Internet architecture. Although centralized network control provides several advantages, attacks on SDN frameworks remain a significant challenge. \n\nTo address this problem, researchers have proposed a machine learning-based method to detect Denial of Service (DoS) attacks on data plane devices, specifically OpenFlow switches resulting from flow-table overflow. They created an SDN dataset using Mininet and extracted features from switch-controller communication traces and OpenFlow switch flow-table snapshots. \n\nThree different algorithms were tested: Neural Network, Support Vector Machines, and Naive Bayes. The results showed that both the neural network and Naive Bayes provided 100% accuracy with the extracted features. \n\nOverall, this machine learning-based approach can be effective in detecting DoS attacks on SDN frameworks and can help improve the security of these systems."}, {"label": 1, "content": "Opportunistic Networks, which include users as a vital component, represent a natural evolution of the mobile Ad Hoc Networks field and offer an example of how the Internet of Things can work. Due to their unique characteristics, Opportunistic Networks require users-centric considerations when designing routing schemes, privacy, and authentication protocols. However, existing mutual authentication schemes for Opportunistic Networks fail to consider users' social behavior and the potential for pre-established contacts to be used in mutual authentication processes. To address these issues, this paper proposes a multiple-level, realistic authentication scheme for short-term and limited-time wireless network environments that involves pre-established contacts, Seed OppNet Identity, and traditional cryptography principles. This scheme offers anonymity and privacy to users and enhances the security of Opportunistic Networks overall."}, {"label": 1, "content": "The MMSE-RISIC equalization algorithm for SC-FDE has been successful in estimating and removing residual inter-symbol interference (RISI), but it still leaves noise interference in the decision data. Moreover, the estimation deviation of RISI can lead to additional disturbances, reducing the accuracy of equalization. As a result, an improved MMSE-RISIC equalization algorithm has been presented, which is extended to the UW-based system of STBC-SC-FDE. \n\nThis new algorithm leverages the correlation between the estimated noise in the UW and the estimated noise in the data, allowing the noise in data to be predicted by the noise in UW, and removed prior to data decision. This leads to an improvement in the accuracy of RISI estimation, resulting in significantly better equalization performance than the original MMSE-RISIC algorithm. \n\nSimulation results have validated the superior performance of the improved MMSE-RISIC equalization."}, {"label": 0, "content": "Both road users and administrators are keen to know the traffic volume at the arbitrary point on the road network. In China, charging systems have been fully established in closed large-regional freeway networks. They have accumulated massive amounts of toll collection data and provided a possible method to forecast unknown traffic volume at any designated cross-section located on a freeway. A systematic method is proposed to derive the traffic volume step-by-step. First, the average traveling speed is obtained for each vehicle on its shortest path. Then, the traveling time is estimated in each road segment. Finally, the historical traffic volume is derived at the designated cross-section. To make the obtained traffic volume data more practical, a deep learning-based autoencoder is used for forecasting the traffic volume and evaluating its prediction accuracy. All these proposed methods are evaluated with a collection of toll data for one month covering more than 5000 km of freeway under a centralized regional charging system. One location is randomly selected as the designated cross section at 2 km from the upstream toll gate on a road segment of the Xi\u2019an ring. The experimental results show the effectiveness and satisfactory accuracy of predicting the traffic volume in the designated cross-section compared with the data captured by the traffic video detection equipment. Rapid and successful prediction from available toll collection data may provide a practical method for deriving the traffic information without installing any additional regularly maintained detectors and equipment on the freeway."}, {"label": 1, "content": "The article proposes a structured approach to building an intelligent software platform that manages complex risks. The platform is divided into a global and local part, with the local component serving as an end-point software for individual systems such as an enterprise or organization. This platform offers not only effective management of complex risks on a single-system scale but also the collection, accumulation, synthesis, and dissemination of integrated risk management models and methods that have proven to be best practices in the industry.\n\nThe local component of the platform enables risk management experts to build hybrid intelligent models without the need for programming skills. This feature ensures that the platform is accessible to a wide range of users, including those who are not familiar with programming.\n\nThe article also provides examples of successful implementation of the proposed solution in projects focused on managing risk. The platform's ability to identify and mitigate complex risks in real-time can help organizations make informed decisions that reduce their exposure to risk.\n\nOverall, the intelligent software platform proposed in this article provides an effective solution for managing complex risks. Its innovative design allows for the collection and synthesis of best practices in risk management, while the local component's flexibility enables risk management specialists to build hybrid intelligent models tailored to individual systems."}, {"label": 0, "content": "The behavior of APT attack has been the hot topic in recent network security study. It is critical to understand the implementation principle of APT attack. In this paper, we analyze the behavior of APT attack in the Ngay campaign from two aspects: network traffic and code implementation. We first set up the attack chain by using network traffic analysis. Then, the vulnerability exploitation process is detailed through reverse code analysis. After that, we illustrate the process of building back door. Lastly, we introduce the obfuscation technology applied in the APT malware samples."}, {"label": 0, "content": "Utilizing intelligent transportation infrastructures can significantly improve the throughput of intersections of Connected Autonomous Vehicles (CAV), where an Intersection Manager (IM) assigns a target velocity to incoming CAVs in order to achieve a high throughput. Since the IM calculates the assigned velocity for a CAV based on the model of the CAV, it's vulnerable to model mismatches and possible external disturbances. As a result, IM must consider a large safety buffer around all CAVs to ensure a safe scheduling, which greatly degrades the throughput. In addition, IM has to assign a relatively lower speed to CAVs that intend to make a turn at the intersection to avoid rollover. This issue reduces the throughput of the intersection even more. In this paper, we propose a space and time-aware technique to manage intersections of CAVs that is robust against external disturbances and model mismatches. In our method, RIM, IM is responsible for assigning a safe Time of Arrival (TOA) and Velocity of Arrival (VOA) to an approaching CAV such that trajectories of CAVs before and inside the intersection does not conflict. Accordingly, CAVs are responsible for determining and tracking an optimal trajectory to reach the intersection at the assigned TOA while driving at VOA. Since CAVs track a position trajectory, the effect of bounded model mismatch and external disturbances can be compensated. In addition, CAVs that intend to make a turn at the intersection do not need to drive at a slow velocity before entering the intersection. Results from conducting experiments on a 1/10 scale intersection of CAVs show that RIM can reduce the position error at the expected TOA by 18X on average in presence of up to 10% model mismatch and an external disturbance with an amplitude of 5% of max range. In total, our technique can achieve 2.7X better throughput on average compared to velocity assignment techniques."}, {"label": 1, "content": "Studying the propagation mechanism of cascading failure and identifying vulnerable lines in the power grid are crucial to prevent and control such failures. However, existing research primarily focuses on identifying vulnerable lines and high-risk pathways, which does not explain the overall propagation characteristics of cascading failure. In this paper, we propose a novel approach that applies sequential pattern mining technology for cascading failure analysis. We begin by introducing the concept of cascading failure pattern (CFP) and propose a cascading failure pattern mining algorithm (CFPMA) based on PrefixSpan. We then calculate the relevance of lines based on the results of the mining algorithm. Our test results on the IEEE 39-bus system show that the proposed method can identify the CFP, which clearly illustrates the overall propagation mechanism of cascading failure. Additionally, our test results under different power flow snapshots reveal that the CFP remains constant across different system statuses."}, {"label": 1, "content": "The Internet of Things (IoT) has become integral in providing various applications by penetrating several aspects of the physical world. These applications generate, exchange, aggregate, and analyze security-critical and privacy-sensitive data which makes them prime targets for attacks. As such, it is crucial that IoT systems have the capability to resist privacy risks and security threats while fulfilling their intended functional requirements and services. To achieve this goal, IoT must implement privacy preserving data manipulation to address the new challenges that arise. One of the main challenges is processing privacy-sensitive data to extract the required information while maintaining privacy. Additionally, privacy factors such as risk assessment and valuation are crucial in facilitating secure trading of sensitive data between data owners and requesters. Furthermore, the security behavior of data owners also plays a pivotal role in protecting privacy in IoT applications. This article aims to provide an overview of privacy preserving techniques used in data aggregation, trading, and analysis. It also explores the balance between data analysis and privacy preservation from the perspective of data analysts, secure data trading from the viewpoint of data owners and requesters, and secure private data aggregation from the data owners' perspective."}, {"label": 0, "content": "In two-settlement electricity markets, virtual bidding may significantly influence Locational Marginal Price (LMP) in day-ahead market. An excessive amount of virtual load may cause congestion on certain transmission lines and LMP difference between certain nodes, which may be manipulated by market participants to arbitrage in other derivative markets, such as the Financial Transmission Rights (FTRs) market. For Independent System Operators (ISOs) and regulators, it is vital to calculate LMP difference caused by virtual bidding in advance to monitor market behaviors and to improve market efficiency. However, the conventional enumeration-based method that calculates LMP differences is time-consuming and inaccurate. In this paper, a fast algorithm is proposed to calculate LMP differences using the parametric programming method. Instead of calculating repetitively, the proposed algorithm finds the turning points where LMP differences change. The case study based on a modified IEEE-9 bus system shows that the proposed algorithm outperforms both in efficiency and accuracy."}, {"label": 0, "content": "High-dimensional and sparse (HiDS) matrices generated by recommender systems contain rich knowledge regarding various desired patterns like users\u2019 potential preferences and community tendency. Latent factor (LF) analysis proves to be highly efficient in extracting such knowledge from an HiDS matrix efficiently. Stochastic gradient descent (SGD) is a highly efficient algorithm for building an LF model. However, current LF models mostly adopt a standard SGD algorithm. Can SGD be extended from various aspects in order to improve the resultant models\u2019 convergence rate and prediction accuracy for missing data? Are such SGD extensions compatible with an LF model? To answer them, this paper carefully investigates eight extended SGD algorithms to propose eight novel LF models. Experimental results on two HiDS matrices generated by real recommender systems show that compared with an LF model with a standard SGD algorithm, an LF model with extended ones can achieve: 1) higher prediction accuracy for missing data; 2) faster convergence rate; and 3) model diversity."}, {"label": 1, "content": "In this article, a novel interferometric synthetic aperture radar (InSAR) baseline estimation method based on ground control points (GCPs) and partitioning is proposed. The method offers an alternative to existing approaches that introduce low-resolution digital elevation models (DEMs) to correct phase jumps between high and low coherence regions during phase unwrapping. Instead of relying on DEMs, the proposed method uses a high coherence regional block to calibrate interferometric parameters. GCPs are used in the calibration process and can be obtained through field measurement or reference to low resolution DEMs. Importantly, the calibration process does not change absolute phase, which avoids local DEM restrictions imposed by low resolution models. Additionally, the block-based coherence map avoids inversion errors due to overall phase deviation. The proposed method was tested using Gaofen-3 InSAR data of Ningbo area and was found to be effective."}, {"label": 1, "content": "Fine-grained classification is a challenging task, primarily due to the small inter-class variance and large intra-class distance between fine-grained categories. The critical factor in solving this problem is to identify the discriminative part of the image accurately. To address this issue, we propose a weakly supervised method that only requires image-level labels for fine-grained classification.\n\nOur model employs a convolutional neural network (CNN), which can locate the discriminative region through attention and automatically focus on subtle features by zooming in on the discriminative region and feeding it to the next CNN. We also incorporate a Squeeze and Excitation (SE) module for channel-wise attention to enhance the model's discriminative ability. Additionally, a spatial constrain loss is utilized to maintain the diversity of the located part.\n\nTo evaluate the performance of our model, we conducted experiments on three datasets: CUB-2011-200, Stanford Dogs, and Stanford Cars. The experimental results demonstrate the effectiveness of our proposed method compared to other methods.\n\nIn summary, our weakly supervised method provides a promising solution for fine-grained classification tasks by accurately locating the discriminative part of the image and enhancing the model's ability to focus on subtler features."}, {"label": 0, "content": "Automatic age estimation from facial images has attracted increasing attention due to its promising potential in real-life computer vision applications. However, due to uncontrollable environments, insufficient and incomplete training data, strong person-specific and large within- age span variations, age estimation has become a challenging problem. Among published age estimation, hierarchical age estimation methods have achieved comparable performance improvement than single level approaches. Most of the published hierarchical approaches have mainly used support vector machines to classify age groups followed by support vector regression for withina- age group age estimation. In this paper, we present a novel hierarchical Gaussian process framework for automatic age estimation. It consists of multi-class Gaussian process classifier to classify the input images into different age groups followed by a warped Gaussian process regression to model group specific aging patterns. In this paper, we separately tune the hyper-parameters for each age group at the regression stage. Compared with existing single level Gaussian process approaches for age estimation, our approach is computationally efficient at both the levels of hierarchy. Partitioning data into different age groups and learning group-wise hyper-parameters is computationally more efficient than learning complete training data. Misclassifications at the group boundaries are compensated at the regression stage by overlapping the neighboring age ranges. Finally, through extensive experiments on two popular aging datasets, the FG-NET and the Morph-II, we demonstrate the effectiveness of our algorithm in improving age estimation performance."}, {"label": 0, "content": "Cloud computing can be used to provide a virtualized platform for running various services, including soft real-time applications such as video streaming. To satisfy an application's real-time requirements, CPU resources are often allocated for the worst case, resulting in system under-utilization or overpaying to the cloud provider under the pay-as-you-go model. To solve this problem, we present Pacer, a framework that provides application developers a platform to implement custom virtual machine-level resource allocation algorithms that utilize real-time application-specific performance feedback from applications running in virtual machines. We also present two example resource allocation algorithms for Pacer that are based on additive-increase-multiplicative-decrease and self-tuning PID control. We apply Pacer to video stream object detection applications to show that Pacer can save more than 50% CPU utilization and use CPU resources more efficiently, while still meeting deadlines for real-time applications."}, {"label": 0, "content": "Voltage optimization on distribution networks is of uttermost importance to Distribution System Operators (DSO). The performance of the entire distribution network depends on the voltage profile of the system. With the current increase in the penetration of Distributed Energy Resources (DERs) on the distribution network, the challenges of optimized voltage profiles become aggravated. For PV connected systems, smart inverters can be used to partake in the voltage regulation and optimization process using their capability of reactive power injection and absorption. This is often referred to as Volt-VAR Optimization (VVO). Choosing the optimal Volt-VAR Curve (VVC) for the smart inverters often becomes challenging. This paper formulates 4 objective functions, and also proposes the use of Genetic Algorithm (GA) for VVC selection with the integration of a high-level penetration of PV smart inverter. The algorithm was tested on the standard IEEE 13 node distribution test feeder without the use of traditional voltage regulating devices such as Voltage regulators and capacitor banks. The result showed that the overall system active power losses can be minimized by carefully selecting the optimal VVC for the scenarios under study. Also, the results show the different dependencies of the minimization of the active power losses in the network on the VVC reactive power absorption and injection axis."}, {"label": 1, "content": "To serve traffic in inter data centers that provide services such as data duplication and virtual machine migration, it is necessary to transfer large amounts of data within a pre-set deadline while tolerating specific latency. In this study, we propose offline routing and spectrum assignment (RSA) schemes for transferring deadline-compliant, voluminous data demands in elastic optical networks. The proposed schemes optimize both time and frequency domains and are initially formulated as an integer linear program (ILP). Additionally, we propose practical scheduling techniques, combining three methods of ordering demands and two RSA schemes. To evaluate the proposed ILP model and scheduling methods, we conduct simulations using realistic network parameters and topologies. Our results show that scalable methods achieve similar spectrum usage performance as the ILP model within reasonable times. Lastly, we provide a `rule-of-thumb' for selecting appropriate scheduling techniques based on the obtained results."}, {"label": 0, "content": "The power load of residential community has the characteristics of wild fluctuations, complex influence factors and difficult forecasting. To deal with that, a short-term load forecasting (STLF) method for residential community based on gated recurrent unit (GRU) neural network was proposed. The least absolute shrinkage and selection operator (Lasso) and partial correlation analysis are used to analyze the influence of temperature, humidity, rainfall and wind speed on the load. It shows that the average temperature affects the change of the load most among various factors, thus the average temperature is added as an input variable to the load forecasting model based on GRU network. The simulation results show that the proposed method is faster within the similar forecasting accuracy, compared with the long short-term memory (LSTM) network and traditional recurrent neural network (RNN). It's proven to be a more effective residential community short-term load forecasting method."}, {"label": 1, "content": "This paper proposes a method for evaluating the reliability of complex software based on a weighted network model for large-scale complex software systems. The model is designed to address the limitations of existing software networks in depicting dependencies of real software systems. The complex software system is initially converted into a complex software network, which represents the internal topology structure in the form of the network. Subsequently, a weighted network model is created based on this topology between nodes to analyze the actual dependencies of each structure within the software system. Lastly, using the weighted network model, the reliability analysis of a real software system is conducted and compared with other technologies. The results demonstrate the effectiveness of the proposed method."}, {"label": 0, "content": "Physical-layer secret key generation (PSKG) technology based on reciprocal wireless channel has been widely studied in point-to-point (P2P) scenarios as it can effectively solve the key distribution problem in traditional security mechanisms. However, the computation and energy cost of PSKG is high when it is extended to group key distribution. The problem of applying PSKG to ensure group secret communication remains open. In this paper, we propose a lightweight group key distribution (LGKD) method for a star network topology environment. In our proposed method, center node and each child node first extract high correlated channel characteristics instead of generating identical P2P keys, respectively. Then, the group key is broadcasted to each child node protected by the P2P channel characteristics with high similarity. Our simulation results verify the feasibility and effectiveness of our proposed group key distribution method."}, {"label": 1, "content": "As a multicarrier modulation system, the OFDM/OQAM system is highly sensitive to system synchronization errors. In this study, we first introduced the current data-aided joint estimation techniques for carrier frequency offset and time offset in time domain for OFDM/OQAM systems and highlighted their limitations. Based on this, we proposed an improved time-domain data-aided joint estimation method for carrier frequency offset and time offset by combining the strengths of the existing methods and introducing an iterative link. The simulation results demonstrated that this method effectively overcomes the limitations of the existing methods and enhances the time-frequency offset estimation performance of OFDM/OQAM systems. Therefore, the proposed method is a highly effective time-frequency offset estimation technique for OFDM/OQAM systems."}, {"label": 1, "content": ".\n\nLipreading, the art of interpreting spoken symbols by looking at them, has numerous applications in various fields such as surveillance, silent movie reconstruction, and aiding individuals with speech and hearing impairments. However, previous research in lipreading has focused only on limited lexicons, resulting in restricted classification of phrases, words, and sentences. Recently, research has expanded to include generating speech from silent video sequences, but without using multiple cameras.\n\nThis paper presents a multi-view speech reading and reconstruction system, named MyLipper, which is vocabulary and language agnostic and real-time. The model uses deep learning-based STCNN+BiGRU architecture to leverage silent video feeds from multiple cameras and generate personalized, intelligent speech for the speaker. Results obtained using MyLipper show over 20% improvement in reconstructed speech's intelligibility, confirming the importance of exploiting multiple views in building an efficient speech reconstruction system. The paper also demonstrates the optimal camera placement for maximum intelligibility of speech and presents reconstructed audios overlaid on corresponding videos from the dataset."}, {"label": 1, "content": "There are numerous challenges in the power system, such as fault diagnosis and condition assessment, that are difficult to tackle due to the existence of uncertain and incomplete information. Despite the significant advancements in science and technology, the problem of multi-factor uncertainties persists, necessitating the selection of appropriate methodologies. To address this issue, a novel uncertainty algorithm is proposed in this paper by incorporating fuzzy theory, rough set theory, and evidence theory. This algorithm is capable of handling imprecise data and conducting comprehensive analyses. In order to demonstrate the efficacy of the proposed algorithm, transformer state assessment is utilized as a case study."}, {"label": 0, "content": "Due to the continuous development of distributed energy and energy storage systems, the VSC based DC and AC-DC hybrid distribution networks have been developed rapidly in recent years. Compared with the traditional AC distribution network, DC distribution network can remove the power conversion links in power supply, storage, and end loads, reducing the complexity and cost of the access system, improving the network efficiency and power quality. This paper introduces several VSC based DC distribution network projects to be implemented in China. The key technologies proposed, including AC/DC conversion, control and protection, fault isolation and restore, and DC transformer, are analyzed together with their technical challenges. This paper also explains the development trends of several DC distribution network related subjects such as DC voltage level, key equipment development, rapid protection & isolating fault, and regional coordinated control etc."}, {"label": 1, "content": "Agriculture is recognized as a crucial aspect to achieve the Zero Hunger objective of the United Nations' Sustainable Development Goals. Among the biggest challenges for farmers is the presence of pests that can cause significant losses in yield. Misuse or overuse of pesticides can also lead to financial losses for farmers. Therefore, proper identification and management of pests can increase productivity and reduce wastage. Deep learning, a branch of Artificial Intelligence, has gained popularity in recent years for its ability to perform tasks like speech recognition and image classification. Convolutional Neural Networks (CNNs) are one of the most widely used architectures for image classification. The classification of images can benefit agriculture, especially for identifying pests. In this study, we examined the impact of dataset size on CNN accuracy in agriculture. We utilized small custom image datasets and CIFAR10 to train VGG16, ResNet, and Inception CNNs. Our findings illustrate that a larger training dataset enhances classification accuracy. We aim to expand this study by examining larger agricultural datasets to implement the results for farmers in rural communities in India."}, {"label": 0, "content": "A regularized optimization problem over a large unstructured graph is studied, where the regularization term is tied to the graph geometry. Typical regularization examples include the total variation and the Laplacian regularizations over the graph. When the graph is a simple path without loops, efficient off-the-shelf algorithms can be used. However, when the graph is large and unstructured, such algorithms cannot be used directly. In this paper, an algorithm, referred to as \u201cSnake,\u201d is proposed to solve such regularized problems over general graphs. The algorithm consists in properly selecting random simple paths in the graph and performing the proximal gradient algorithm over these simple paths. This algorithm is an instance of a new general stochastic proximal gradient algorithm, whose convergence is proven. Applications to trend filtering and graph inpainting are provided among others. Numerical experiments are conducted over large graphs."}, {"label": 0, "content": "Virtual reality (VR) applications make use of 360\u00b0 omnidirectional video content for creating immersive experience to the user. In order to utilize current 2D video compression standards, such content must be projected onto a 2D image plane. However, the projection from spherical to 2D domain introduces deformations in the projected content due to the different sampling characteristics of the 2D plane. Such deformations are not favorable for the motion models of the current video coding standards. Consequently, omnidirectional video is not efficiently compressible with current codecs. In this work, a geometry-based motion vector scaling method is proposed in order to compress the motion information of omnidirectional content efficiently. The proposed method applies a scaling technique, based on the location in the 360\u00b0 video, to the motion information of the neighboring blocks in order to provide a uniform motion behavior in a certain part of the content. The uniform motion behavior provides optimal candidates for efficiently predicting the motion vectors of the current block. The conducted experiments illustrated that the proposed method provides up to 2.2% bitrate reduction and on average around 1% bitrate reduction for the content with high motion characteristics in the VTM test model of Versatile Video Coding (H.266/VVC) standard."}, {"label": 0, "content": "Diseases such as positional obstructive sleep apnea and pressure sores are closely related to body postures on a bed. To estimate body postures reliably and comfortably, we proposed a novel classification system using unconstrained measurements of ballistocardiogram (BCG) signals. A flexiblepiezo-electric polymer film sensor was embedded in a mattress to collect BCG data. The amplitude features based on the morphology of BCG were applied to Bayesian classifier with piecewisesmoothing correction. Twelve healthy subjects participated in the experiment. The final average prediction accuracies of four common body postures (supine, left lateral, prone and right lateral) on the bed all exceeded 97%, and a list of kappa coefficients calculated from classification results of subjects demonstrated almost perfect agreement. Overall, the developed system represents a brand new thinking in building an unobtrusive solution for body posture monitoring in our daily lives."}, {"label": 1, "content": "The unbalanced structure of distribution networks poses challenges to their analysis and computation. As more renewable distributed generation is integrated into these networks, accurate and effective sensitivity analysis methods become increasingly important for facilitating control and optimization procedures. To address this need, a sensitivity analysis method for unbalanced distribution networks has been developed in this paper. \n\nThis method makes use of existing measurement data to construct a linearized power flow model. Sensitivity data is obtained by directly solving linear equations. Numerical tests have demonstrated that this method achieves higher accuracy compared to traditional methods. \n\nOverall, the proposed sensitivity analysis method is a promising approach to address the challenges posed by unbalanced distribution networks. Its effectiveness in optimizing control procedures and facilitating renewable energy integration makes it a valuable contribution to the field."}, {"label": 0, "content": "Conventionally, the signal component frequencies are estimated by spectral peak search process, and suffered with the common signal mismatch problem (SMP). MVDR and CCA are typical nonparametric methods in magnitude squared coherence (MSC) spectral estimation. In this paper, a scalar cost function is developed based on the CCA MSC spectrum, where local peak indicates the estimation of signal frequency. Then, a gradient-based adaptive-step algorithm is presented to find the local peaks. In simulations, the proposed algorithm improves the frequency estimation accuracy, and SMP is avoided."}, {"label": 1, "content": "Indoor localization is a notoriously difficult problem, primarily because GPS signals are often unavailable. However, recent advances in technology have proposed using radio frequency fingerprinting techniques that rely on received signal strength (RSS) measurements to identify indoor locations. Despite its potential, RSS measurements have been challenging to model accurately in complex indoor environments, as they are time-varying. In this paper, we propose utilizing dictionary learning (DL) to generate high-quality fingerprints that incorporate channel characteristics for each location. To do this, we present an enhanced DL algorithm that leverages prior information about the channel distribution and produces fingerprints in real-time. Our simulation results indicate that our approach is highly effective in improving indoor localization accuracy."}, {"label": 1, "content": "The digital revolution in both academia and industry is being led by IoT, which has brought convenience to people's daily lives. However, security and privacy issues have become challenges. Blockchain, a decentralized database based on cryptographic techniques, is a promising solution for IoT security, which may transform various areas, including manufacture, finance, and trading.\n\nThe centralized model, which is struggling to meet some specified demands in IoT, can be replaced by the blockchain framework. In this article, we investigate typical security and privacy issues in IoT and develop a framework to integrate blockchain with IoT, which can provide great assurance for IoT data and various functionalities with desirable scalability like authentication, decentralized payment, and so on. We also suggest some possible solutions to these security and privacy issues in IoT based on blockchain and Ethereum to exhibit how blockchain contributes to IoT."}, {"label": 0, "content": "Effectiveness increase of information system design is a relevant objective. To solve the problem network model of information system organization in the conditions of uncertainty has been developed, operation numerical characteristics and parameters of network graph have been counted, estimate of probability of completion of work complex to a fixed term has been determined. As methods of study, methods of network planning and management in the conditions of uncertainty have been used. The results of the study allow increasing effectiveness of information system design."}, {"label": 0, "content": "Protozoa detection and identification play important roles in many practical domains such as parasitology, scientific research, biological treatment processes, and environmental quality evaluation. Traditional laboratory methods for protozoan identification are time-consuming and require expert knowledge and expensive equipment. Another approach is using micrographs to identify the species of protozoans that can save a lot of time and reduce the cost. However, the existing methods in this approach only identify the species when the protozoan are already segmented. These methods study features of shapes and sizes. In this work, we detect and identify in the images of cysts and oocysts of various species such as: Giardia lamblia, Iodamoeba butschilii, Toxoplasma gondi, Cyclospora cayetanensis, Balantidium coli, Sarcocystis, Cystoisospora belli and Acanthamoeba, which have round shapes in common and affect seriously to human and animal health. We propose Segmentation-driven RetinaNet to automatically detect, segment, and identify protozoans in their micrographs. By applying multiple techniques such as transfer learning, and data augmentation techniques, and dividing training samples into life-cycle stages of protozoans, we successfully overcome the lack of data issue in applying deep learning for this problem. Even though there are at most 5 samples per life-cycle category in the training data, our proposed method still achieves promising results and outperforms the original RetinaNet on our protozoa dataset."}, {"label": 0, "content": "A below average throughput of Information Technology students specializing in software development is a challenge that many Universities and Universities of Technology in South Africa face. Contributing factors to this phenomenon are varied at best, but one of the identified factors are that students in this field, especially first year students, find it difficult to conceptualize the associated information and manner of thinking required to become successful in their studies. This is especially true when considering object orientated programming concepts and paradigms that students are required to master as part of their studies. Literary evidence suggests that a high level of working memory, which is associated with abstract thinking ability, is required when learning and applying object orientated programming concepts. The problem becomes more evident and serious if we consider that the Information and Communication Technology sector of a country is largely dependent on the graduating student populous in terms of growing the sector sustainably. A specialized software instrument was developed and tested in an attempt to affect a change in the abstract thinking ability of students from a student sample at a University of Technology. The results of this study focusses on the effect that the instrument realized on the academic performance of first year students related to particularly to object orientated programming and their abstract thinking ability in general as gauged by, amongst other instruments, the General Scholastic Ability Test, or GSAT, rather than focusing on the instrument itself."}, {"label": 1, "content": "The installation of flexible alternating current transmission system (FACTS) devices is a widely used approach for reducing ohmic losses in power networks. However, determining their optimal location and control parameters is crucial for their effectiveness. One such device is the interphase power controller (IPC), which can alter active power flow in network branches. While IPCs have not been explicitly discussed in placement studies, an efficient optimization method is introduced in this paper, utilizing a genetic algorithm (GA) combined with optimal power flow (OPF) for IPC placement. This method proposes a novel way to enter the steady state model of IPC into the power flow equations and determines the optimal values of decision variables to obtain energy-efficient transmission system solutions. The simulation results on IEEE 30-bus and 118-bus test systems suggest that the GA-based optimization process yields optimal solutions for this mixed integer placement problem."}, {"label": 1, "content": "Recently, hyperspectral images (HSI) have become increasingly important in remote sensing applications. One of the fundamental issues in remote sensing is HSI classification, which has become a hot topic in the remote sensing community. To address the problem of overfitting limited training samples, we implemented a regularized convolutional neural network (CNN) using dropout and regularization strategies. While many studies have shown that integrating spectrum with spatial context is an effective way to classify HSI, the scaling issue has not been fully explored. In this study, we propose a highly efficient deep feature extraction and classification method for spectral-spatial HSI, which fully utilizes multiscale spatial features obtained using a guided filter. This method is the first attempt to lean a CNN for spectral and multiscale spatial features. Our experimental results, using various datasets such as Indian Pines, Pavia University, and Salinas, show that this approach achieves a 3% improvement in accuracy compared to its counterparts."}, {"label": 1, "content": "As big data continues to gain popularity and mobile devices become more prevalent, the significance of mobile data mining is becoming increasingly apparent. While mobile data mining presents certain advantages, it is limited in its ability to handle large datasets efficiently. To address this issue, we enhanced traditional mobile data mining procedures by incorporating cloud computing technology. Our proposed model, called MobileWeka2, demonstrated its ability to process large datasets effectively through experimentation with various data sets. These experimental results confirm the feasibility of this model in addressing the challenges of traditional mobile data mining."}, {"label": 1, "content": "In recent years, EMG has gained a lot of attention as a human interface tool. We have seen good results in hand motion recognition and personal authentication using wrist EMG. However, combining them simultaneously has been a challenge. To address this, we have measured EMG using dry-type sensors attached to the wrist and carried out both hand motion recognition and personal authentication.\n\nOur approach used a multi-input and multi-output model of a Convolutional Neural Network (CNN) and a filter size of 3x3 for the convolution layers. We did not include pooling layers in this paper. Our method achieved an average accuracy of 94.5% for hand motion recognition and 94.6% for personal authentication. \n\nIn the conventional method, personal authentication was classified into two classes only, but our proposed method carried out multi-class classification. We extracted 128x8 input data from the measuring unit. \n\nOverall, our proposed method demonstrated improved performance compared to the conventional method. In the proposed method, the average accuracy of hand motion recognition was 94.6%, and the average accuracy of personal authentication was 95.0%. This study underscores the great potential of combining hand motion recognition and personal authentication using dry-type sensors and CNN models."}, {"label": 1, "content": "The Energy Imbalance Market (EIM) is a great opportunity to integrate more variable renewable energy sources into the grid. However, volatile weather conditions add challenges to accurately forecasting photovoltaic (PV) power, which is crucial for grid stability and market operation. Most existing forecasting methods heavily rely on measurement accuracy, and the adaptability of these methods to complex weather conditions is rarely addressed. \n\nTo address these issues, this paper introduces a weather classification multivariate adaptive regression spline (MARS) forecasting model. This model is designed for complex weather conditions in all seasons, can be updated incrementally, and has high computational efficiency that meets EIM operations' requirements. The study uses a data set that includes historical power and meteorological parameters from a small-scale PV platform to train MARS models with forecast horizons ranging from 15 minutes to 24 hours in different seasons. \n\nThe analysis and testing of this novel model shows that it outperforms existing methods in terms of accuracy, adaptability, and efficiency. The MARS model can better handle complex weather conditions and adjust to changes, making it an excellent tool for enhancing grid stability and market operation."}, {"label": 0, "content": "This paper addresses the problem of vehicular localization on the road and proposes a stochastic solution that leverages vehicle-to-vehicle communication as well as the knowledge that vehicles acquire regarding their approximate locations. Such knowledge is inferred from generated GPS readings together with distance measurements calculated using the beacons broadcasted periodically by other neighboring vehicles. Furthermore, the proposed solution methodology also adopts the locations of stationary RoadSide Units (RSUs) as fixed reference points that help in determining the locations of vehicles whenever these vehicles navigate through the RSUs' coverage ranges. It is shown here that the additional position measurements received from neighboring vehicles lead to a remarkably accurate estimate of the position of a certain target vehicle. An analytical framework is established in this paper with the objective of formulating the target vehicle's position estimate problem using particle filters. The validity, reliability and accuracy of the presented mathematical formulae are verified through extensive simulations using a combination of the Network Simulator (NS-3) and the Simulation for Urban MObility (SUMO) that were used to generate realistic vehicular mobility traces."}, {"label": 0, "content": "With the operation of substantial HVDC lines, the risk of cascading failures is increased greatly. The propagation mechanism of cascading failures in AC/DC hybrid systems is analyzed, and a complete cascading failures chains search model for AC/DC systems in large-scale power grids is established. A method for rapid evaluation of AC faults to DC is proposed, which is divided into two steps. At first, the short circuit ratio of the system after the occurrence of the failure chain is calculated, and the extreme failure chain is identified. Time domain simulation is performed to whether it will trigger DC block. For the non-extreme accident chain, the magnitude of the fault is quantified by the area where the voltage after the fault exceeds the normal voltage value. The Leven berg-Marquardt neural network (LMNN) is used to quickly calculate the voltage dips degree, and the time domain simulation check is performed on the severe fault. The line failure probability index and the line failure risk index are defined. The pruning search is carried out according to the line outage risk after the occurrence of the accident chain at each level, which reduces the search space ensuring the search accuracy and greatly improves the search efficiency. Finally, the effectiveness of the method proposed to search high-risk cascading failures chains in AC-DC Hybrid Power System is verified by the simulations concerning the Shandong power grid of China."}, {"label": 0, "content": "Wireless Sensor Network (WSN) is an emerging next-generation sensor network that has a wide range of application prospects. The localization technology is one of the most important key technologies for WSN. However, in a complex indoor environment, fluctuations in received signal strength can seriously degrade positioning accuracy. In this paper, we propose a fingerprint localization method based on received signal strength (RSS) distance and improved weighted k-Nearest Neighbor (KNN) algorithm. The fingerprint database is established in the off-line phase. The real-time RSS values of the on-line measurement points are measured, and the two-stage RSS distance is calculated using the Euclidean distance. Finally, in order to solve the problem of non-Gaussian distribution of measurement noise, we use an improved weighted KNN algorithm to calculate the final position coordinates of the measurement point. Simulation results show that this method can reduce the influence of signal strength fluctuations and improve the positioning accuracy."}, {"label": 0, "content": "The sale of electric vehicles (EVs) is rapidly increasing around the world due to EVs' efficiency and energy security. Charging systems play a vital role in electric vehicle. Charging systems can be categorized into three levels according to Society of Automatic Engineers (SAE). This paper presents the topologies of three types of charging systems. The charging systems are simulated in RT-Lab real-time simulator. The input ac for Level 1 and Level 2 charging systems is single-phase. The charging system consists of two diode bridge rectifiers, a power factor correction (PFC) boost circuit, a DC/AC converter, an LLC resonant converter, and a high frequency transformer. Constant current/constant voltage (CC/CV) control is employed for the DC/AC converter. The Level 3 charging system uses a three-phase source as input and its bi-directional converter is equipped with reactive power and DC bus voltage control. Three testbeds are setup to simulate the three types of charging systems with different charging power levels. A 10 kWh-battery will be charged. Simulation results demonstrate the expected charging performance of the charging systems."}, {"label": 0, "content": "In this paper, we present a method that utilizes computer vision, specifically projective geometry, to map a known distribution of points on a sphere - with known diameter - along with an arbitrary image of these points on an image plane to identify the configuration of the camera. In other words, knowing the sets of 2D-3D corresponding points, one can extract the camera matrix and dissect it into parameters of interest: intrinsics and extrinsics. The method that is validated by code shows in detail how to setup a theoretical world and camera coordinate frame, and then through the knowledge of the correspondence, displays the solution to the optimization problem. The results are then analyzed noting the relative error between the retrieved and actual camera matrices."}, {"label": 0, "content": "Flood is one of the common types of natural disaster in Indonesia, we need a system that can predict the arrival of the flood is important for the Indonesian people, especially people who live a certain area of the river flow. Some parameters that can be used to predict the flood are water level and rainfall around the river. Modeling system to predict the flood must have the prediction results as accurate as possible in order to produce a good system in predicting floods. Therefore, in this study proposed method of artificial neural network to analyze flood prediction ability by using artificial neural network In this study case using artificial neural network Radial Basis Function. Radial Basis Function is a model of artificial neural network architecture consisting of three layers of which are the input layer, hidden layer, and output layer. The data used for the training and testing process are data of water level and rainfall data in 2015 in Dayeuhkolot. Prediction results in the training and testing process resulted in MAPE values are 0.047% and 1.05% for water level data and 4.97% and 29.1% for rainfall data with combination of hidden node = 35, learning rate = 0.2 and Spread constant = 1.1 with the target epoch maximum termination of 5000 epoch."}, {"label": 0, "content": "The scheduling of mixed-criticality (MC) systems with graceful degradation is considered, where LO-criticality tasks are guaranteed some service in HI mode in the form of minimum cumulative completion rates. First, we present an easy to implement admission-control procedure to determine which LO-criticality jobs to complete in HI mode. Then, we propose a demand-bound-function-based MC schedulability test that runs in pseudo-polynomial time for such systems under EDF-VD scheduling, wherein two virtual deadline setting heuristics are considered. Furthermore, we discuss a mechanism for the system to switch back from HI to LO mode and quantify the maximum time duration such recovery process would take. Finally, we show the effectiveness of our proposed method by experimental evaluation in comparison to state-of-the-art MC schedulers."}, {"label": 0, "content": "High-resolution optical imagery can provide detailed information of urban land objects for impervious surface extraction, while airborne light detection and ranging (LiDAR) data can provide height features of land objects. Therefore, synergistic use of high-resolution imagery and LiDAR data is considered as an effective method to improve impervious surfaces extraction. In this paper, a novel hierarchical multiscale super-pixel-based classification method is proposed and applied to the urban impervious surfaces extraction from WorldView-2 and normalized digital surface model (nDSM) images derived from airborne LiDAR data. Three subsets in rural, rural-urban, and urban subsets are selected as the study areas. First, we split nonground and ground objects based on nDSM thresholds. Second, a hierarchical multiresolution segmentation method is used to generate nonground and ground super pixels. Then, we determine the multiscale input images based on the size of super pixels. Third, we construct optimal deep residual network (ResNet) and Spatial Pyramid Pooling (SPP-net) to train the model using multiscale input images. Finally, we use our deep models to predict hierarchically total super pixels in three subsets and generate the classification and impervious surfaces results. Our proposed method adopts hierarchical classification based on LiDAR nDSM height, which significantly improves the impervious surfaces extraction accuracies. Then, the deep residual network is applied further on multispectral and height fused data to extract urban impervious surfaces. Moreover, we propose an adaptive method to determine multiscale input images based on the segmentation of super pixels, which are inputs into the ResNet+SPP-net to train the deep model. Our proposed method reduces the uncertainty of multiscale input images and extracts better multiscale features. The results of the experiment show that our proposed method has a significant superiority to traditional pixel-based method and single scale method for urban impervious surfaces extraction."}, {"label": 0, "content": "In this paper, we propose an uplink scheduling scheme via downlink signal design for wireless powered communication networks (WPCNs). Although harvest-then-transmit protocols and related optimal resource allocation problems were studied, explicit methods of transmitting the scheduling information have not been considered in prior works. For uplink scheduling, we propose a design of the downlink energy signal with a power level modulation, which conveys the scheduling information to users. Hybrid-access point allocates different power levels to the subslots of the downlink energy signal, and the users recognize their uplink subslot lengths from their corresponding downlink subslots' power levels. The scheduling can be optimized based on the user channel state with respect to the sum rate. We formulate the sum throughput maximization problem for the proposed scheme, which is shown to be a convex optimization problem. We also study the proposed scheme in a noisy environment. The solution to the problem provides the optimal downlink and uplink slot lengths. The numerical results confirm that the throughput of the proposed WPCN scheme outperforms that of the conventional schemes. The improvement is shown even in imperfect synchronization scenario."}, {"label": 0, "content": "In this paper, we address the problem of com-putational and networking virtual resources embedding across multiple Infrastructure-as-a-Service (IaaS) providers. This issue, usually referred to as the Virtual Network Embedding (VNE) problem, requires two phases of operation in such a context: the multicloud virtual network requests (VNRs) splitting, followed by the intracloud VNR segments mapping. This paper focuses on the splitting phase problem, by proposing a splitting strategy based on two optimization approaches, with the objective of improving the performance and the quality of service (QoS) of resulting mapped VNR segments. An Integer Linear Program (ILP) is used to formalize our splitting strategy as a mathematical minimization problem with constraints. The ILP model is first solved with the exact approach. Subsequently, a metaheuristic approach based on the Tabu Search (TS) is proposed in order to find optimal or near-optimal solutions in polynomial solving time. The simulation results obtained show the efficiency of the proposed VNRs splitting approaches according to several performance criteria. Solution costs of the heuristic are on average close to the exact solution, with an average cost gap ranging from 0% to a maximum of 2.05%, performed in a highly reduced computing time. In comparison with other baseline approaches, the acceptance rate and the delay are improved by approximately 15%, while preventing QoS violations."}, {"label": 1, "content": "This paper presents a method for identifying major power quality disturbance sources in a regional power grid through monitoring data correlation analysis among multiple grid nodes. The proposed method involves two main steps. In the first step, strong-correlated branches that significantly affect voltage distortion in problematic nodes are identified using correlation calculation between the voltage quality index in each node and the current quality index in each branch. In the second step, the contribution of each strong-correlated current branch to node voltage distortion is quantified through partial correlation analysis. Finally, the major disturbance sources are identified by estimating their contribution.\n\nThe proposed method was validated through a case study conducted on a regional power grid that includes multiple types of disturbance sources. The results showed that the method can accurately identify the major disturbance sources responsible for node voltage distortion. Thus, providing effective guidance for disturbance source governance."}]